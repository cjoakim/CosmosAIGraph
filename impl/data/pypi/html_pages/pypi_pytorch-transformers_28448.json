{
  "libtype": "pypi",
  "libname": "pytorch-transformers",
  "url": "https://pypi.org/project/pytorch-transformers/",
  "html": "<!DOCTYPE html><html lang=\"en\" dir=\"ltr\">  <head>    <meta charset=\"utf-8\">    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">    <meta name=\"defaultLanguage\" content=\"en\">    <meta name=\"availableLanguages\" content=\"en, es, fr, ja, pt_BR, uk, el, de, zh_Hans, zh_Hant, ru, he, eo\">    <title>pytorch-transformers \u00b7 PyPI</title>    <meta name=\"description\" content=\"Repository of pre-trained NLP Transformer models: BERT &amp; RoBERTa, GPT &amp; GPT-2, Transformer-XL, XLNet and XLM\">    <link rel=\"stylesheet\" href=\"/static/css/warehouse-ltr.99b3104d.css\">    <link rel=\"stylesheet\" href=\"/static/css/fontawesome.b50b476c.css\">    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Source+Sans+3:400,400italic,600,600italic,700,700italic%7CSource+Code+Pro:500\">    <noscript>      <link rel=\"stylesheet\" href=\"/static/css/noscript.0673c9ea.css\">    </noscript>    <link rel=\"icon\" href=\"/static/images/favicon.35549fe8.ico\" type=\"image/x-icon\">    <link rel=\"alternate\" type=\"application/rss+xml\" title=\"RSS: 40 latest updates\" href=\"/rss/updates.xml\">    <link rel=\"alternate\" type=\"application/rss+xml\" title=\"RSS: 40 newest packages\" href=\"/rss/packages.xml\"><link rel=\"alternate\" type=\"application/rss+xml\" title=\"RSS: latest releases for pytorch-transformers\" href=\"/rss/project/pytorch-transformers/releases.xml\">    <link rel=\"canonical\" href=\"https://pypi.org/project/pytorch-transformers/\">    <meta property=\"og:url\" content=\"https://pypi.org/project/pytorch-transformers/\">    <meta property=\"og:site_name\" content=\"PyPI\">    <meta property=\"og:type\" content=\"website\">    <meta property=\"og:image\" content=\"https://pypi.org/static/images/twitter.abaf4b19.webp\">    <meta property=\"og:title\" content=\"pytorch-transformers\">    <meta property=\"og:description\" content=\"Repository of pre-trained NLP Transformer models: BERT &amp; RoBERTa, GPT &amp; GPT-2, Transformer-XL, XLNet and XLM\">    <link rel=\"search\" type=\"application/opensearchdescription+xml\" title=\"PyPI\" href=\"/opensearch.xml\">    <script asyncdata-ga-id=\"UA-55961911-1\"data-ga4-id=\"G-RW7D75DF8V\"            src=\"/static/js/warehouse.dd4295c4.js\">    </script><script>MathJax = {  tex: {    inlineMath: [['$', '$'], ['\\\\(', '\\\\)']]  },};</script><script async  src=\"https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-svg.js\"  integrity=\"sha256-1CldwzdEg2k1wTmf7s5RWVd7NMXI/7nxxjJM2C4DqII=\"  crossorigin=\"anonymous\"></script><script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-55961911-1\"></script><script async src=\"https://www.googletagmanager.com/gtag/js?id=G-RW7D75DF8V\"></script><script defer src=\"https://www.fastly-insights.com/insights.js?k=6a52360a-f306-421e-8ed5-7417d0d4a4e9&dnt=true\"></script>    <script async        src=\"https://media.ethicalads.io/media/client/v1.4.0/ethicalads.min.js\"        integrity=\"sha256-U3hKDidudIaxBDEzwGJApJgPEf2mWk6cfMWghrAa6i0= sha384-UcmsCqcNRSLW/dV3Lo1oCi2/VaurXbib6p4HyUEOeIa/4OpsrnucrugAefzVZJfI sha512-q4t1L4xEjGV2R4hzqCa41P8jrgFUS8xTb8rdNv4FGvw7FpydVj/kkxBJHOiaoxHa8olCcx1Slk9K+3sNbsM4ug==\"        crossorigin=\"anonymous\"    ></script>  </head>  <body data-controller=\"viewport-toggle\">    <!-- Accessibility: this link should always be the first piece of content inside the body-->    <a href=\"#content\" class=\"skip-to-content\">Skip to main content</a>    <button type=\"button\" class=\"button button--primary button--switch-to-mobile hidden\" data-viewport-toggle-target=\"switchToMobile\" data-action=\"viewport-toggle#switchToMobile\">Switch to mobile version    </button>    <div id=\"sticky-notifications\" class=\"stick-to-top js-stick-to-top\">      <!-- Add browser warning. Will show for ie9 and below -->      <!--[if IE]>      <div class=\"notification-bar notification-bar--warning\" role=\"status\">        <span class=\"notification-bar__icon\">          <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"></i>          <span class=\"sr-only\">Warning</span>        </span>        <span class=\"notification-bar__message\">You are using an unsupported browser, upgrade to a newer version.</span>      </div>      <![endif]-->      <noscript>      <div class=\"notification-bar notification-bar--warning\" role=\"status\">        <span class=\"notification-bar__icon\">          <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"></i>          <span class=\"sr-only\">Warning</span>        </span>        <span class=\"notification-bar__message\">Some features may not work without JavaScript. Please try enabling it if you encounter problems.</span>      </div>      </noscript><div data-html-include=\"/_includes/notification-banners/\"></div>    </div><div data-html-include=\"/_includes/flash-messages/\"></div><div data-html-include=\"/_includes/session-notifications/\"></div>    <header class=\"site-header \">      <div class=\"site-container\">        <div class=\"split-layout\">          <div class=\"split-layout\">            <div>              <a class=\"site-header__logo\" href=\"/\">                <img alt=\"PyPI\" src=\"/static/images/logo-small.2a411bc6.svg\">              </a>            </div>            <form class=\"search-form search-form--primary\" action=\"/search/\" role=\"search\">              <label for=\"search\" class=\"sr-only\">Search PyPI</label>              <input id=\"search\" class=\"search-form__search\" type=\"text\" name=\"q\" placeholder=\"Search projects\" value=\"\" autocomplete=\"off\" autocapitalize=\"off\" spellcheck=\"false\" data-controller=\"search-focus\" data-action=\"keydown@window->search-focus#focusSearchField\" data-search-focus-target=\"searchField\">              <button type=\"submit\" class=\"search-form__button\">                <i class=\"fa fa-search\" aria-hidden=\"true\"></i>                <span class=\"sr-only\">Search</span>              </button>            </form>          </div><div data-html-include=\"/_includes/current-user-indicator/\">            <div id=\"user-indicator\" class=\"horizontal-menu horizontal-menu--light horizontal-menu--tall\">  <nav class=\"horizontal-menu horizontal-menu--light horizontal-menu--tall hide-on-tablet\" aria-label=\"Main navigation\">    <ul>      <li class=\"horizontal-menu__item\"><a href=\"/help/\" class=\"horizontal-menu__link\">Help</a></li>      <li class=\"horizontal-menu__item\"><a href=\"/sponsors/\" class=\"horizontal-menu__link\">Sponsors</a></li>      <li class=\"horizontal-menu__item\"><a href=\"/account/login/\" class=\"horizontal-menu__link\">Log in</a></li>      <li class=\"horizontal-menu__item\"><a href=\"/account/register/\" class=\"horizontal-menu__link\">Register</a></li>    </ul>  </nav>  <nav class=\"dropdown dropdown--on-menu hidden show-on-tablet\" aria-label=\"Main navigation\">    <button type=\"button\" class=\"horizontal-menu__link dropdown__trigger\" aria-haspopup=\"true\" aria-expanded=\"false\" aria-label=\"View menu\">Menu      <span class=\"dropdown__trigger-caret\">        <i class=\"fa fa-caret-down\" aria-hidden=\"true\"></i>      </span>    </button>    <ul class=\"dropdown__content\" aria-hidden=\"true\" aria-label=\"Main menu\">      <li><a class=\"dropdown__link\" href=\"/help/\">Help</a></li>      <li><a class=\"dropdown__link\" href=\"/sponsors/\">Sponsors</a></li>      <li><a class=\"dropdown__link\" href=\"/account/login/\">Log in</a></li>      <li><a class=\"dropdown__link\" href=\"/account/register/\">Register</a></li>    </ul>  </nav></div></div>        </div>      </div>    </header>    <div class=\"mobile-search\">      <form class=\"search-form search-form--fullwidth\" action=\"/search/\" role=\"search\">        <label for=\"mobile-search\" class=\"sr-only\">Search PyPI</label>        <input id=\"mobile-search\" class=\"search-form__search\" type=\"text\" name=\"q\" placeholder=\"Search projects\" value=\"\" autocomplete=\"off\" autocapitalize=\"off\" spellcheck=\"false\">                <button type=\"submit\" class=\"search-form__button\">          <i class=\"fa fa-search\" aria-hidden=\"true\"></i>          <span class=\"sr-only\">Search</span>        </button>      </form>    </div>    <main id=\"content\"><div class=\"hidden\"  data-controller=\"github-repo-stats\"  data-github-repo-stats-github-repo-info-outlet=\".github-repo-info\"  data-github-repo-stats-url-value=\"https://api.github.com/repos/huggingface/pytorch-transformers\"  data-github-repo-stats-issue-url-value=\"https://api.github.com/search/issues?q=repo:huggingface/pytorch-transformers+type:issue+state:open&amp;per_page=1\"></div><div class=\"banner\">  <div class=\"package-header\">    <div class=\"package-header__left\">      <h1 class=\"package-header__name\">        pytorch-transformers 1.2.0      </h1>      <div data-controller=\"clipboard\">        <p class=\"package-header__pip-instructions\">          <span id=\"pip-command\" data-clipboard-target=\"source\">pip install pytorch-transformers</span>          <button type=\"button\" class=\"copy-tooltip copy-tooltip-s\" data-action=\"clipboard#copy\" data-clipboard-target=\"tooltip\" data-clipboard-tooltip-value=\"Copy to clipboard\">            <i class=\"fa fa-copy\" aria-hidden=\"true\"></i>            <span class=\"sr-only\">Copy PIP instructions</span>          </button>        </p>      </div>    </div>    <div class=\"package-header__right\">      <a class=\"status-badge status-badge--good\" href=\"/project/pytorch-transformers/\">        <span>Latest version</span>      </a>      <p class=\"package-header__date\">Released: <time datetime=\"2019-09-04T11:36:34+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Sep 4, 2019</time>      </p>    </div>  </div></div><div class=\"horizontal-section horizontal-section--grey horizontal-section--thin\">  <div class=\"site-container\"><div data-html-include=\"/_includes/administer-project-include/pytorch-transformers\"></div>    <div class=\"split-layout split-layout--middle package-description\">      <p class=\"package-description__summary\">Repository of pre-trained NLP Transformer models: BERT &amp; RoBERTa, GPT &amp; GPT-2, Transformer-XL, XLNet and XLM</p><div data-html-include=\"/_includes/edit-project-button/pytorch-transformers\"></div>    </div>  </div></div><div data-controller=\"project-tabs\">  <div class=\"tabs-container\">    <div class=\"vertical-tabs\">      <div class=\"vertical-tabs__tabs\">        <div class=\"sidebar-section\">          <h3 class=\"sidebar-section__title\">Navigation</h3>          <nav aria-label=\"Navigation for pytorch-transformers\">            <ul class=\"vertical-tabs__list\" role=\"tablist\">              <li role=\"tab\">                <a id=\"description-tab\" href=\"#description\" data-project-tabs-target=\"tab\" data-action=\"project-tabs#onTabClick\" class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--is-active\" aria-selected=\"true\" aria-label=\"Project description. Focus will be moved to the description.\">                  <i class=\"fa fa-align-left\" aria-hidden=\"true\"></i>Project description                </a>              </li>              <li role=\"tab\">                <a id=\"history-tab\" href=\"#history\" data-project-tabs-target=\"tab\" data-action=\"project-tabs#onTabClick\" class=\"vertical-tabs__tab vertical-tabs__tab--with-icon\" aria-label=\"Release history. Focus will be moved to the history panel.\">                  <i class=\"fa fa-history\" aria-hidden=\"true\"></i>Release history                </a>              </li>              <li role=\"tab\">                <a id=\"files-tab\" href=\"#files\" data-project-tabs-target=\"tab\" data-action=\"project-tabs#onTabClick\" class=\"vertical-tabs__tab vertical-tabs__tab--with-icon\" aria-label=\"Download files. Focus will be moved to the project files.\">                  <i class=\"fa fa-download\" aria-hidden=\"true\"></i>Download files                </a>              </li>            </ul>          </nav>        </div><div class=\"sidebar-section\">  <h3 class=\"sidebar-section__title\">Project links</h3>  <ul class=\"vertical-tabs__list\">    <li>      <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\" href=\"https://github.com/huggingface/pytorch-transformers\" rel=\"nofollow\">        <i class=\"fas fa-home\" aria-hidden=\"true\"></i>Homepage      </a>    </li>  </ul></div><div class=\"sidebar-section\">  <h3 class=\"sidebar-section__title\">Statistics</h3>  <div class=\"hidden github-repo-info\" data-controller=\"github-repo-info\">GitHub statistics:    <ul class=\"vertical-tabs__list\">      <li>        <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\"           data-github-repo-info-target=\"stargazersUrl\" rel=\"noopener\">          <i class=\"fa fa-star\" aria-hidden=\"true\"></i>          <strong>Stars:</strong>          <span data-github-repo-info-target=\"stargazersCount\"></span>        </a>      </li>      <li>        <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\"           data-github-repo-info-target=\"forksUrl\" rel=\"noopener\">          <i class=\"fa fa-code-branch\" aria-hidden=\"true\"></i>          <strong>Forks:</strong>          <span data-github-repo-info-target=\"forksCount\"></span>        </a>      </li>      <li>        <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\"           data-github-repo-info-target=\"openIssuesUrl\" rel=\"noopener\">          <i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>          <strong>Open issues:</strong>          <span data-github-repo-info-target=\"openIssuesCount\"></span>        </a>      </li>      <li>        <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\"           data-github-repo-info-target=\"openPRsUrl\" rel=\"noopener\">          <i class=\"fa fa-code-pull-request\" aria-hidden=\"true\"></i>          <strong>Open PRs:</strong>          <span data-github-repo-info-target=\"openPRsCount\"></span>        </a>      </li>    </ul>  </div>  <p>View statistics for this project via <a href=\"https://libraries.io/pypi/pytorch-transformers\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Libraries.io</a>, or by using <a href=\"https://packaging.python.org/guides/analyzing-pypi-package-downloads/\" target=\"_blank\" rel=\"noopener\">our public dataset on Google BigQuery</a>  </p></div><div class=\"sidebar-section\">  <h3 class=\"sidebar-section__title\">Meta</h3>  <p><strong>License:</strong> Apache Software License (Apache)</p>    <p><strong>Author:</strong> <a href=\"mailto:thomas@huggingface.co\">Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Google AI Language Team Authors, Open AI team Authors</a></p>  <p class=\"tags\">    <i class=\"fa fa-tags\" aria-hidden=\"true\"></i>    <span class=\"sr-only\">Tags</span>    <span class=\"package-keyword\">      NLP,    </span>    <span class=\"package-keyword\">      deep,    </span>    <span class=\"package-keyword\">      learning,    </span>    <span class=\"package-keyword\">      transformer,    </span>    <span class=\"package-keyword\">      pytorch,    </span>    <span class=\"package-keyword\">      BERT,    </span>    <span class=\"package-keyword\">      GPT,    </span>    <span class=\"package-keyword\">      GPT-2,    </span>    <span class=\"package-keyword\">      google,    </span>    <span class=\"package-keyword\">      openai,    </span>    <span class=\"package-keyword\">      CMU    </span>  </p></div><div class=\"sidebar-section\">    <h3 class=\"sidebar-section__title\">Maintainers</h3>      <span class=\"sidebar-section__maintainer\">        <a href=\"/user/Thomwolf/\" aria-label=\"Thomwolf\">          <span class=\"sidebar-section__user-gravatar\">            <img src=\"https://pypi-camo.freetls.fastly.net/b23c4580fa4e4509fd26c0a20f6a32862cf6e4f3/68747470733a2f2f7365637572652e67726176617461722e636f6d2f6176617461722f36346632346335653561383531656337396538306230633131616566393762653f73697a653d3530\" height=\"50\" width=\"50\" alt=\"Avatar for Thomwolf from gravatar.com\" title=\"Avatar for Thomwolf from gravatar.com\">          </span>          <span class=\"sidebar-section__user-gravatar-text\">            Thomwolf          </span>        </a>      </span></div><div class=\"sidebar-section\">  <h3 class=\"sidebar-section__title\">Classifiers</h3>  <ul class=\"sidebar-section__classifiers\">    <li>      <strong>Intended Audience</strong>      <ul>        <li>          <a href=\"/search/?c=Intended+Audience+%3A%3A+Science%2FResearch\">            Science/Research          </a>        </li>      </ul>    </li>    <li>      <strong>License</strong>      <ul>        <li>          <a href=\"/search/?c=License+%3A%3A+OSI+Approved+%3A%3A+Apache+Software+License\">            OSI Approved :: Apache Software License          </a>        </li>      </ul>    </li>    <li>      <strong>Programming Language</strong>      <ul>        <li>          <a href=\"/search/?c=Programming+Language+%3A%3A+Python+%3A%3A+3\">            Python :: 3          </a>        </li>      </ul>    </li>    <li>      <strong>Topic</strong>      <ul>        <li>          <a href=\"/search/?c=Topic+%3A%3A+Scientific%2FEngineering+%3A%3A+Artificial+Intelligence\">            Scientific/Engineering :: Artificial Intelligence          </a>        </li>      </ul>    </li>  </ul></div><div class=\"sidebar-section\" data-ea-publisher=\"psf\" data-ea-type=\"psf\" data-ea-keywords=\"pypi-sidebar\"></div>      </div>      <div class=\"vertical-tabs__panel\">        <!-- mobile menu -->        <nav aria-label=\"Navigation for pytorch-transformers\">          <ul class=\"vertical-tabs__list\" role=\"tablist\">            <li role=\"tab\">              <a id=\"mobile-description-tab\" href=\"#description\" data-project-tabs-target=\"mobileTab\" data-action=\"project-tabs#onTabClick\" class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--mobile vertical-tabs__tab--no-top-border vertical-tabs__tab--is-active\" aria-selected=\"true\" aria-label=\"Project description. Focus will be moved to the description.\">                <i class=\"fa fa-align-left\" aria-hidden=\"true\"></i>Project description              </a>            </li>            <li role=\"tab\">              <a id=\"mobile-data-tab\" href=\"#data\" data-project-tabs-target=\"mobileTab\" data-action=\"project-tabs#onTabClick\" class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--mobile\" aria-label=\"Project details. Focus will be moved to the project details.\">                <i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>Project details              </a>            </li>            <li role=\"tab\">              <a id=\"mobile-history-tab\" href=\"#history\" data-project-tabs-target=\"mobileTab\" data-action=\"project-tabs#onTabClick\" class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--mobile\" aria-label=\"Release history. Focus will be moved to the history panel.\">              <i class=\"fa fa-history\" aria-hidden=\"true\"></i>Release history            </a>            </li>            <li role=\"tab\">              <a id=\"mobile-files-tab\" href=\"#files\" data-project-tabs-target=\"mobileTab\" data-action=\"project-tabs#onTabClick\" class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--mobile\" aria-label=\"Download files. Focus will be moved to the project files.\">                <i class=\"fa fa-download\" aria-hidden=\"true\"></i>Download files              </a>            </li>          </ul>        </nav>        <div id=\"description\" data-project-tabs-target=\"content\" class=\"vertical-tabs__content\" role=\"tabpanel\" aria-labelledby=\"description-tab mobile-description-tab\" tabindex=\"-1\">          <h2 class=\"page-title\">Project description</h2>          <div class=\"project-description\">            <h1>\ud83d\udc7e PyTorch-Transformers</h1><p><a href=\"https://circleci.com/gh/huggingface/pytorch-transformers\" rel=nofollow><img src=\"https://pypi-camo.freetls.fastly.net/734018d847f89ffa66a9183ff7cfd8e30cb4fcfa/68747470733a2f2f636972636c6563692e636f6d2f67682f68756767696e67666163652f7079746f7263682d7472616e73666f726d6572732e7376673f7374796c653d737667\" alt=CircleCI></a></p><p>PyTorch-Transformers (formerly known as <code>pytorch-pretrained-bert</code>) is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP).</p><p>The library currently contains PyTorch implementations, pre-trained model weights, usage scripts and conversion utilities for the following models:</p><ol><li><strong><a href=\"https://github.com/google-research/bert\" rel=nofollow>BERT</a></strong> (from Google) released with the paper <a href=\"https://arxiv.org/abs/1810.04805\" rel=nofollow>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.</li><li><strong><a href=\"https://github.com/openai/finetune-transformer-lm\" rel=nofollow>GPT</a></strong> (from OpenAI) released with the paper <a href=\"https://blog.openai.com/language-unsupervised/\" rel=nofollow>Improving Language Understanding by Generative Pre-Training</a> by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.</li><li><strong><a href=\"https://blog.openai.com/better-language-models/\" rel=nofollow>GPT-2</a></strong> (from OpenAI) released with the paper <a href=\"https://blog.openai.com/better-language-models/\" rel=nofollow>Language Models are Unsupervised Multitask Learners</a> by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.</li><li><strong><a href=\"https://github.com/kimiyoung/transformer-xl\" rel=nofollow>Transformer-XL</a></strong> (from Google/CMU) released with the paper <a href=\"https://arxiv.org/abs/1901.02860\" rel=nofollow>Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a> by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.</li><li><strong><a href=\"https://github.com/zihangdai/xlnet/\" rel=nofollow>XLNet</a></strong> (from Google/CMU) released with the paper <a href=\"https://arxiv.org/abs/1906.08237\" rel=nofollow>\u200bXLNet: Generalized Autoregressive Pretraining for Language Understanding</a> by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.</li><li><strong><a href=\"https://github.com/facebookresearch/XLM/\" rel=nofollow>XLM</a></strong> (from Facebook) released together with the paper <a href=\"https://arxiv.org/abs/1901.07291\" rel=nofollow>Cross-lingual Language Model Pretraining</a> by Guillaume Lample and Alexis Conneau.</li><li><strong><a href=\"https://github.com/pytorch/fairseq/tree/master/examples/roberta\" rel=nofollow>RoBERTa</a></strong> (from Facebook), released together with the paper a <a href=\"https://arxiv.org/abs/1907.11692\" rel=nofollow>Robustly Optimized BERT Pretraining Approach</a> by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.</li><li><strong><a href=\"https://github.com/huggingface/pytorch-transformers/tree/master/examples/distillation\" rel=nofollow>DistilBERT</a></strong> (from HuggingFace), released together with the blogpost <a href=\"https://medium.com/huggingface/distilbert-8cf3380435b5\" rel=nofollow>Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of\u00a0BERT</a> by Victor Sanh, Lysandre Debut and Thomas Wolf.</li></ol><p>These implementations have been tested on several datasets (see the example scripts) and should match the performances of the original implementations (e.g. ~93 F1 on SQuAD for BERT Whole-Word-Masking, ~88 F1 on RocStories for OpenAI GPT, ~18.3 perplexity on WikiText 103 for Transformer-XL, ~0.916 Peason R coefficient on STS-B for XLNet). You can find more details on the performances in the Examples section of the <a href=\"https://huggingface.co/pytorch-transformers/examples.html\" rel=nofollow>documentation</a>.</p><table><thead><tr><th>Section</th><th>Description</th></tr></thead><tbody><tr><td><a href=#installation rel=nofollow>Installation</a></td><td>How to install the package</td></tr><tr><td><a href=#quick-tour rel=nofollow>Quick tour: Usage</a></td><td>Tokenizers &amp; models usage: Bert and GPT-2</td></tr><tr><td><a href=#quick-tour-of-the-fine-tuningusage-scripts rel=nofollow>Quick tour: Fine-tuning/usage scripts</a></td><td>Using provided scripts: GLUE, SQuAD and Text generation</td></tr><tr><td><a href=#Migrating-from-pytorch-pretrained-bert-to-pytorch-transformers rel=nofollow>Migrating from pytorch-pretrained-bert to pytorch-transformers</a></td><td>Migrating your code from pytorch-pretrained-bert to pytorch-transformers</td></tr><tr><td><a href=\"https://huggingface.co/pytorch-transformers/\" rel=nofollow>Documentation</a></td><td>Full API documentation and more</td></tr></tbody></table><h2>Installation</h2><p>This repo is tested on Python 2.7 and 3.5+ (examples are tested only on python 3.5+) and PyTorch 1.0.0+</p><h3>With pip</h3><p>PyTorch-Transformers can be installed by pip as follows:</p><pre lang=bash>pip<span class=w> </span>install<span class=w> </span>pytorch-transformers</pre><h3>From source</h3><p>Clone the repository and run:</p><pre lang=bash>pip<span class=w> </span>install<span class=w> </span><span class=o>[</span>--editable<span class=o>]</span><span class=w> </span>.</pre><h3>Tests</h3><p>A series of tests is included for the library and the example scripts. Library tests can be found in the <a href=\"https://github.com/huggingface/pytorch-transformers/tree/master/pytorch_transformers/tests\" rel=nofollow>tests folder</a> and examples tests in the <a href=\"https://github.com/huggingface/pytorch-transformers/tree/master/examples\" rel=nofollow>examples folder</a>.</p><p>These tests can be run using <code>pytest</code> (install pytest if needed with <code>pip install pytest</code>).</p><p>You can run the tests from the root of the cloned repository with the commands:</p><pre lang=bash>python<span class=w> </span>-m<span class=w> </span>pytest<span class=w> </span>-sv<span class=w> </span>./pytorch_transformers/tests/python<span class=w> </span>-m<span class=w> </span>pytest<span class=w> </span>-sv<span class=w> </span>./examples/</pre><h3>Do you want to run a Transformer model on a mobile device?</h3><p>You should check out our <a href=\"https://github.com/huggingface/swift-coreml-transformers\" rel=nofollow><code>swift-coreml-transformers</code></a> repo.</p><p>It contains an example of a conversion script from a Pytorch trained Transformer model (here, <code>GPT-2</code>) to a CoreML model that runs on iOS devices.</p><p>At some point in the future, you'll be able to seamlessly move from pre-training or fine-tuning models in PyTorch to productizing them in CoreML,or prototype a model or an app in CoreML then research its hyperparameters or architecture from PyTorch. Super exciting!</p><h2>Quick tour</h2><p>Let's do a very quick overview of PyTorch-Transformers. Detailed examples for each model architecture (Bert, GPT, GPT-2, Transformer-XL, XLNet and XLM) can be found in the <a href=\"https://huggingface.co/pytorch-transformers/\" rel=nofollow>full documentation</a>.</p><pre lang=python3><span class=kn>import</span> <span class=nn>torch</span><span class=kn>from</span> <span class=nn>pytorch_transformers</span> <span class=kn>import</span> <span class=o>*</span><span class=c1># PyTorch-Transformers has a unified API</span><span class=c1># for 7 transformer architectures and 30 pretrained weights.</span><span class=c1>#          Model          | Tokenizer          | Pretrained weights shortcut</span><span class=n>MODELS</span> <span class=o>=</span> <span class=p>[(</span><span class=n>BertModel</span><span class=p>,</span>       <span class=n>BertTokenizer</span><span class=p>,</span>      <span class=s1>'bert-base-uncased'</span><span class=p>),</span>          <span class=p>(</span><span class=n>OpenAIGPTModel</span><span class=p>,</span>  <span class=n>OpenAIGPTTokenizer</span><span class=p>,</span> <span class=s1>'openai-gpt'</span><span class=p>),</span>          <span class=p>(</span><span class=n>GPT2Model</span><span class=p>,</span>       <span class=n>GPT2Tokenizer</span><span class=p>,</span>      <span class=s1>'gpt2'</span><span class=p>),</span>          <span class=p>(</span><span class=n>TransfoXLModel</span><span class=p>,</span>  <span class=n>TransfoXLTokenizer</span><span class=p>,</span> <span class=s1>'transfo-xl-wt103'</span><span class=p>),</span>          <span class=p>(</span><span class=n>XLNetModel</span><span class=p>,</span>      <span class=n>XLNetTokenizer</span><span class=p>,</span>     <span class=s1>'xlnet-base-cased'</span><span class=p>),</span>          <span class=p>(</span><span class=n>XLMModel</span><span class=p>,</span>        <span class=n>XLMTokenizer</span><span class=p>,</span>       <span class=s1>'xlm-mlm-enfr-1024'</span><span class=p>),</span>          <span class=p>(</span><span class=n>RobertaModel</span><span class=p>,</span>    <span class=n>RobertaTokenizer</span><span class=p>,</span>   <span class=s1>'roberta-base'</span><span class=p>)]</span><span class=c1># Let's encode some text in a sequence of hidden-states using each model:</span><span class=k>for</span> <span class=n>model_class</span><span class=p>,</span> <span class=n>tokenizer_class</span><span class=p>,</span> <span class=n>pretrained_weights</span> <span class=ow>in</span> <span class=n>MODELS</span><span class=p>:</span>    <span class=c1># Load pretrained model/tokenizer</span>    <span class=n>tokenizer</span> <span class=o>=</span> <span class=n>tokenizer_class</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>pretrained_weights</span><span class=p>)</span>    <span class=n>model</span> <span class=o>=</span> <span class=n>model_class</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>pretrained_weights</span><span class=p>)</span>    <span class=c1># Encode text</span>    <span class=n>input_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>\"Here is some text to encode\"</span><span class=p>,</span> <span class=n>add_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)])</span>  <span class=c1># Add special tokens takes care of adding [CLS], [SEP], &lt;s&gt;... tokens in the right way for each model.</span>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>        <span class=n>last_hidden_states</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>  <span class=c1># Models outputs are now tuples</span><span class=c1># Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.</span><span class=n>BERT_MODEL_CLASSES</span> <span class=o>=</span> <span class=p>[</span><span class=n>BertModel</span><span class=p>,</span> <span class=n>BertForPreTraining</span><span class=p>,</span> <span class=n>BertForMaskedLM</span><span class=p>,</span> <span class=n>BertForNextSentencePrediction</span><span class=p>,</span>                      <span class=n>BertForSequenceClassification</span><span class=p>,</span> <span class=n>BertForMultipleChoice</span><span class=p>,</span> <span class=n>BertForTokenClassification</span><span class=p>,</span>                      <span class=n>BertForQuestionAnswering</span><span class=p>]</span><span class=c1># All the classes for an architecture can be initiated from pretrained weights for this architecture</span><span class=c1># Note that additional weights added for fine-tuning are only initialized</span><span class=c1># and need to be trained on the down-stream task</span><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>BertTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>'bert-base-uncased'</span><span class=p>)</span><span class=k>for</span> <span class=n>model_class</span> <span class=ow>in</span> <span class=n>BERT_MODEL_CLASSES</span><span class=p>:</span>    <span class=c1># Load pretrained model/tokenizer</span>    <span class=n>model</span> <span class=o>=</span> <span class=n>model_class</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>'bert-base-uncased'</span><span class=p>)</span><span class=c1># Models can return full list of hidden-states &amp; attentions weights at each layer</span><span class=n>model</span> <span class=o>=</span> <span class=n>model_class</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>pretrained_weights</span><span class=p>,</span>                                    <span class=n>output_hidden_states</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>                                    <span class=n>output_attentions</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span><span class=n>input_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>\"Let's see all hidden-states and attentions on this text\"</span><span class=p>)])</span><span class=n>all_hidden_states</span><span class=p>,</span> <span class=n>all_attentions</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)[</span><span class=o>-</span><span class=mi>2</span><span class=p>:]</span><span class=c1># Models are compatible with Torchscript</span><span class=n>model</span> <span class=o>=</span> <span class=n>model_class</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>pretrained_weights</span><span class=p>,</span> <span class=n>torchscript</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span><span class=n>traced_model</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>jit</span><span class=o>.</span><span class=n>trace</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=p>(</span><span class=n>input_ids</span><span class=p>,))</span><span class=c1># Simple serialization for models and tokenizers</span><span class=n>model</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s1>'./directory/to/save/'</span><span class=p>)</span>  <span class=c1># save</span><span class=n>model</span> <span class=o>=</span> <span class=n>model_class</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>'./directory/to/save/'</span><span class=p>)</span>  <span class=c1># re-load</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s1>'./directory/to/save/'</span><span class=p>)</span>  <span class=c1># save</span><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>tokenizer_class</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>'./directory/to/save/'</span><span class=p>)</span>  <span class=c1># re-load</span><span class=c1># SOTA examples for GLUE, SQUAD, text generation...</span></pre><h2>Quick tour of the fine-tuning/usage scripts</h2><p>The library comprises several example scripts with SOTA performances for NLU and NLG tasks:</p><ul><li><code>run_glue.py</code>: an example fine-tuning Bert, XLNet and XLM on nine different GLUE tasks (<em>sequence-level classification</em>)</li><li><code>run_squad.py</code>: an example fine-tuning Bert, XLNet and XLM on the question answering dataset SQuAD 2.0 (<em>token-level classification</em>)</li><li><code>run_generation.py</code>: an example using GPT, GPT-2, Transformer-XL and XLNet for conditional language generation</li><li>other model-specific examples (see the documentation).</li></ul><p>Here are three quick usage examples for these scripts:</p><h3><code>run_glue.py</code>: Fine-tuning on GLUE tasks for sequence classification</h3><p>The <a href=\"https://gluebenchmark.com/\" rel=nofollow>General Language Understanding Evaluation (GLUE) benchmark</a> is a collection of nine sentence- or sentence-pair language understanding tasks for evaluating and analyzing natural language understanding systems.</p><p>Before running anyone of these GLUE tasks you should download the<a href=\"https://gluebenchmark.com/tasks\" rel=nofollow>GLUE data</a> by running<a href=\"https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e\" rel=nofollow>this script</a>and unpack it to some directory <code>$GLUE_DIR</code>.</p><p>You should also install the additional packages required by the examples:</p><pre lang=shell>pip<span class=w> </span>install<span class=w> </span>-r<span class=w> </span>./examples/requirements.txt</pre><pre lang=shell><span class=nb>export</span><span class=w> </span><span class=nv>GLUE_DIR</span><span class=o>=</span>/path/to/glue<span class=nb>export</span><span class=w> </span><span class=nv>TASK_NAME</span><span class=o>=</span>MRPCpython<span class=w> </span>./examples/run_glue.py<span class=w> </span><span class=se>\\</span><span class=w>    </span>--model_type<span class=w> </span>bert<span class=w> </span><span class=se>\\</span><span class=w>    </span>--model_name_or_path<span class=w> </span>bert-base-uncased<span class=w> </span><span class=se>\\</span><span class=w>    </span>--task_name<span class=w> </span><span class=nv>$TASK_NAME</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--do_train<span class=w> </span><span class=se>\\</span><span class=w>    </span>--do_eval<span class=w> </span><span class=se>\\</span><span class=w>    </span>--do_lower_case<span class=w> </span><span class=se>\\</span><span class=w>    </span>--data_dir<span class=w> </span><span class=nv>$GLUE_DIR</span>/<span class=nv>$TASK_NAME</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--max_seq_length<span class=w> </span><span class=m>128</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--per_gpu_eval_batch_size<span class=o>=</span><span class=m>8</span><span class=w>   </span><span class=se>\\</span><span class=w>    </span>--per_gpu_train_batch_size<span class=o>=</span><span class=m>8</span><span class=w>   </span><span class=se>\\</span><span class=w>    </span>--learning_rate<span class=w> </span>2e-5<span class=w> </span><span class=se>\\</span><span class=w>    </span>--num_train_epochs<span class=w> </span><span class=m>3</span>.0<span class=w> </span><span class=se>\\</span><span class=w>    </span>--output_dir<span class=w> </span>/tmp/<span class=nv>$TASK_NAME</span>/</pre><p>where task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.</p><p>The dev set results will be present within the text file 'eval_results.txt' in the specified output_dir. In case of MNLI, since there are two separate dev sets, matched and mismatched, there will be a separate output folder called '/tmp/MNLI-MM/' in addition to '/tmp/MNLI/'.</p><h4>Fine-tuning XLNet model on the STS-B regression task</h4><p>This example code fine-tunes XLNet on the STS-B corpus using parallel training on a server with 4 V100 GPUs.Parallel training is a simple way to use several GPUs (but is slower and less flexible than distributed training, see below).</p><pre lang=shell><span class=nb>export</span><span class=w> </span><span class=nv>GLUE_DIR</span><span class=o>=</span>/path/to/gluepython<span class=w> </span>./examples/run_glue.py<span class=w> </span><span class=se>\\</span><span class=w>    </span>--model_type<span class=w> </span>xlnet<span class=w> </span><span class=se>\\</span><span class=w>    </span>--model_name_or_path<span class=w> </span>xlnet-large-cased<span class=w> </span><span class=se>\\</span><span class=w>    </span>--do_train<span class=w>  </span><span class=se>\\</span><span class=w>    </span>--do_eval<span class=w>   </span><span class=se>\\</span><span class=w>    </span>--task_name<span class=o>=</span>sts-b<span class=w>     </span><span class=se>\\</span><span class=w>    </span>--data_dir<span class=o>=</span><span class=si>${</span><span class=nv>GLUE_DIR</span><span class=si>}</span>/STS-B<span class=w>  </span><span class=se>\\</span><span class=w>    </span>--output_dir<span class=o>=</span>./proc_data/sts-b-110<span class=w>   </span><span class=se>\\</span><span class=w>    </span>--max_seq_length<span class=o>=</span><span class=m>128</span><span class=w>   </span><span class=se>\\</span><span class=w>    </span>--per_gpu_eval_batch_size<span class=o>=</span><span class=m>8</span><span class=w>   </span><span class=se>\\</span><span class=w>    </span>--per_gpu_train_batch_size<span class=o>=</span><span class=m>8</span><span class=w>   </span><span class=se>\\</span><span class=w>    </span>--gradient_accumulation_steps<span class=o>=</span><span class=m>1</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--max_steps<span class=o>=</span><span class=m>1200</span><span class=w>  </span><span class=se>\\</span><span class=w>    </span>--model_name<span class=o>=</span>xlnet-large-cased<span class=w>   </span><span class=se>\\</span><span class=w>    </span>--overwrite_output_dir<span class=w>   </span><span class=se>\\</span><span class=w>    </span>--overwrite_cache<span class=w> </span><span class=se>\\</span><span class=w>    </span>--warmup_steps<span class=o>=</span><span class=m>120</span></pre><p>On this machine we thus have a batch size of 32, please increase <code>gradient_accumulation_steps</code> to reach the same batch size if you have a smaller machine. These hyper-parameters should result in a Pearson correlation coefficient of <code>+0.917</code> on the development set.</p><h4>Fine-tuning Bert model on the MRPC classification task</h4><p>This example code fine-tunes the Bert Whole Word Masking model on the Microsoft Research Paraphrase Corpus (MRPC) corpus using distributed training on 8 V100 GPUs to reach a F1 &gt; 92.</p><pre lang=bash>python<span class=w> </span>-m<span class=w> </span>torch.distributed.launch<span class=w> </span>--nproc_per_node<span class=w> </span><span class=m>8</span><span class=w> </span>./examples/run_glue.py<span class=w>   </span><span class=se>\\</span><span class=w>    </span>--model_type<span class=w> </span>bert<span class=w> </span><span class=se>\\</span><span class=w>    </span>--model_name_or_path<span class=w> </span>bert-large-uncased-whole-word-masking<span class=w> </span><span class=se>\\</span><span class=w>    </span>--task_name<span class=w> </span>MRPC<span class=w> </span><span class=se>\\</span><span class=w>    </span>--do_train<span class=w>   </span><span class=se>\\</span><span class=w>    </span>--do_eval<span class=w>   </span><span class=se>\\</span><span class=w>    </span>--do_lower_case<span class=w>   </span><span class=se>\\</span><span class=w>    </span>--data_dir<span class=w> </span><span class=nv>$GLUE_DIR</span>/MRPC/<span class=w>   </span><span class=se>\\</span><span class=w>    </span>--max_seq_length<span class=w> </span><span class=m>128</span><span class=w>   </span><span class=se>\\</span><span class=w>    </span>--per_gpu_eval_batch_size<span class=o>=</span><span class=m>8</span><span class=w>   </span><span class=se>\\</span><span class=w>    </span>--per_gpu_train_batch_size<span class=o>=</span><span class=m>8</span><span class=w>   </span><span class=se>\\</span><span class=w>    </span>--learning_rate<span class=w> </span>2e-5<span class=w>   </span><span class=se>\\</span><span class=w>    </span>--num_train_epochs<span class=w> </span><span class=m>3</span>.0<span class=w>  </span><span class=se>\\</span><span class=w>    </span>--output_dir<span class=w> </span>/tmp/mrpc_output/<span class=w> </span><span class=se>\\</span><span class=w>    </span>--overwrite_output_dir<span class=w>   </span><span class=se>\\</span><span class=w>    </span>--overwrite_cache<span class=w> </span><span class=se>\\</span></pre><p>Training with these hyper-parameters gave us the following results:</p><pre lang=bash><span class=w>  </span><span class=nv>acc</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=m>0</span>.8823529411764706<span class=w>  </span><span class=nv>acc_and_f1</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=m>0</span>.901702786377709<span class=w>  </span><span class=nv>eval_loss</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=m>0</span>.3418912578906332<span class=w>  </span><span class=nv>f1</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=m>0</span>.9210526315789473<span class=w>  </span><span class=nv>global_step</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=m>174</span><span class=w>  </span><span class=nv>loss</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=m>0</span>.07231863956341798</pre><h3><code>run_squad.py</code>: Fine-tuning on SQuAD for question-answering</h3><p>This example code fine-tunes BERT on the SQuAD dataset using distributed training on 8 V100 GPUs and Bert Whole Word Masking uncased model to reach a F1 &gt; 93 on SQuAD:</p><pre lang=bash>python<span class=w> </span>-m<span class=w> </span>torch.distributed.launch<span class=w> </span>--nproc_per_node<span class=o>=</span><span class=m>8</span><span class=w> </span>./examples/run_squad.py<span class=w> </span><span class=se>\\</span><span class=w>    </span>--model_type<span class=w> </span>bert<span class=w> </span><span class=se>\\</span><span class=w>    </span>--model_name_or_path<span class=w> </span>bert-large-uncased-whole-word-masking<span class=w> </span><span class=se>\\</span><span class=w>    </span>--do_train<span class=w> </span><span class=se>\\</span><span class=w>    </span>--do_eval<span class=w> </span><span class=se>\\</span><span class=w>    </span>--do_lower_case<span class=w> </span><span class=se>\\</span><span class=w>    </span>--train_file<span class=w> </span><span class=nv>$SQUAD_DIR</span>/train-v1.1.json<span class=w> </span><span class=se>\\</span><span class=w>    </span>--predict_file<span class=w> </span><span class=nv>$SQUAD_DIR</span>/dev-v1.1.json<span class=w> </span><span class=se>\\</span><span class=w>    </span>--learning_rate<span class=w> </span>3e-5<span class=w> </span><span class=se>\\</span><span class=w>    </span>--num_train_epochs<span class=w> </span><span class=m>2</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--max_seq_length<span class=w> </span><span class=m>384</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--doc_stride<span class=w> </span><span class=m>128</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--output_dir<span class=w> </span>../models/wwm_uncased_finetuned_squad/<span class=w> </span><span class=se>\\</span><span class=w>    </span>--per_gpu_eval_batch_size<span class=o>=</span><span class=m>3</span><span class=w>   </span><span class=se>\\</span><span class=w>    </span>--per_gpu_train_batch_size<span class=o>=</span><span class=m>3</span><span class=w>   </span><span class=se>\\</span></pre><p>Training with these hyper-parameters gave us the following results:</p><pre lang=bash>python<span class=w> </span><span class=nv>$SQUAD_DIR</span>/evaluate-v1.1.py<span class=w> </span><span class=nv>$SQUAD_DIR</span>/dev-v1.1.json<span class=w> </span>../models/wwm_uncased_finetuned_squad/predictions.json<span class=o>{</span><span class=s2>\"exact_match\"</span>:<span class=w> </span><span class=m>86</span>.91579943235573,<span class=w> </span><span class=s2>\"f1\"</span>:<span class=w> </span><span class=m>93</span>.1532499015869<span class=o>}</span></pre><p>This is the model provided as <code>bert-large-uncased-whole-word-masking-finetuned-squad</code>.</p><h3><code>run_generation.py</code>: Text generation with GPT, GPT-2, Transformer-XL and XLNet</h3><p>A conditional generation script is also included to generate text from a prompt.The generation script includes the <a href=\"https://github.com/rusiaaman/XLNet-gen#methodology\" rel=nofollow>tricks</a> proposed by by Aman Rusia to get high quality generation with memory models like Transformer-XL and XLNet (include a predefined text to make short inputs longer).</p><p>Here is how to run the script with the small version of OpenAI GPT-2 model:</p><pre lang=shell>python<span class=w> </span>./examples/run_generation.py<span class=w> </span><span class=se>\\</span><span class=w>    </span>--model_type<span class=o>=</span>gpt2<span class=w> </span><span class=se>\\</span><span class=w>    </span>--length<span class=o>=</span><span class=m>20</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--model_name_or_path<span class=o>=</span>gpt2<span class=w> </span><span class=se>\\</span></pre><h2>Migrating from pytorch-pretrained-bert to pytorch-transformers</h2><p>Here is a quick summary of what you should take care of when migrating from <code>pytorch-pretrained-bert</code> to <code>pytorch-transformers</code></p><h3>Models always output <code>tuples</code></h3><p>The main breaking change when migrating from <code>pytorch-pretrained-bert</code> to <code>pytorch-transformers</code> is that the models forward method always outputs a <code>tuple</code> with various elements depending on the model and the configuration parameters.</p><p>The exact content of the tuples for each model are detailed in the models' docstrings and the <a href=\"https://huggingface.co/pytorch-transformers/\" rel=nofollow>documentation</a>.</p><p>In pretty much every case, you will be fine by taking the first element of the output as the output you previously used in <code>pytorch-pretrained-bert</code>.</p><p>Here is a <code>pytorch-pretrained-bert</code> to <code>pytorch-transformers</code> conversion example for a <code>BertForSequenceClassification</code> classification model:</p><pre lang=python3><span class=c1># Let's load our model</span><span class=n>model</span> <span class=o>=</span> <span class=n>BertForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>'bert-base-uncased'</span><span class=p>)</span><span class=c1># If you used to have this line in pytorch-pretrained-bert:</span><span class=n>loss</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>labels</span><span class=o>=</span><span class=n>labels</span><span class=p>)</span><span class=c1># Now just use this line in pytorch-transformers to extract the loss from the output tuple:</span><span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>labels</span><span class=o>=</span><span class=n>labels</span><span class=p>)</span><span class=n>loss</span> <span class=o>=</span> <span class=n>outputs</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=c1># In pytorch-transformers you can also have access to the logits:</span><span class=n>loss</span><span class=p>,</span> <span class=n>logits</span> <span class=o>=</span> <span class=n>outputs</span><span class=p>[:</span><span class=mi>2</span><span class=p>]</span><span class=c1># And even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)</span><span class=n>model</span> <span class=o>=</span> <span class=n>BertForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>'bert-base-uncased'</span><span class=p>,</span> <span class=n>output_attentions</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span><span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>labels</span><span class=o>=</span><span class=n>labels</span><span class=p>)</span><span class=n>loss</span><span class=p>,</span> <span class=n>logits</span><span class=p>,</span> <span class=n>attentions</span> <span class=o>=</span> <span class=n>outputs</span></pre><h3>Serialization</h3><p>Breaking change in the <code>from_pretrained()</code>method:</p><ol><li><p>Models are now set in evaluation mode by default when instantiated with the <code>from_pretrained()</code> method. To train them don't forget to set them back in training mode (<code>model.train()</code>) to activate the dropout modules.</p></li><li><p>The additional <code>*input</code> and <code>**kwargs</code> arguments supplied to the <code>from_pretrained()</code> method used to be directly passed to the underlying model's class <code>__init__()</code> method. They are now used to update the model configuration attribute instead which can break derived model classes build based on the previous <code>BertForSequenceClassification</code> examples. We are working on a way to mitigate this breaking change in <a href=\"https://github.com/huggingface/pytorch-transformers/pull/866\" rel=nofollow>#866</a> by forwarding the the model <code>__init__()</code> method (i) the provided positional arguments and (ii) the keyword arguments which do not match any configuration class attributes.</p></li></ol><p>Also, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method <code>save_pretrained(save_directory)</code> if you were using any other serialization method before.</p><p>Here is an example:</p><pre lang=python3><span class=c1>### Let's load a model and tokenizer</span><span class=n>model</span> <span class=o>=</span> <span class=n>BertForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>'bert-base-uncased'</span><span class=p>)</span><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>BertTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>'bert-base-uncased'</span><span class=p>)</span><span class=c1>### Do some stuff to our model and tokenizer</span><span class=c1># Ex: add new tokens to the vocabulary and embeddings of our model</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>add_tokens</span><span class=p>([</span><span class=s1>'[SPECIAL_TOKEN_1]'</span><span class=p>,</span> <span class=s1>'[SPECIAL_TOKEN_2]'</span><span class=p>])</span><span class=n>model</span><span class=o>.</span><span class=n>resize_token_embeddings</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>tokenizer</span><span class=p>))</span><span class=c1># Train our model</span><span class=n>train</span><span class=p>(</span><span class=n>model</span><span class=p>)</span><span class=c1>### Now let's save our model and tokenizer to a directory</span><span class=n>model</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s1>'./my_saved_model_directory/'</span><span class=p>)</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s1>'./my_saved_model_directory/'</span><span class=p>)</span><span class=c1>### Reload the model and the tokenizer</span><span class=n>model</span> <span class=o>=</span> <span class=n>BertForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>'./my_saved_model_directory/'</span><span class=p>)</span><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>BertTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>'./my_saved_model_directory/'</span><span class=p>)</span></pre><h3>Optimizers: BertAdam &amp; OpenAIAdam are now AdamW, schedules are standard PyTorch schedules</h3><p>The two optimizers previously included, <code>BertAdam</code> and <code>OpenAIAdam</code>, have been replaced by a single <code>AdamW</code> optimizer which has a few differences:</p><ul><li>it only implements weights decay correction,</li><li>schedules are now externals (see below),</li><li>gradient clipping is now also external (see below).</li></ul><p>The new optimizer <code>AdamW</code> matches PyTorch <code>Adam</code> optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping.</p><p>The schedules are now standard <a href=\"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\" rel=nofollow>PyTorch learning rate schedulers</a> and not part of the optimizer anymore.</p><p>Here is a conversion examples from <code>BertAdam</code> with a linear warmup and decay schedule to <code>AdamW</code> and the same schedule:</p><pre lang=python3><span class=c1># Parameters:</span><span class=n>lr</span> <span class=o>=</span> <span class=mf>1e-3</span><span class=n>max_grad_norm</span> <span class=o>=</span> <span class=mf>1.0</span><span class=n>num_total_steps</span> <span class=o>=</span> <span class=mi>1000</span><span class=n>num_warmup_steps</span> <span class=o>=</span> <span class=mi>100</span><span class=n>warmup_proportion</span> <span class=o>=</span> <span class=nb>float</span><span class=p>(</span><span class=n>num_warmup_steps</span><span class=p>)</span> <span class=o>/</span> <span class=nb>float</span><span class=p>(</span><span class=n>num_total_steps</span><span class=p>)</span>  <span class=c1># 0.1</span><span class=c1>### Previously BertAdam optimizer was instantiated like this:</span><span class=n>optimizer</span> <span class=o>=</span> <span class=n>BertAdam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>,</span> <span class=n>schedule</span><span class=o>=</span><span class=s1>'warmup_linear'</span><span class=p>,</span> <span class=n>warmup</span><span class=o>=</span><span class=n>warmup_proportion</span><span class=p>,</span> <span class=n>t_total</span><span class=o>=</span><span class=n>num_total_steps</span><span class=p>)</span><span class=c1>### and used like this:</span><span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>train_data</span><span class=p>:</span>    <span class=n>loss</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span><span class=c1>### In PyTorch-Transformers, optimizer and schedules are splitted and instantiated like this:</span><span class=n>optimizer</span> <span class=o>=</span> <span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>,</span> <span class=n>correct_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>  <span class=c1># To reproduce BertAdam specific behavior set correct_bias=False</span><span class=n>scheduler</span> <span class=o>=</span> <span class=n>WarmupLinearSchedule</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>warmup_steps</span><span class=o>=</span><span class=n>num_warmup_steps</span><span class=p>,</span> <span class=n>t_total</span><span class=o>=</span><span class=n>num_total_steps</span><span class=p>)</span>  <span class=c1># PyTorch scheduler</span><span class=c1>### and used like this:</span><span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>train_data</span><span class=p>:</span>    <span class=n>loss</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>    <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>clip_grad_norm_</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>max_grad_norm</span><span class=p>)</span>  <span class=c1># Gradient clipping is not in AdamW anymore (so you can use amp without issue)</span>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span></pre><h2>Citation</h2><p>At the moment, there is no paper associated to PyTorch-Transformers but we are working on preparing one. In the meantime, please include a mention of the library and a link to the present repository if you use this work in a published or open-source project.</p>          </div>        </div>        <div id=\"data\" data-project-tabs-target=\"content\" class=\"vertical-tabs__content\" role=\"tabpanel\" aria-labelledby=\"mobile-data-tab\" tabindex=\"-1\">          <h2 class=\"page-title\">Project details</h2><div class=\"sidebar-section\">  <h3 class=\"sidebar-section__title\">Project links</h3>  <ul class=\"vertical-tabs__list\">    <li>      <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\" href=\"https://github.com/huggingface/pytorch-transformers\" rel=\"nofollow\">        <i class=\"fas fa-home\" aria-hidden=\"true\"></i>Homepage      </a>    </li>  </ul></div><div class=\"sidebar-section\">  <h3 class=\"sidebar-section__title\">Statistics</h3>  <div class=\"hidden github-repo-info\" data-controller=\"github-repo-info\">GitHub statistics:    <ul class=\"vertical-tabs__list\">      <li>        <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\"           data-github-repo-info-target=\"stargazersUrl\" rel=\"noopener\">          <i class=\"fa fa-star\" aria-hidden=\"true\"></i>          <strong>Stars:</strong>          <span data-github-repo-info-target=\"stargazersCount\"></span>        </a>      </li>      <li>        <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\"           data-github-repo-info-target=\"forksUrl\" rel=\"noopener\">          <i class=\"fa fa-code-branch\" aria-hidden=\"true\"></i>          <strong>Forks:</strong>          <span data-github-repo-info-target=\"forksCount\"></span>        </a>      </li>      <li>        <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\"           data-github-repo-info-target=\"openIssuesUrl\" rel=\"noopener\">          <i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>          <strong>Open issues:</strong>          <span data-github-repo-info-target=\"openIssuesCount\"></span>        </a>      </li>      <li>        <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\"           data-github-repo-info-target=\"openPRsUrl\" rel=\"noopener\">          <i class=\"fa fa-code-pull-request\" aria-hidden=\"true\"></i>          <strong>Open PRs:</strong>          <span data-github-repo-info-target=\"openPRsCount\"></span>        </a>      </li>    </ul>  </div>  <p>View statistics for this project via <a href=\"https://libraries.io/pypi/pytorch-transformers\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Libraries.io</a>, or by using <a href=\"https://packaging.python.org/guides/analyzing-pypi-package-downloads/\" target=\"_blank\" rel=\"noopener\">our public dataset on Google BigQuery</a>  </p></div><div class=\"sidebar-section\">  <h3 class=\"sidebar-section__title\">Meta</h3>  <p><strong>License:</strong> Apache Software License (Apache)</p>    <p><strong>Author:</strong> <a href=\"mailto:thomas@huggingface.co\">Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Google AI Language Team Authors, Open AI team Authors</a></p>  <p class=\"tags\">    <i class=\"fa fa-tags\" aria-hidden=\"true\"></i>    <span class=\"sr-only\">Tags</span>    <span class=\"package-keyword\">      NLP,    </span>    <span class=\"package-keyword\">      deep,    </span>    <span class=\"package-keyword\">      learning,    </span>    <span class=\"package-keyword\">      transformer,    </span>    <span class=\"package-keyword\">      pytorch,    </span>    <span class=\"package-keyword\">      BERT,    </span>    <span class=\"package-keyword\">      GPT,    </span>    <span class=\"package-keyword\">      GPT-2,    </span>    <span class=\"package-keyword\">      google,    </span>    <span class=\"package-keyword\">      openai,    </span>    <span class=\"package-keyword\">      CMU    </span>  </p></div><div class=\"sidebar-section\">    <h3 class=\"sidebar-section__title\">Maintainers</h3>      <span class=\"sidebar-section__maintainer\">        <a href=\"/user/Thomwolf/\" aria-label=\"Thomwolf\">          <span class=\"sidebar-section__user-gravatar\">            <img src=\"https://pypi-camo.freetls.fastly.net/b23c4580fa4e4509fd26c0a20f6a32862cf6e4f3/68747470733a2f2f7365637572652e67726176617461722e636f6d2f6176617461722f36346632346335653561383531656337396538306230633131616566393762653f73697a653d3530\" height=\"50\" width=\"50\" alt=\"Avatar for Thomwolf from gravatar.com\" title=\"Avatar for Thomwolf from gravatar.com\">          </span>          <span class=\"sidebar-section__user-gravatar-text\">            Thomwolf          </span>        </a>      </span></div><div class=\"sidebar-section\">  <h3 class=\"sidebar-section__title\">Classifiers</h3>  <ul class=\"sidebar-section__classifiers\">    <li>      <strong>Intended Audience</strong>      <ul>        <li>          <a href=\"/search/?c=Intended+Audience+%3A%3A+Science%2FResearch\">            Science/Research          </a>        </li>      </ul>    </li>    <li>      <strong>License</strong>      <ul>        <li>          <a href=\"/search/?c=License+%3A%3A+OSI+Approved+%3A%3A+Apache+Software+License\">            OSI Approved :: Apache Software License          </a>        </li>      </ul>    </li>    <li>      <strong>Programming Language</strong>      <ul>        <li>          <a href=\"/search/?c=Programming+Language+%3A%3A+Python+%3A%3A+3\">            Python :: 3          </a>        </li>      </ul>    </li>    <li>      <strong>Topic</strong>      <ul>        <li>          <a href=\"/search/?c=Topic+%3A%3A+Scientific%2FEngineering+%3A%3A+Artificial+Intelligence\">            Scientific/Engineering :: Artificial Intelligence          </a>        </li>      </ul>    </li>  </ul></div>          <br>        </div>        <div id=\"history\" data-project-tabs-target=\"content\" class=\"vertical-tabs__content\" role=\"tabpanel\" aria-labelledby=\"history-tab mobile-history-tab\" tabindex=\"-1\">          <h2 class=\"page-title split-layout\">            <span>Release history</span>            <span class=\"reset-text margin-top\">              <a href=\"/help/#project-release-notifications\">Release notifications</a> |              <a href=\"/rss/project/pytorch-transformers/releases.xml\">RSS feed <i class=\"fa fa-rss\" aria-hidden=\"true\"></i></a>            </span>          </h2>          <div class=\"release-timeline\">            <div class=\"release release--latest release--current\">              <div class=\"release__meta\">                <span class=\"badge\">This version</span>              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/blue-cube.572a5bfb.svg\">              </div>              <a class=\"card release__card\" href=\"/project/pytorch-transformers/1.2.0/\">                <p class=\"release__version\">                  1.2.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2019-09-04T11:36:34+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Sep 4, 2019</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/pytorch-transformers/1.1.0/\">                <p class=\"release__version\">                  1.1.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2019-08-15T21:14:28+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Aug 15, 2019</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/pytorch-transformers/1.0.0/\">                <p class=\"release__version\">                  1.0.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2019-07-16T14:13:32+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Jul 16, 2019</time>                </p>              </a>            </div>            <div class=\"release release--oldest\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/pytorch-transformers/0.7.0/\">                <p class=\"release__version\">                  0.7.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2019-07-05T10:23:15+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Jul 5, 2019</time>                </p>              </a>            </div>          </div>        </div>          <div id=\"files\" data-project-tabs-target=\"content\" class=\"vertical-tabs__content\" role=\"tabpanel\" aria-labelledby=\"files-tab mobile-files-tab\" tabindex=\"-1\">            <h2 class=\"page-title\">Download files</h2>            <p>Download the file for your platform. If you're not sure which to choose, learn more about <a href=\"https://packaging.python.org/tutorials/installing-packages/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">installing packages</a>.</p>            <h3>Source Distribution            </h3>                  <div class=\"file\">      <div class=\"file__graphic\">        <i class=\"far fa-file\" aria-hidden=\"true\"></i>      </div>      <div class=\"card file__card\">        <a href=\"https://files.pythonhosted.org/packages/39/46/60ade12cd10f3e3dc7cb109361a73ebd8a8530c35bed71f681d2588aa277/pytorch_transformers-1.2.0.tar.gz\">          pytorch_transformers-1.2.0.tar.gz        </a>        (152.3 kB        <a href=\"#copy-hash-modal-f161e187-600b-46c2-a075-74da62a1615f\">view hashes</a>)        <p class=\"file__meta\">          Uploaded <time datetime=\"2019-09-04T11:36:39+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Sep 4, 2019</time>          <code>source</code>        </p>      </div>    </div>            <h3>Built Distributions            </h3>                <div class=\"file\">      <div class=\"file__graphic\">        <i class=\"far fa-file\" aria-hidden=\"true\"></i>      </div>      <div class=\"card file__card\">        <a href=\"https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl\">          pytorch_transformers-1.2.0-py3-none-any.whl        </a>        (176.4 kB        <a href=\"#copy-hash-modal-b6679123-71b9-4f1b-a779-1890b4032687\">view hashes</a>)        <p class=\"file__meta\">          Uploaded <time datetime=\"2019-09-04T11:36:36+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Sep 4, 2019</time>          <code>py3</code>        </p>      </div>    </div>    <div class=\"file\">      <div class=\"file__graphic\">        <i class=\"far fa-file\" aria-hidden=\"true\"></i>      </div>      <div class=\"card file__card\">        <a href=\"https://files.pythonhosted.org/packages/a9/be/587dd871b8abcdb34bbfbfac976b403bb22bc5c41d3d18cff329bf669fd3/pytorch_transformers-1.2.0-py2-none-any.whl\">          pytorch_transformers-1.2.0-py2-none-any.whl        </a>        (176.4 kB        <a href=\"#copy-hash-modal-f88d2072-548c-40f5-b9ab-add05edc2774\">view hashes</a>)        <p class=\"file__meta\">          Uploaded <time datetime=\"2019-09-04T11:36:34+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Sep 4, 2019</time>          <code>py2</code>        </p>      </div>    </div>          </div><div id=\"copy-hash-modal-f161e187-600b-46c2-a075-74da62a1615f\" class=\"modal modal--wide\">  <div class=\"modal__content\" role=\"dialog\">    <a href=\"#modal-close\" title=\"Close\" class=\"modal__close\">      <i class=\"fa fa-times\" aria-hidden=\"true\"></i>      <span class=\"sr-only\">Close</span>    </a>    <div class=\"modal__body\">      <h3 class=\"modal__title\"><a href=\"https://pip.pypa.io/en/stable/topics/secure-installs/#hash-checking-mode\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Hashes</a> for pytorch_transformers-1.2.0.tar.gz      </h3>      <table class=\"table table--hashes\">        <caption class=\"sr-only\">Hashes for pytorch_transformers-1.2.0.tar.gz</caption>        <thead>          <tr>            <th scope=\"col\">Algorithm</th>            <th scope=\"col\">Hash digest</th>            <th></th>          </tr>        </thead>        <tbody>          <tr data-controller=\"clipboard\">            <th scope=\"row\">SHA256</th>            <td><code data-clipboard-target=\"source\">293e4a864ae9d9401f9fba13f16b8696e4a1cb38bcd0b56562d03af5489daeb9</code></td>            <td class=\"table__align-right\">              <button type=\"button\" class=\"button button--small copy-tooltip copy-tooltip-w\" data-action=\"clipboard#copy\" data-clipboard-target=\"tooltip\" data-clipboard-tooltip-value=\"Copy to clipboard\">Copy              </button>            </td>          </tr>          <tr data-controller=\"clipboard\">            <th scope=\"row\">MD5</th>            <td><code data-clipboard-target=\"source\">48e994040a609c050cf559764e430cb6</code></td>            <td class=\"table__align-right\">              <button type=\"button\" class=\"button button--small copy-tooltip copy-tooltip-w\" data-action=\"clipboard#copy\" data-clipboard-target=\"tooltip\" data-clipboard-tooltip-value=\"Copy to clipboard\">Copy              </button>            </td>          </tr>          <tr data-controller=\"clipboard\">            <th scope=\"row\">BLAKE2b-256</th>            <td><code data-clipboard-target=\"source\">394660ade12cd10f3e3dc7cb109361a73ebd8a8530c35bed71f681d2588aa277</code></td>            <td class=\"table__align-right\">              <button type=\"button\" class=\"button button--small copy-tooltip copy-tooltip-w\" data-action=\"clipboard#copy\" data-clipboard-target=\"tooltip\" data-clipboard-tooltip-value=\"Copy to clipboard\">Copy              </button>            </td>          </tr>        </tbody>      </table>    </div>    <div class=\"modal__footer\">      <a href=\"#modal-close\" class=\"button button--primary modal__action\">Close</a>    </div>  </div></div><div id=\"copy-hash-modal-b6679123-71b9-4f1b-a779-1890b4032687\" class=\"modal modal--wide\">  <div class=\"modal__content\" role=\"dialog\">    <a href=\"#modal-close\" title=\"Close\" class=\"modal__close\">      <i class=\"fa fa-times\" aria-hidden=\"true\"></i>      <span class=\"sr-only\">Close</span>    </a>    <div class=\"modal__body\">      <h3 class=\"modal__title\"><a href=\"https://pip.pypa.io/en/stable/topics/secure-installs/#hash-checking-mode\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Hashes</a> for pytorch_transformers-1.2.0-py3-none-any.whl      </h3>      <table class=\"table table--hashes\">        <caption class=\"sr-only\">Hashes for pytorch_transformers-1.2.0-py3-none-any.whl</caption>        <thead>          <tr>            <th scope=\"col\">Algorithm</th>            <th scope=\"col\">Hash digest</th>            <th></th>          </tr>        </thead>        <tbody>          <tr data-controller=\"clipboard\">            <th scope=\"row\">SHA256</th>            <td><code data-clipboard-target=\"source\">bdb606fe1f2d27586710ed03cfa49dbbd80215c38bf965862daada0c137fd7ce</code></td>            <td class=\"table__align-right\">              <button type=\"button\" class=\"button button--small copy-tooltip copy-tooltip-w\" data-action=\"clipboard#copy\" data-clipboard-target=\"tooltip\" data-clipboard-tooltip-value=\"Copy to clipboard\">Copy              </button>            </td>          </tr>          <tr data-controller=\"clipboard\">            <th scope=\"row\">MD5</th>            <td><code data-clipboard-target=\"source\">2dba40cda929caa2d2c8c843bf327336</code></td>            <td class=\"table__align-right\">              <button type=\"button\" class=\"button button--small copy-tooltip copy-tooltip-w\" data-action=\"clipboard#copy\" data-clipboard-target=\"tooltip\" data-clipboard-tooltip-value=\"Copy to clipboard\">Copy              </button>            </td>          </tr>          <tr data-controller=\"clipboard\">            <th scope=\"row\">BLAKE2b-256</th>            <td><code data-clipboard-target=\"source\">a3b7d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b</code></td>            <td class=\"table__align-right\">              <button type=\"button\" class=\"button button--small copy-tooltip copy-tooltip-w\" data-action=\"clipboard#copy\" data-clipboard-target=\"tooltip\" data-clipboard-tooltip-value=\"Copy to clipboard\">Copy              </button>            </td>          </tr>        </tbody>      </table>    </div>    <div class=\"modal__footer\">      <a href=\"#modal-close\" class=\"button button--primary modal__action\">Close</a>    </div>  </div></div><div id=\"copy-hash-modal-f88d2072-548c-40f5-b9ab-add05edc2774\" class=\"modal modal--wide\">  <div class=\"modal__content\" role=\"dialog\">    <a href=\"#modal-close\" title=\"Close\" class=\"modal__close\">      <i class=\"fa fa-times\" aria-hidden=\"true\"></i>      <span class=\"sr-only\">Close</span>    </a>    <div class=\"modal__body\">      <h3 class=\"modal__title\"><a href=\"https://pip.pypa.io/en/stable/topics/secure-installs/#hash-checking-mode\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Hashes</a> for pytorch_transformers-1.2.0-py2-none-any.whl      </h3>      <table class=\"table table--hashes\">        <caption class=\"sr-only\">Hashes for pytorch_transformers-1.2.0-py2-none-any.whl</caption>        <thead>          <tr>            <th scope=\"col\">Algorithm</th>            <th scope=\"col\">Hash digest</th>            <th></th>          </tr>        </thead>        <tbody>          <tr data-controller=\"clipboard\">            <th scope=\"row\">SHA256</th>            <td><code data-clipboard-target=\"source\">15f12a04424c0f6d3a7c7b57d6c79628dc9c117a204fb7db8c1ea330c77a6898</code></td>            <td class=\"table__align-right\">              <button type=\"button\" class=\"button button--small copy-tooltip copy-tooltip-w\" data-action=\"clipboard#copy\" data-clipboard-target=\"tooltip\" data-clipboard-tooltip-value=\"Copy to clipboard\">Copy              </button>            </td>          </tr>          <tr data-controller=\"clipboard\">            <th scope=\"row\">MD5</th>            <td><code data-clipboard-target=\"source\">885d028952907e3b9425567892e89d6f</code></td>            <td class=\"table__align-right\">              <button type=\"button\" class=\"button button--small copy-tooltip copy-tooltip-w\" data-action=\"clipboard#copy\" data-clipboard-target=\"tooltip\" data-clipboard-tooltip-value=\"Copy to clipboard\">Copy              </button>            </td>          </tr>          <tr data-controller=\"clipboard\">            <th scope=\"row\">BLAKE2b-256</th>            <td><code data-clipboard-target=\"source\">a9be587dd871b8abcdb34bbfbfac976b403bb22bc5c41d3d18cff329bf669fd3</code></td>            <td class=\"table__align-right\">              <button type=\"button\" class=\"button button--small copy-tooltip copy-tooltip-w\" data-action=\"clipboard#copy\" data-clipboard-target=\"tooltip\" data-clipboard-tooltip-value=\"Copy to clipboard\">Copy              </button>            </td>          </tr>        </tbody>      </table>    </div>    <div class=\"modal__footer\">      <a href=\"#modal-close\" class=\"button button--primary modal__action\">Close</a>    </div>  </div></div>      </div>    </div>  </div></div>    </main>    <footer class=\"footer\">      <div class=\"footer__logo\">        <img src=\"/static/images/white-cube.2351a86c.svg\" alt=\"\" class=\"-js-white-cube\">      </div>      <div class=\"footer__menus\">        <div class=\"footer__menu\">          <h2>Help</h2>          <nav aria-label=\"Help navigation\">            <ul>              <li><a href=\"https://packaging.python.org/tutorials/installing-packages/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Installing packages</a></li>              <li><a href=\"https://packaging.python.org/tutorials/packaging-projects/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Uploading packages</a></li>              <li><a href=\"https://packaging.python.org/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">User guide</a></li>              <li><a href=\"https://www.python.org/dev/peps/pep-0541/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Project name retention</a></li>              <li><a href=\"/help/\">FAQs</a></li>            </ul>          </nav>        </div>        <div class=\"footer__menu\">          <h2>About PyPI</h2>          <nav aria-label=\"About PyPI navigation\">            <ul>              <li><a href=\"https://twitter.com/PyPI\" title=\"External link\" target=\"_blank\" rel=\"noopener\">PyPI on Twitter</a></li>              <li><a href=\"https://dtdg.co/pypi\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Infrastructure dashboard</a></li>              <li><a href=\"/stats/\">Statistics</a></li>              <li><a href=\"/trademarks/\">Logos & trademarks</a></li>              <li><a href=\"/sponsors/\">Our sponsors</a></li>            </ul>          </nav>        </div>        <div class=\"footer__menu\">          <h2>Contributing to PyPI</h2>          <nav aria-label=\"How to contribute navigation\">            <ul>              <li><a href=\"/help/#feedback\">Bugs and feedback</a></li>              <li><a href=\"https://github.com/pypi/warehouse\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Contribute on GitHub</a></li>              <li><a href=\"https://hosted.weblate.org/projects/pypa/warehouse/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Translate PyPI</a></li>              <li><a href=\"/sponsors/\">Sponsor PyPI</a></li>              <li><a href=\"https://github.com/pypi/warehouse/graphs/contributors\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Development credits</a></li>            </ul>          </nav>        </div>        <div class=\"footer__menu\">          <h2>Using PyPI</h2>          <nav aria-label=\"Using PyPI navigation\">            <ul>              <li><a href=\"https://github.com/pypa/.github/blob/main/CODE_OF_CONDUCT.md\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Code of conduct</a></li>              <li><a href=\"/security/\">Report security issue</a></li>              <li><a href=\"https://www.python.org/privacy/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Privacy policy</a></li>              <li><a href=\"/policy/terms-of-use/\">Terms of use</a></li>              <li><a href=\"/policy/acceptable-use-policy/\">Acceptable Use Policy</a></li>            </ul>          </nav>        </div>      </div>      <hr class=\"footer__divider\">      <div class=\"footer__text\">        <p>Status:<a href=\"https://status.python.org/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">          <span data-statuspage-domain=\"https://2p66nmmycsj3.statuspage.io\">all systems operational</span></a>        </p>        <p>Developed and maintained by the Python community, for the Python community.          <br>          <a href=\"https://donate.pypi.org\">Donate today!</a>        </p>        <p>          \"PyPI\", \"Python Package Index\", and the blocks logos are registered <a href=\"/trademarks/\">trademarks</a> of the <a href=\"https://python.org/psf-landing\" target=\"_blank\" rel=\"noopener\">Python Software Foundation</a>.<br>        </p>        <p>          \u00a9 2024 <a href=\"https://www.python.org/psf-landing/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Python Software Foundation</a><br>          <a href=\"/sitemap/\">Site map</a>        </p>      </div>      <div class=\"centered hide-on-desktop\">        <button type=\"button\" class=\"button button--switch-to-desktop hidden\" data-viewport-toggle-target=\"switchToDesktop\" data-action=\"viewport-toggle#switchToDesktop\">Switch to desktop version        </button>      </div>    </footer>    <div class=\"language-switcher\">      <form action=\"/locale/\">        <ul>          <li>            <button              class=\"language-switcher__selected\"              name=\"locale_id\" value=\"en\" type=\"submit\"            >              English            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"es\" type=\"submit\"            >              espa\u00f1ol            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"fr\" type=\"submit\"            >              fran\u00e7ais            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"ja\" type=\"submit\"            >              \u65e5\u672c\u8a9e            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"pt_BR\" type=\"submit\"            >              portugu\u00eas (Brasil)            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"uk\" type=\"submit\"            >              \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"el\" type=\"submit\"            >              \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"de\" type=\"submit\"            >              Deutsch            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"zh_Hans\" type=\"submit\"            >              \u4e2d\u6587 (\u7b80\u4f53)            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"zh_Hant\" type=\"submit\"            >              \u4e2d\u6587 (\u7e41\u9ad4)            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"ru\" type=\"submit\"            >              \u0440\u0443\u0441\u0441\u043a\u0438\u0439            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"he\" type=\"submit\"            >              \u05e2\u05d1\u05e8\u05d9\u05ea            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"eo\" type=\"submit\"            >              esperanto            </button>          </li>        </ul>      </form>    </div><div class=\"sponsors\">  <p class=\"sponsors__title\">Supported by</p>  <div class=\"sponsors__divider\"></div>        <a class=\"sponsors__sponsor\" target=\"_blank\" rel=\"noopener\" href=\"https://aws.amazon.com/\">          <img class=sponsors__image src=\"https://pypi-camo.freetls.fastly.net/ed7074cadad1a06f56bc520ad9bd3e00d0704c5b/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f6177732d77686974652d6c6f676f2d7443615473387a432e706e67\" alt=AWS loading=lazy>          <span class=\"sponsors__name\">AWS</span>          <span class=\"sponsors__service\">            Cloud computing and Security Sponsor          </span>        </a>        <a class=\"sponsors__sponsor\" target=\"_blank\" rel=\"noopener\" href=\"https://www.datadoghq.com/\">          <img class=sponsors__image src=\"https://pypi-camo.freetls.fastly.net/8855f7c063a3bdb5b0ce8d91bfc50cf851cc5c51/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f64617461646f672d77686974652d6c6f676f2d6668644c4e666c6f2e706e67\" alt=Datadog loading=lazy>          <span class=\"sponsors__name\">Datadog</span>          <span class=\"sponsors__service\">            Monitoring          </span>        </a>        <a class=\"sponsors__sponsor\" target=\"_blank\" rel=\"noopener\" href=\"https://www.fastly.com/\">          <img class=sponsors__image src=\"https://pypi-camo.freetls.fastly.net/df6fe8829cbff2d7f668d98571df1fd011f36192/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f666173746c792d77686974652d6c6f676f2d65684d3077735f6f2e706e67\" alt=Fastly loading=lazy>          <span class=\"sponsors__name\">Fastly</span>          <span class=\"sponsors__service\">            CDN          </span>        </a>        <a class=\"sponsors__sponsor\" target=\"_blank\" rel=\"noopener\" href=\"https://careers.google.com/\">          <img class=sponsors__image src=\"https://pypi-camo.freetls.fastly.net/420cc8cf360bac879e24c923b2f50ba7d1314fb0/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f676f6f676c652d77686974652d6c6f676f2d616734424e3774332e706e67\" alt=Google loading=lazy>          <span class=\"sponsors__name\">Google</span>          <span class=\"sponsors__service\">            Download Analytics          </span>        </a>        <a class=\"sponsors__sponsor\" target=\"_blank\" rel=\"noopener\" href=\"https://www.python.org/psf/sponsors/#microsoft\">          <img class=sponsors__image src=\"https://pypi-camo.freetls.fastly.net/524d1ce72f7772294ca4c1fe05d21dec8fa3f8ea/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f6d6963726f736f66742d77686974652d6c6f676f2d5a443172685444462e706e67\" alt=Microsoft loading=lazy>          <span class=\"sponsors__name\">Microsoft</span>          <span class=\"sponsors__service\">            PSF Sponsor          </span>        </a>        <a class=\"sponsors__sponsor\" target=\"_blank\" rel=\"noopener\" href=\"https://www.pingdom.com/\">          <img class=sponsors__image src=\"https://pypi-camo.freetls.fastly.net/d01053c02f3a626b73ffcb06b96367fdbbf9e230/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f70696e67646f6d2d77686974652d6c6f676f2d67355831547546362e706e67\" alt=Pingdom loading=lazy>          <span class=\"sponsors__name\">Pingdom</span>          <span class=\"sponsors__service\">            Monitoring          </span>        </a>        <a class=\"sponsors__sponsor\" target=\"_blank\" rel=\"noopener\" href=\"https://getsentry.com/for/python\">          <img class=sponsors__image src=\"https://pypi-camo.freetls.fastly.net/67af7117035e2345bacb5a82e9aa8b5b3e70701d/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f73656e7472792d77686974652d6c6f676f2d4a2d6b64742d706e2e706e67\" alt=Sentry loading=lazy>          <span class=\"sponsors__name\">Sentry</span>          <span class=\"sponsors__service\">            Error logging          </span>        </a>        <a class=\"sponsors__sponsor\" target=\"_blank\" rel=\"noopener\" href=\"https://statuspage.io\">          <img class=sponsors__image src=\"https://pypi-camo.freetls.fastly.net/b611884ff90435a0575dbab7d9b0d3e60f136466/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f737461747573706167652d77686974652d6c6f676f2d5467476c6a4a2d502e706e67\" alt=StatusPage loading=lazy>          <span class=\"sponsors__name\">StatusPage</span>          <span class=\"sponsors__service\">            Status page          </span>        </a></div>  </body></html>",
  "embeddings": []
}