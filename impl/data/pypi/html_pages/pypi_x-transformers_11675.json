{
  "libtype": "pypi",
  "libname": "x-transformers",
  "url": "https://github.com/lucidrains/x-transformers",
  "html": "<!DOCTYPE html><html  lang=\"en\"    data-color-mode=\"auto\" data-light-theme=\"light\" data-dark-theme=\"dark\"  data-a11y-animated-images=\"system\" data-a11y-link-underlines=\"true\"  >  <head>    <meta charset=\"utf-8\">  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">  <link rel=\"dns-prefetch\" href=\"https://avatars.githubusercontent.com\">  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">  <link rel=\"dns-prefetch\" href=\"https://user-images.githubusercontent.com/\">  <link rel=\"preconnect\" href=\"https://github.githubassets.com\" crossorigin>  <link rel=\"preconnect\" href=\"https://avatars.githubusercontent.com\">    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/light-0eace2597ca3.css\" /><link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/dark-a167e256da9c.css\" /><link data-color-theme=\"dark_dimmed\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/dark_dimmed-d11f2cf8009b.css\" /><link data-color-theme=\"dark_high_contrast\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/dark_high_contrast-ea7373db06c8.css\" /><link data-color-theme=\"dark_colorblind\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/dark_colorblind-afa99dcf40f7.css\" /><link data-color-theme=\"light_colorblind\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/light_colorblind-af6c685139ba.css\" /><link data-color-theme=\"light_high_contrast\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/light_high_contrast-578cdbc8a5a9.css\" /><link data-color-theme=\"light_tritanopia\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/light_tritanopia-5cb699a7e247.css\" /><link data-color-theme=\"dark_tritanopia\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/dark_tritanopia-9b32204967c6.css\" />    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/primer-primitives-2ef2a46b27ee.css\" />    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/primer-711f412bb361.css\" />    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/global-4803cd254267.css\" />    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/github-f4d857cbc96a.css\" />  <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/repository-6247ca238fd4.css\" /><link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/code-6d7b4ef0ea51.css\" />    <script type=\"application/json\" id=\"client-env\">{\"locale\":\"en\",\"featureFlags\":[\"code_vulnerability_scanning\",\"copilot_conversational_ux_history_refs\",\"copilot_chat_attach_knowledge\",\"copilot_chat_knowledge_base_copy\",\"copilot_smell_icebreaker_ux\",\"copilot_implicit_context\",\"docset_management_ui\",\"copilot_chat_settings\",\"failbot_handle_non_errors\",\"geojson_azure_maps\",\"image_metric_tracking\",\"marketing_forms_api_integration_contact_request\",\"marketing_pages_search_explore_provider\",\"turbo_experiment_risky\",\"sample_network_conn_type\",\"no_character_key_shortcuts_in_inputs\",\"custom_inp\",\"remove_child_patch\"]}</script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/wp-runtime-47578fb192fd.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_dompurify_dist_purify_js-6890e890956f.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_stacktrace-parser_dist_stack-trace-parser_esm_js-node_modules_github_bro-a4c183-79f9611c275b.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_hydro-analytics-client_dist_analytics-client_js-node_modules_gith-6a10dd-e66ebda625fb.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_failbot_failbot_ts-479802999bcc.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/environment-fe7570f3bc38.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_selector-observer_dist_index_esm_js-9f960d9b217c.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_behaviors_dist_esm_focus-zone_js-086f7a27bac0.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_relative-time-element_dist_index_js-c76945c5961a.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_delegated-events_dist_index_js-node_modules_github_details-dialog-elemen-29dc30-a2a71f11a507.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_auto-complete-element_dist_index_js-12366198e7a5.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_text-expander-element_dist_index_js-8a621df59e80.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_filter-input-element_dist_index_js-node_modules_github_remote-inp-b7d8f4-654130b7cde5.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_file-attachment-element_dist_index_js-node_modules_primer_view-co-5dccdf-e5e2b9fa3c0c.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/github-elements-e4eda4896b4e.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/element-registry-b99c9d8fad1d.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_catalyst_lib_index_js-node_modules_github_hydro-analytics-client_-978abc0-add939c751ce.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_lit-html_lit-html_js-5b376145beff.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_mini-throttle_dist_index_js-node_modules_github_alive-client_dist-bf5aa2-1b562c29ab8e.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_morphdom_dist_morphdom-esm_js-5bff297a06de.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_turbo_dist_turbo_es2017-esm_js-c91f4ad18b62.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_color-convert_index_js-72c9fbde5ad4.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_scroll-anchoring_dist_scro-231ccf-aa129238d13b.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_behaviors_dist_esm_dimensions_js-node_modules_github_jtml_lib_index_js-95b84ee6bc34.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_session-resume_dist_index_js-node_modules_primer_behaviors_dist_e-da6ec6-3f39339c9d98.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_paste-markdown_dist_index_esm_js-node_modules_github_quote-select-67e0dc-1aa35af077a4.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_updatable-content_ts-ee3fc84d7fb0.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_behaviors_task-list_ts-app_assets_modules_github_onfocus_ts-app_ass-421cec-9de4213015af.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_sticky-scroll-into-view_ts-94209c43e6af.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_behaviors_ajax-error_ts-app_assets_modules_github_behaviors_include-467754-f9bd433e9591.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_behaviors_commenting_edit_ts-app_assets_modules_github_behaviors_ht-83c235-9285faa0e011.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_blob-anchor_ts-app_assets_modules_github_filter-sort_ts-app_assets_-c96432-da3733f430b8.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/behaviors-1fb9e5061509.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_delegated-events_dist_index_js-node_modules_github_catalyst_lib_index_js-d0256ebff5cd.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/notifications-global-352d84c6cc82.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_virtualized-list_es_index_js-node_modules_github_template-parts_lib_index_js-878844713bc9.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_delegated-events_dist_inde-c537341-c7f6a41a084c.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_ref-selector_ts-b593b93f23f5.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/codespaces-1a8626dd714a.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_filter-input-element_dist_index_js-node_modules_github_mini-throt-08ab15-3e0517baca99.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_file-attachment-element_dist_index_js-node_modules_github_mini-th-55cf52-e14cb4b719b4.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/repositories-69068e0899f9.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/code-menu-614feb194539.js\"></script>    <title>GitHub - lucidrains/x-transformers: A simple but complete full-attention transformer with a set of promising experimental features from various papers</title>  <meta name=\"route-pattern\" content=\"/:user_id/:repository\" data-turbo-transient>  <meta name=\"route-controller\" content=\"files\" data-turbo-transient>  <meta name=\"route-action\" content=\"disambiguate\" data-turbo-transient>      <meta name=\"current-catalog-service-hash\" content=\"82c569b93da5c18ed649ebd4c2c79437db4611a6a1373e805a3cb001c64130b7\">  <meta name=\"request-id\" content=\"CBCC:93F6D:C962BD:128ADB6:65E793E6\" data-pjax-transient=\"true\"/><meta name=\"html-safe-nonce\" content=\"f752605d2dadf0930c6258f931a4ff0b44fc5292504c67f9473e27a97380fea8\" data-pjax-transient=\"true\"/><meta name=\"visitor-payload\" content=\"eyJyZWZlcnJlciI6IiIsInJlcXVlc3RfaWQiOiJDQkNDOjkzRjZEOkM5NjJCRDoxMjhBREI2OjY1RTc5M0U2IiwidmlzaXRvcl9pZCI6IjYzMTU1MzY1NzMzNzMzMjIyMTQiLCJyZWdpb25fZWRnZSI6ImlhZCIsInJlZ2lvbl9yZW5kZXIiOiJpYWQifQ==\" data-pjax-transient=\"true\"/><meta name=\"visitor-hmac\" content=\"484c47995825a02015d3553e7ede86bdc1fa919d4ecd97066d794ad28e4c6b48\" data-pjax-transient=\"true\"/>    <meta name=\"hovercard-subject-tag\" content=\"repository:306980630\" data-turbo-transient>  <meta name=\"github-keyboard-shortcuts\" content=\"repository,copilot\" data-turbo-transient=\"true\" />    <meta name=\"selected-link\" value=\"repo_source\" data-turbo-transient>  <link rel=\"assets\" href=\"https://github.githubassets.com/\">    <meta name=\"google-site-verification\" content=\"c1kuD-K2HIVF635lypcsWPoD4kilo5-jA_wBFyT4uMY\">  <meta name=\"google-site-verification\" content=\"KT5gs8h0wvaagLKAVWq8bbeNwnZZK1r1XQysX3xurLU\">  <meta name=\"google-site-verification\" content=\"ZzhVyEFwb7w3e0-uOTltm8Jsck2F5StVihD0exw2fsA\">  <meta name=\"google-site-verification\" content=\"GXs5KoUUkNCoaAZn7wPN-t01Pywp9M3sEjnt_3_ZWPc\">  <meta name=\"google-site-verification\" content=\"Apib7-x98H0j5cPqHWwSMm6dNU4GmODRoqxLiDzdx9I\"><meta name=\"octolytics-url\" content=\"https://collector.github.com/github/collect\" />  <meta name=\"analytics-location\" content=\"/&lt;user-name&gt;/&lt;repo-name&gt;\" data-turbo-transient=\"true\" />        <meta name=\"user-login\" content=\"\">      <meta name=\"viewport\" content=\"width=device-width\">          <meta name=\"description\" content=\"A simple but complete full-attention transformer with a set of promising experimental features from various papers - lucidrains/x-transformers\">      <link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"/opensearch.xml\" title=\"GitHub\">    <link rel=\"fluid-icon\" href=\"https://github.com/fluidicon.png\" title=\"GitHub\">    <meta property=\"fb:app_id\" content=\"1401488693436528\">    <meta name=\"apple-itunes-app\" content=\"app-id=1477376905, app-argument=https://github.com/lucidrains/x-transformers\" />      <meta name=\"twitter:image:src\" content=\"https://opengraph.githubassets.com/8c8299fd373c90ee8239e0d142a7d85538db8fdc26b7cb745813dd55b55bf39b/lucidrains/x-transformers\" /><meta name=\"twitter:site\" content=\"@github\" /><meta name=\"twitter:card\" content=\"summary_large_image\" /><meta name=\"twitter:title\" content=\"GitHub - lucidrains/x-transformers: A simple but complete full-attention transformer with a set of promising experimental features from various papers\" /><meta name=\"twitter:description\" content=\"A simple but complete full-attention transformer with a set of promising experimental features from various papers - lucidrains/x-transformers\" />      <meta property=\"og:image\" content=\"https://opengraph.githubassets.com/8c8299fd373c90ee8239e0d142a7d85538db8fdc26b7cb745813dd55b55bf39b/lucidrains/x-transformers\" /><meta property=\"og:image:alt\" content=\"A simple but complete full-attention transformer with a set of promising experimental features from various papers - lucidrains/x-transformers\" /><meta property=\"og:image:width\" content=\"1200\" /><meta property=\"og:image:height\" content=\"600\" /><meta property=\"og:site_name\" content=\"GitHub\" /><meta property=\"og:type\" content=\"object\" /><meta property=\"og:title\" content=\"GitHub - lucidrains/x-transformers: A simple but complete full-attention transformer with a set of promising experimental features from various papers\" /><meta property=\"og:url\" content=\"https://github.com/lucidrains/x-transformers\" /><meta property=\"og:description\" content=\"A simple but complete full-attention transformer with a set of promising experimental features from various papers - lucidrains/x-transformers\" />              <meta name=\"hostname\" content=\"github.com\">        <meta name=\"expected-hostname\" content=\"github.com\">  <meta http-equiv=\"x-pjax-version\" content=\"b9fa4cafade57d606c6dcfafff1d08bd597980af7b9837ed473fdf0cdea8a3bc\" data-turbo-track=\"reload\">  <meta http-equiv=\"x-pjax-csp-version\" content=\"5dcfbec3488c5fd5a334e287ce6a17058b7d4beb91db2d4d184e4d55bbf1d7d7\" data-turbo-track=\"reload\">  <meta http-equiv=\"x-pjax-css-version\" content=\"d33c7c2fcff40783f3002896023f41e2c17ec62b12ddbe7434e2001d743fb853\" data-turbo-track=\"reload\">  <meta http-equiv=\"x-pjax-js-version\" content=\"4ba4a7cc07194c8d5f24291dea4fbc790ffd83ba40beacaf8d0117187b571b4d\" data-turbo-track=\"reload\">  <meta name=\"turbo-cache-control\" content=\"no-preview\" data-turbo-transient=\"\">      <meta data-hydrostats=\"publish\">  <meta name=\"go-import\" content=\"github.com/lucidrains/x-transformers git https://github.com/lucidrains/x-transformers.git\">  <meta name=\"octolytics-dimension-user_id\" content=\"108653\" /><meta name=\"octolytics-dimension-user_login\" content=\"lucidrains\" /><meta name=\"octolytics-dimension-repository_id\" content=\"306980630\" /><meta name=\"octolytics-dimension-repository_nwo\" content=\"lucidrains/x-transformers\" /><meta name=\"octolytics-dimension-repository_public\" content=\"true\" /><meta name=\"octolytics-dimension-repository_is_fork\" content=\"false\" /><meta name=\"octolytics-dimension-repository_network_root_id\" content=\"306980630\" /><meta name=\"octolytics-dimension-repository_network_root_nwo\" content=\"lucidrains/x-transformers\" />    <link rel=\"canonical\" href=\"https://github.com/lucidrains/x-transformers\" data-turbo-transient>  <meta name=\"turbo-body-classes\" content=\"logged-out env-production page-responsive\">  <meta name=\"browser-stats-url\" content=\"https://api.github.com/_private/browser/stats\">  <meta name=\"browser-errors-url\" content=\"https://api.github.com/_private/browser/errors\">  <link rel=\"mask-icon\" href=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" color=\"#000000\">  <link rel=\"alternate icon\" class=\"js-site-favicon\" type=\"image/png\" href=\"https://github.githubassets.com/favicons/favicon.png\">  <link rel=\"icon\" class=\"js-site-favicon\" type=\"image/svg+xml\" href=\"https://github.githubassets.com/favicons/favicon.svg\"><meta name=\"theme-color\" content=\"#1e2327\"><meta name=\"color-scheme\" content=\"light dark\" />  <link rel=\"manifest\" href=\"/manifest.json\" crossOrigin=\"use-credentials\">  </head>  <body class=\"logged-out env-production page-responsive\" style=\"word-wrap: break-word;\">    <div data-turbo-body class=\"logged-out env-production page-responsive\" style=\"word-wrap: break-word;\">          <div class=\"position-relative js-header-wrapper \">      <a href=\"#start-of-content\" class=\"px-2 py-4 color-bg-accent-emphasis color-fg-on-emphasis show-on-focus js-skip-to-content\">Skip to content</a>      <span data-view-component=\"true\" class=\"progress-pjax-loader Progress position-fixed width-full\">    <span style=\"width: 0%;\" data-view-component=\"true\" class=\"Progress-item progress-pjax-loader-bar left-0 top-0 color-bg-accent-emphasis\"></span></span>              <script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Button_IconButton_js-node_modules_primer_react_lib--23bcad-a89698f38643.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/keyboard-shortcuts-dialog-a23eda2bcf8d.js\"></script><react-partial  partial-name=\"keyboard-shortcuts-dialog\"  data-ssr=\"false\">    <script type=\"application/json\" data-target=\"react-partial.embeddedData\">{\"props\":{}}</script>  <div data-target=\"react-partial.reactRoot\"></div></react-partial>                          <script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_delegated-events_dist_inde-94fd67-99519581d0f8.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/sessions-585a7232e50a.js\"></script><header class=\"Header-old header-logged-out js-details-container Details position-relative f4 py-3\" role=\"banner\" data-color-mode=light data-light-theme=light data-dark-theme=dark>  <button type=\"button\" class=\"Header-backdrop d-lg-none border-0 position-fixed top-0 left-0 width-full height-full js-details-target\" aria-label=\"Toggle navigation\">    <span class=\"d-none\">Toggle navigation</span>  </button>  <div class=\" d-flex flex-column flex-lg-row flex-items-center p-responsive height-full position-relative z-1\">    <div class=\"d-flex flex-justify-between flex-items-center width-full width-lg-auto\">      <a class=\"mr-lg-3 color-fg-inherit flex-order-2\" href=\"https://github.com/\" aria-label=\"Homepage\" data-ga-click=\"(Logged out) Header, go to homepage, icon:logo-wordmark\">        <svg height=\"32\" aria-hidden=\"true\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"32\" data-view-component=\"true\" class=\"octicon octicon-mark-github\">    <path d=\"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z\"></path></svg>      </a>      <div class=\"flex-1\">        <a href=\"/login?return_to=https%3A%2F%2Fgithub.com%2Flucidrains%2Fx-transformers\"          class=\"d-inline-block d-lg-none flex-order-1 f5 no-underline border color-border-default rounded-2 px-2 py-1 color-fg-inherit\"          data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/x-transformers&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"746f69153e1c58b969068598aa9622fa70168054c43eb89952edf603d0e8022e\"          data-ga-click=\"(Logged out) Header, clicked Sign in, text:sign-in\">          Sign in        </a>      </div>      <div class=\"flex-1 flex-order-2 text-right\">        <button aria-label=\"Toggle navigation\" aria-expanded=\"false\" type=\"button\" data-view-component=\"true\" class=\"js-details-target Button--link Button--medium Button d-lg-none color-fg-inherit p-1\">  <span class=\"Button-content\">    <span class=\"Button-label\"><div class=\"HeaderMenu-toggle-bar rounded my-1\"></div>            <div class=\"HeaderMenu-toggle-bar rounded my-1\"></div>            <div class=\"HeaderMenu-toggle-bar rounded my-1\"></div></span>  </span></button>      </div>    </div>    <div class=\"HeaderMenu--logged-out p-responsive height-fit position-lg-relative d-lg-flex flex-column flex-auto pt-7 pb-4 top-0\">      <div class=\"header-menu-wrapper d-flex flex-column flex-self-end flex-lg-row flex-justify-between flex-auto p-3 p-lg-0 rounded rounded-lg-0 mt-3 mt-lg-0\">          <nav class=\"mt-0 px-3 px-lg-0 mb-3 mb-lg-0\" aria-label=\"Global\">            <ul class=\"d-lg-flex list-style-none\">                <li class=\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\">      <button type=\"button\" class=\"HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target\" aria-expanded=\"false\">        Product        <svg opacity=\"0.5\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-chevron-down HeaderMenu-icon ml-1\">    <path d=\"M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z\"></path></svg>      </button>      <div class=\"HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 d-lg-flex dropdown-menu-wide\">          <div class=\"px-lg-4 border-lg-right mb-4 mb-lg-0 pr-lg-7\">            <ul class=\"list-style-none f5\" >                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}\" href=\"/features/actions\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-workflow color-fg-subtle mr-3\">    <path d=\"M1 3a2 2 0 0 1 2-2h6.5a2 2 0 0 1 2 2v6.5a2 2 0 0 1-2 2H7v4.063C7 16.355 7.644 17 8.438 17H12.5v-2.5a2 2 0 0 1 2-2H21a2 2 0 0 1 2 2V21a2 2 0 0 1-2 2h-6.5a2 2 0 0 1-2-2v-2.5H8.437A2.939 2.939 0 0 1 5.5 15.562V11.5H3a2 2 0 0 1-2-2Zm2-.5a.5.5 0 0 0-.5.5v6.5a.5.5 0 0 0 .5.5h6.5a.5.5 0 0 0 .5-.5V3a.5.5 0 0 0-.5-.5ZM14.5 14a.5.5 0 0 0-.5.5V21a.5.5 0 0 0 .5.5H21a.5.5 0 0 0 .5-.5v-6.5a.5.5 0 0 0-.5-.5Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Actions</div>        Automate any workflow      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}\" href=\"/features/packages\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-package color-fg-subtle mr-3\">    <path d=\"M12.876.64V.639l8.25 4.763c.541.313.875.89.875 1.515v9.525a1.75 1.75 0 0 1-.875 1.516l-8.25 4.762a1.748 1.748 0 0 1-1.75 0l-8.25-4.763a1.75 1.75 0 0 1-.875-1.515V6.917c0-.625.334-1.202.875-1.515L11.126.64a1.748 1.748 0 0 1 1.75 0Zm-1 1.298L4.251 6.34l7.75 4.474 7.75-4.474-7.625-4.402a.248.248 0 0 0-.25 0Zm.875 19.123 7.625-4.402a.25.25 0 0 0 .125-.216V7.639l-7.75 4.474ZM3.501 7.64v8.803c0 .09.048.172.125.216l7.625 4.402v-8.947Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Packages</div>        Host and manage packages      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}\" href=\"/features/security\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-shield-check color-fg-subtle mr-3\">    <path d=\"M16.53 9.78a.75.75 0 0 0-1.06-1.06L11 13.19l-1.97-1.97a.75.75 0 0 0-1.06 1.06l2.5 2.5a.75.75 0 0 0 1.06 0l5-5Z\"></path><path d=\"m12.54.637 8.25 2.675A1.75 1.75 0 0 1 22 4.976V10c0 6.19-3.771 10.704-9.401 12.83a1.704 1.704 0 0 1-1.198 0C5.77 20.705 2 16.19 2 10V4.976c0-.758.489-1.43 1.21-1.664L11.46.637a1.748 1.748 0 0 1 1.08 0Zm-.617 1.426-8.25 2.676a.249.249 0 0 0-.173.237V10c0 5.46 3.28 9.483 8.43 11.426a.199.199 0 0 0 .14 0C17.22 19.483 20.5 15.461 20.5 10V4.976a.25.25 0 0 0-.173-.237l-8.25-2.676a.253.253 0 0 0-.154 0Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Security</div>        Find and fix vulnerabilities      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}\" href=\"/features/codespaces\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-codespaces color-fg-subtle mr-3\">    <path d=\"M3.5 3.75C3.5 2.784 4.284 2 5.25 2h13.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 18.75 13H5.25a1.75 1.75 0 0 1-1.75-1.75Zm-2 12c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v4a1.75 1.75 0 0 1-1.75 1.75H3.25a1.75 1.75 0 0 1-1.75-1.75ZM5.25 3.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h13.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Zm-2 12a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h17.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25Z\"></path><path d=\"M10 17.75a.75.75 0 0 1 .75-.75h6.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Codespaces</div>        Instant dev environments      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}\" href=\"/features/copilot\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-copilot color-fg-subtle mr-3\">    <path d=\"M23.922 16.992c-.861 1.495-5.859 5.023-11.922 5.023-6.063 0-11.061-3.528-11.922-5.023A.641.641 0 0 1 0 16.736v-2.869a.841.841 0 0 1 .053-.22c.372-.935 1.347-2.292 2.605-2.656.167-.429.414-1.055.644-1.517a10.195 10.195 0 0 1-.052-1.086c0-1.331.282-2.499 1.132-3.368.397-.406.89-.717 1.474-.952 1.399-1.136 3.392-2.093 6.122-2.093 2.731 0 4.767.957 6.166 2.093.584.235 1.077.546 1.474.952.85.869 1.132 2.037 1.132 3.368 0 .368-.014.733-.052 1.086.23.462.477 1.088.644 1.517 1.258.364 2.233 1.721 2.605 2.656a.832.832 0 0 1 .053.22v2.869a.641.641 0 0 1-.078.256ZM12.172 11h-.344a4.323 4.323 0 0 1-.355.508C10.703 12.455 9.555 13 7.965 13c-1.725 0-2.989-.359-3.782-1.259a2.005 2.005 0 0 1-.085-.104L4 11.741v6.585c1.435.779 4.514 2.179 8 2.179 3.486 0 6.565-1.4 8-2.179v-6.585l-.098-.104s-.033.045-.085.104c-.793.9-2.057 1.259-3.782 1.259-1.59 0-2.738-.545-3.508-1.492a4.323 4.323 0 0 1-.355-.508h-.016.016Zm.641-2.935c.136 1.057.403 1.913.878 2.497.442.544 1.134.938 2.344.938 1.573 0 2.292-.337 2.657-.751.384-.435.558-1.15.558-2.361 0-1.14-.243-1.847-.705-2.319-.477-.488-1.319-.862-2.824-1.025-1.487-.161-2.192.138-2.533.529-.269.307-.437.808-.438 1.578v.021c0 .265.021.562.063.893Zm-1.626 0c.042-.331.063-.628.063-.894v-.02c-.001-.77-.169-1.271-.438-1.578-.341-.391-1.046-.69-2.533-.529-1.505.163-2.347.537-2.824 1.025-.462.472-.705 1.179-.705 2.319 0 1.211.175 1.926.558 2.361.365.414 1.084.751 2.657.751 1.21 0 1.902-.394 2.344-.938.475-.584.742-1.44.878-2.497Z\"></path><path d=\"M14.5 14.25a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Zm-5 0a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Copilot</div>        Write better code with AI      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}\" href=\"/features/code-review\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-code-review color-fg-subtle mr-3\">    <path d=\"M10.3 6.74a.75.75 0 0 1-.04 1.06l-2.908 2.7 2.908 2.7a.75.75 0 1 1-1.02 1.1l-3.5-3.25a.75.75 0 0 1 0-1.1l3.5-3.25a.75.75 0 0 1 1.06.04Zm3.44 1.06a.75.75 0 1 1 1.02-1.1l3.5 3.25a.75.75 0 0 1 0 1.1l-3.5 3.25a.75.75 0 1 1-1.02-1.1l2.908-2.7-2.908-2.7Z\"></path><path d=\"M1.5 4.25c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v12.5a1.75 1.75 0 0 1-1.75 1.75h-9.69l-3.573 3.573A1.458 1.458 0 0 1 5 21.043V18.5H3.25a1.75 1.75 0 0 1-1.75-1.75ZM3.25 4a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h2.5a.75.75 0 0 1 .75.75v3.19l3.72-3.72a.749.749 0 0 1 .53-.22h10a.25.25 0 0 0 .25-.25V4.25a.25.25 0 0 0-.25-.25Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Code review</div>        Manage code changes      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}\" href=\"/features/issues\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-issue-opened color-fg-subtle mr-3\">    <path d=\"M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1ZM2.5 12a9.5 9.5 0 0 0 9.5 9.5 9.5 9.5 0 0 0 9.5-9.5A9.5 9.5 0 0 0 12 2.5 9.5 9.5 0 0 0 2.5 12Zm9.5 2a2 2 0 1 1-.001-3.999A2 2 0 0 1 12 14Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Issues</div>        Plan and track work      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}\" href=\"/features/discussions\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-comment-discussion color-fg-subtle mr-3\">    <path d=\"M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 14.25 14H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 15.543V14H1.75A1.75 1.75 0 0 1 0 12.25v-9.5C0 1.784.784 1 1.75 1ZM1.5 2.75v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Z\"></path><path d=\"M22.5 8.75a.25.25 0 0 0-.25-.25h-3.5a.75.75 0 0 1 0-1.5h3.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 22.25 20H21v1.543a1.457 1.457 0 0 1-2.487 1.03L15.939 20H10.75A1.75 1.75 0 0 1 9 18.25v-1.465a.75.75 0 0 1 1.5 0v1.465c0 .138.112.25.25.25h5.5a.75.75 0 0 1 .53.22l2.72 2.72v-2.19a.75.75 0 0 1 .75-.75h2a.25.25 0 0 0 .25-.25v-9.5Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Discussions</div>        Collaborate outside of code      </div>    </a></li>            </ul>          </div>          <div class=\"px-lg-4\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"product-explore-heading\">Explore</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"product-explore-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to All features&quot;,&quot;label&quot;:&quot;ref_cta:All features;&quot;}\" href=\"/features\">      All features    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Documentation&quot;,&quot;label&quot;:&quot;ref_cta:Documentation;&quot;}\" href=\"https://docs.github.com\">      Documentation    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to GitHub Skills&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Skills;&quot;}\" href=\"https://skills.github.com/\">      GitHub Skills    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Blog&quot;,&quot;label&quot;:&quot;ref_cta:Blog;&quot;}\" href=\"https://github.blog\">      Blog    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>            </ul>          </div>      </div></li>                <li class=\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\">      <button type=\"button\" class=\"HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target\" aria-expanded=\"false\">        Solutions        <svg opacity=\"0.5\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-chevron-down HeaderMenu-icon ml-1\">    <path d=\"M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z\"></path></svg>      </button>      <div class=\"HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 px-lg-4\">          <div class=\"border-bottom pb-3 mb-3\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"solutions-for-heading\">For</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"solutions-for-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Enterprise&quot;,&quot;label&quot;:&quot;ref_cta:Enterprise;&quot;}\" href=\"/enterprise\">      Enterprise    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Teams&quot;,&quot;label&quot;:&quot;ref_cta:Teams;&quot;}\" href=\"/team\">      Teams    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Startups&quot;,&quot;label&quot;:&quot;ref_cta:Startups;&quot;}\" href=\"/enterprise/startups\">      Startups    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Education&quot;,&quot;label&quot;:&quot;ref_cta:Education;&quot;}\" href=\"https://education.github.com\">      Education    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>            </ul>          </div>          <div class=\"border-bottom pb-3 mb-3\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"solutions-by-solution-heading\">By Solution</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"solutions-by-solution-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to CI/CD &amp;amp; Automation&quot;,&quot;label&quot;:&quot;ref_cta:CI/CD &amp;amp; Automation;&quot;}\" href=\"/solutions/ci-cd/\">      CI/CD &amp; Automation    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to DevOps&quot;,&quot;label&quot;:&quot;ref_cta:DevOps;&quot;}\" href=\"/solutions/devops/\">      DevOps    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to DevSecOps&quot;,&quot;label&quot;:&quot;ref_cta:DevSecOps;&quot;}\" href=\"https://resources.github.com/devops/fundamentals/devsecops/\">      DevSecOps    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>            </ul>          </div>          <div class=\"\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"solutions-resources-heading\">Resources</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"solutions-resources-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Learning Pathways&quot;,&quot;label&quot;:&quot;ref_cta:Learning Pathways;&quot;}\" href=\"https://resources.github.com/learn/pathways/\">      Learning Pathways    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to White papers, Ebooks, Webinars&quot;,&quot;label&quot;:&quot;ref_cta:White papers, Ebooks, Webinars;&quot;}\" href=\"https://resources.github.com/\">      White papers, Ebooks, Webinars    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Customer Stories&quot;,&quot;label&quot;:&quot;ref_cta:Customer Stories;&quot;}\" href=\"/customer-stories\">      Customer Stories    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Partners&quot;,&quot;label&quot;:&quot;ref_cta:Partners;&quot;}\" href=\"https://partner.github.com/\">      Partners    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>            </ul>          </div>      </div></li>                <li class=\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\">      <button type=\"button\" class=\"HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target\" aria-expanded=\"false\">        Open Source        <svg opacity=\"0.5\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-chevron-down HeaderMenu-icon ml-1\">    <path d=\"M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z\"></path></svg>      </button>      <div class=\"HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 px-lg-4\">          <div class=\"border-bottom pb-3 mb-3\">            <ul class=\"list-style-none f5\" >                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}\" href=\"/sponsors\">            <div>        <div class=\"color-fg-default h4\">GitHub Sponsors</div>        Fund open source developers      </div>    </a></li>            </ul>          </div>          <div class=\"border-bottom pb-3 mb-3\">            <ul class=\"list-style-none f5\" >                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}\" href=\"/readme\">            <div>        <div class=\"color-fg-default h4\">The ReadME Project</div>        GitHub community articles      </div>    </a></li>            </ul>          </div>          <div class=\"\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"open-source-repositories-heading\">Repositories</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"open-source-repositories-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Topics&quot;,&quot;label&quot;:&quot;ref_cta:Topics;&quot;}\" href=\"/topics\">      Topics    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Trending&quot;,&quot;label&quot;:&quot;ref_cta:Trending;&quot;}\" href=\"/trending\">      Trending    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Collections&quot;,&quot;label&quot;:&quot;ref_cta:Collections;&quot;}\" href=\"/collections\">      Collections    </a></li>            </ul>          </div>      </div></li>                <li class=\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\">    <a class=\"HeaderMenu-link no-underline px-0 px-lg-2 py-3 py-lg-2 d-block d-lg-inline-block\" data-analytics-event=\"{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}\" href=\"/pricing\">Pricing</a></li>            </ul>          </nav>        <div class=\"d-lg-flex flex-items-center mb-3 mb-lg-0 text-center text-lg-left ml-3\" style=\"\">                <qbsearch-input class=\"search-input\" data-scope=\"repo:lucidrains/x-transformers\" data-custom-scopes-path=\"/search/custom_scopes\" data-delete-custom-scopes-csrf=\"4AI1wxGAasTfXAqLEg_LO5yuDPJ3dbWmWJiGnfwxT_gKtPm3HZdOKg5Uqfn30TCvYXPGAEdnVi4BtqsuiQ8RRg\" data-max-custom-scopes=\"10\" data-header-redesign-enabled=\"false\" data-initial-value=\"\" data-blackbird-suggestions-path=\"/search/suggestions\" data-jump-to-suggestions-path=\"/_graphql/GetSuggestedNavigationDestinations\" data-current-repository=\"lucidrains/x-transformers\" data-current-org=\"\" data-current-owner=\"lucidrains\" data-logged-in=\"false\" data-copilot-chat-enabled=\"false\" data-blackbird-indexed-repo-csrf=\"<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=ygwITdX6CAD7zGdTFwoxSleZ1TXCXsoUXzbs8ENB2rsOSq9irVtJw0OkBMZZV%2FfQZuXHgmEezc8zbJ%2F4WzDl9vpX8Acw4%2BJaMFA15U6jH5QC5DjJA2EREF%2FEroxXGGkcP5m4fzQFVfZYVdqmMbRuUdrRSG9rG%2FZx2zfkcu7V00ERaFdKbvxNXlK6honju%2FIzAR0N7N%2BkD6%2Bv87yMez0lFP3rqDyeiErj5xQ7b7E%2FG3Yc56d%2FP4wbrkChx15hIVl3ZokoBSucod%2FCPEcXwqQWJ0fvrdNamAl06%2BBoVGcJ%2B0BfOFybktu2cTgUvjAzvGjKJVC%2FPkwlF6hJnDWIjRx27WzcniOLUQN6t9FyY9q3hTjecnGeuAy3nloRcUnMsiAo%2FGjoLDqU7n5yt3eAqb88aYj%2FsK44ITG9ZRkfvA60xfq5O00i1f5XuoUufMtV2IHGgj2ubPdZB7AEoFoNgCk13rrp5BDt8%2Fn4ehLcm7%2BWIG8YGiG7gLOJJJmBw4HMnu0%2FOv2DJqe4nOSnfzfJya1nGEHm3kz%2BbkzvM8Q%3D--E4NGJ41vt1IT79jt--WoczK5HVgz%2Becracf9xxGg%3D%3D&quot; />\">  <div    class=\"search-input-container search-with-dialog position-relative d-flex flex-row flex-items-center mr-4 rounded\"    data-action=\"click:qbsearch-input#searchInputContainerClicked\"  >      <button        type=\"button\"        class=\"header-search-button placeholder  input-button form-control d-flex flex-1 flex-self-stretch flex-items-center no-wrap width-full py-0 pl-2 pr-0 text-left border-0 box-shadow-none\"        data-target=\"qbsearch-input.inputButton\"        placeholder=\"Search or jump to...\"        data-hotkey=s,/        autocapitalize=\"off\"        data-action=\"click:qbsearch-input#handleExpand\"      >        <div class=\"mr-2 color-fg-muted\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-search\">    <path d=\"M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z\"></path></svg>        </div>        <span class=\"flex-1\" data-target=\"qbsearch-input.inputButtonText\">Search or jump to...</span>          <div class=\"d-flex\" data-target=\"qbsearch-input.hotkeyIndicator\">            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"22\" height=\"20\" aria-hidden=\"true\" class=\"mr-1\"><path fill=\"none\" stroke=\"#979A9C\" opacity=\".4\" d=\"M3.5.5h12c1.7 0 3 1.3 3 3v13c0 1.7-1.3 3-3 3h-12c-1.7 0-3-1.3-3-3v-13c0-1.7 1.3-3 3-3z\"></path><path fill=\"#979A9C\" d=\"M11.8 6L8 15.1h-.9L10.8 6h1z\"></path></svg>          </div>      </button>    <input type=\"hidden\" name=\"type\" class=\"js-site-search-type-field\">    <div class=\"Overlay--hidden \" data-modal-dialog-overlay>  <modal-dialog data-action=\"close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose\" data-target=\"qbsearch-input.searchSuggestionsDialog\" role=\"dialog\" id=\"search-suggestions-dialog\" aria-modal=\"true\" aria-labelledby=\"search-suggestions-dialog-header\" data-view-component=\"true\" class=\"Overlay Overlay--width-large Overlay--height-auto\">      <h1 id=\"search-suggestions-dialog-header\" class=\"sr-only\">Search code, repositories, users, issues, pull requests...</h1>    <div class=\"Overlay-body Overlay-body--paddingNone\">                <div data-view-component=\"true\">        <div class=\"search-suggestions position-fixed width-full color-shadow-large border color-fg-default color-bg-default overflow-hidden d-flex flex-column query-builder-container\"          style=\"border-radius: 12px;\"          data-target=\"qbsearch-input.queryBuilderContainer\"          hidden        >          <!-- '\"` --><!-- </textarea></xmp> --></option></form><form id=\"query-builder-test-form\" action=\"\" accept-charset=\"UTF-8\" method=\"get\">  <query-builder data-target=\"qbsearch-input.queryBuilder\" id=\"query-builder-query-builder-test\" data-filter-key=\":\" data-view-component=\"true\" class=\"QueryBuilder search-query-builder\">    <div class=\"FormControl FormControl--fullWidth\">      <label id=\"query-builder-test-label\" for=\"query-builder-test\" class=\"FormControl-label sr-only\">        Search      </label>      <div        class=\"QueryBuilder-StyledInput width-fit \"        data-target=\"query-builder.styledInput\"      >          <span id=\"query-builder-test-leadingvisual-wrap\" class=\"FormControl-input-leadingVisualWrap QueryBuilder-leadingVisualWrap\">            <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-search FormControl-input-leadingVisual\">    <path d=\"M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z\"></path></svg>          </span>        <div data-target=\"query-builder.styledInputContainer\" class=\"QueryBuilder-StyledInputContainer\">          <div            aria-hidden=\"true\"            class=\"QueryBuilder-StyledInputContent\"            data-target=\"query-builder.styledInputContent\"          ></div>          <div class=\"QueryBuilder-InputWrapper\">            <div aria-hidden=\"true\" class=\"QueryBuilder-Sizer\" data-target=\"query-builder.sizer\"></div>            <input id=\"query-builder-test\" name=\"query-builder-test\" value=\"\" autocomplete=\"off\" type=\"text\" role=\"combobox\" spellcheck=\"false\" aria-expanded=\"false\" aria-describedby=\"validation-1b68690a-eff9-48b5-a978-cbee8bd476e3\" data-target=\"query-builder.input\" data-action=\"          input:query-builder#inputChange          blur:query-builder#inputBlur          keydown:query-builder#inputKeydown          focus:query-builder#inputFocus        \" data-view-component=\"true\" class=\"FormControl-input QueryBuilder-Input FormControl-medium\" />          </div>        </div>          <span class=\"sr-only\" id=\"query-builder-test-clear\">Clear</span>          <button role=\"button\" id=\"query-builder-test-clear-button\" aria-labelledby=\"query-builder-test-clear query-builder-test-label\" data-target=\"query-builder.clearButton\" data-action=\"                click:query-builder#clear                focus:query-builder#clearButtonFocus                blur:query-builder#clearButtonBlur              \" variant=\"small\" hidden=\"hidden\" type=\"button\" data-view-component=\"true\" class=\"Button Button--iconOnly Button--invisible Button--medium mr-1 px-2 py-0 d-flex flex-items-center rounded-1 color-fg-muted\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x-circle-fill Button-visual\">    <path d=\"M2.343 13.657A8 8 0 1 1 13.658 2.343 8 8 0 0 1 2.343 13.657ZM6.03 4.97a.751.751 0 0 0-1.042.018.751.751 0 0 0-.018 1.042L6.94 8 4.97 9.97a.749.749 0 0 0 .326 1.275.749.749 0 0 0 .734-.215L8 9.06l1.97 1.97a.749.749 0 0 0 1.275-.326.749.749 0 0 0-.215-.734L9.06 8l1.97-1.97a.749.749 0 0 0-.326-1.275.749.749 0 0 0-.734.215L8 6.94Z\"></path></svg></button>      </div>      <template id=\"search-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-search\">    <path d=\"M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z\"></path></svg></template><template id=\"code-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code\">    <path d=\"m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z\"></path></svg></template><template id=\"file-code-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-file-code\">    <path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0 1 14.25 15h-9a.75.75 0 0 1 0-1.5h9a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 10 4.25V1.5H5.75a.25.25 0 0 0-.25.25v2.5a.75.75 0 0 1-1.5 0Zm1.72 4.97a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734l1.47-1.47-1.47-1.47a.75.75 0 0 1 0-1.06ZM3.28 7.78 1.81 9.25l1.47 1.47a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018l-2-2a.75.75 0 0 1 0-1.06l2-2a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Zm8.22-6.218V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg></template><template id=\"history-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-history\">    <path d=\"m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z\"></path></svg></template><template id=\"repo-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo\">    <path d=\"M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z\"></path></svg></template><template id=\"bookmark-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-bookmark\">    <path d=\"M3 2.75C3 1.784 3.784 1 4.75 1h6.5c.966 0 1.75.784 1.75 1.75v11.5a.75.75 0 0 1-1.227.579L8 11.722l-3.773 3.107A.751.751 0 0 1 3 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.91l3.023-2.489a.75.75 0 0 1 .954 0l3.023 2.49V2.75a.25.25 0 0 0-.25-.25Z\"></path></svg></template><template id=\"plus-circle-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-plus-circle\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm7.25-3.25v2.5h2.5a.75.75 0 0 1 0 1.5h-2.5v2.5a.75.75 0 0 1-1.5 0v-2.5h-2.5a.75.75 0 0 1 0-1.5h2.5v-2.5a.75.75 0 0 1 1.5 0Z\"></path></svg></template><template id=\"circle-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-dot-fill\">    <path d=\"M8 4a4 4 0 1 1 0 8 4 4 0 0 1 0-8Z\"></path></svg></template><template id=\"trash-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-trash\">    <path d=\"M11 1.75V3h2.25a.75.75 0 0 1 0 1.5H2.75a.75.75 0 0 1 0-1.5H5V1.75C5 .784 5.784 0 6.75 0h2.5C10.216 0 11 .784 11 1.75ZM4.496 6.675l.66 6.6a.25.25 0 0 0 .249.225h5.19a.25.25 0 0 0 .249-.225l.66-6.6a.75.75 0 0 1 1.492.149l-.66 6.6A1.748 1.748 0 0 1 10.595 15h-5.19a1.75 1.75 0 0 1-1.741-1.575l-.66-6.6a.75.75 0 1 1 1.492-.15ZM6.5 1.75V3h3V1.75a.25.25 0 0 0-.25-.25h-2.5a.25.25 0 0 0-.25.25Z\"></path></svg></template><template id=\"team-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-people\">    <path d=\"M2 5.5a3.5 3.5 0 1 1 5.898 2.549 5.508 5.508 0 0 1 3.034 4.084.75.75 0 1 1-1.482.235 4 4 0 0 0-7.9 0 .75.75 0 0 1-1.482-.236A5.507 5.507 0 0 1 3.102 8.05 3.493 3.493 0 0 1 2 5.5ZM11 4a3.001 3.001 0 0 1 2.22 5.018 5.01 5.01 0 0 1 2.56 3.012.749.749 0 0 1-.885.954.752.752 0 0 1-.549-.514 3.507 3.507 0 0 0-2.522-2.372.75.75 0 0 1-.574-.73v-.352a.75.75 0 0 1 .416-.672A1.5 1.5 0 0 0 11 5.5.75.75 0 0 1 11 4Zm-5.5-.5a2 2 0 1 0-.001 3.999A2 2 0 0 0 5.5 3.5Z\"></path></svg></template><template id=\"project-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-project\">    <path d=\"M1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0ZM1.5 1.75v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25ZM11.75 3a.75.75 0 0 1 .75.75v7.5a.75.75 0 0 1-1.5 0v-7.5a.75.75 0 0 1 .75-.75Zm-8.25.75a.75.75 0 0 1 1.5 0v5.5a.75.75 0 0 1-1.5 0ZM8 3a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 3Z\"></path></svg></template><template id=\"pencil-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-pencil\">    <path d=\"M11.013 1.427a1.75 1.75 0 0 1 2.474 0l1.086 1.086a1.75 1.75 0 0 1 0 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 0 1-.927-.928l.929-3.25c.081-.286.235-.547.445-.758l8.61-8.61Zm.176 4.823L9.75 4.81l-6.286 6.287a.253.253 0 0 0-.064.108l-.558 1.953 1.953-.558a.253.253 0 0 0 .108-.064Zm1.238-3.763a.25.25 0 0 0-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 0 0 0-.354Z\"></path></svg></template><template id=\"copilot-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copilot\">    <path d=\"M7.998 15.035c-4.562 0-7.873-2.914-7.998-3.749V9.338c.085-.628.677-1.686 1.588-2.065.013-.07.024-.143.036-.218.029-.183.06-.384.126-.612-.201-.508-.254-1.084-.254-1.656 0-.87.128-1.769.693-2.484.579-.733 1.494-1.124 2.724-1.261 1.206-.134 2.262.034 2.944.765.05.053.096.108.139.165.044-.057.094-.112.143-.165.682-.731 1.738-.899 2.944-.765 1.23.137 2.145.528 2.724 1.261.566.715.693 1.614.693 2.484 0 .572-.053 1.148-.254 1.656.066.228.098.429.126.612.012.076.024.148.037.218.924.385 1.522 1.471 1.591 2.095v1.872c0 .766-3.351 3.795-8.002 3.795Zm0-1.485c2.28 0 4.584-1.11 5.002-1.433V7.862l-.023-.116c-.49.21-1.075.291-1.727.291-1.146 0-2.059-.327-2.71-.991A3.222 3.222 0 0 1 8 6.303a3.24 3.24 0 0 1-.544.743c-.65.664-1.563.991-2.71.991-.652 0-1.236-.081-1.727-.291l-.023.116v4.255c.419.323 2.722 1.433 5.002 1.433ZM6.762 2.83c-.193-.206-.637-.413-1.682-.297-1.019.113-1.479.404-1.713.7-.247.312-.369.789-.369 1.554 0 .793.129 1.171.308 1.371.162.181.519.379 1.442.379.853 0 1.339-.235 1.638-.54.315-.322.527-.827.617-1.553.117-.935-.037-1.395-.241-1.614Zm4.155-.297c-1.044-.116-1.488.091-1.681.297-.204.219-.359.679-.242 1.614.091.726.303 1.231.618 1.553.299.305.784.54 1.638.54.922 0 1.28-.198 1.442-.379.179-.2.308-.578.308-1.371 0-.765-.123-1.242-.37-1.554-.233-.296-.693-.587-1.713-.7Z\"></path><path d=\"M6.25 9.037a.75.75 0 0 1 .75.75v1.501a.75.75 0 0 1-1.5 0V9.787a.75.75 0 0 1 .75-.75Zm4.25.75v1.501a.75.75 0 0 1-1.5 0V9.787a.75.75 0 0 1 1.5 0Z\"></path></svg></template><template id=\"workflow-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-workflow\">    <path d=\"M0 1.75C0 .784.784 0 1.75 0h3.5C6.216 0 7 .784 7 1.75v3.5A1.75 1.75 0 0 1 5.25 7H4v4a1 1 0 0 0 1 1h4v-1.25C9 9.784 9.784 9 10.75 9h3.5c.966 0 1.75.784 1.75 1.75v3.5A1.75 1.75 0 0 1 14.25 16h-3.5A1.75 1.75 0 0 1 9 14.25v-.75H5A2.5 2.5 0 0 1 2.5 11V7h-.75A1.75 1.75 0 0 1 0 5.25Zm1.75-.25a.25.25 0 0 0-.25.25v3.5c0 .138.112.25.25.25h3.5a.25.25 0 0 0 .25-.25v-3.5a.25.25 0 0 0-.25-.25Zm9 9a.25.25 0 0 0-.25.25v3.5c0 .138.112.25.25.25h3.5a.25.25 0 0 0 .25-.25v-3.5a.25.25 0 0 0-.25-.25Z\"></path></svg></template><template id=\"book-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-book\">    <path d=\"M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z\"></path></svg></template><template id=\"code-review-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code-review\">    <path d=\"M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 13H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25v-8.5C0 1.784.784 1 1.75 1ZM1.5 2.75v8.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-8.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Zm5.28 1.72a.75.75 0 0 1 0 1.06L5.31 7l1.47 1.47a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018l-2-2a.75.75 0 0 1 0-1.06l2-2a.75.75 0 0 1 1.06 0Zm2.44 0a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L10.69 7 9.22 5.53a.75.75 0 0 1 0-1.06Z\"></path></svg></template><template id=\"codespaces-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-codespaces\">    <path d=\"M0 11.25c0-.966.784-1.75 1.75-1.75h12.5c.966 0 1.75.784 1.75 1.75v3A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25Zm2-9.5C2 .784 2.784 0 3.75 0h8.5C13.216 0 14 .784 14 1.75v5a1.75 1.75 0 0 1-1.75 1.75h-8.5A1.75 1.75 0 0 1 2 6.75Zm1.75-.25a.25.25 0 0 0-.25.25v5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-5a.25.25 0 0 0-.25-.25Zm-2 9.5a.25.25 0 0 0-.25.25v3c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-3a.25.25 0 0 0-.25-.25Z\"></path><path d=\"M7 12.75a.75.75 0 0 1 .75-.75h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z\"></path></svg></template><template id=\"comment-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-comment\">    <path d=\"M1 2.75C1 1.784 1.784 1 2.75 1h10.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 13.25 12H9.06l-2.573 2.573A1.458 1.458 0 0 1 4 13.543V12H2.75A1.75 1.75 0 0 1 1 10.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h4.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path></svg></template><template id=\"comment-discussion-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-comment-discussion\">    <path d=\"M1.75 1h8.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 10.25 10H7.061l-2.574 2.573A1.458 1.458 0 0 1 2 11.543V10h-.25A1.75 1.75 0 0 1 0 8.25v-5.5C0 1.784.784 1 1.75 1ZM1.5 2.75v5.5c0 .138.112.25.25.25h1a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h3.5a.25.25 0 0 0 .25-.25v-5.5a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25Zm13 2a.25.25 0 0 0-.25-.25h-.5a.75.75 0 0 1 0-1.5h.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 14.25 12H14v1.543a1.458 1.458 0 0 1-2.487 1.03L9.22 12.28a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l2.22 2.22v-2.19a.75.75 0 0 1 .75-.75h1a.25.25 0 0 0 .25-.25Z\"></path></svg></template><template id=\"organization-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-organization\">    <path d=\"M1.75 16A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0h8.5C11.216 0 12 .784 12 1.75v12.5c0 .085-.006.168-.018.25h2.268a.25.25 0 0 0 .25-.25V8.285a.25.25 0 0 0-.111-.208l-1.055-.703a.749.749 0 1 1 .832-1.248l1.055.703c.487.325.779.871.779 1.456v5.965A1.75 1.75 0 0 1 14.25 16h-3.5a.766.766 0 0 1-.197-.026c-.099.017-.2.026-.303.026h-3a.75.75 0 0 1-.75-.75V14h-1v1.25a.75.75 0 0 1-.75.75Zm-.25-1.75c0 .138.112.25.25.25H4v-1.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 .75.75v1.25h2.25a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25ZM3.75 6h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5ZM3 3.75A.75.75 0 0 1 3.75 3h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 3.75Zm4 3A.75.75 0 0 1 7.75 6h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 7 6.75ZM7.75 3h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5ZM3 9.75A.75.75 0 0 1 3.75 9h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 9.75ZM7.75 9h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5Z\"></path></svg></template><template id=\"rocket-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-rocket\">    <path d=\"M14.064 0h.186C15.216 0 16 .784 16 1.75v.186a8.752 8.752 0 0 1-2.564 6.186l-.458.459c-.314.314-.641.616-.979.904v3.207c0 .608-.315 1.172-.833 1.49l-2.774 1.707a.749.749 0 0 1-1.11-.418l-.954-3.102a1.214 1.214 0 0 1-.145-.125L3.754 9.816a1.218 1.218 0 0 1-.124-.145L.528 8.717a.749.749 0 0 1-.418-1.11l1.71-2.774A1.748 1.748 0 0 1 3.31 4h3.204c.288-.338.59-.665.904-.979l.459-.458A8.749 8.749 0 0 1 14.064 0ZM8.938 3.623h-.002l-.458.458c-.76.76-1.437 1.598-2.02 2.5l-1.5 2.317 2.143 2.143 2.317-1.5c.902-.583 1.74-1.26 2.499-2.02l.459-.458a7.25 7.25 0 0 0 2.123-5.127V1.75a.25.25 0 0 0-.25-.25h-.186a7.249 7.249 0 0 0-5.125 2.123ZM3.56 14.56c-.732.732-2.334 1.045-3.005 1.148a.234.234 0 0 1-.201-.064.234.234 0 0 1-.064-.201c.103-.671.416-2.273 1.15-3.003a1.502 1.502 0 1 1 2.12 2.12Zm6.94-3.935c-.088.06-.177.118-.266.175l-2.35 1.521.548 1.783 1.949-1.2a.25.25 0 0 0 .119-.213ZM3.678 8.116 5.2 5.766c.058-.09.117-.178.176-.266H3.309a.25.25 0 0 0-.213.119l-1.2 1.95ZM12 5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg></template><template id=\"shield-check-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-shield-check\">    <path d=\"m8.533.133 5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667l5.25-1.68a1.748 1.748 0 0 1 1.066 0Zm-.61 1.429.001.001-5.25 1.68a.251.251 0 0 0-.174.237V7c0 1.36.275 2.666 1.057 3.859.784 1.194 2.121 2.342 4.366 3.298a.196.196 0 0 0 .154 0c2.245-.957 3.582-2.103 4.366-3.297C13.225 9.666 13.5 8.358 13.5 7V3.48a.25.25 0 0 0-.174-.238l-5.25-1.68a.25.25 0 0 0-.153 0ZM11.28 6.28l-3.5 3.5a.75.75 0 0 1-1.06 0l-1.5-1.5a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l.97.97 2.97-2.97a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z\"></path></svg></template><template id=\"heart-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-heart\">    <path d=\"m8 14.25.345.666a.75.75 0 0 1-.69 0l-.008-.004-.018-.01a7.152 7.152 0 0 1-.31-.17 22.055 22.055 0 0 1-3.434-2.414C2.045 10.731 0 8.35 0 5.5 0 2.836 2.086 1 4.25 1 5.797 1 7.153 1.802 8 3.02 8.847 1.802 10.203 1 11.75 1 13.914 1 16 2.836 16 5.5c0 2.85-2.045 5.231-3.885 6.818a22.066 22.066 0 0 1-3.744 2.584l-.018.01-.006.003h-.002ZM4.25 2.5c-1.336 0-2.75 1.164-2.75 3 0 2.15 1.58 4.144 3.365 5.682A20.58 20.58 0 0 0 8 13.393a20.58 20.58 0 0 0 3.135-2.211C12.92 9.644 14.5 7.65 14.5 5.5c0-1.836-1.414-3-2.75-3-1.373 0-2.609.986-3.029 2.456a.749.749 0 0 1-1.442 0C6.859 3.486 5.623 2.5 4.25 2.5Z\"></path></svg></template><template id=\"server-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-server\">    <path d=\"M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v4c0 .372-.116.717-.314 1 .198.283.314.628.314 1v4a1.75 1.75 0 0 1-1.75 1.75H1.75A1.75 1.75 0 0 1 0 12.75v-4c0-.358.109-.707.314-1a1.739 1.739 0 0 1-.314-1v-4C0 1.784.784 1 1.75 1ZM1.5 2.75v4c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Zm.25 5.75a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25ZM7 4.75A.75.75 0 0 1 7.75 4h4.5a.75.75 0 0 1 0 1.5h-4.5A.75.75 0 0 1 7 4.75ZM7.75 10h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM3 4.75A.75.75 0 0 1 3.75 4h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 4.75ZM3.75 10h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5Z\"></path></svg></template><template id=\"globe-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-globe\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM5.78 8.75a9.64 9.64 0 0 0 1.363 4.177c.255.426.542.832.857 1.215.245-.296.551-.705.857-1.215A9.64 9.64 0 0 0 10.22 8.75Zm4.44-1.5a9.64 9.64 0 0 0-1.363-4.177c-.307-.51-.612-.919-.857-1.215a9.927 9.927 0 0 0-.857 1.215A9.64 9.64 0 0 0 5.78 7.25Zm-5.944 1.5H1.543a6.507 6.507 0 0 0 4.666 5.5c-.123-.181-.24-.365-.352-.552-.715-1.192-1.437-2.874-1.581-4.948Zm-2.733-1.5h2.733c.144-2.074.866-3.756 1.58-4.948.12-.197.237-.381.353-.552a6.507 6.507 0 0 0-4.666 5.5Zm10.181 1.5c-.144 2.074-.866 3.756-1.58 4.948-.12.197-.237.381-.353.552a6.507 6.507 0 0 0 4.666-5.5Zm2.733-1.5a6.507 6.507 0 0 0-4.666-5.5c.123.181.24.365.353.552.714 1.192 1.436 2.874 1.58 4.948Z\"></path></svg></template><template id=\"issue-opened-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-issue-opened\">    <path d=\"M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path><path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z\"></path></svg></template><template id=\"device-mobile-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-device-mobile\">    <path d=\"M3.75 0h8.5C13.216 0 14 .784 14 1.75v12.5A1.75 1.75 0 0 1 12.25 16h-8.5A1.75 1.75 0 0 1 2 14.25V1.75C2 .784 2.784 0 3.75 0ZM3.5 1.75v12.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25ZM8 13a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z\"></path></svg></template><template id=\"package-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-package\">    <path d=\"m8.878.392 5.25 3.045c.54.314.872.89.872 1.514v6.098a1.75 1.75 0 0 1-.872 1.514l-5.25 3.045a1.75 1.75 0 0 1-1.756 0l-5.25-3.045A1.75 1.75 0 0 1 1 11.049V4.951c0-.624.332-1.201.872-1.514L7.122.392a1.75 1.75 0 0 1 1.756 0ZM7.875 1.69l-4.63 2.685L8 7.133l4.755-2.758-4.63-2.685a.248.248 0 0 0-.25 0ZM2.5 5.677v5.372c0 .09.047.171.125.216l4.625 2.683V8.432Zm6.25 8.271 4.625-2.683a.25.25 0 0 0 .125-.216V5.677L8.75 8.432Z\"></path></svg></template><template id=\"credit-card-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-credit-card\">    <path d=\"M10.75 9a.75.75 0 0 0 0 1.5h1.5a.75.75 0 0 0 0-1.5h-1.5Z\"></path><path d=\"M0 3.75C0 2.784.784 2 1.75 2h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 14H1.75A1.75 1.75 0 0 1 0 12.25ZM14.5 6.5h-13v5.75c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25Zm0-2.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25V5h13Z\"></path></svg></template><template id=\"play-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-play\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z\"></path></svg></template><template id=\"gift-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-gift\">    <path d=\"M2 2.75A2.75 2.75 0 0 1 4.75 0c.983 0 1.873.42 2.57 1.232.268.318.497.668.68 1.042.183-.375.411-.725.68-1.044C9.376.42 10.266 0 11.25 0a2.75 2.75 0 0 1 2.45 4h.55c.966 0 1.75.784 1.75 1.75v2c0 .698-.409 1.301-1 1.582v4.918A1.75 1.75 0 0 1 13.25 16H2.75A1.75 1.75 0 0 1 1 14.25V9.332C.409 9.05 0 8.448 0 7.75v-2C0 4.784.784 4 1.75 4h.55c-.192-.375-.3-.8-.3-1.25ZM7.25 9.5H2.5v4.75c0 .138.112.25.25.25h4.5Zm1.5 0v5h4.5a.25.25 0 0 0 .25-.25V9.5Zm0-4V8h5.5a.25.25 0 0 0 .25-.25v-2a.25.25 0 0 0-.25-.25Zm-7 0a.25.25 0 0 0-.25.25v2c0 .138.112.25.25.25h5.5V5.5h-5.5Zm3-4a1.25 1.25 0 0 0 0 2.5h2.309c-.233-.818-.542-1.401-.878-1.793-.43-.502-.915-.707-1.431-.707ZM8.941 4h2.309a1.25 1.25 0 0 0 0-2.5c-.516 0-1 .205-1.43.707-.337.392-.646.975-.879 1.793Z\"></path></svg></template><template id=\"code-square-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code-square\">    <path d=\"M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25Zm7.47 3.97a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L10.69 8 9.22 6.53a.75.75 0 0 1 0-1.06ZM6.78 6.53 5.31 8l1.47 1.47a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215l-2-2a.75.75 0 0 1 0-1.06l2-2a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z\"></path></svg></template><template id=\"device-desktop-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-device-desktop\">    <path d=\"M14.25 1c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 14.25 12h-3.727c.099 1.041.52 1.872 1.292 2.757A.752.752 0 0 1 11.25 16h-6.5a.75.75 0 0 1-.565-1.243c.772-.885 1.192-1.716 1.292-2.757H1.75A1.75 1.75 0 0 1 0 10.25v-7.5C0 1.784.784 1 1.75 1ZM1.75 2.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25ZM9.018 12H6.982a5.72 5.72 0 0 1-.765 2.5h3.566a5.72 5.72 0 0 1-.765-2.5Z\"></path></svg></template>        <div class=\"position-relative\">                <ul                  role=\"listbox\"                  class=\"ActionListWrap QueryBuilder-ListWrap\"                  aria-label=\"Suggestions\"                  data-action=\"                    combobox-commit:query-builder#comboboxCommit                    mousedown:query-builder#resultsMousedown                  \"                  data-target=\"query-builder.resultsList\"                  data-persist-list=false                  id=\"query-builder-test-results\"                ></ul>        </div>      <div class=\"FormControl-inlineValidation\" id=\"validation-1b68690a-eff9-48b5-a978-cbee8bd476e3\" hidden=\"hidden\">        <span class=\"FormControl-inlineValidation--visual\">          <svg aria-hidden=\"true\" height=\"12\" viewBox=\"0 0 12 12\" version=\"1.1\" width=\"12\" data-view-component=\"true\" class=\"octicon octicon-alert-fill\">    <path d=\"M4.855.708c.5-.896 1.79-.896 2.29 0l4.675 8.351a1.312 1.312 0 0 1-1.146 1.954H1.33A1.313 1.313 0 0 1 .183 9.058ZM7 7V3H5v4Zm-1 3a1 1 0 1 0 0-2 1 1 0 0 0 0 2Z\"></path></svg>        </span>        <span></span></div>    </div>    <div data-target=\"query-builder.screenReaderFeedback\" aria-live=\"polite\" aria-atomic=\"true\" class=\"sr-only\"></div></query-builder></form>          <div class=\"d-flex flex-row color-fg-muted px-3 text-small color-bg-default search-feedback-prompt\">            <a target=\"_blank\" href=\"https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax\" data-view-component=\"true\" class=\"Link color-fg-accent text-normal ml-2\">              Search syntax tips</a>            <div class=\"d-flex flex-1\"></div>          </div>        </div></div>    </div></modal-dialog></div>  </div>  <div data-action=\"click:qbsearch-input#retract\" class=\"dark-backdrop position-fixed\" hidden data-target=\"qbsearch-input.darkBackdrop\"></div>  <div class=\"color-fg-default\">    <dialog-helper>  <dialog data-target=\"qbsearch-input.feedbackDialog\" data-action=\"close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose\" id=\"feedback-dialog\" aria-modal=\"true\" aria-labelledby=\"feedback-dialog-title\" aria-describedby=\"feedback-dialog-description\" data-view-component=\"true\" class=\"Overlay Overlay-whenNarrow Overlay--size-medium Overlay--motion-scaleFade\">    <div data-view-component=\"true\" class=\"Overlay-header\">  <div class=\"Overlay-headerContentWrap\">    <div class=\"Overlay-titleWrap\">      <h1 class=\"Overlay-title \" id=\"feedback-dialog-title\">        Provide feedback      </h1>    </div>    <div class=\"Overlay-actionWrap\">      <button data-close-dialog-id=\"feedback-dialog\" aria-label=\"Close\" type=\"button\" data-view-component=\"true\" class=\"close-button Overlay-closeButton\"><svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg></button>    </div>  </div></div>      <scrollable-region data-labelled-by=\"feedback-dialog-title\">        <div data-view-component=\"true\" class=\"Overlay-body\">        <!-- '\"` --><!-- </textarea></xmp> --></option></form><form id=\"code-search-feedback-form\" data-turbo=\"false\" action=\"/search/feedback\" accept-charset=\"UTF-8\" method=\"post\"><input type=\"hidden\" data-csrf=\"true\" name=\"authenticity_token\" value=\"PqbBarzL5jZjdtTaDbr+r9MfrjXhWl2ruvm4ObrBSCtNz965OYqMY3MJrCAuzRkvzAXWgMfWQUXqqI9CX/GwZw==\" />          <p>We read every piece of feedback, and take your input very seriously.</p>          <textarea name=\"feedback\" class=\"form-control width-full mb-2\" style=\"height: 120px\" id=\"feedback\"></textarea>          <input name=\"include_email\" id=\"include_email\" aria-label=\"Include my email address so I can be contacted\" class=\"form-control mr-2\" type=\"checkbox\">          <label for=\"include_email\" style=\"font-weight: normal\">Include my email address so I can be contacted</label></form></div>      </scrollable-region>      <div data-view-component=\"true\" class=\"Overlay-footer Overlay-footer--alignEnd\">          <button data-close-dialog-id=\"feedback-dialog\" type=\"button\" data-view-component=\"true\" class=\"btn\">    Cancel</button>          <button form=\"code-search-feedback-form\" data-action=\"click:qbsearch-input#submitFeedback\" type=\"submit\" data-view-component=\"true\" class=\"btn-primary btn\">    Submit feedback</button></div></dialog></dialog-helper>    <custom-scopes data-target=\"qbsearch-input.customScopesManager\">    <dialog-helper>  <dialog data-target=\"custom-scopes.customScopesModalDialog\" data-action=\"close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose\" id=\"custom-scopes-dialog\" aria-modal=\"true\" aria-labelledby=\"custom-scopes-dialog-title\" aria-describedby=\"custom-scopes-dialog-description\" data-view-component=\"true\" class=\"Overlay Overlay-whenNarrow Overlay--size-medium Overlay--motion-scaleFade\">    <div data-view-component=\"true\" class=\"Overlay-header Overlay-header--divided\">  <div class=\"Overlay-headerContentWrap\">    <div class=\"Overlay-titleWrap\">      <h1 class=\"Overlay-title \" id=\"custom-scopes-dialog-title\">        Saved searches      </h1>        <h2 id=\"custom-scopes-dialog-description\" class=\"Overlay-description\">Use saved searches to filter your results more quickly</h2>    </div>    <div class=\"Overlay-actionWrap\">      <button data-close-dialog-id=\"custom-scopes-dialog\" aria-label=\"Close\" type=\"button\" data-view-component=\"true\" class=\"close-button Overlay-closeButton\"><svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg></button>    </div>  </div></div>      <scrollable-region data-labelled-by=\"custom-scopes-dialog-title\">        <div data-view-component=\"true\" class=\"Overlay-body\">        <div data-target=\"custom-scopes.customScopesModalDialogFlash\"></div>        <div hidden class=\"create-custom-scope-form\" data-target=\"custom-scopes.createCustomScopeForm\">        <!-- '\"` --><!-- </textarea></xmp> --></option></form><form id=\"custom-scopes-dialog-form\" data-turbo=\"false\" action=\"/search/custom_scopes\" accept-charset=\"UTF-8\" method=\"post\"><input type=\"hidden\" data-csrf=\"true\" name=\"authenticity_token\" value=\"SJVBw+rErXcYF7R6YpLXerwTnPhpEZ/VlLM4Xm1Yb5bsuPOK/jXxNeWnpzpIt/RUxNPfxWcCD1hmPuTd0NAs5w==\" />          <div data-target=\"custom-scopes.customScopesModalDialogFlash\"></div>          <input type=\"hidden\" id=\"custom_scope_id\" name=\"custom_scope_id\" data-target=\"custom-scopes.customScopesIdField\">          <div class=\"form-group\">            <label for=\"custom_scope_name\">Name</label>            <auto-check src=\"/search/custom_scopes/check_name\" required>              <input                type=\"text\"                name=\"custom_scope_name\"                id=\"custom_scope_name\"                data-target=\"custom-scopes.customScopesNameField\"                class=\"form-control\"                autocomplete=\"off\"                placeholder=\"github-ruby\"                required                maxlength=\"50\">              <input type=\"hidden\" data-csrf=\"true\" value=\"iQcbOxUBW83aWFaFlXgnhViq79mQiGh0ynHwDb3CMBrsJWPbsN+Zes9D/Sh7Y9BL9YkdQ5sReB2JhhfM7hfRDw==\" />            </auto-check>          </div>          <div class=\"form-group\">            <label for=\"custom_scope_query\">Query</label>            <input              type=\"text\"              name=\"custom_scope_query\"              id=\"custom_scope_query\"              data-target=\"custom-scopes.customScopesQueryField\"              class=\"form-control\"              autocomplete=\"off\"              placeholder=\"(repo:mona/a OR repo:mona/b) AND lang:python\"              required              maxlength=\"500\">          </div>          <p class=\"text-small color-fg-muted\">            To see all available qualifiers, see our <a class=\"Link--inTextBlock\" href=\"https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax\">documentation</a>.          </p></form>        </div>        <div data-target=\"custom-scopes.manageCustomScopesForm\">          <div data-target=\"custom-scopes.list\"></div>        </div></div>      </scrollable-region>      <div data-view-component=\"true\" class=\"Overlay-footer Overlay-footer--alignEnd Overlay-footer--divided\">          <button data-action=\"click:custom-scopes#customScopesCancel\" type=\"button\" data-view-component=\"true\" class=\"btn\">    Cancel</button>          <button form=\"custom-scopes-dialog-form\" data-action=\"click:custom-scopes#customScopesSubmit\" data-target=\"custom-scopes.customScopesSubmitButton\" type=\"submit\" data-view-component=\"true\" class=\"btn-primary btn\">    Create saved search</button></div></dialog></dialog-helper>    </custom-scopes>  </div></qbsearch-input><input type=\"hidden\" data-csrf=\"true\" class=\"js-data-jump-to-suggestions-path-csrf\" value=\"9LHOcYVZH7jbUL2y4YeEHsJ6QVKrd3QEF1YiTQPa8TEM14dk8wQbPlsd+sZe6NdOqZ5OsabQXD+14liAOfZujg==\" />          <div class=\"position-relative mr-lg-3 d-lg-inline-block\">            <a href=\"/login?return_to=https%3A%2F%2Fgithub.com%2Flucidrains%2Fx-transformers\"              class=\"HeaderMenu-link HeaderMenu-link--sign-in flex-shrink-0 no-underline d-block d-lg-inline-block border border-lg-0 rounded rounded-lg-0 p-2 p-lg-0\"              data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/x-transformers&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"746f69153e1c58b969068598aa9622fa70168054c43eb89952edf603d0e8022e\"              data-ga-click=\"(Logged out) Header, clicked Sign in, text:sign-in\">              Sign in            </a>          </div>            <a href=\"/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=lucidrains%2Fx-transformers\"              class=\"HeaderMenu-link HeaderMenu-link--sign-up flex-shrink-0 d-none d-lg-inline-block no-underline border color-border-default rounded px-2 py-1\"              data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/x-transformers&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"746f69153e1c58b969068598aa9622fa70168054c43eb89952edf603d0e8022e\"              data-analytics-event=\"{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/&lt;user-name&gt;/&lt;repo-name&gt;;ref_cta:Sign up;ref_loc:header logged out&quot;}\"            >              Sign up            </a>        </div>      </div>    </div>  </div></header>      <div hidden=\"hidden\" data-view-component=\"true\" class=\"js-stale-session-flash stale-session-flash flash flash-warn flash-full mb-3\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-alert\">    <path d=\"M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>        <span class=\"js-stale-session-flash-signed-in\" hidden>You signed in with another tab or window. <a class=\"Link--inTextBlock\" href=\"\">Reload</a> to refresh your session.</span>        <span class=\"js-stale-session-flash-signed-out\" hidden>You signed out in another tab or window. <a class=\"Link--inTextBlock\" href=\"\">Reload</a> to refresh your session.</span>        <span class=\"js-stale-session-flash-switched\" hidden>You switched accounts on another tab or window. <a class=\"Link--inTextBlock\" href=\"\">Reload</a> to refresh your session.</span>    <button id=\"icon-button-dc773a18-cdbb-465b-b714-f23af06cec17\" aria-labelledby=\"tooltip-74444ae0-74b0-43aa-af2a-afca60701153\" type=\"button\" data-view-component=\"true\" class=\"Button Button--iconOnly Button--invisible Button--medium flash-close js-flash-close\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x Button-visual\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg></button><tool-tip id=\"tooltip-74444ae0-74b0-43aa-af2a-afca60701153\" for=\"icon-button-dc773a18-cdbb-465b-b714-f23af06cec17\" popover=\"manual\" data-direction=\"s\" data-type=\"label\" data-view-component=\"true\" class=\"sr-only position-absolute\">Dismiss alert</tool-tip>  </div>    </div>  <div id=\"start-of-content\" class=\"show-on-focus\"></div>    <div id=\"js-flash-container\" data-turbo-replace>  <template class=\"js-flash-template\">    <div class=\"flash flash-full   {{ className }}\">  <div class=\"px-2\" >    <button autofocus class=\"flash-close js-flash-close\" type=\"button\" aria-label=\"Dismiss this message\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg>    </button>    <div aria-atomic=\"true\" role=\"alert\" class=\"js-flash-alert\">            <div>{{ message }}</div>    </div>  </div></div>  </template></div>        <include-fragment class=\"js-notification-shelf-include-fragment\" data-base-src=\"https://github.com/notifications/beta/shelf\"></include-fragment>  <div    class=\"application-main \"    data-commit-hovercards-enabled    data-discussion-hovercards-enabled    data-issue-and-pr-hovercards-enabled  >        <div itemscope itemtype=\"http://schema.org/SoftwareSourceCode\" class=\"\">    <main id=\"js-repo-pjax-container\" >                <div id=\"repository-container-header\"  class=\"pt-3 hide-full-screen\" style=\"background-color: var(--page-header-bgColor, var(--color-page-header-bg));\" data-turbo-replace>      <div class=\"d-flex flex-wrap flex-justify-end mb-3  px-3 px-md-4 px-lg-5\" style=\"gap: 1rem;\">        <div class=\"flex-auto min-width-0 width-fit mr-3\">              <div class=\" d-flex flex-wrap flex-items-center wb-break-word f3 text-normal\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo color-fg-muted mr-2\">    <path d=\"M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z\"></path></svg>        <span class=\"author flex-self-stretch\" itemprop=\"author\">      <a class=\"url fn\" rel=\"author\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/lucidrains/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"/lucidrains\">        lucidrains</a>    </span>    <span class=\"mx-1 flex-self-stretch color-fg-muted\">/</span>    <strong itemprop=\"name\" class=\"mr-2 flex-self-stretch\">      <a data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" href=\"/lucidrains/x-transformers\">x-transformers</a>    </strong>    <span></span><span class=\"Label Label--secondary v-align-middle mr-1\">Public</span>  </div>        </div>        <div id=\"repository-details-container\" data-turbo-replace>            <ul class=\"pagehead-actions flex-shrink-0 d-none d-md-inline\" style=\"padding: 2px 0;\">            <li>          <include-fragment src=\"/lucidrains/x-transformers/sponsor_button\"></include-fragment>        </li>        <li>            <a href=\"/login?return_to=%2Flucidrains%2Fx-transformers\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/x-transformers&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"cb5d6ed3c82d649f3a46b29f8bdb894b7ffd2697b5c4ea1c12593587548a5f62\" aria-label=\"You must be signed in to change notification settings\" data-view-component=\"true\" class=\"tooltipped tooltipped-s btn-sm btn\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-bell mr-2\">    <path d=\"M8 16a2 2 0 0 0 1.985-1.75c.017-.137-.097-.25-.235-.25h-3.5c-.138 0-.252.113-.235.25A2 2 0 0 0 8 16ZM3 5a5 5 0 0 1 10 0v2.947c0 .05.015.098.042.139l1.703 2.555A1.519 1.519 0 0 1 13.482 13H2.518a1.516 1.516 0 0 1-1.263-2.36l1.703-2.554A.255.255 0 0 0 3 7.947Zm5-3.5A3.5 3.5 0 0 0 4.5 5v2.947c0 .346-.102.683-.294.97l-1.703 2.556a.017.017 0 0 0-.003.01l.001.006c0 .002.002.004.004.006l.006.004.007.001h10.964l.007-.001.006-.004.004-.006.001-.007a.017.017 0 0 0-.003-.01l-1.703-2.554a1.745 1.745 0 0 1-.294-.97V5A3.5 3.5 0 0 0 8 1.5Z\"></path></svg>Notifications</a>  </li>  <li>          <a icon=\"repo-forked\" id=\"fork-button\" href=\"/login?return_to=%2Flucidrains%2Fx-transformers\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;repo details fork button&quot;,&quot;repository_id&quot;:306980630,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/x-transformers&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"b363884a14a0cbf69ae9cd47307a40c005433b52ce46fbb28cf48166204a8599\" data-view-component=\"true\" class=\"btn-sm btn\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo-forked mr-2\">    <path d=\"M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z\"></path></svg>Fork    <span id=\"repo-network-counter\" data-pjax-replace=\"true\" data-turbo-replace=\"true\" title=\"343\" data-view-component=\"true\" class=\"Counter\">343</span></a>  </li>  <li>        <div data-view-component=\"true\" class=\"BtnGroup d-flex\">        <a href=\"/login?return_to=%2Flucidrains%2Fx-transformers\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:306980630,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/x-transformers&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"45d8c29a85542343e9fc9736027ebb06fad625a6d0bccb6ddfb2ad9d0771194e\" aria-label=\"You must be signed in to star a repository\" data-view-component=\"true\" class=\"tooltipped tooltipped-s btn-sm btn BtnGroup-item\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-star v-align-text-bottom d-inline-block mr-2\">    <path d=\"M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z\"></path></svg><span data-view-component=\"true\" class=\"d-inline\">          Star</span>          <span id=\"repo-stars-counter-star\" aria-label=\"3947 users starred this repository\" data-singular-suffix=\"user starred this repository\" data-plural-suffix=\"users starred this repository\" data-turbo-replace=\"true\" title=\"3,947\" data-view-component=\"true\" class=\"Counter js-social-count\">3.9k</span></a>        <button aria-label=\"You must be signed in to add this repository to a list\" type=\"button\" disabled=\"disabled\" data-view-component=\"true\" class=\"btn-sm btn BtnGroup-item px-2\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-triangle-down\">    <path d=\"m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z\"></path></svg></button></div>  </li>    <li>            </li></ul>        </div>      </div>        <div id=\"responsive-meta-container\" data-turbo-replace>      <div class=\"d-block d-md-none mb-2 px-3 px-md-4 px-lg-5\">      <p class=\"f4 mb-3 \">        A simple but complete full-attention transformer with a set of promising experimental features from various papers      </p>          <h3 class=\"sr-only\">License</h3>  <div class=\"mb-2\">    <a href=\"/lucidrains/x-transformers/blob/main/LICENSE\"      class=\"Link--muted\"            data-analytics-event=\"{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:license&quot;}\"    >      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-law mr-2\">    <path d=\"M8.75.75V2h.985c.304 0 .603.08.867.231l1.29.736c.038.022.08.033.124.033h2.234a.75.75 0 0 1 0 1.5h-.427l2.111 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.006.005-.01.01-.045.04c-.21.176-.441.327-.686.45C14.556 10.78 13.88 11 13 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L12.178 4.5h-.162c-.305 0-.604-.079-.868-.231l-1.29-.736a.245.245 0 0 0-.124-.033H8.75V13h2.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1 0-1.5h2.5V3.5h-.984a.245.245 0 0 0-.124.033l-1.289.737c-.265.15-.564.23-.869.23h-.162l2.112 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.016.015-.045.04c-.21.176-.441.327-.686.45C4.556 10.78 3.88 11 3 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L2.178 4.5H1.75a.75.75 0 0 1 0-1.5h2.234a.249.249 0 0 0 .125-.033l1.288-.737c.265-.15.564-.23.869-.23h.984V.75a.75.75 0 0 1 1.5 0Zm2.945 8.477c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L13 6.327Zm-10 0c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L3 6.327Z\"></path></svg>     MIT license    </a>  </div>    <div class=\"mb-3\">        <a class=\"Link--secondary no-underline mr-3\" href=\"/lucidrains/x-transformers/stargazers\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-star mr-1\">    <path d=\"M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z\"></path></svg>          <span class=\"text-bold\">3.9k</span>          stars</a>        <a class=\"Link--secondary no-underline mr-3\" href=\"/lucidrains/x-transformers/forks\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo-forked mr-1\">    <path d=\"M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z\"></path></svg>          <span class=\"text-bold\">343</span>          forks</a>          <a class=\"Link--secondary no-underline mr-3 d-inline-block\" href=\"/lucidrains/x-transformers/branches\">            <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-git-branch mr-1\">    <path d=\"M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z\"></path></svg>            <span>Branches</span></a>          <a class=\"Link--secondary no-underline d-inline-block\" href=\"/lucidrains/x-transformers/tags\">            <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-tag mr-1\">    <path d=\"M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z\"></path></svg>            <span>Tags</span></a>        <a class=\"Link--secondary no-underline d-inline-block\" href=\"/lucidrains/x-transformers/activity\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-pulse mr-1\">    <path d=\"M6 2c.306 0 .582.187.696.471L10 10.731l1.304-3.26A.751.751 0 0 1 12 7h3.25a.75.75 0 0 1 0 1.5h-2.742l-1.812 4.528a.751.751 0 0 1-1.392 0L6 4.77 4.696 8.03A.75.75 0 0 1 4 8.5H.75a.75.75 0 0 1 0-1.5h2.742l1.812-4.529A.751.751 0 0 1 6 2Z\"></path></svg>          <span>Activity</span></a>    </div>      <div class=\"d-flex flex-wrap gap-2\">        <div class=\"flex-1\">            <div data-view-component=\"true\" class=\"BtnGroup d-flex\">        <a href=\"/login?return_to=%2Flucidrains%2Fx-transformers\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:306980630,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/x-transformers&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"45d8c29a85542343e9fc9736027ebb06fad625a6d0bccb6ddfb2ad9d0771194e\" aria-label=\"You must be signed in to star a repository\" data-view-component=\"true\" class=\"tooltipped tooltipped-s btn-sm btn btn-block BtnGroup-item\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-star v-align-text-bottom d-inline-block mr-2\">    <path d=\"M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z\"></path></svg><span data-view-component=\"true\" class=\"d-inline\">          Star</span></a>        <button aria-label=\"You must be signed in to add this repository to a list\" type=\"button\" disabled=\"disabled\" data-view-component=\"true\" class=\"btn-sm btn BtnGroup-item px-2\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-triangle-down\">    <path d=\"m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z\"></path></svg></button></div>        </div>        <div class=\"flex-1\">                <a href=\"/login?return_to=%2Flucidrains%2Fx-transformers\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/x-transformers&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"cb5d6ed3c82d649f3a46b29f8bdb894b7ffd2697b5c4ea1c12593587548a5f62\" aria-label=\"You must be signed in to change notification settings\" data-view-component=\"true\" class=\"tooltipped tooltipped-s btn-sm btn btn-block\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-bell mr-2\">    <path d=\"M8 16a2 2 0 0 0 1.985-1.75c.017-.137-.097-.25-.235-.25h-3.5c-.138 0-.252.113-.235.25A2 2 0 0 0 8 16ZM3 5a5 5 0 0 1 10 0v2.947c0 .05.015.098.042.139l1.703 2.555A1.519 1.519 0 0 1 13.482 13H2.518a1.516 1.516 0 0 1-1.263-2.36l1.703-2.554A.255.255 0 0 0 3 7.947Zm5-3.5A3.5 3.5 0 0 0 4.5 5v2.947c0 .346-.102.683-.294.97l-1.703 2.556a.017.017 0 0 0-.003.01l.001.006c0 .002.002.004.004.006l.006.004.007.001h10.964l.007-.001.006-.004.004-.006.001-.007a.017.017 0 0 0-.003-.01l-1.703-2.554a1.745 1.745 0 0 1-.294-.97V5A3.5 3.5 0 0 0 8 1.5Z\"></path></svg>Notifications</a>        </div>          <span>                      </span>      </div>  </div></div>          <nav data-pjax=\"#js-repo-pjax-container\" aria-label=\"Repository\" data-view-component=\"true\" class=\"js-repo-nav js-sidenav-container-pjax js-responsive-underlinenav overflow-hidden UnderlineNav px-3 px-md-4 px-lg-5\">  <ul data-view-component=\"true\" class=\"UnderlineNav-body list-style-none\">      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"code-tab\" href=\"/lucidrains/x-transformers\" data-tab-item=\"i0code-tab\" data-selected-links=\"repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches repo_packages repo_deployments repo_attestations /lucidrains/x-transformers\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g c\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Code&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" aria-current=\"page\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item selected\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z\"></path></svg>        <span data-content=\"Code\">Code</span>          <span id=\"code-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"Not available\" data-view-component=\"true\" class=\"Counter\"></span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"issues-tab\" href=\"/lucidrains/x-transformers/issues\" data-tab-item=\"i1issues-tab\" data-selected-links=\"repo_issues repo_labels repo_milestones /lucidrains/x-transformers/issues\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g i\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Issues&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-issue-opened UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path><path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z\"></path></svg>        <span data-content=\"Issues\">Issues</span>          <span id=\"issues-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"61\" data-view-component=\"true\" class=\"Counter\">61</span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"pull-requests-tab\" href=\"/lucidrains/x-transformers/pulls\" data-tab-item=\"i2pull-requests-tab\" data-selected-links=\"repo_pulls checks /lucidrains/x-transformers/pulls\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g p\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Pull requests&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-git-pull-request UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M1.5 3.25a2.25 2.25 0 1 1 3 2.122v5.256a2.251 2.251 0 1 1-1.5 0V5.372A2.25 2.25 0 0 1 1.5 3.25Zm5.677-.177L9.573.677A.25.25 0 0 1 10 .854V2.5h1A2.5 2.5 0 0 1 13.5 5v5.628a2.251 2.251 0 1 1-1.5 0V5a1 1 0 0 0-1-1h-1v1.646a.25.25 0 0 1-.427.177L7.177 3.427a.25.25 0 0 1 0-.354ZM3.75 2.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm0 9.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm8.25.75a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Z\"></path></svg>        <span data-content=\"Pull requests\">Pull requests</span>          <span id=\"pull-requests-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"1\" data-view-component=\"true\" class=\"Counter\">1</span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"actions-tab\" href=\"/lucidrains/x-transformers/actions\" data-tab-item=\"i3actions-tab\" data-selected-links=\"repo_actions /lucidrains/x-transformers/actions\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g a\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Actions&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-play UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z\"></path></svg>        <span data-content=\"Actions\">Actions</span>          <span id=\"actions-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"Not available\" data-view-component=\"true\" class=\"Counter\"></span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"projects-tab\" href=\"/lucidrains/x-transformers/projects\" data-tab-item=\"i4projects-tab\" data-selected-links=\"repo_projects new_repo_project repo_project /lucidrains/x-transformers/projects\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g b\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Projects&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-table UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25ZM6.5 6.5v8h7.75a.25.25 0 0 0 .25-.25V6.5Zm8-1.5V1.75a.25.25 0 0 0-.25-.25H6.5V5Zm-13 1.5v7.75c0 .138.112.25.25.25H5v-8ZM5 5V1.5H1.75a.25.25 0 0 0-.25.25V5Z\"></path></svg>        <span data-content=\"Projects\">Projects</span>          <span id=\"projects-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"1\" data-view-component=\"true\" class=\"Counter\">1</span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"security-tab\" href=\"/lucidrains/x-transformers/security\" data-tab-item=\"i5security-tab\" data-selected-links=\"security overview alerts policy token_scanning code_scanning /lucidrains/x-transformers/security\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g s\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Security&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-shield UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M7.467.133a1.748 1.748 0 0 1 1.066 0l5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667Zm.61 1.429a.25.25 0 0 0-.153 0l-5.25 1.68a.25.25 0 0 0-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.196.196 0 0 0 .154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.251.251 0 0 0-.174-.237l-5.25-1.68ZM8.75 4.75v3a.75.75 0 0 1-1.5 0v-3a.75.75 0 0 1 1.5 0ZM9 10.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>        <span data-content=\"Security\">Security</span>          <include-fragment src=\"/lucidrains/x-transformers/security/overall-count\" accept=\"text/fragment+html\"></include-fragment>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"insights-tab\" href=\"/lucidrains/x-transformers/pulse\" data-tab-item=\"i6insights-tab\" data-selected-links=\"repo_graphs repo_contributors dependency_graph dependabot_updates pulse people community /lucidrains/x-transformers/pulse\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Insights&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-graph UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M1.5 1.75V13.5h13.75a.75.75 0 0 1 0 1.5H.75a.75.75 0 0 1-.75-.75V1.75a.75.75 0 0 1 1.5 0Zm14.28 2.53-5.25 5.25a.75.75 0 0 1-1.06 0L7 7.06 4.28 9.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.25-3.25a.75.75 0 0 1 1.06 0L10 7.94l4.72-4.72a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z\"></path></svg>        <span data-content=\"Insights\">Insights</span>          <span id=\"insights-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"Not available\" data-view-component=\"true\" class=\"Counter\"></span>    </a></li></ul>    <div style=\"visibility:hidden;\" data-view-component=\"true\" class=\"UnderlineNav-actions js-responsive-underlinenav-overflow position-absolute pr-3 pr-md-4 pr-lg-5 right-0\">      <action-menu data-select-variant=\"none\" data-view-component=\"true\">  <focus-group direction=\"vertical\" mnemonics retain>    <button id=\"action-menu-c9c3f0ee-0842-49b8-86ff-f92e48d35705-button\" popovertarget=\"action-menu-c9c3f0ee-0842-49b8-86ff-f92e48d35705-overlay\" aria-controls=\"action-menu-c9c3f0ee-0842-49b8-86ff-f92e48d35705-list\" aria-haspopup=\"true\" aria-labelledby=\"tooltip-42ef25c0-3042-46bd-a02d-8d354d6e0fb9\" type=\"button\" data-view-component=\"true\" class=\"Button Button--iconOnly Button--secondary Button--medium UnderlineNav-item\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-kebab-horizontal Button-visual\">    <path d=\"M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path></svg></button><tool-tip id=\"tooltip-42ef25c0-3042-46bd-a02d-8d354d6e0fb9\" for=\"action-menu-c9c3f0ee-0842-49b8-86ff-f92e48d35705-button\" popover=\"manual\" data-direction=\"s\" data-type=\"label\" data-view-component=\"true\" class=\"sr-only position-absolute\">Additional navigation options</tool-tip><anchored-position id=\"action-menu-c9c3f0ee-0842-49b8-86ff-f92e48d35705-overlay\" anchor=\"action-menu-c9c3f0ee-0842-49b8-86ff-f92e48d35705-button\" align=\"start\" side=\"outside-bottom\" anchor-offset=\"normal\" popover=\"auto\" data-view-component=\"true\">  <div data-view-component=\"true\" class=\"Overlay Overlay--size-auto\">          <div data-view-component=\"true\" class=\"Overlay-body Overlay-body--paddingNone\">          <div data-view-component=\"true\">  <ul aria-labelledby=\"action-menu-c9c3f0ee-0842-49b8-86ff-f92e48d35705-button\" id=\"action-menu-c9c3f0ee-0842-49b8-86ff-f92e48d35705-list\" role=\"menu\" data-view-component=\"true\" class=\"ActionListWrap--inset ActionListWrap\">      <li hidden=\"hidden\" data-menu-item=\"i0code-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-3a4fe606-39c7-44de-89d5-1f94f864a24b\" href=\"/lucidrains/x-transformers\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code\">    <path d=\"m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Code</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i1issues-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-bdeda295-867c-4261-9b60-8233ab6b18df\" href=\"/lucidrains/x-transformers/issues\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-issue-opened\">    <path d=\"M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path><path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Issues</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i2pull-requests-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-81ca6e03-83d2-499a-a83a-ee35e36436e9\" href=\"/lucidrains/x-transformers/pulls\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-git-pull-request\">    <path d=\"M1.5 3.25a2.25 2.25 0 1 1 3 2.122v5.256a2.251 2.251 0 1 1-1.5 0V5.372A2.25 2.25 0 0 1 1.5 3.25Zm5.677-.177L9.573.677A.25.25 0 0 1 10 .854V2.5h1A2.5 2.5 0 0 1 13.5 5v5.628a2.251 2.251 0 1 1-1.5 0V5a1 1 0 0 0-1-1h-1v1.646a.25.25 0 0 1-.427.177L7.177 3.427a.25.25 0 0 1 0-.354ZM3.75 2.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm0 9.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm8.25.75a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Pull requests</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i3actions-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-f75f196d-e1c5-4c7e-b9d9-8d55aa569669\" href=\"/lucidrains/x-transformers/actions\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-play\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Actions</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i4projects-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-49874b40-3346-4f27-a998-a29ca47fca63\" href=\"/lucidrains/x-transformers/projects\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-table\">    <path d=\"M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25ZM6.5 6.5v8h7.75a.25.25 0 0 0 .25-.25V6.5Zm8-1.5V1.75a.25.25 0 0 0-.25-.25H6.5V5Zm-13 1.5v7.75c0 .138.112.25.25.25H5v-8ZM5 5V1.5H1.75a.25.25 0 0 0-.25.25V5Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Projects</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i5security-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-65fbc24d-f59b-407a-8ee4-2aeb660065cd\" href=\"/lucidrains/x-transformers/security\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-shield\">    <path d=\"M7.467.133a1.748 1.748 0 0 1 1.066 0l5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667Zm.61 1.429a.25.25 0 0 0-.153 0l-5.25 1.68a.25.25 0 0 0-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.196.196 0 0 0 .154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.251.251 0 0 0-.174-.237l-5.25-1.68ZM8.75 4.75v3a.75.75 0 0 1-1.5 0v-3a.75.75 0 0 1 1.5 0ZM9 10.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Security</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i6insights-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-313b0eae-d838-4fd5-b09b-c7dcc60fb7b9\" href=\"/lucidrains/x-transformers/pulse\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-graph\">    <path d=\"M1.5 1.75V13.5h13.75a.75.75 0 0 1 0 1.5H.75a.75.75 0 0 1-.75-.75V1.75a.75.75 0 0 1 1.5 0Zm14.28 2.53-5.25 5.25a.75.75 0 0 1-1.06 0L7 7.06 4.28 9.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.25-3.25a.75.75 0 0 1 1.06 0L10 7.94l4.72-4.72a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Insights</span></a>    </li></ul>  </div></div>      </div></anchored-position>  </focus-group></action-menu></div></nav>  </div>  <turbo-frame id=\"repo-content-turbo-frame\" target=\"_top\" data-turbo-action=\"advance\" class=\"\">    <div id=\"repo-content-pjax-container\" class=\"repository-content \" >                <h1 class='sr-only'>lucidrains/x-transformers</h1>  <div class=\"clearfix container-xl px-md-4 px-lg-5 px-3\">    <div>  <div id=\"spoof-warning\" class=\"mt-0 pb-3\" hidden aria-hidden>  <div data-view-component=\"true\" class=\"flash flash-warn mt-0 clearfix\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-alert float-left mt-1\">    <path d=\"M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>      <div class=\"overflow-hidden\">This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.</div>  </div></div>  <include-fragment src=\"/lucidrains/x-transformers/spoofed_commit_check/5fa952a2a489eb4d5e3785fd56c357692dec20e4\" data-test-selector=\"spoofed-commit-check\"></include-fragment>  <div style=\"max-width: 100%\" data-view-component=\"true\" class=\"Layout Layout--flowRow-until-md react-repos-overview-margin Layout--sidebarPosition-end Layout--sidebarPosition-flowRow-end\">  <div data-view-component=\"true\" class=\"Layout-main\">        <script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/react-lib-1fbfc5be2c18.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_octicons-react_dist_index_esm_js-node_modules_primer_react_lib-es-2e8e7c-adc8451a70cf.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Box_Box_js-8f8c5e2a2cbf.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Button_Button_js-67fe00b5266a.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_ActionList_index_js-2dd4d13d3ae6.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Overlay_Overlay_js-node_modules_primer_react_lib-es-fa1130-829932cf63db.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Text_Text_js-node_modules_primer_react_lib-esm_Text-85a14b-236dc9716ad0.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_ActionMenu_ActionMenu_js-eaf74522e470.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_react-router-dom_dist_index_js-3b41341d50fe.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Dialog_js-node_modules_primer_react_lib-esm_Label_L-857e1c-77794958a54a.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_UnderlineNav_index_js-89fa5806aa3c.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_AvatarStack_AvatarStack_js-node_modules_primer_reac-e445e7-175b51e43dcc.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_react-core_create-browser-history_ts-ui_packages_react-core_AppContextProvider_ts-809ab9-bf008735d0bb.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_paths_index_ts-7137b25aa38b.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_ref-selector_RefSelector_tsx-dbbdef4348e2.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_commit-attribution_index_ts-ui_packages_commit-checks-status_index_ts-ui_packages-ffbe33-4c4ddf7d268d.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_react-code-view_components_directory_DirectoryContent_index_ts-app_assets_-1fd1f5-c96303590595.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/repos-overview-523b8f59ec33.js\"></script><react-partial  partial-name=\"repos-overview\"  data-ssr=\"true\">    <script type=\"application/json\" data-target=\"react-partial.embeddedData\">{\"props\":{\"initialPayload\":{\"allShortcutsEnabled\":false,\"path\":\"/\",\"repo\":{\"id\":306980630,\"defaultBranch\":\"main\",\"name\":\"x-transformers\",\"ownerLogin\":\"lucidrains\",\"currentUserCanPush\":false,\"isFork\":false,\"isEmpty\":false,\"createdAt\":\"2020-10-24T22:13:25.000Z\",\"ownerAvatar\":\"https://avatars.githubusercontent.com/u/108653?v=4\",\"public\":true,\"private\":false,\"isOrgOwned\":false},\"currentUser\":null,\"refInfo\":{\"name\":\"main\",\"listCacheKey\":\"v0:1709072178.0\",\"canEdit\":false,\"refType\":\"branch\",\"currentOid\":\"5fa952a2a489eb4d5e3785fd56c357692dec20e4\"},\"tree\":{\"items\":[{\"name\":\".github\",\"path\":\".github\",\"contentType\":\"directory\"},{\"name\":\"examples\",\"path\":\"examples\",\"contentType\":\"directory\"},{\"name\":\"images\",\"path\":\"images\",\"contentType\":\"directory\"},{\"name\":\"x_transformers\",\"path\":\"x_transformers\",\"contentType\":\"directory\"},{\"name\":\".gitignore\",\"path\":\".gitignore\",\"contentType\":\"file\"},{\"name\":\"LICENSE\",\"path\":\"LICENSE\",\"contentType\":\"file\"},{\"name\":\"README.md\",\"path\":\"README.md\",\"contentType\":\"file\"},{\"name\":\"setup.py\",\"path\":\"setup.py\",\"contentType\":\"file\"}],\"templateDirectorySuggestionUrl\":null,\"readme\":null,\"totalCount\":8,\"showBranchInfobar\":false},\"fileTree\":null,\"fileTreeProcessingTime\":null,\"foldersToFetch\":[],\"treeExpanded\":false,\"symbolsExpanded\":false,\"isOverview\":true,\"overview\":{\"banners\":{\"shouldRecommendReadme\":false,\"isPersonalRepo\":false,\"showUseActionBanner\":false,\"actionSlug\":null,\"actionId\":null,\"showProtectBranchBanner\":false,\"recentlyTouchedDataChannel\":null,\"publishBannersInfo\":{\"dismissActionNoticePath\":\"/settings/dismiss-notice/publish_action_from_repo\",\"releasePath\":\"/lucidrains/x-transformers/releases/new?marketplace=true\",\"showPublishActionBanner\":false},\"interactionLimitBanner\":null,\"showInvitationBanner\":false,\"inviterName\":null},\"codeButton\":{\"contactPath\":\"/contact\",\"isEnterprise\":false,\"local\":{\"protocolInfo\":{\"httpAvailable\":true,\"sshAvailable\":null,\"httpUrl\":\"https://github.com/lucidrains/x-transformers.git\",\"showCloneWarning\":null,\"sshUrl\":null,\"sshCertificatesRequired\":null,\"sshCertificatesAvailable\":null,\"ghCliUrl\":\"gh repo clone lucidrains/x-transformers\",\"defaultProtocol\":\"http\",\"newSshKeyUrl\":\"/settings/ssh/new\",\"setProtocolPath\":\"/users/set_protocol\"},\"platformInfo\":{\"cloneUrl\":\"https://desktop.github.com\",\"showVisualStudioCloneButton\":false,\"visualStudioCloneUrl\":\"https://windows.github.com\",\"showXcodeCloneButton\":false,\"xcodeCloneUrl\":\"https://developer.apple.com\",\"zipballUrl\":\"/lucidrains/x-transformers/archive/refs/heads/main.zip\"}},\"newCodespacePath\":\"/codespaces/new?hide_repo_select=true\\u0026repo=306980630\"},\"popovers\":{\"rename\":null,\"renamedParentRepo\":null},\"commitCount\":\"493\",\"overviewFiles\":[{\"displayName\":\"README.md\",\"repoName\":\"x-transformers\",\"refName\":\"main\",\"path\":\"README.md\",\"preferredFileType\":\"readme\",\"tabName\":\"README\",\"richText\":\"\\u003carticle class=\\\"markdown-body entry-content container-lg\\\" itemprop=\\\"text\\\"\\u003e\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003ex-transformers\\u003c/h2\\u003e\\u003ca id=\\\"user-content-x-transformers\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: x-transformers\\\" href=\\\"#x-transformers\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://badge.fury.io/py/x-transformers\\\" rel=\\\"nofollow\\\"\\u003e\\u003cimg src=\\\"https://camo.githubusercontent.com/6978d11285aa67ea58e35a57f96d02e2bf03aa6912d2aa6459e84be7d06bb999/68747470733a2f2f62616467652e667572792e696f2f70792f782d7472616e73666f726d6572732e737667\\\" alt=\\\"PyPI version\\\" data-canonical-src=\\\"https://badge.fury.io/py/x-transformers.svg\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eA concise but fully-featured transformer, complete with a set of promising e\\u003cstrong\\u003ex\\u003c/strong\\u003eperimental features from various papers.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eInstall\\u003c/h2\\u003e\\u003ca id=\\\"user-content-install\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Install\\\" href=\\\"#install\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"$ pip install x-transformers\\\"\\u003e\\u003cpre\\u003e$ pip install x-transformers\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eUsage\\u003c/h2\\u003e\\u003ca id=\\\"user-content-usage\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Usage\\\" href=\\\"#usage\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eFull encoder / decoder\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import XTransformer\\n\\nmodel = XTransformer(\\n    dim = 512,\\n    enc_num_tokens = 256,\\n    enc_depth = 6,\\n    enc_heads = 8,\\n    enc_max_seq_len = 1024,\\n    dec_num_tokens = 256,\\n    dec_depth = 6,\\n    dec_heads = 8,\\n    dec_max_seq_len = 1024,\\n    tie_token_emb = True      # tie embeddings of encoder and decoder\\n)\\n\\nsrc = torch.randint(0, 256, (1, 1024))\\nsrc_mask = torch.ones_like(src).bool()\\ntgt = torch.randint(0, 256, (1, 1024))\\n\\nloss = model(src, tgt, mask = src_mask) # (1, 1024, 512)\\nloss.backward()\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eXTransformer\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eXTransformer\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eenc_num_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eenc_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eenc_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eenc_max_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edec_num_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edec_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edec_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edec_max_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003etie_token_emb\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e      \\u003cspan class=\\\"pl-c\\\"\\u003e# tie embeddings of encoder and decoder\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003esrc\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-s1\\\"\\u003esrc_mask\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eones_like\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003esrc\\u003c/span\\u003e).\\u003cspan class=\\\"pl-en\\\"\\u003ebool\\u003c/span\\u003e()\\n\\u003cspan class=\\\"pl-s1\\\"\\u003etgt\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e))\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003esrc\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003etgt\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003emask\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003esrc_mask\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1024, 512)\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ebackward\\u003c/span\\u003e()\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eDecoder-only (GPT-like)\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 12,\\n        heads = 8\\n    )\\n).cuda()\\n\\nx = torch.randint(0, 256, (1, 1024)).cuda()\\n\\nmodel(x) # (1, 1024, 20000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e12\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e\\n    )\\n).\\u003cspan class=\\\"pl-en\\\"\\u003ecuda\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e)).\\u003cspan class=\\\"pl-en\\\"\\u003ecuda\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1024, 20000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eGPT3 would be approximately the following (but you wouldn't be able to run it anyways)\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"\\ngpt3 = TransformerWrapper(\\n    num_tokens = 50000,\\n    max_seq_len = 2048,\\n    attn_layers = Decoder(\\n        dim = 12288,\\n        depth = 96,\\n        heads = 96,\\n        attn_dim_head = 128\\n    )\\n).cuda()\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-s1\\\"\\u003egpt3\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e50000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e12288\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e96\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e96\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_dim_head\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e\\n    )\\n).\\u003cspan class=\\\"pl-en\\\"\\u003ecuda\\u003c/span\\u003e()\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eEncoder-only (BERT-like)\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Encoder(\\n        dim = 512,\\n        depth = 12,\\n        heads = 8\\n    )\\n).cuda()\\n\\nx = torch.randint(0, 256, (1, 1024)).cuda()\\nmask = torch.ones_like(x).bool()\\n\\nmodel(x, mask = mask) # (1, 1024, 20000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e12\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e\\n    )\\n).\\u003cspan class=\\\"pl-en\\\"\\u003ecuda\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e)).\\u003cspan class=\\\"pl-en\\\"\\u003ecuda\\u003c/span\\u003e()\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emask\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eones_like\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e).\\u003cspan class=\\\"pl-en\\\"\\u003ebool\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003emask\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003emask\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1024, 20000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eState of the art image classification (\\u003ca href=\\\"https://arxiv.org/abs/2205.01580\\\" rel=\\\"nofollow\\\"\\u003eSimpleViT\\u003c/a\\u003e)\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import ViTransformerWrapper, Encoder\\n\\nmodel = ViTransformerWrapper(\\n    image_size = 256,\\n    patch_size = 32,\\n    num_classes = 1000,\\n    attn_layers = Encoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n    )\\n)\\n\\nimg = torch.randn(1, 3, 256, 256)\\nmodel(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eImage -\\u0026gt; caption\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import ViTransformerWrapper, TransformerWrapper, Encoder, Decoder\\n\\nencoder = ViTransformerWrapper(\\n    image_size = 256,\\n    patch_size = 32,\\n    attn_layers = Encoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8\\n    )\\n)\\n\\ndecoder = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        cross_attend = True\\n    )\\n)\\n\\nimg = torch.randn(1, 3, 256, 256)\\ncaption = torch.randint(0, 20000, (1, 1024))\\n\\nencoded = encoder(img, return_embeddings = True)\\ndecoder(caption, context = encoded) # (1, 1024, 20000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eencoder\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003edecoder\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003ecross_attend\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ecaption\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e))\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eencoded\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003eencoder\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ereturn_embeddings\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-en\\\"\\u003edecoder\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ecaption\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003econtext\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eencoded\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1024, 20000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2209.06794\\\" rel=\\\"nofollow\\\"\\u003ePaLI\\u003c/a\\u003e, state of the art language-vision model\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import ViTransformerWrapper, XTransformer, Encoder\\n\\n# PaLI composes of\\n# 1. vision transformer (ViTransformerWrapper) +\\n# 2. encoder-decoder transformer (XTransformer)\\n\\nvit = ViTransformerWrapper(\\n    image_size = 256,\\n    patch_size = 32,\\n    attn_layers = Encoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8\\n    )\\n)\\n\\npali = XTransformer(\\n    dim = 512,\\n    enc_num_tokens = 256,\\n    enc_depth = 6,\\n    enc_heads = 8,\\n    enc_max_seq_len = 1024,\\n    dec_num_tokens = 256,\\n    dec_depth = 6,\\n    dec_heads = 8,\\n    dec_max_seq_len = 1024\\n)\\n\\n# training data\\n\\nimg = torch.randn(1, 3, 256, 256)               # images\\nprompt = torch.randint(0, 256, (1, 1024))       # prompt\\nprompt_mask = torch.ones(1, 1024).bool()        # prompt text mask\\noutput_text = torch.randint(0, 256, (1, 1024))  # target output text\\n\\n# train\\n\\nimg_embeds = vit(\\n    img,\\n    return_embeddings = True\\n)\\n\\nloss = pali(\\n    prompt,\\n    output_text,\\n    mask = prompt_mask,\\n    src_prepend_embeds = img_embeds             # will preprend image embeddings to encoder text embeddings before attention\\n)\\n\\nloss.backward()\\n\\n# do the above for many steps on a 17B parameter model\\n# attention is all you need\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eXTransformer\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# PaLI composes of\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# 1. vision transformer (ViTransformerWrapper) +\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# 2. encoder-decoder transformer (XTransformer)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003evit\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epali\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eXTransformer\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eenc_num_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eenc_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eenc_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eenc_max_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edec_num_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edec_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edec_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edec_max_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# training data\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)               \\u003cspan class=\\\"pl-c\\\"\\u003e# images\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eprompt\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e))       \\u003cspan class=\\\"pl-c\\\"\\u003e# prompt\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eprompt_mask\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eones\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e).\\u003cspan class=\\\"pl-en\\\"\\u003ebool\\u003c/span\\u003e()        \\u003cspan class=\\\"pl-c\\\"\\u003e# prompt text mask\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eoutput_text\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e))  \\u003cspan class=\\\"pl-c\\\"\\u003e# target output text\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# train\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg_embeds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003evit\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ereturn_embeddings\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003epali\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eprompt\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eoutput_text\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emask\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eprompt_mask\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003esrc_prepend_embeds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eimg_embeds\\u003c/span\\u003e             \\u003cspan class=\\\"pl-c\\\"\\u003e# will preprend image embeddings to encoder text embeddings before attention\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ebackward\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# do the above for many steps on a 17B parameter model\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# attention is all you need\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eDropouts\\u003c/h2\\u003e\\u003ca id=\\\"user-content-dropouts\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Dropouts\\\" href=\\\"#dropouts\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    emb_dropout = 0.1,         # dropout after embedding\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        layer_dropout = 0.1,   # stochastic depth - dropout entire layer\\n        attn_dropout = 0.1,    # dropout post-attention\\n        ff_dropout = 0.1       # feedforward dropout\\n    )\\n)\\n\\nx = torch.randint(0, 20000, (1, 1024))\\nmodel(x)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,         \\u003cspan class=\\\"pl-c\\\"\\u003e# dropout after embedding\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003elayer_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,   \\u003cspan class=\\\"pl-c\\\"\\u003e# stochastic depth - dropout entire layer\\u003c/span\\u003e\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,    \\u003cspan class=\\\"pl-c\\\"\\u003e# dropout post-attention\\u003c/span\\u003e\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eff_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e       \\u003cspan class=\\\"pl-c\\\"\\u003e# feedforward dropout\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eFeatures\\u003c/h2\\u003e\\u003ca id=\\\"user-content-features\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Features\\\" href=\\\"#features\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eFlash Attention\\u003c/h3\\u003e\\u003ca id=\\\"user-content-flash-attention\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Flash Attention\\\" href=\\\"#flash-attention\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/flash-attention.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/flash-attention.png\\\" width=\\\"500px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eWhat originally started off as \\u003ca href=\\\"https://arxiv.org/abs/2112.05682\\\" rel=\\\"nofollow\\\"\\u003ea short paper\\u003c/a\\u003e from Markus Rabe culminated as a practical fused attention CUDA kernel, named \\u003ca href=\\\"https://arxiv.org/abs/2205.14135\\\" rel=\\\"nofollow\\\"\\u003eFlash Attention\\u003c/a\\u003e by \\u003ca href=\\\"https://tridao.me/\\\" rel=\\\"nofollow\\\"\\u003eTri Dao\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe technique processes the attention matrix in tiles, only keeping track of the running softmax and exponentiated weighted sums. By recomputing on the backwards pass in a tiled fashion, one is able to keep the memory linear with respect to sequence length. This allows a lot of recent models  to be able to reach for longer context lengths without worrying about the memory bottleneck.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eOther engineering decisions made by Tri Dao led to its enormous success, namely minimizing HBM accesses so that both the forwards and backwards outperform naive attention. In other words, flash attention is not only more memory efficient, but faster as well, making it a necessity for training transformers.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eMetaAI has recently added the ability to use \\u003ca href=\\\"https://github.com/hazyresearch/flash-attention\\\"\\u003eTri Dao's CUDA kernel\\u003c/a\\u003e through the \\u003ca href=\\\"https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\\\" rel=\\\"nofollow\\\"\\u003escaled_dot_product_attention\\u003c/a\\u003e function in Pytorch 2.0. (They also have a \\u003ccode\\u003emem_efficient\\u003c/code\\u003e attention, which is identical to flash attention design, just that the tiles are traversed differently)\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\\\" rel=\\\"nofollow\\\"\\u003eLlama\\u003c/a\\u003e was trained using Flash Attention. The only reason to avoid it is if you require operating on the attention matrix (dynamic positional bias, talking heads, residual attention).\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use it in this repository by setting \\u003ccode\\u003eattn_flash\\u003c/code\\u003e to \\u003ccode\\u003eTrue\\u003c/code\\u003e and enjoy the immediate memory savings and increase in speed.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eex.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        attn_flash = True # just set this to True if you have pytorch 2.0 installed\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_flash\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e \\u003cspan class=\\\"pl-c\\\"\\u003e# just set this to True if you have pytorch 2.0 installed\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eAugmenting Self-attention with Persistent Memory\\u003c/h3\\u003e\\u003ca id=\\\"user-content-augmenting-self-attention-with-persistent-memory\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Augmenting Self-attention with Persistent Memory\\\" href=\\\"#augmenting-self-attention-with-persistent-memory\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/all-attention.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/all-attention.png\\\" width=\\\"500px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/1907.01470\\\" rel=\\\"nofollow\\\"\\u003ehttps://arxiv.org/abs/1907.01470\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eProposes adding learned memory key / values prior to attention. They were able to remove feedforwards altogether and attain similar performance to the original transformers. I have found that keeping the feedforwards and adding the memory key / values leads to even better performance.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"from x_transformers import Decoder, Encoder\\n\\nenc = Encoder(\\n    dim = 512,\\n    depth = 6,\\n    heads = 8,\\n    attn_num_mem_kv = 16 # 16 memory key / values\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eenc\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_num_mem_kv\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e \\u003cspan class=\\\"pl-c\\\"\\u003e# 16 memory key / values\\u003c/span\\u003e\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eMemory Transformers\\u003c/h3\\u003e\\u003ca id=\\\"user-content-memory-transformers\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Memory Transformers\\\" href=\\\"#memory-transformers\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/memory-transformer.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/memory-transformer.png\\\" width=\\\"500px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2006.11527\\\" rel=\\\"nofollow\\\"\\u003ehttps://arxiv.org/abs/2006.11527\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eProposes adding learned tokens, akin to CLS tokens, named memory tokens, that is passed through the attention layers alongside the input tokens. This setting is compatible with both encoder and decoder training.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    num_memory_tokens = 20, # 20 memory tokens\\n    attn_layers = Encoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_memory_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c\\\"\\u003e# 20 memory tokens\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate: MetaAI researchers \\u003ca href=\\\"https://arxiv.org/abs/2309.16588\\\" rel=\\\"nofollow\\\"\\u003ehave found\\u003c/a\\u003e that adding memory tokens (they call them register tokens), alleviates outliers (which is suspected now to be a pathology of attention networks unable to \\u003ca href=\\\"https://arxiv.org/abs/2306.12929\\\" rel=\\\"nofollow\\\"\\u003eattend to nothing\\u003c/a\\u003e).\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eTransformers Without Tears\\u003c/h3\\u003e\\u003ca id=\\\"user-content-transformers-without-tears\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Transformers Without Tears\\\" href=\\\"#transformers-without-tears\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/scalenorm.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/scalenorm.png\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/1910.05895\\\" rel=\\\"nofollow\\\"\\u003ehttps://arxiv.org/abs/1910.05895\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThey experiment with alternatives to Layer normalization and found one that is both effective and simpler. Researchers have shared with me this leads to faster convergence.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        use_scalenorm = True # set to True to use for all layers\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003euse_scalenorm\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e \\u003cspan class=\\\"pl-c\\\"\\u003e# set to True to use for all layers\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can also use the l2 normalized embeddings proposed as part of \\u003ccode\\u003efixnorm\\u003c/code\\u003e. I have found it leads to improved convergence, when paired with small initialization (proposed by \\u003ca href=\\\"https://github.com/BlinkDL\\\"\\u003eBlinkDL\\u003c/a\\u003e). The small initialization will be taken care of as long as \\u003ccode\\u003el2norm_embed\\u003c/code\\u003e is set to \\u003ccode\\u003eTrue\\u003c/code\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    l2norm_embed = True,    # set this to True for l2 normalized embedding + small init\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003el2norm_embed\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e,    \\u003cspan class=\\\"pl-c\\\"\\u003e# set this to True for l2 normalized embedding + small init\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eAlong the same lines of l2 normalized embeddings, Huggingface's \\u003ca href=\\\"https://huggingface.co/bigscience/bloom\\\" rel=\\\"nofollow\\\"\\u003e175B parameter BLOOM\\u003c/a\\u003e also places a layernorm right after the embeddings and just before the tokens enter the attention layers. This was corroborated by Yandex's \\u003ca href=\\\"https://github.com/yandex/YaLM-100B\\\"\\u003e100B parameter YaLM\\u003c/a\\u003e to stabilize training.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eIt is recommended you either have either \\u003ccode\\u003el2norm_embed\\u003c/code\\u003e or \\u003ccode\\u003epost_emb_norm\\u003c/code\\u003e set to \\u003ccode\\u003eTrue\\u003c/code\\u003e but not both, as they probably serve the same purpose.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    post_emb_norm = True,    # set this to True to layernorm summed token + pos embeddings\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epost_emb_norm\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e,    \\u003cspan class=\\\"pl-c\\\"\\u003e# set this to True to layernorm summed token + pos embeddings\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eRoot Mean Square Layer Normalization\\u003c/h3\\u003e\\u003ca id=\\\"user-content-root-mean-square-layer-normalization\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Root Mean Square Layer Normalization\\\" href=\\\"#root-mean-square-layer-normalization\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/1910.07467\\\" rel=\\\"nofollow\\\"\\u003ehttps://arxiv.org/abs/1910.07467\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe authors propose to replace layer normalization with a simpler alternative, without mean centering and the learned bias. An investigative paper found this to be the \\u003ca href=\\\"https://arxiv.org/abs/2102.11972\\\" rel=\\\"nofollow\\\"\\u003ebest performing normalization variant\\u003c/a\\u003e. It was also used in Deepmind's latest large language models, \\u003ca href=\\\"https://deepmind.com/research/publications/2021/improving-language-models-by-retrieving-from-trillions-of-tokens\\\" rel=\\\"nofollow\\\"\\u003eRetro\\u003c/a\\u003e and \\u003ca href=\\\"https://arxiv.org/abs/2112.11446\\\" rel=\\\"nofollow\\\"\\u003eGopher\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        use_rmsnorm = True # set to true to use for all layers\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003euse_rmsnorm\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e \\u003cspan class=\\\"pl-c\\\"\\u003e# set to true to use for all layers\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003cem\\u003eJuly 2023\\u003c/em\\u003e \\u003ca href=\\\"https://arxiv.org/abs/2307.14995\\\" rel=\\\"nofollow\\\"\\u003eA linear attention paper\\u003c/a\\u003e has experiments to show that removing the learned multiplicative gamma led to no performance degradation. This simplifies the RMS normalization to a satisfying \\u003ccode\\u003el2norm(x) * sqrt(dim)\\u003c/code\\u003e.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        use_simple_rmsnorm = True # set to true to use for all layers\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003euse_simple_rmsnorm\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e \\u003cspan class=\\\"pl-c\\\"\\u003e# set to true to use for all layers\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eGLU Variants Improve Transformer\\u003c/h3\\u003e\\u003ca id=\\\"user-content-glu-variants-improve-transformer\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: GLU Variants Improve Transformer\\\" href=\\\"#glu-variants-improve-transformer\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/ffglu.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/ffglu.png\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2002.05202\\\" rel=\\\"nofollow\\\"\\u003ehttps://arxiv.org/abs/2002.05202\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eNoam Shazeer paper that explores gating in the feedforward, finding that simple gating with GELU leads to significant improvements. This variant also showed up in the latest mT5 architecture. You should always turn this on (I may eventually turn it on by default).\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        ff_glu = True # set to true to use for all feedforwards\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eff_glu\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e \\u003cspan class=\\\"pl-c\\\"\\u003e# set to true to use for all feedforwards\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe \\u003ca href=\\\"https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\\\" rel=\\\"nofollow\\\"\\u003ePaLM\\u003c/a\\u003e language model also chose to use the Swish GLU variant. You can turn this on by setting two flags\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        ff_swish = True, # set this to True\\n        ff_glu = True    # set to true to use for all feedforwards\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eff_swish\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c\\\"\\u003e# set this to True\\u003c/span\\u003e\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eff_glu\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e    \\u003cspan class=\\\"pl-c\\\"\\u003e# set to true to use for all feedforwards\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eNo Bias in Feedforward\\u003c/h3\\u003e\\u003ca id=\\\"user-content-no-bias-in-feedforward\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: No Bias in Feedforward\\\" href=\\\"#no-bias-in-feedforward\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eStarting with \\u003ca href=\\\"https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\\\" rel=\\\"nofollow\\\"\\u003ePaLM\\u003c/a\\u003e, there begun a trend to remove biases from the transformer all together. \\u003ca href=\\\"https://github.com/borisdayma\\\"\\u003eBoris Dayma\\u003c/a\\u003e has run a number of experiments that showed removing biases from feedforwards led to increased throughput without any loss of accuracy. This was corroborated by \\u003ca href=\\\"https://arxiv.org/abs/2212.14034\\\" rel=\\\"nofollow\\\"\\u003eyet another paper\\u003c/a\\u003e investigating transformer architecture variants.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can turn off the feedforward bias as follows\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        ff_no_bias = True  # set this to True\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eff_no_bias\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c\\\"\\u003e# set this to True\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eReLU\u00b2\\u003c/h3\\u003e\\u003ca id=\\\"user-content-relu\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: ReLU\u00b2\\\" href=\\\"#relu\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2109.08668\\\" rel=\\\"nofollow\\\"\\u003ehttps://arxiv.org/abs/2109.08668\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis paper used neural architecture search and found an activation, Relu Squared, that is both simpler and performs better than GELU, in the autoregressive language model setting. I have confirmed this in my independent experiments. However, if one were using the GLU variant from above, GELU still performs better. Pending further corroboration.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        ff_relu_squared = True\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eff_relu_squared\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eExplicit Sparse Transformer: Concentrated Attention Through Explicit Selection\\u003c/h3\\u003e\\u003ca id=\\\"user-content-explicit-sparse-transformer-concentrated-attention-through-explicit-selection\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection\\\" href=\\\"#explicit-sparse-transformer-concentrated-attention-through-explicit-selection\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/topk-attention.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/topk-attention.png\\\" width=\\\"500px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/1912.11637\\\" rel=\\\"nofollow\\\"\\u003ehttps://arxiv.org/abs/1912.11637\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis paper proposes an efficient way to sparsify attention by zeroing all dot-product query/key values not within the top k values. The show that this cheap method was as effective as other more expensive operations like sparsemax or entmax15. This technique comes with the cost of an extra hyperparameter (the top k values to keep). The paper recommends a value of \\u003ccode\\u003ek = 8\\u003c/code\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        attn_sparse_topk = 8 # keep only the top 8 values before attention (softmax)\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_sparse_topk\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e \\u003cspan class=\\\"pl-c\\\"\\u003e# keep only the top 8 values before attention (softmax)\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eTalking-Heads Attention\\u003c/h3\\u003e\\u003ca id=\\\"user-content-talking-heads-attention\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Talking-Heads Attention\\\" href=\\\"#talking-heads-attention\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/talking-heads.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/talking-heads.png\\\" width=\\\"500px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2003.02436\\\" rel=\\\"nofollow\\\"\\u003ehttps://arxiv.org/abs/2003.02436\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eA Noam Shazeer paper that proposes mixing information between heads pre and post attention (softmax). This comes with the cost of extra memory and compute.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        attn_talking_heads = True  # turn on information exchange between attention heads\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_talking_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c\\\"\\u003e# turn on information exchange between attention heads\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eOne Write-Head Is All You Need\\u003c/h3\\u003e\\u003ca id=\\\"user-content-one-write-head-is-all-you-need\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: One Write-Head Is All You Need\\\" href=\\\"#one-write-head-is-all-you-need\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/1911.02150\\\" rel=\\\"nofollow\\\"\\u003ehttps://arxiv.org/abs/1911.02150\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYet another Noam Shazeer paper (he's a legend) that proposes to only have one head for the key / values, but multi-headed queries. This paper was largely ignored for a while, but recently validated at scale in \\u003ca href=\\\"https://arxiv.org/abs/2203.07814\\\" rel=\\\"nofollow\\\"\\u003eAlphaCode\\u003c/a\\u003e as well as \\u003ca href=\\\"https://arxiv.org/abs/2204.02311\\\" rel=\\\"nofollow\\\"\\u003ePaLM\\u003c/a\\u003e. It has the property of being memory efficient when decoding extremely large language models. You can use it with one keyword argument as shown below.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        attn_one_kv_head = True\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_one_kv_head\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis has been further generalized in \\u003ca href=\\\"https://arxiv.org/abs/2305.13245\\\" rel=\\\"nofollow\\\"\\u003ea recent paper\\u003c/a\\u003e to allow for groups of query heads to attend to a single key / value head. You can use this by specifying the \\u003ccode\\u003eattn_kv_heads\\u003c/code\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 12,\\n        heads = 8,\\n        attn_kv_heads = 2 # say you want 4 query heads to attend to 1 key / value head\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e12\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_kv_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e \\u003cspan class=\\\"pl-c\\\"\\u003e# say you want 4 query heads to attend to 1 key / value head\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eAttention on Attention for Image Captioning\\u003c/h3\\u003e\\u003ca id=\\\"user-content-attention-on-attention-for-image-captioning\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Attention on Attention for Image Captioning\\\" href=\\\"#attention-on-attention-for-image-captioning\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/attention-on-attention.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/attention-on-attention.png\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/1908.06954\\\" rel=\\\"nofollow\\\"\\u003ehttps://arxiv.org/abs/1908.06954\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis paper proposes to add a gated linear unit at the end of the attention layer, further gated by the original queries. Although this is not widely used outside of visual question / answering, I suspect it should lead to improvements after seeing the success of the feedforward GLU variant.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate: After some experimentation, I found this variant actually performs worse, but if it were to be modified to not concatenate the queries before gating, it performs much better. That is what we will be using in this repository.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Encoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        attn_on_attn = True  # gate output of attention layer, by queries\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_on_attn\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c\\\"\\u003e# gate output of attention layer, by queries\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eIntra-attention Gating on Values\\u003c/h3\\u003e\\u003ca id=\\\"user-content-intra-attention-gating-on-values\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Intra-attention Gating on Values\\\" href=\\\"#intra-attention-gating-on-values\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/gate_values.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/gate_values.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://github.com/deepmind/alphafold\\\"\\u003eAlphafold2\\u003c/a\\u003e had a peculiar variant of attention where they gate the aggregated values with the input, presumably to have the block have more control over the update.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eA quick test shows a small but noticeable improvement, on about the same order as attention on attention.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Encoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        attn_gate_values = True  # gate aggregated values with the input\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_gate_values\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c\\\"\\u003e# gate aggregated values with the input\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eImproving Transformer Models by Reordering their Sublayers\\u003c/h3\\u003e\\u003ca id=\\\"user-content-improving-transformer-models-by-reordering-their-sublayers\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Improving Transformer Models by Reordering their Sublayers\\\" href=\\\"#improving-transformer-models-by-reordering-their-sublayers\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/sandwich.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/sandwich.png\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/sandwich-2.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/sandwich-2.png\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/1911.03864\\\" rel=\\\"nofollow\\\"\\u003ehttps://arxiv.org/abs/1911.03864\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis paper proposes to break from the normal fixed pattern of alternating attention and feedforwards, but to have blocks of only attention at the beginning followed by blocks of feedforwards at the end. This was further corroborated by a paper by Nvidia that reduces the number of attention layers to be 1/3rd of the feedforwards without loss in performance.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe amount of interleaving is controlled by a \\\"sandwich coefficient\\\", which they found to be optimal at a value of \\u003ccode\\u003e6\\u003c/code\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can experiment with this feature as shown below\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Encoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        sandwich_coef = 6  # interleave attention and feedforwards with sandwich coefficient of 6\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003esandwich_coef\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c\\\"\\u003e# interleave attention and feedforwards with sandwich coefficient of 6\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eUnderstanding and Improving Transformer From a Multi-Particle Dynamic System Point of View\\u003c/h3\\u003e\\u003ca id=\\\"user-content-understanding-and-improving-transformer-from-a-multi-particle-dynamic-system-point-of-view\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View\\\" href=\\\"#understanding-and-improving-transformer-from-a-multi-particle-dynamic-system-point-of-view\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/macaron-1.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/macaron-1.png\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/macaron-2.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/macaron-2.png\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/1906.02762\\\" rel=\\\"nofollow\\\"\\u003ehttps://arxiv.org/abs/1906.02762\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe authors propose to view the success of transformers from a dynamical systems point of view, and then proposes an improvement based on mathematics of that POV. Specifically, they propose to place the attention layer in between two feedforward layers. This was adopted by a paper using transformers for speech recognition, the \\u003ca href=\\\"https://arxiv.org/abs/2005.08100\\\" rel=\\\"nofollow\\\"\\u003eConformer\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Encoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        macaron = True  # use macaron configuration\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003emacaron\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c\\\"\\u003e# use macaron configuration\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eT5's Simplified Relative Positional Encoding\\u003c/h3\\u003e\\u003ca id=\\\"user-content-t5s-simplified-relative-positional-encoding\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: T5's Simplified Relative Positional Encoding\\\" href=\\\"#t5s-simplified-relative-positional-encoding\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/1910.10683\\\" rel=\\\"nofollow\\\"\\u003ehttps://arxiv.org/abs/1910.10683\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eT5 is one of the most successful encoder / decoder transformer architectures trained to date. They invented a new simplified relative positional encoding based on learned bias values that are added to the attention matrix pre-softmax. This bias is shared and injected into each attention layer. I have decided to include this because it offers a cheap way to have relative positional encoding (superior to absolute positional), and I have read papers that suggest having positional encoding added to each layer (vs only before the first) is beneficial.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        rel_pos_bias = True  # adds relative positional bias to all attention layers, a la T5\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003erel_pos_bias\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c\\\"\\u003e# adds relative positional bias to all attention layers, a la T5\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eResidual Attention\\u003c/h3\\u003e\\u003ca id=\\\"user-content-residual-attention\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Residual Attention\\\" href=\\\"#residual-attention\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/residual_attn.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/residual_attn.png\\\" width=\\\"500px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2012.11747\\\" rel=\\\"nofollow\\\"\\u003ehttps://arxiv.org/abs/2012.11747\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis paper from Google proposes residualizing the pre-attention scores across all layers. At the cost of no extra parameters, they show improvement on top of regular attention networks. If you turn on this setting, be aware that the best results in the paper used post-normalization, in which case a learning warmup will be needed. The authors also reported that they could use a higher learning rate and get even better gains in the same amount of steps. (In the paper they use \\u003ccode\\u003e2e-4\\u003c/code\\u003e vs \\u003ccode\\u003e1e-4\\u003c/code\\u003e for vanilla transformer)\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Encoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Encoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        pre_norm = False,       # in the paper, residual attention had best results with post-layernorm\\n        residual_attn = True    # add residual attention\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003epre_norm\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eFalse\\u003c/span\\u003e,       \\u003cspan class=\\\"pl-c\\\"\\u003e# in the paper, residual attention had best results with post-layernorm\\u003c/span\\u003e\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eresidual_attn\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e    \\u003cspan class=\\\"pl-c\\\"\\u003e# add residual attention\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eI also tried residualizing cross attention and may have noticed an improvement in convergence. You can try it by setting the \\u003ccode\\u003ecross_residual_attn\\u003c/code\\u003e keyword to \\u003ccode\\u003eTrue\\u003c/code\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import XTransformer\\n\\nmodel = XTransformer(\\n    dim = 512,\\n    enc_num_tokens = 256,\\n    enc_depth = 6,\\n    enc_heads = 8,\\n    enc_max_seq_len = 1024,\\n    dec_num_tokens = 256,\\n    dec_depth = 6,\\n    dec_heads = 8,\\n    dec_max_seq_len = 1024,\\n    dec_cross_residual_attn = True     # residualize cross attention\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eXTransformer\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eXTransformer\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eenc_num_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eenc_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eenc_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eenc_max_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edec_num_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edec_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edec_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edec_max_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edec_cross_residual_attn\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e     \\u003cspan class=\\\"pl-c\\\"\\u003e# residualize cross attention\\u003c/span\\u003e\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eTransformer-XL recurrence\\u003c/h3\\u003e\\u003ca id=\\\"user-content-transformer-xl-recurrence\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Transformer-XL recurrence\\\" href=\\\"#transformer-xl-recurrence\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can also do Transformer-XL recurrence, by simply passing in a \\u003ccode\\u003emax_mem_len\\u003c/code\\u003e in the \\u003ccode\\u003eTransformerWrapper\\u003c/code\\u003e class, and then making sure your \\u003ccode\\u003eDecoder\\u003c/code\\u003e has \\u003ccode\\u003erel_pos_bias\\u003c/code\\u003e (or \\u003ccode\\u003erotary_pos_emb\\u003c/code\\u003e) set to \\u003ccode\\u003eTrue\\u003c/code\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThen, you can retrieve the memories at each step with the \\u003ccode\\u003ereturn_mems\\u003c/code\\u003e keyword and pass it to the next iteration.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel_xl = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 512,\\n    max_mem_len = 2048,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        rel_pos_bias = True\\n    )\\n)\\n\\nseg1 = torch.randint(0, 20000, (1, 512))\\nseg2 = torch.randint(0, 20000, (1, 512))\\nseg3 = torch.randint(0, 20000, (1, 512))\\n\\nlogits1, mems1  = model_xl(seg1, return_mems = True)\\nlogits2, mems2  = model_xl(seg2, mems = mems1, return_mems = True)\\nlogits3, mems3  = model_xl(seg3, mems = mems2, return_mems = True)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel_xl\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_mem_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003erel_pos_bias\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eseg1\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eseg2\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eseg3\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e))\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003elogits1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003emems1\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emodel_xl\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eseg1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ereturn_mems\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003elogits2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003emems2\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emodel_xl\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eseg2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003emems\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003emems1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ereturn_mems\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003elogits3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003emems3\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emodel_xl\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eseg3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003emems\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003emems2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ereturn_mems\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eSetting up the logic for training and sampling from transformer xl can be a bit overwhelming. This repository offers a simple wrapper that should make this easy, with the \\u003ccode\\u003eXLAutoregressiveWrapper\\u003c/code\\u003e.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"# pass in the above model_xl\\n\\nxl_wrapper = XLAutoregressiveWrapper(model_xl)\\n\\nseg = torch.randint(0, 20000, (1, 4096)).cuda()  # sequence exceeding max length, automatically segmented and memory managed\\n\\nloss = xl_wrapper(seg)\\nloss.backward()\\n\\n# then, after much training\\n\\nprime = seg[:, :1024]   # if prime exceeds max length, memory will be caught up before generating\\n\\ngenerated = xl_wrapper.generate(prime, 4096)  # (1, 4096)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-c\\\"\\u003e# pass in the above model_xl\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003exl_wrapper\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eXLAutoregressiveWrapper\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003emodel_xl\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eseg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e4096\\u003c/span\\u003e)).\\u003cspan class=\\\"pl-en\\\"\\u003ecuda\\u003c/span\\u003e()  \\u003cspan class=\\\"pl-c\\\"\\u003e# sequence exceeding max length, automatically segmented and memory managed\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003exl_wrapper\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eseg\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ebackward\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# then, after much training\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eprime\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eseg\\u003c/span\\u003e[:, :\\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e]   \\u003cspan class=\\\"pl-c\\\"\\u003e# if prime exceeds max length, memory will be caught up before generating\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003egenerated\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003exl_wrapper\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003egenerate\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eprime\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e4096\\u003c/span\\u003e)  \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 4096)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eEnhanced recurrence\\u003c/h3\\u003e\\u003ca id=\\\"user-content-enhanced-recurrence\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Enhanced recurrence\\\" href=\\\"#enhanced-recurrence\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/enhanced-recurrence.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/enhanced-recurrence.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2012.15688\\\" rel=\\\"nofollow\\\"\\u003eThis paper\\u003c/a\\u003e proposes a simple technique to enhance the range of Transformer-XL. They simply route the memory segment of a layer to the layer below it, for the next recurrent step. You can enable this by setting \\u003ccode\\u003eshift_mem_down = 1\\u003c/code\\u003e. You can also shift down arbitrary number of layers by setting this value to \\u003ccode\\u003e\\u0026gt; 1\\u003c/code\\u003e.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel_xl = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 512,\\n    max_mem_len = 2048,\\n    shift_mem_down = 1,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        rotary_pos_emb = True\\n    )\\n)\\n\\nseg1 = torch.randint(0, 20000, (1, 512))\\nseg2 = torch.randint(0, 20000, (1, 512))\\nseg3 = torch.randint(0, 20000, (1, 512))\\n\\nlogits1, mems1  = model_xl(seg1, return_mems = True)\\nlogits2, mems2  = model_xl(seg2, mems = mems1, return_mems = True) # mems1 of layer N are automatically routed to the layer N-1\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel_xl\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_mem_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eshift_mem_down\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003erotary_pos_emb\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eseg1\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eseg2\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eseg3\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e))\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003elogits1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003emems1\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emodel_xl\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eseg1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ereturn_mems\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003elogits2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003emems2\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emodel_xl\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eseg2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003emems\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003emems1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ereturn_mems\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# mems1 of layer N are automatically routed to the layer N-1\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eGated residual\\u003c/h3\\u003e\\u003ca id=\\\"user-content-gated-residual\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Gated residual\\\" href=\\\"#gated-residual\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/gating.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/gating.png\\\" width=\\\"500px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/1910.06764\\\" rel=\\\"nofollow\\\"\\u003ehttps://arxiv.org/abs/1910.06764\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe authors propose gating the residual connections in the transformer network and demonstrate increased stability and performance for Transformer-XL in a variety of reinforcement learning tasks.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    max_mem_len = 2048,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 16,\\n        gate_residual = True\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_mem_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003egate_residual\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eRotary Positional Embeddings\\u003c/h3\\u003e\\u003ca id=\\\"user-content-rotary-positional-embeddings\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Rotary Positional Embeddings\\\" href=\\\"#rotary-positional-embeddings\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/rotary.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/rotary.png\\\" width=\\\"500px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eDeveloped in Beijing, this new technique quickly gained interest in the NLP circles. In short, it allows you to endow the transformer with relative positional embeddings at the cost of no learned parameters. You apply a rotary operation to the queries and keys prior to their dot product in attention. The big idea is injecting positions through rotations.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eHighly recommend that you have this turned on whenever you are working on an ordered sequence.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        rotary_pos_emb = True  # turns on rotary positional embeddings\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003erotary_pos_emb\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c\\\"\\u003e# turns on rotary positional embeddings\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate (12/2022): Rotary embedding has since been hugely successful, widely adopted in many large language models, including the largest in the world, PaLM. However, it has been uncovered in the ALiBi paper that rotary embeddings cannot length extrapolate well. This was recently addressed in \\u003ca href=\\\"https://arxiv.org/abs/2212.10554v1\\\" rel=\\\"nofollow\\\"\\u003ea Microsoft research paper\\u003c/a\\u003e. They propose a way to unobtrusively add the same decay as in ALiBi, and found that this resolves the extrapolation problem. You can use it in this repository by setting \\u003ccode\\u003erotary_xpos = True\\u003c/code\\u003e. Like ALiBi, it would enforce the attention to be local. You can set the receptive field with \\u003ccode\\u003erotary_xpos_scale_base\\u003c/code\\u003e value, which defaults to \\u003ccode\\u003e512\\u003c/code\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        rotary_xpos = True   # modified rotary to extrapolate well beyond length at which it was trained\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003erotary_xpos\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e   \\u003cspan class=\\\"pl-c\\\"\\u003e# modified rotary to extrapolate well beyond length at which it was trained\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eDynamic Positional Bias\\u003c/h3\\u003e\\u003ca id=\\\"user-content-dynamic-positional-bias\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Dynamic Positional Bias\\\" href=\\\"#dynamic-positional-bias\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/dynamic-pos-bias.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/dynamic-pos-bias.png\\\" width=\\\"150px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis technique bears roots from the field of vision transformers, where researchers are trying to have relative positions generalize to larger resolutions (without having to retrain the entire network). It was used in two recent papers, \\u003ca href=\\\"https://arxiv.org/abs/2108.00154\\\" rel=\\\"nofollow\\\"\\u003eCrossFormer\\u003c/a\\u003e, as well as \\u003ca href=\\\"https://arxiv.org/abs/2111.09883\\\" rel=\\\"nofollow\\\"\\u003eSwinV2\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://github.com/cfoster0\\\"\\u003eCharles Foster\\u003c/a\\u003e first tried this for a language model, and found that it works. Later on \\u003ca href=\\\"https://github.com/bob80333\\\"\\u003eEric Engelhart\\u003c/a\\u003e produced experimental results that show the same type of extrapolation holds, even for 1d sequences.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eEric trained at sequence lengths of 128, and showed that it generalized well to 1024. In addition, he showed that linear positions was better than log (used in SwinV2), for language.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eLinear distances\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/dynamic-pos-bias-linear.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/dynamic-pos-bias-linear.png\\\" width=\\\"600px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eLog distances\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/dynamic-pos-bias-log.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/dynamic-pos-bias-log.png\\\" width=\\\"600px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eNegative control - Sinusoidal\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/dynamic-pos-bias-sinusoidal.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/dynamic-pos-bias-sinusoidal.png\\\" width=\\\"600px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eMore of Eric's experimental results can be found \\u003ca href=\\\"https://github.com/bob80333/investigating_extrapolation\\\"\\u003ehere\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use this type of relative position if you wish to train at smaller sequence lengths and have it generalize to longer ones, for both autoregressive and bidirectional models.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate: \\u003ca href=\\\"https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding/discussion/460121\\\" rel=\\\"nofollow\\\"\\u003eFirst place RNA folding using dynamic positional bias\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 256,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        dynamic_pos_bias = True,                # set this to True\\n        dynamic_pos_bias_log_distance = False   # whether to use log distance, as in SwinV2\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edynamic_pos_bias\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e,                \\u003cspan class=\\\"pl-c\\\"\\u003e# set this to True\\u003c/span\\u003e\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edynamic_pos_bias_log_distance\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eFalse\\u003c/span\\u003e   \\u003cspan class=\\\"pl-c\\\"\\u003e# whether to use log distance, as in SwinV2\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eALiBi Positional Embedding\\u003c/h3\\u003e\\u003ca id=\\\"user-content-alibi-positional-embedding\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: ALiBi Positional Embedding\\\" href=\\\"#alibi-positional-embedding\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://ofir.io/train_short_test_long.pdf\\\" rel=\\\"nofollow\\\"\\u003eThis paper\\u003c/a\\u003e proposes to simply apply a static linear bias to the attention matrix. The authors show this is not only effective as a relative positional encoding, but also allows the attention net to extrapolate to greater sequences length than what it was trained on, for autoregressive language models.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis repository also offers a bidirectional variant (nonsymmetric), proposed by the authors \\u003ca href=\\\"https://github.com/ofirpress/attention_with_linear_biases/issues/5\\\" data-hovercard-type=\\\"issue\\\" data-hovercard-url=\\\"/ofirpress/attention_with_linear_biases/issues/5/hovercard\\\"\\u003ehere\\u003c/a\\u003e. However, this is untested. If you need bidirectional length extrapolation, the safest option would be Dynamic Position Bias\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate: It may be that ALiBi enforces a strong local attention across the heads, and may hinder it from attending at distances greater than 1k. To avoid any issues with global message passing, I've decided to introduce another hyperparameter \\u003ccode\\u003ealibi_num_heads\\u003c/code\\u003e, so one can specify less heads for the ALiBi bias\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate: There are reports that ALiBi outperform Rotary embeddings for pretraining and downstream fine-tuning.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate: \\u003ca href=\\\"https://arxiv.org/abs/2305.19466\\\" rel=\\\"nofollow\\\"\\u003eNew paper\\u003c/a\\u003e shows that no positional embedding can length extrapolate even than explicit ones\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        alibi_pos_bias = True, # turns on ALiBi positional embedding\\n        alibi_num_heads = 4    # only use ALiBi for 4 out of the 8 heads, so other 4 heads can still attend far distances\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003ealibi_pos_bias\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c\\\"\\u003e# turns on ALiBi positional embedding\\u003c/span\\u003e\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003ealibi_num_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e    \\u003cspan class=\\\"pl-c\\\"\\u003e# only use ALiBi for 4 out of the 8 heads, so other 4 heads can still attend far distances\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eShifted Tokens\\u003c/h3\\u003e\\u003ca id=\\\"user-content-shifted-tokens\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Shifted Tokens\\\" href=\\\"#shifted-tokens\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eAn \\u003ca href=\\\"https://github.com/BlinkDL\\\"\\u003eindependent researcher\\u003c/a\\u003e has found that shifting a subset of the feature dimension along the sequence dimension by 1 token helps with convergence (\\u003ca href=\\\"https://zhuanlan.zhihu.com/p/191393788\\\" rel=\\\"nofollow\\\"\\u003eTime-mixing\\u003c/a\\u003e). I have tested this for the autoregressive case and can confirm that it leads to greatly improved convergence. This also lines up with \\u003ca href=\\\"https://arxiv.org/abs/2106.07477\\\" rel=\\\"nofollow\\\"\\u003ethe results\\u003c/a\\u003e of some papers in the vision domain.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eTo use it, simply set \\u003ccode\\u003eshift_tokens = 1\\u003c/code\\u003e (or to whatever number of shifts you desire). The feature dimension will be divided by \\u003ccode\\u003eshift_tokens + 1\\u003c/code\\u003e and then each chunk will be shifted \\u003ccode\\u003e[0, shift_tokens]\\u003c/code\\u003e respectively\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate: new experiments by @sdtblck suggests this may only work for character-level training\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate: after more experiments, it seems that in the context of BPE encoding, with rotary turned on, there is no benefit to shifting. for character-level training, shifting may still improve a tiny bit\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate: When doing BPE encoded tokens, it seems that shift of 2 will bottleneck the dimensions (divided by 5). It is recommended you always do a shift of 1, unless if you are working with character level.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        shift_tokens = 1\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eshift_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eIf you want finer control over how much is shifted per block (whether attention or feedforward), simply pass in a tuple of size that is equal to the number of layers.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        shift_tokens = (1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0) # 12 blocks, attention and feedforward alternating, with progressively less shifting\\n    )\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eshift_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# 12 blocks, attention and feedforward alternating, with progressively less shifting\\u003c/span\\u003e\\n    )\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eSandwich Norm\\u003c/h3\\u003e\\u003ca id=\\\"user-content-sandwich-norm\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Sandwich Norm\\\" href=\\\"#sandwich-norm\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/sandwich_norm.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/sandwich_norm.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis technique first made an appearance in \\u003ca href=\\\"https://arxiv.org/abs/2105.13290\\\" rel=\\\"nofollow\\\"\\u003ethe CoqView paper\\u003c/a\\u003e, a Chinese version of the famous text-to-image transformer DALL-E. They propose, when using pre-layernorm, to add an extra layernorm to all the branch outputs. I have found this to be very effective for a number of projects, when facing instability during training.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        sandwich_norm = True # set this to True\\n    )\\n)\\n\\nx = torch.randint(0, 20000, (1, 1024))\\nmodel(x)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003esandwich_norm\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e \\u003cspan class=\\\"pl-c\\\"\\u003e# set this to True\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eResiDual\\u003c/h3\\u003e\\u003ca id=\\\"user-content-residual\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: ResiDual\\\" href=\\\"#residual\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/resi_dual.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/resi_dual.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2304.14802\\\" rel=\\\"nofollow\\\"\\u003eThis Microsoft paper\\u003c/a\\u003e proposes yet another normalization configuration, combining both pre and post layernorm. They claim this hybridization reduces representation collapse (known to be an issue with pre-layernorm with increasing depth), while maintaining stability and reducing vanishing gradients (issues with post-layernorm). Initial experiments on my end show it to work no worse than pre-layernorm or sandwich norm. More study needed by the public to see if this is actually a winning technique.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        resi_dual = True,               # set this to True\\n        resi_dual_scale = 0.1           # in appendix, they said on fp16 the prenorm residual is prone to overflow. they claim by scaling it at each layer by a factor, it would prevent the overflow, and keep results the same (as layernorms are invariant to scaling of the input)\\n    )\\n)\\n\\nx = torch.randint(0, 20000, (1, 1024))\\nmodel(x)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eresi_dual\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e,               \\u003cspan class=\\\"pl-c\\\"\\u003e# set this to True\\u003c/span\\u003e\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eresi_dual_scale\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e           \\u003cspan class=\\\"pl-c\\\"\\u003e# in appendix, they said on fp16 the prenorm residual is prone to overflow. they claim by scaling it at each layer by a factor, it would prevent the overflow, and keep results the same (as layernorms are invariant to scaling of the input)\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eNormformer\\u003c/h3\\u003e\\u003ca id=\\\"user-content-normformer\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Normformer\\\" href=\\\"#normformer\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/normformer.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/normformer.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis \\u003ca href=\\\"https://openreview.net/forum?id=GMYWzWztDx5\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e uncovers an issue with pre-norm transformers where gradients are mismatched between the early and later layers. They propose 4 changes, of which I will be offering 3.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe first change is to offer per head scaling after aggregating the values in attention. My experiments show a slight improvement in convergence.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        attn_head_scale = True  # set this to True\\n    )\\n)\\n\\nx = torch.randint(0, 20000, (1, 1024))\\nmodel(x)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_head_scale\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c\\\"\\u003e# set this to True\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe second change is an extra layernorm right after the activation in the feedforward. I have also verified a slight improvement, at the cost of extra compute.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        ff_post_act_ln = True # set this to True\\n    )\\n)\\n\\nx = torch.randint(0, 20000, (1, 1024))\\nmodel(x)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eff_post_act_ln\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e \\u003cspan class=\\\"pl-c\\\"\\u003e# set this to True\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eFor the residual scaling, you simply have to set \\u003ccode\\u003escale_residual = True\\u003c/code\\u003e. I have noticed slight improvements, but occasional instability as well, so use with caution.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        scale_residual = True # set this to True\\n    )\\n)\\n\\nx = torch.randint(0, 20000, (1, 1024))\\nmodel(x)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003escale_residual\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e \\u003cspan class=\\\"pl-c\\\"\\u003e# set this to True\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe last change is a layernorm right after the outwards projection in attention. This is actually identical to the sandwich norm proposed by the Coqview paper, so you can use this by simply setting \\u003ccode\\u003esandwich_norm = True\\u003c/code\\u003e, although it would also add it to the feedforward layer.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eCosine Sim Attention\\u003c/h3\\u003e\\u003ca id=\\\"user-content-cosine-sim-attention\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Cosine Sim Attention\\\" href=\\\"#cosine-sim-attention\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/cosine-sim-attention.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/cosine-sim-attention.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis \\u003ca href=\\\"https://arxiv.org/abs/2010.04245\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e proposes to l2 normalize the queries and keys along the head dimension before the dot product (cosine similarity), with the additional change of the scale being learned rather than static. The normalization prevents the attention operation from overflowing, and removes any need for numerical stability measures prior to softmax. Both are perennial problems when training transformers.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis was validated at scale recently by the training of \\u003ca href=\\\"https://arxiv.org/abs/2111.09883\\\" rel=\\\"nofollow\\\"\\u003ea 3B parameter vision transformer\\u003c/a\\u003e. The SwinV2 paper also proposes to change the pre-layernorm to a post-layernorm for further stability.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eI have validated that this works just as well as dot product attention in an autoregressive setting, if one were to initialize the temperature as proposed in the QK-norm paper (as a function of the sequence length).\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis flavor of attention also has \\u003ca href=\\\"https://arxiv.org/abs/2111.05498\\\" rel=\\\"nofollow\\\"\\u003ea connection\\u003c/a\\u003e to sparse distributed memory. \\u003ca href=\\\"https://www.youtube.com/watch?v=THIIk7LR9_8\\\" rel=\\\"nofollow\\\"\\u003e[youtube talk]\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate: I have discovered a way to remove the learned temperature altogether, by grouping the feature dimension and doing l2-normalization on each group. This allows the queries and keys to have a similarity that is upper bounded by the number of groups. A group size of 8 or 16 was sufficient in my tests. Decided to name this technique \\\"Grouped QK Normalization\\\". The drawback is that I believe an attention head dimension 32 is too small to use this tactic (a dimension often used in vision)\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate 2: Tero Karras has successfully used cosine sim attention in \\u003ca href=\\\"https://arxiv.org/abs/2312.02696\\\" rel=\\\"nofollow\\\"\\u003ea new paper\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use it as follows\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        attn_qk_norm = True,       # set this to True\\n        attn_qk_norm_groups = 8    # number of groups in the feature dimension for l2norm, similarity scores will be bounded between [-group, group]. determines how sharp the attention can be\\n    )\\n)\\n\\nx = torch.randint(0, 20000, (1, 1024))\\nmodel(x)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_qk_norm\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e,       \\u003cspan class=\\\"pl-c\\\"\\u003e# set this to True\\u003c/span\\u003e\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_qk_norm_groups\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e    \\u003cspan class=\\\"pl-c\\\"\\u003e# number of groups in the feature dimension for l2norm, similarity scores will be bounded between [-group, group]. determines how sharp the attention can be\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eAnother update: Simply scaling the cosine similarity (group of 1) with a fixed constant (10) may work too\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n        attn_qk_norm = True,       # set to True\\n        attn_qk_norm_scale = 10    # new scale on the similarity, with groups of 1\\n    )\\n)\\n\\nx = torch.randint(0, 20000, (1, 1024))\\nmodel(x)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_qk_norm\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e,       \\u003cspan class=\\\"pl-c\\\"\\u003e# set to True\\u003c/span\\u003e\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_qk_norm_scale\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e10\\u003c/span\\u003e    \\u003cspan class=\\\"pl-c\\\"\\u003e# new scale on the similarity, with groups of 1\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eQK RMSNorm\\u003c/h3\\u003e\\u003ca id=\\\"user-content-qk-rmsnorm\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: QK RMSNorm\\\" href=\\\"#qk-rmsnorm\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/qknorm-analysis.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/qknorm-analysis.png\\\" width=\\\"450px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate: Google Brain has proven out something similar to cosine sim attention in \\u003ca href=\\\"https://arxiv.org/abs/2302.05442\\\" rel=\\\"nofollow\\\"\\u003ea 22B parameter model\\u003c/a\\u003e. In their papers, they have analysis showing that the normalization resulted in not only extra stability, but also better results in the end (due to less need to adjust learning rate when increasing parameter count).\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eWe are nearing the point of wiping out a source of transformer training instability with one simple intervention, in my opinion. The only slight difference in the paper is that they still have a learned scale across the feature dimension (per use of rmsnorm). Not sure how critical this is, but just to make sure we don't miss anything, I will include this here. You can use this by setting \\u003ccode\\u003eqk_norm_dim_scale = True\\u003c/code\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate: \\u003ca href=\\\"https://twitter.com/Tim_Dettmers/status/1625531080513306627\\\" rel=\\\"nofollow\\\"\\u003eCounterpoint from Tim Dettmers\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate 2: \\u003ca href=\\\"https://arxiv.org/abs/2305.19268\\\" rel=\\\"nofollow\\\"\\u003eCounter\\u003c/a\\u003e to Tim's assertion that outliers are needed, and potentially even \\u003ca href=\\\"https://arxiv.org/abs/2306.12929\\\" rel=\\\"nofollow\\\"\\u003esome solutions\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate 3: Used by \\u003ca href=\\\"https://www.adept.ai/blog/persimmon-8b\\\" rel=\\\"nofollow\\\"\\u003e8B parameter LLM\\u003c/a\\u003e successfully\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate 4: a MetaAI group found that they can \\u003ca href=\\\"https://arxiv.org/abs/2309.16588\\\" rel=\\\"nofollow\\\"\\u003ealleviate outliers\\u003c/a\\u003e by adding \\u003ccode\\u003eregister tokens\\u003c/code\\u003e, also known as \\u003ccode\\u003ememory tokens\\u003c/code\\u003e from earlier literature (Burtsev et al). Perhaps what should be tried next is see if qk norm can be improved in the presence of memory tokens.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 12,\\n        heads = 8,\\n        attn_qk_norm = True,\\n        attn_qk_norm_dim_scale = True # set this to True, in addition to `attn_qk_norm = True`\\n    )\\n)\\n\\nx = torch.randint(0, 256, (1, 1024))\\nmodel(x)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e12\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_qk_norm\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_qk_norm_dim_scale\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e \\u003cspan class=\\\"pl-c\\\"\\u003e# set this to True, in addition to `attn_qk_norm = True`\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eTurning off absolute positional embedding\\u003c/h3\\u003e\\u003ca id=\\\"user-content-turning-off-absolute-positional-embedding\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Turning off absolute positional embedding\\\" href=\\\"#turning-off-absolute-positional-embedding\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eA number of papers have hinted that causal transformers (\\u003ccode\\u003eDecoder\\u003c/code\\u003e) can learn absolute positions in the absence of added embeddings of any sort. This was recently thoroughly investigated \\u003ca href=\\\"https://arxiv.org/abs/2203.16634\\\" rel=\\\"nofollow\\\"\\u003ehere\\u003c/a\\u003e. You can turn off the absolute positional embedding by setting \\u003ccode\\u003euse_abs_pos_emb = False\\u003c/code\\u003e in the \\u003ccode\\u003eTransformerWrapper\\u003c/code\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eGiven \\u003ca href=\\\"https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\\\" rel=\\\"nofollow\\\"\\u003ePaLM\\u003c/a\\u003e, the trend going forward may be to forgo absolute positional embedding (again, for causal transformers only), and add relative positional embeddings with RoPE, ALiBi, etc.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUpdate: \\u003ca href=\\\"https://arxiv.org/abs/2305.19466\\\" rel=\\\"nofollow\\\"\\u003eThis paper\\u003c/a\\u003e shows that in the absence of any engineered absolute or relative positional embeddings, decoders can generate implicit positions, and even length generalize better than solutions of the past. They were unaware of dynamic positional bias, however.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    use_abs_pos_emb = False,   # set this to False\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 6,\\n        heads = 8,\\n    )\\n)\\n\\nx = torch.randint(0, 20000, (1, 1024))\\nmodel(x)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003euse_abs_pos_emb\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eFalse\\u003c/span\\u003e,   \\u003cspan class=\\\"pl-c\\\"\\u003e# set this to False\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eForgetful Causal Mask\\u003c/h3\\u003e\\u003ca id=\\\"user-content-forgetful-causal-mask\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Forgetful Causal Mask\\\" href=\\\"#forgetful-causal-mask\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/fcm.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/fcm.png\\\" width=\\\"450px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2210.13432\\\" rel=\\\"nofollow\\\"\\u003eThis paper\\u003c/a\\u003e shows convincing results that one can combine masking (from masked language modeling) with autoregressive training, leading to significantly better results.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use this by setting the \\u003ccode\\u003emask_prob\\u003c/code\\u003e on the \\u003ccode\\u003eAutoregressiveWrapper\\u003c/code\\u003e class\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import TransformerWrapper, Decoder, AutoregressiveWrapper\\n\\nmodel = TransformerWrapper(\\n    num_tokens = 20000,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 12,\\n        heads = 8\\n    )\\n)\\n\\nmodel = AutoregressiveWrapper(\\n    model,\\n    mask_prob = 0.15  # in paper, they use 15%, same as BERT\\n).cuda()\\n\\n# mock data\\n\\nx = torch.randint(0, 20000, (1, 1024)).cuda()\\n\\n# derive cross entropy loss, masking all taken care of\\n\\nloss = model(x)\\nloss.backward()\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eAutoregressiveWrapper\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e12\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eAutoregressiveWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emask_prob\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.15\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c\\\"\\u003e# in paper, they use 15%, same as BERT\\u003c/span\\u003e\\n).\\u003cspan class=\\\"pl-en\\\"\\u003ecuda\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# mock data\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e)).\\u003cspan class=\\\"pl-en\\\"\\u003ecuda\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# derive cross entropy loss, masking all taken care of\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ebackward\\u003c/span\\u003e()\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eMiscellaneous\\u003c/h2\\u003e\\u003ca id=\\\"user-content-miscellaneous\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Miscellaneous\\\" href=\\\"#miscellaneous\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eCross Attention\\u003c/h3\\u003e\\u003ca id=\\\"user-content-cross-attention\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Cross Attention\\\" href=\\\"#cross-attention\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import Encoder, CrossAttender\\n\\nenc = Encoder(dim = 512, depth = 6)\\nmodel = CrossAttender(dim = 512, depth = 6)\\n\\nnodes = torch.randn(1, 1, 512)\\nnode_masks = torch.ones(1, 1).bool()\\n\\nneighbors = torch.randn(1, 5, 512)\\nneighbor_masks = torch.ones(1, 5).bool()\\n\\nencoded_neighbors = enc(neighbors, mask = neighbor_masks)\\nmodel(nodes, context = encoded_neighbors, mask = node_masks, context_mask = neighbor_masks) # (1, 1, 512)\\n\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eCrossAttender\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eenc\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eCrossAttender\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003enodes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003enode_masks\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eones\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e).\\u003cspan class=\\\"pl-en\\\"\\u003ebool\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eneighbors\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e5\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eneighbor_masks\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eones\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e5\\u003c/span\\u003e).\\u003cspan class=\\\"pl-en\\\"\\u003ebool\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eencoded_neighbors\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003eenc\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eneighbors\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003emask\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eneighbor_masks\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003enodes\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003econtext\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eencoded_neighbors\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003emask\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003enode_masks\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003econtext_mask\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eneighbor_masks\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1, 512)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eContinuous Embeddings\\u003c/h3\\u003e\\u003ca id=\\\"user-content-continuous-embeddings\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Continuous Embeddings\\\" href=\\\"#continuous-embeddings\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import ContinuousTransformerWrapper, Decoder\\n\\nmodel = ContinuousTransformerWrapper(\\n    dim_in = 32,\\n    dim_out = 100,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 12,\\n        heads = 8\\n    )\\n)\\n\\nx = torch.randn((1, 1024, 32))\\n\\nmodel(x) # (1, 1024, 100)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eContinuousTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eContinuousTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim_in\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim_out\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e100\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e12\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e((\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e))\\n\\n\\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1024, 100)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can also train a transformer that accepts continuous values autoregressively easily, in the same scheme as done successfully in \\u003ca href=\\\"https://arxiv.org/abs/2112.05329\\\" rel=\\\"nofollow\\\"\\u003ethis paper\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom x_transformers import ContinuousTransformerWrapper, Decoder\\nfrom x_transformers import ContinuousAutoregressiveWrapper\\n\\nmodel = ContinuousTransformerWrapper(\\n    dim_in = 777,\\n    dim_out = 777,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 12,\\n        heads = 8\\n    )\\n)\\n\\n# wrap it with the continuous autoregressive wrapper\\n\\nmodel = ContinuousAutoregressiveWrapper(model)\\n\\n# mock data\\n\\nx = torch.randn((1, 1024, 777))\\nmask = torch.ones(1, 1024).bool()\\n\\n# train on a lot of data above\\n\\nloss = model(x, mask = mask)\\nloss.backward\\n\\n# then generate\\n\\nstart_emb = torch.randn(1, 777)\\ngenerated = model.generate(start_emb, 17) # (17, 777)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eContinuousTransformerWrapper\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eContinuousAutoregressiveWrapper\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eContinuousTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim_in\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e777\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim_out\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e777\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e12\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# wrap it with the continuous autoregressive wrapper\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eContinuousAutoregressiveWrapper\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# mock data\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e((\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e777\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emask\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eones\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e).\\u003cspan class=\\\"pl-en\\\"\\u003ebool\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# train on a lot of data above\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003emask\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003emask\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003ebackward\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# then generate\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003estart_emb\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e777\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003egenerated\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003egenerate\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003estart_emb\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e17\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (17, 777)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003exVal - Continuous and Discrete\\u003c/h3\\u003e\\u003ca id=\\\"user-content-xval---continuous-and-discrete\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: xVal - Continuous and Discrete\\\" href=\\\"#xval---continuous-and-discrete\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/x-transformers/blob/main/images/xval.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/x-transformers/raw/main/images/xval.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis is promising work that resulted from the collaboration across many institutes (collectively known as Polymathic AI). They found that by offering a continuously scaled number token to the transformer, the transformer was able to generalize arithmetic and forecasting tasks better than the alternative encoding schemes.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis is corroborated by some \\u003ca href=\\\"https://github.com/lucidrains/tab-transformer-pytorch#ft-transformer\\\"\\u003eprior work\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\n\\nfrom x_transformers import (\\n    Decoder,\\n    XValTransformerWrapper,\\n    XValAutoregressiveWrapper\\n)\\n\\nmodel = XValTransformerWrapper(\\n    num_tokens = 4,\\n    numerical_token_id = 3,\\n    max_seq_len = 1024,\\n    attn_layers = Decoder(\\n        dim = 512,\\n        depth = 12,\\n        heads = 8\\n    )\\n)\\n\\n# wrap it with the xval autoregressive wrapper\\n\\nmodel = XValAutoregressiveWrapper(model)\\n\\n# mock data\\n\\nids = torch.randint(0, 4, (1, 777))\\nnums = torch.randn(1, 777)\\nmask = torch.ones(1, 777).bool()\\n\\n# train on a lot of data above\\n\\nloss = model(ids, nums, mask = mask)\\nloss.backward()\\n\\n# then generate\\n\\nstart_ids = torch.randint(0, 4, (1, 1))\\nstart_nums = torch.randn(1, 1)\\n\\nids_out, num_out, is_number_mask = model.generate(start_ids, start_nums, 17)\\n\\n# (1, 17), (1, 17), (1, 17)\\n\\n# discrete, continuous, mask for discrete / continuous\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e (\\n    \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-v\\\"\\u003eXValTransformerWrapper\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-v\\\"\\u003eXValAutoregressiveWrapper\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eXValTransformerWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enumerical_token_id\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eattn_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDecoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e12\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# wrap it with the xval autoregressive wrapper\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eXValAutoregressiveWrapper\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# mock data\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eids\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e777\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-s1\\\"\\u003enums\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e777\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emask\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eones\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e777\\u003c/span\\u003e).\\u003cspan class=\\\"pl-en\\\"\\u003ebool\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# train on a lot of data above\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eids\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003enums\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003emask\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003emask\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ebackward\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# then generate\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003estart_ids\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e))\\n\\u003cspan class=\\\"pl-s1\\\"\\u003estart_nums\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eids_out\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003enum_out\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eis_number_mask\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003egenerate\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003estart_ids\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003estart_nums\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e17\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 17), (1, 17), (1, 17)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# discrete, continuous, mask for discrete / continuous\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eCitations\\u003c/h2\\u003e\\u003ca id=\\\"user-content-citations\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Citations\\\" href=\\\"#citations\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{vaswani2017attention,\\n    title   = {Attention Is All You Need},\\n    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\\n    year    = {2017},\\n    eprint  = {1706.03762},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CL}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003evaswani2017attention\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAttention Is All You Need\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAshish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2017\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e1706.03762\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CL\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{DBLP:journals/corr/abs-1907-01470,\\n    author    = {Sainbayar Sukhbaatar and\\n               Edouard Grave and\\n               Guillaume Lample and\\n               Herv{\\\\'{e}} J{\\\\'{e}}gou and\\n               Armand Joulin},\\n    title     = {Augmenting Self-attention with Persistent Memory},\\n    journal   = {CoRR},\\n    volume    = {abs/1907.01470},\\n    year      = {2019},\\n    url       = {http://arxiv.org/abs/1907.01470}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eDBLP:journals/corr/abs-1907-01470\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eSainbayar Sukhbaatar and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e               Edouard Grave and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e               Guillaume Lample and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e               Herv{\\\\'{e}} J{\\\\'{e}}gou and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e               Armand Joulin\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAugmenting Self-attention with Persistent Memory\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eCoRR\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003evolume\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eabs/1907.01470\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e      = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2019\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e       = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttp://arxiv.org/abs/1907.01470\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{1910.05895,\\n    author  = {Toan Q. Nguyen and Julian Salazar},\\n    title   = {Transformers without Tears: Improving the Normalization of Self-Attention},\\n    year    = {2019},\\n    eprint  = {arXiv:1910.05895},\\n    doi     = {10.5281/zenodo.3525484},\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003e1910.05895\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eToan Q. Nguyen and Julian Salazar\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eTransformers without Tears: Improving the Normalization of Self-Attention\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2019\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv:1910.05895\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003edoi\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e10.5281/zenodo.3525484\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{shazeer2020glu,\\n    title   = {GLU Variants Improve Transformer},\\n    author  = {Noam Shazeer},\\n    year    = {2020},\\n    url     = {https://arxiv.org/abs/2002.05202}    \\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eshazeer2020glu\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eGLU Variants Improve Transformer\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eNoam Shazeer\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2020\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://arxiv.org/abs/2002.05202\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e    \\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Zoph2022STMoEDS,\\n    title   = {ST-MoE: Designing Stable and Transferable Sparse Expert Models},\\n    author  = {Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam M. Shazeer and William Fedus},\\n    year    = {2022}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eZoph2022STMoEDS\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eST-MoE: Designing Stable and Transferable Sparse Expert Models\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eBarret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam M. Shazeer and William Fedus\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{bhojanapalli2020lowrank,\\n    title   = {Low-Rank Bottleneck in Multi-head Attention Models},\\n    author  = {Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},\\n    year    = {2020},\\n    eprint  = {2002.07028}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ebhojanapalli2020lowrank\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eLow-Rank Bottleneck in Multi-head Attention Models\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eSrinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2020\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2002.07028\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{burtsev2020memory,\\n    title   = {Memory Transformer}, \\n    author  = {Mikhail S. Burtsev and Grigory V. Sapunov},\\n    year    = {2020},\\n    eprint  = {2006.11527},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CL}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eburtsev2020memory\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eMemory Transformer\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eMikhail S. Burtsev and Grigory V. Sapunov\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2020\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2006.11527\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CL\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{zhao2019explicit,\\n    title   = {Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection}, \\n    author  = {Guangxiang Zhao and Junyang Lin and Zhiyuan Zhang and Xuancheng Ren and Qi Su and Xu Sun},\\n    year    = {2019},\\n    eprint  = {1912.11637},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CL}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ezhao2019explicit\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eExplicit Sparse Transformer: Concentrated Attention Through Explicit Selection\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eGuangxiang Zhao and Junyang Lin and Zhiyuan Zhang and Xuancheng Ren and Qi Su and Xu Sun\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2019\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e1912.11637\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CL\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{correia2019adaptively,\\n    title   = {Adaptively Sparse Transformers},\\n    author  = {Gon\u00e7alo M. Correia and Vlad Niculae and Andr\u00e9 F. T. Martins},\\n    year    = {2019},\\n    eprint  = {1909.00015},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CL}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ecorreia2019adaptively\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAdaptively Sparse Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eGon\u00e7alo M. Correia and Vlad Niculae and Andr\u00e9 F. T. Martins\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2019\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e1909.00015\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CL\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{shazeer2020talkingheads,\\n    title   = {Talking-Heads Attention}, \\n    author  = {Noam Shazeer and Zhenzhong Lan and Youlong Cheng and Nan Ding and Le Hou},\\n    year    = {2020},\\n    eprint  = {2003.02436},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.LG}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eshazeer2020talkingheads\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eTalking-Heads Attention\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eNoam Shazeer and Zhenzhong Lan and Youlong Cheng and Nan Ding and Le Hou\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2020\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2003.02436\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.LG\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{press2020improving,\\n    title   = {Improving Transformer Models by Reordering their Sublayers}, \\n    author  = {Ofir Press and Noah A. Smith and Omer Levy},\\n    year    = {2020},\\n    eprint  = {1911.03864},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CL}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003epress2020improving\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eImproving Transformer Models by Reordering their Sublayers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eOfir Press and Noah A. Smith and Omer Levy\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2020\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e1911.03864\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CL\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{lu2019understanding,\\n    title   = {Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View}, \\n    author  = {Yiping Lu and Zhuohan Li and Di He and Zhiqing Sun and Bin Dong and Tao Qin and Liwei Wang and Tie-Yan Liu},\\n    year    = {2019},\\n    eprint  = {1906.02762},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.LG}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003elu2019understanding\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eUnderstanding and Improving Transformer From a Multi-Particle Dynamic System Point of View\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eYiping Lu and Zhuohan Li and Di He and Zhiqing Sun and Bin Dong and Tao Qin and Liwei Wang and Tie-Yan Liu\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2019\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e1906.02762\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.LG\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{ke2020rethinking,\\n    title     = {Rethinking Positional Encoding in Language Pre-training},\\n    author    = {Guolin Ke and Di He and Tie-Yan Liu},\\n    year      = {2020},\\n    eprint    = {2006.15595},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CL}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eke2020rethinking\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eRethinking Positional Encoding in Language Pre-training\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eGuolin Ke and Di He and Tie-Yan Liu\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e      = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2020\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2006.15595\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CL\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{dosovitskiy2020image,\\n    title   = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\\n    author  = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},\\n    year    = {2020},\\n    eprint  = {2010.11929},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003edosovitskiy2020image\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAlexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2020\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2010.11929\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{huang2019attention,\\n    title   = {Attention on Attention for Image Captioning},\\n    author  = {Lun Huang and Wenmin Wang and Jie Chen and Xiao-Yong Wei},\\n    year    = {2019},\\n    eprint  = {1908.06954},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ehuang2019attention\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAttention on Attention for Image Captioning\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eLun Huang and Wenmin Wang and Jie Chen and Xiao-Yong Wei\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2019\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e1908.06954\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{raffel2020exploring,\\n    title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, \\n    author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\\n    year    = {2020},\\n    eprint  = {1910.10683},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.LG}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eraffel2020exploring\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eColin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2020\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e1910.10683\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.LG\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{martins-etal-2020-sparse,\\n    title   = \\u0026quot;Sparse Text Generation\\u0026quot;,\\n    author  = \\u0026quot;Martins, Pedro Henrique  and\\n        Marinho, Zita  and\\n        Martins, Andr{\\\\'e} F. T.\\u0026quot;,\\n    booktitle = \\u0026quot;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\\u0026quot;,\\n    month   = nov,\\n    year    = \\u0026quot;2020\\u0026quot;,\\n    address = \\u0026quot;Online\\u0026quot;,\\n    publisher = \\u0026quot;Association for Computational Linguistics\\u0026quot;,\\n    url     = \\u0026quot;https://www.aclweb.org/anthology/2020.emnlp-main.348\\u0026quot;\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003emartins-etal-2020-sparse\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003eSparse Text Generation\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003eMartins, Pedro Henrique  and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e        Marinho, Zita  and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e        Martins, Andr{\\\\'e} F. T.\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ebooktitle\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003eProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003emonth\\u003c/span\\u003e   = nov,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e2020\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eaddress\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003eOnline\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003epublisher\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003eAssociation for Computational Linguistics\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003ehttps://www.aclweb.org/anthology/2020.emnlp-main.348\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{he2020realformer,\\n    title   = {RealFormer: Transformer Likes Residual Attention},\\n    author  = {Ruining He and Anirudh Ravula and Bhargav Kanagal and Joshua Ainslie},\\n    year    = {2020},\\n    eprint  = {2012.11747},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.LG}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ehe2020realformer\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eRealFormer: Transformer Likes Residual Attention\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eRuining He and Anirudh Ravula and Bhargav Kanagal and Joshua Ainslie\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2020\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2012.11747\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.LG\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{carion2020endtoend,\\n    title   = {End-to-End Object Detection with Transformers},\\n    author  = {Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko},\\n    year    = {2020},\\n    eprint  = {2005.12872},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ecarion2020endtoend\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eEnd-to-End Object Detection with Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eNicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2020\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2005.12872\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{press2021ALiBi,\\n    title   = {Train Short, Test Long: Attention with Linear Biases Enable Input Length Extrapolation},\\n    author  = {Ofir Press and Noah A. Smith and Mike Lewis},\\n    year    = {2021},\\n    url     = {https://ofir.io/train_short_test_long.pdf}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003epress2021ALiBi\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eTrain Short, Test Long: Attention with Linear Biases Enable Input Length Extrapolation\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eOfir Press and Noah A. Smith and Mike Lewis\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://ofir.io/train_short_test_long.pdf\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{parisotto2019stabilizing,\\n    title     = {Stabilizing Transformers for Reinforcement Learning},\\n    author    = {Emilio Parisotto and H. Francis Song and Jack W. Rae and Razvan Pascanu and Caglar Gulcehre and Siddhant M. Jayakumar and Max Jaderberg and Raphael Lopez Kaufman and Aidan Clark and Seb Noury and Matthew M. Botvinick and Nicolas Heess and Raia Hadsell},\\n    year      = {2019},\\n    eprint    = {1910.06764},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.LG}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eparisotto2019stabilizing\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eStabilizing Transformers for Reinforcement Learning\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eEmilio Parisotto and H. Francis Song and Jack W. Rae and Razvan Pascanu and Caglar Gulcehre and Siddhant M. Jayakumar and Max Jaderberg and Raphael Lopez Kaufman and Aidan Clark and Seb Noury and Matthew M. Botvinick and Nicolas Heess and Raia Hadsell\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e      = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2019\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e1910.06764\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.LG\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{narang2021transformer,\\n    title       = {Do Transformer Modifications Transfer Across Implementations and Applications?},\\n    author      = {Sharan Narang and Hyung Won Chung and Yi Tay and William Fedus and Thibault Fevry and Michael Matena and Karishma Malkan and Noah Fiedel and Noam Shazeer and Zhenzhong Lan and Yanqi Zhou and Wei Li and Nan Ding and Jake Marcus and Adam Roberts and Colin Raffel},\\n    year        = {2021},\\n    eprint      = {2102.11972},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.LG}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003enarang2021transformer\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e       = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eDo Transformer Modifications Transfer Across Implementations and Applications?\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e      = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eSharan Narang and Hyung Won Chung and Yi Tay and William Fedus and Thibault Fevry and Michael Matena and Karishma Malkan and Noah Fiedel and Noam Shazeer and Zhenzhong Lan and Yanqi Zhou and Wei Li and Nan Ding and Jake Marcus and Adam Roberts and Colin Raffel\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e        = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e      = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2102.11972\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.LG\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{zhang2019root,\\n    title   = {Root Mean Square Layer Normalization},\\n    author  = {Biao Zhang and Rico Sennrich},\\n    year    = {2019},\\n    eprint  = {1910.07467},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.LG}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ezhang2019root\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eRoot Mean Square Layer Normalization\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eBiao Zhang and Rico Sennrich\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2019\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e1910.07467\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.LG\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Qin2023ScalingTT,\\n    title   = {Scaling TransNormer to 175 Billion Parameters},\\n    author  = {Zhen Qin and Dong Li and Weigao Sun and Weixuan Sun and Xuyang Shen and Xiaodong Han and Yunshen Wei and Baohong Lv and Fei Yuan and Xiao Luo and Y. Qiao and Yiran Zhong},\\n    year    = {2023},\\n    url     = {https://api.semanticscholar.org/CorpusID:260203124}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eQin2023ScalingTT\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eScaling TransNormer to 175 Billion Parameters\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eZhen Qin and Dong Li and Weigao Sun and Weixuan Sun and Xuyang Shen and Xiaodong Han and Yunshen Wei and Baohong Lv and Fei Yuan and Xiao Luo and Y. Qiao and Yiran Zhong\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://api.semanticscholar.org/CorpusID:260203124\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{su2021roformer,\\n    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},\\n    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\\n    year    = {2021},\\n    eprint  = {2104.09864},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CL}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003esu2021roformer\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eRoFormer: Enhanced Transformer with Rotary Position Embedding\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eJianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2104.09864\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CL\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Chen2023ExtendingCW,\\n    title   = {Extending Context Window of Large Language Models via Positional Interpolation},\\n    author  = {Shouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian},\\n    year    = {2023}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eChen2023ExtendingCW\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eExtending Context Window of Large Language Models via Positional Interpolation\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eShouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Sun2022ALT,\\n  title     = {A Length-Extrapolatable Transformer},\\n  author    = {Yutao Sun and Li Dong and Barun Patra and Shuming Ma and Shaohan Huang and Alon Benhaim and Vishrav Chaudhary and Xia Song and Furu Wei},\\n  year      = {2022}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eSun2022ALT\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eA Length-Extrapolatable Transformer\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eYutao Sun and Li Dong and Barun Patra and Shuming Ma and Shaohan Huang and Alon Benhaim and Vishrav Chaudhary and Xia Song and Furu Wei\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e      = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@Article{AlphaFold2021,\\n    author  = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\\\\v{Z}}{\\\\'\\\\i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A A and Ballard, Andrew J and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},\\n    journal = {Nature},\\n    title   = {Highly accurate protein structure prediction with {AlphaFold}},\\n    year    = {2021},\\n    doi     = {10.1038/s41586-021-03819-2},\\n    note    = {(Accelerated article preview)},\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@Article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eAlphaFold2021\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eJumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\\\\v{Z}}{\\\\'\\\\i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A A and Ballard, Andrew J and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eNature\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eHighly accurate protein structure prediction with {AlphaFold}\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003edoi\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e10.1038/s41586-021-03819-2\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003enote\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e(Accelerated article preview)\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@software{peng_bo_2021_5196578,\\n    author       = {PENG Bo},\\n    title        = {BlinkDL/RWKV-LM: 0.01},\\n    month        = {aug},\\n    year         = {2021},\\n    publisher    = {Zenodo},\\n    version      = {0.01},\\n    doi          = {10.5281/zenodo.5196578},\\n    url          = {https://doi.org/10.5281/zenodo.5196578}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@software\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003epeng_bo_2021_5196578\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e       = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ePENG Bo\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e        = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eBlinkDL/RWKV-LM: 0.01\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003emonth\\u003c/span\\u003e        = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eaug\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e         = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003epublisher\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eZenodo\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eversion\\u003c/span\\u003e      = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e0.01\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003edoi\\u003c/span\\u003e          = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e10.5281/zenodo.5196578\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e          = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://doi.org/10.5281/zenodo.5196578\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{csord\u00e1s2021devil,\\n    title   = {The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers},\\n    author  = {R\u00f3bert Csord\u00e1s and Kazuki Irie and J\u00fcrgen Schmidhuber},\\n    year    = {2021},\\n    eprint  = {2108.12284},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.LG}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ecsord\u00e1s2021devil\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eThe Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eR\u00f3bert Csord\u00e1s and Kazuki Irie and J\u00fcrgen Schmidhuber\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2108.12284\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.LG\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{so2021primer,\\n    title   = {Primer: Searching for Efficient Transformers for Language Modeling}, \\n    author  = {David R. So and Wojciech Ma\u0144ke and Hanxiao Liu and Zihang Dai and Noam Shazeer and Quoc V. Le},\\n    year    = {2021},\\n    eprint  = {2109.08668},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.LG}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eso2021primer\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ePrimer: Searching for Efficient Transformers for Language Modeling\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eDavid R. So and Wojciech Ma\u0144ke and Hanxiao Liu and Zihang Dai and Noam Shazeer and Quoc V. Le\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2109.08668\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.LG\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{ding2021erniedoc,\\n    title   = {ERNIE-Doc: A Retrospective Long-Document Modeling Transformer}, \\n    author  = {Siyu Ding and Junyuan Shang and Shuohuan Wang and Yu Sun and Hao Tian and Hua Wu and Haifeng Wang},\\n    year    = {2021},\\n    eprint  = {2012.15688},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CL}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eding2021erniedoc\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eERNIE-Doc: A Retrospective Long-Document Modeling Transformer\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eSiyu Ding and Junyuan Shang and Shuohuan Wang and Yu Sun and Hao Tian and Hua Wu and Haifeng Wang\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2012.15688\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CL\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{ding2021cogview,\\n    title   = {CogView: Mastering Text-to-Image Generation via Transformers},\\n    author  = {Ming Ding and Zhuoyi Yang and Wenyi Hong and Wendi Zheng and Chang Zhou and Da Yin and Junyang Lin and Xu Zou and Zhou Shao and Hongxia Yang and Jie Tang},\\n    year    = {2021},\\n    eprint  = {2105.13290},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eding2021cogview\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eCogView: Mastering Text-to-Image Generation via Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eMing Ding and Zhuoyi Yang and Wenyi Hong and Wendi Zheng and Chang Zhou and Da Yin and Junyang Lin and Xu Zou and Zhou Shao and Hongxia Yang and Jie Tang\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2105.13290\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{anonymous2022normformer,\\n    title   = {NormFormer: Improved Transformer Pretraining with Extra Normalization},\\n    author  = {Anonymous},\\n    booktitle = {Submitted to The Tenth International Conference on Learning Representations },\\n    year    = {2022},\\n    url     = {https://openreview.net/forum?id=GMYWzWztDx5},\\n    note    = {under review}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eanonymous2022normformer\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eNormFormer: Improved Transformer Pretraining with Extra Normalization\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAnonymous\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ebooktitle\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eSubmitted to The Tenth International Conference on Learning Representations \\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://openreview.net/forum?id=GMYWzWztDx5\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003enote\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eunder review\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{henry2020querykey,\\n    title   = {Query-Key Normalization for Transformers},\\n    author  = {Alex Henry and Prudhvi Raj Dachapally and Shubham Pawar and Yuxuan Chen},\\n    year    = {2020},\\n    eprint  = {2010.04245},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CL}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ehenry2020querykey\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eQuery-Key Normalization for Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAlex Henry and Prudhvi Raj Dachapally and Shubham Pawar and Yuxuan Chen\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2020\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2010.04245\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CL\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{liu2021swin,\\n    title   = {Swin Transformer V2: Scaling Up Capacity and Resolution},\\n    author  = {Ze Liu and Han Hu and Yutong Lin and Zhuliang Yao and Zhenda Xie and Yixuan Wei and Jia Ning and Yue Cao and Zheng Zhang and Li Dong and Furu Wei and Baining Guo},\\n    year    = {2021},\\n    eprint  = {2111.09883},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eliu2021swin\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eSwin Transformer V2: Scaling Up Capacity and Resolution\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eZe Liu and Han Hu and Yutong Lin and Zhuliang Yao and Zhenda Xie and Yixuan Wei and Jia Ning and Yue Cao and Zheng Zhang and Li Dong and Furu Wei and Baining Guo\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2111.09883\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{Haviv2022TransformerLM,\\n    title   = {Transformer Language Models without Positional Encodings Still Learn Positional Information},\\n    author  = {Adi Haviv and Ori Ram and Ofir Press and Peter Izsak and Omer Levy},\\n    journal = {ArXiv},\\n    year    = {2022},\\n    volume  = {abs/2203.16634}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eHaviv2022TransformerLM\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eTransformer Language Models without Positional Encodings Still Learn Positional Information\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAdi Haviv and Ori Ram and Ofir Press and Peter Izsak and Omer Levy\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eArXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003evolume\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eabs/2203.16634\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{chowdhery2022PaLM,\\n    title   = {PaLM: Scaling Language Modeling with Pathways},\\n    author  = {Chowdhery, Aakanksha et al},\\n    year    = {2022}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003echowdhery2022PaLM\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ePaLM: Scaling Language Modeling with Pathways\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eChowdhery, Aakanksha et al\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{Shazeer2019FastTD,\\n    title   = {Fast Transformer Decoding: One Write-Head is All You Need},\\n    author  = {Noam M. Shazeer},\\n    journal = {ArXiv},\\n    year    = {2019},\\n    volume  = {abs/1911.02150}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eShazeer2019FastTD\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eFast Transformer Decoding: One Write-Head is All You Need\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eNoam M. Shazeer\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eArXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2019\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003evolume\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eabs/1911.02150\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{Ainslie2023GQATG,\\n    title   = {GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},\\n    author  = {Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebr'on and Sumit K. Sanghai},\\n    journal = {ArXiv},\\n    year    = {2023},\\n    volume  = {abs/2305.13245},\\n    url     = {https://api.semanticscholar.org/CorpusID:258833177}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eAinslie2023GQATG\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eJoshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebr'on and Sumit K. Sanghai\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eArXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003evolume\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eabs/2305.13245\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://api.semanticscholar.org/CorpusID:258833177\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{schlag2020enhancing,\\n    title   = {Enhancing the Transformer with explicit relational encoding for math problem solving},\\n    author  = {Imanol Schlag and Paul Smolensky and Roland Fernandez and Nebojsa Jojic and J{\\\\\\u0026quot;u}rgen Schmidhuber and Jianfeng Gao},\\n    year    = {2020},\\n    url     = {https://openreview.net/forum?id=B1xfElrKPr}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eschlag2020enhancing\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eEnhancing the Transformer with explicit relational encoding for math problem solving\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eImanol Schlag and Paul Smolensky and Roland Fernandez and Nebojsa Jojic and J{\\\\\\\"u}rgen Schmidhuber and Jianfeng Gao\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2020\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://openreview.net/forum?id=B1xfElrKPr\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{Liu2022FCMFC,\\n    title   = {FCM: Forgetful Causal Masking Makes Causal Language Models Better Zero-Shot Learners},\\n    author  = {Hao Liu and Xinyang Geng and Lisa Lee and Igor Mordatch and Sergey Levine and Sharan Narang and P. Abbeel},\\n    journal = {ArXiv},\\n    year    = {2022},\\n    volume  = {abs/2210.13432}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eLiu2022FCMFC\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eFCM: Forgetful Causal Masking Makes Causal Language Models Better Zero-Shot Learners\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eHao Liu and Xinyang Geng and Lisa Lee and Igor Mordatch and Sergey Levine and Sharan Narang and P. Abbeel\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eArXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003evolume\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eabs/2210.13432\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Huang2016DeepNW,\\n    title   = {Deep Networks with Stochastic Depth},\\n    author  = {Gao Huang and Yu Sun and Zhuang Liu and Daniel Sedra and Kilian Q. Weinberger},\\n    booktitle = {European Conference on Computer Vision},\\n    year    = {2016}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eHuang2016DeepNW\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eDeep Networks with Stochastic Depth\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eGao Huang and Yu Sun and Zhuang Liu and Daniel Sedra and Kilian Q. Weinberger\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ebooktitle\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eEuropean Conference on Computer Vision\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2016\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Hua2022TransformerQI,\\n    title   = {Transformer Quality in Linear Time},\\n    author  = {Weizhe Hua and Zihang Dai and Hanxiao Liu and Quoc V. Le},\\n    booktitle = {International Conference on Machine Learning},\\n    year    = {2022}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eHua2022TransformerQI\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eTransformer Quality in Linear Time\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eWeizhe Hua and Zihang Dai and Hanxiao Liu and Quoc V. Le\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ebooktitle\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eInternational Conference on Machine Learning\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{Chang2022MaskGITMG,\\n    title   = {MaskGIT: Masked Generative Image Transformer},\\n    author  = {Huiwen Chang and Han Zhang and Lu Jiang and Ce Liu and William T. Freeman},\\n    journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n    year    = {2022},\\n    pages   = {11305-11315}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eChang2022MaskGITMG\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eMaskGIT: Masked Generative Image Transformer\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eHuiwen Chang and Han Zhang and Lu Jiang and Ce Liu and William T. Freeman\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003epages\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e11305-11315\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{Lezama2022ImprovedMI,\\n    title   = {Improved Masked Image Generation with Token-Critic},\\n    author  = {Jos{\\\\'e} Lezama and Huiwen Chang and Lu Jiang and Irfan Essa},\\n    journal = {ArXiv},\\n    year    = {2022},\\n    volume  = {abs/2209.04439}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eLezama2022ImprovedMI\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eImproved Masked Image Generation with Token-Critic\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eJos{\\\\'e} Lezama and Huiwen Chang and Lu Jiang and Irfan Essa\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eArXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003evolume\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eabs/2209.04439\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{https://doi.org/10.48550/arxiv.2302.01327,\\n    doi     = {10.48550/ARXIV.2302.01327},\\n    url     = {https://arxiv.org/abs/2302.01327},\\n    author  = {Kumar, Manoj and Dehghani, Mostafa and Houlsby, Neil},\\n    title   = {Dual PatchNorm},\\n    publisher = {arXiv},\\n    year    = {2023},\\n    copyright = {Creative Commons Attribution 4.0 International}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ehttps://doi.org/10.48550/arxiv.2302.01327\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003edoi\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e10.48550/ARXIV.2302.01327\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://arxiv.org/abs/2302.01327\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eKumar, Manoj and Dehghani, Mostafa and Houlsby, Neil\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eDual PatchNorm\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003epublisher\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ecopyright\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eCreative Commons Attribution 4.0 International\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{dao2022flashattention,\\n    title   = {Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},\\n    author  = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\\\\'e}, Christopher},\\n    booktitle = {Advances in Neural Information Processing Systems},\\n    year    = {2022}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003edao2022flashattention\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eFlash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eDao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\\\\'e}, Christopher\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ebooktitle\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAdvances in Neural Information Processing Systems\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{Xie2023ResiDualTW,\\n  title     = {ResiDual: Transformer with Dual Residual Connections},\\n  author    = {Shufang Xie and Huishuai Zhang and Junliang Guo and Xu Tan and Jiang Bian and Hany Hassan Awadalla and Arul Menezes and Tao Qin and Rui Yan},\\n  journal   = {ArXiv},\\n  year      = {2023},\\n  volume    = {abs/2304.14802}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eXie2023ResiDualTW\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eResiDual: Transformer with Dual Residual Connections\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eShufang Xie and Huishuai Zhang and Junliang Guo and Xu Tan and Jiang Bian and Hany Hassan Awadalla and Arul Menezes and Tao Qin and Rui Yan\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eArXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e      = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003evolume\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eabs/2304.14802\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Dehghani2023ScalingVT,\\n    title   = {Scaling Vision Transformers to 22 Billion Parameters},\\n    author  = {Mostafa Dehghani and Josip Djolonga and Basil Mustafa and Piotr Padlewski and Jonathan Heek and Justin Gilmer and Andreas Steiner and Mathilde Caron and Robert Geirhos and Ibrahim M. Alabdulmohsin and Rodolphe Jenatton and Lucas Beyer and Michael Tschannen and Anurag Arnab and Xiao Wang and Carlos Riquelme and Matthias Minderer and Joan Puigcerver and Utku Evci and Manoj Kumar and Sjoerd van Steenkiste and Gamaleldin F. Elsayed and Aravindh Mahendran and Fisher Yu and Avital Oliver and Fantine Huot and Jasmijn Bastings and Mark Collier and Alexey A. Gritsenko and Vighnesh Birodkar and Cristina Nader Vasconcelos and Yi Tay and Thomas Mensink and Alexander Kolesnikov and Filip Paveti'c and Dustin Tran and Thomas Kipf and Mario Luvci'c and Xiaohua Zhai and Daniel Keysers and Jeremiah Harmsen and Neil Houlsby},\\n    year    = {2023}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eDehghani2023ScalingVT\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eScaling Vision Transformers to 22 Billion Parameters\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eMostafa Dehghani and Josip Djolonga and Basil Mustafa and Piotr Padlewski and Jonathan Heek and Justin Gilmer and Andreas Steiner and Mathilde Caron and Robert Geirhos and Ibrahim M. Alabdulmohsin and Rodolphe Jenatton and Lucas Beyer and Michael Tschannen and Anurag Arnab and Xiao Wang and Carlos Riquelme and Matthias Minderer and Joan Puigcerver and Utku Evci and Manoj Kumar and Sjoerd van Steenkiste and Gamaleldin F. Elsayed and Aravindh Mahendran and Fisher Yu and Avital Oliver and Fantine Huot and Jasmijn Bastings and Mark Collier and Alexey A. Gritsenko and Vighnesh Birodkar and Cristina Nader Vasconcelos and Yi Tay and Thomas Mensink and Alexander Kolesnikov and Filip Paveti'c and Dustin Tran and Thomas Kipf and Mario Luvci'c and Xiaohua Zhai and Daniel Keysers and Jeremiah Harmsen and Neil Houlsby\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{Beyer2022BetterPV,\\n    title   = {Better plain ViT baselines for ImageNet-1k},\\n    author  = {Lucas Beyer and Xiaohua Zhai and Alexander Kolesnikov},\\n    journal = {ArXiv},\\n    year    = {2022},\\n    volume  = {abs/2205.01580}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eBeyer2022BetterPV\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eBetter plain ViT baselines for ImageNet-1k\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eLucas Beyer and Xiaohua Zhai and Alexander Kolesnikov\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eArXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003evolume\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eabs/2205.01580\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{Kazemnejad2023TheIO,\\n    title   = {The Impact of Positional Encoding on Length Generalization in Transformers},\\n    author  = {Amirhossein Kazemnejad and Inkit Padhi and Karthikeyan Natesan Ramamurthy and Payel Das and Siva Reddy},\\n    journal = {ArXiv},\\n    year    = {2023},\\n    volume  = {abs/2305.19466}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eKazemnejad2023TheIO\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eThe Impact of Positional Encoding on Length Generalization in Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAmirhossein Kazemnejad and Inkit Padhi and Karthikeyan Natesan Ramamurthy and Payel Das and Siva Reddy\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eArXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003evolume\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eabs/2305.19466\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{bloc97-2023\\n    title   = {NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.},\\n    author  = {/u/bloc97},\\n    url     = {https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ebloc97-2023\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eNTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e/u/bloc97\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Zoph2022STMoEDS,\\n    title   = {ST-MoE: Designing Stable and Transferable Sparse Expert Models},\\n    author  = {Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam M. Shazeer and William Fedus},\\n    year    = {2022}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eZoph2022STMoEDS\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eST-MoE: Designing Stable and Transferable Sparse Expert Models\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eBarret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam M. Shazeer and William Fedus\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{Lan2019ALBERTAL,\\n    title   = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\\n    author  = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\\n    journal = {ArXiv},\\n    year    = {2019},\\n    volume  = {abs/1909.11942},\\n    url     = {https://api.semanticscholar.org/CorpusID:202888986}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eLan2019ALBERTAL\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eALBERT: A Lite BERT for Self-supervised Learning of Language Representations\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eZhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eArXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2019\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003evolume\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eabs/1909.11942\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://api.semanticscholar.org/CorpusID:202888986\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Li2022ContrastiveDO,\\n    title   = {Contrastive Decoding: Open-ended Text Generation as Optimization},\\n    author  = {Xiang Lisa Li and Ari Holtzman and Daniel Fried and Percy Liang and Jason Eisner and Tatsunori Hashimoto and Luke Zettlemoyer and Mike Lewis},\\n    booktitle = {Annual Meeting of the Association for Computational Linguistics},\\n    year    = {2022},\\n    url     = {https://api.semanticscholar.org/CorpusID:253157949}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eLi2022ContrastiveDO\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eContrastive Decoding: Open-ended Text Generation as Optimization\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eXiang Lisa Li and Ari Holtzman and Daniel Fried and Percy Liang and Jason Eisner and Tatsunori Hashimoto and Luke Zettlemoyer and Mike Lewis\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ebooktitle\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAnnual Meeting of the Association for Computational Linguistics\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://api.semanticscholar.org/CorpusID:253157949\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{OBrien2023ContrastiveDI,\\n    title   = {Contrastive Decoding Improves Reasoning in Large Language Models},\\n    author  = {Sean O'Brien and Mike Lewis},\\n    year    = {2023},\\n    url     = {https://api.semanticscholar.org/CorpusID:261884427}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eOBrien2023ContrastiveDI\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eContrastive Decoding Improves Reasoning in Large Language Models\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eSean O'Brien and Mike Lewis\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://api.semanticscholar.org/CorpusID:261884427\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Darcet2023VisionTN,\\n    title   = {Vision Transformers Need Registers},\\n    author  = {Timoth'ee Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski},\\n    year    = {2023},\\n    url     = {https://api.semanticscholar.org/CorpusID:263134283}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eDarcet2023VisionTN\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eVision Transformers Need Registers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eTimoth'ee Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://api.semanticscholar.org/CorpusID:263134283\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{Bondarenko2023QuantizableTR,\\n    title   = {Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing},\\n    author  = {Yelysei Bondarenko and Markus Nagel and Tijmen Blankevoort},\\n    journal = {ArXiv},\\n    year    = {2023},\\n    volume  = {abs/2306.12929},\\n    url     = {https://api.semanticscholar.org/CorpusID:259224568}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eBondarenko2023QuantizableTR\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eQuantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eYelysei Bondarenko and Markus Nagel and Tijmen Blankevoort\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eArXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003evolume\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eabs/2306.12929\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://api.semanticscholar.org/CorpusID:259224568\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Golkar2023xValAC,\\n    title   = {xVal: A Continuous Number Encoding for Large Language Models},\\n    author  = {Siavash Golkar and Mariel Pettee and Michael Eickenberg and Alberto Bietti and M. Cranmer and G{\\\\'e}raud Krawezik and Francois Lanusse and Michael McCabe and Ruben Ohana and Liam Parker and Bruno R{\\\\'e}galdo-Saint Blancard and Tiberiu Te\u015fileanu and Kyunghyun Cho and Shirley Ho},\\n    year    = {2023},\\n    url     = {https://api.semanticscholar.org/CorpusID:263622222}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eGolkar2023xValAC\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003exVal: A Continuous Number Encoding for Large Language Models\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eSiavash Golkar and Mariel Pettee and Michael Eickenberg and Alberto Bietti and M. Cranmer and G{\\\\'e}raud Krawezik and Francois Lanusse and Michael McCabe and Ruben Ohana and Liam Parker and Bruno R{\\\\'e}galdo-Saint Blancard and Tiberiu Te\u015fileanu and Kyunghyun Cho and Shirley Ho\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://api.semanticscholar.org/CorpusID:263622222\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{Rafailov2023DirectPO,\\n    title   = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},\\n    author  = {Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},\\n    journal = {ArXiv},\\n    year    = {2023},\\n    volume  = {abs/2305.18290},\\n    url     = {https://api.semanticscholar.org/CorpusID:258959321}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eRafailov2023DirectPO\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eDirect Preference Optimization: Your Language Model is Secretly a Reward Model\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eRafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eArXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003evolume\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eabs/2305.18290\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://api.semanticscholar.org/CorpusID:258959321\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003cem\\u003esolve intelligence... then use that to solve everything else.\\u003c/em\\u003e - Demis Hassabis\\u003c/p\\u003e\\n\\u003c/article\\u003e\",\"loaded\":true,\"timedOut\":false,\"errorMessage\":null,\"headerInfo\":{\"toc\":[{\"level\":2,\"text\":\"x-transformers\",\"anchor\":\"x-transformers\",\"htmlText\":\"x-transformers\"},{\"level\":2,\"text\":\"Install\",\"anchor\":\"install\",\"htmlText\":\"Install\"},{\"level\":2,\"text\":\"Usage\",\"anchor\":\"usage\",\"htmlText\":\"Usage\"},{\"level\":2,\"text\":\"Dropouts\",\"anchor\":\"dropouts\",\"htmlText\":\"Dropouts\"},{\"level\":2,\"text\":\"Features\",\"anchor\":\"features\",\"htmlText\":\"Features\"},{\"level\":3,\"text\":\"Flash Attention\",\"anchor\":\"flash-attention\",\"htmlText\":\"Flash Attention\"},{\"level\":3,\"text\":\"Augmenting Self-attention with Persistent Memory\",\"anchor\":\"augmenting-self-attention-with-persistent-memory\",\"htmlText\":\"Augmenting Self-attention with Persistent Memory\"},{\"level\":3,\"text\":\"Memory Transformers\",\"anchor\":\"memory-transformers\",\"htmlText\":\"Memory Transformers\"},{\"level\":3,\"text\":\"Transformers Without Tears\",\"anchor\":\"transformers-without-tears\",\"htmlText\":\"Transformers Without Tears\"},{\"level\":3,\"text\":\"Root Mean Square Layer Normalization\",\"anchor\":\"root-mean-square-layer-normalization\",\"htmlText\":\"Root Mean Square Layer Normalization\"},{\"level\":3,\"text\":\"GLU Variants Improve Transformer\",\"anchor\":\"glu-variants-improve-transformer\",\"htmlText\":\"GLU Variants Improve Transformer\"},{\"level\":3,\"text\":\"No Bias in Feedforward\",\"anchor\":\"no-bias-in-feedforward\",\"htmlText\":\"No Bias in Feedforward\"},{\"level\":3,\"text\":\"ReLU\u00b2\",\"anchor\":\"relu\",\"htmlText\":\"ReLU\u00b2\"},{\"level\":3,\"text\":\"Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection\",\"anchor\":\"explicit-sparse-transformer-concentrated-attention-through-explicit-selection\",\"htmlText\":\"Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection\"},{\"level\":3,\"text\":\"Talking-Heads Attention\",\"anchor\":\"talking-heads-attention\",\"htmlText\":\"Talking-Heads Attention\"},{\"level\":3,\"text\":\"One Write-Head Is All You Need\",\"anchor\":\"one-write-head-is-all-you-need\",\"htmlText\":\"One Write-Head Is All You Need\"},{\"level\":3,\"text\":\"Attention on Attention for Image Captioning\",\"anchor\":\"attention-on-attention-for-image-captioning\",\"htmlText\":\"Attention on Attention for Image Captioning\"},{\"level\":3,\"text\":\"Intra-attention Gating on Values\",\"anchor\":\"intra-attention-gating-on-values\",\"htmlText\":\"Intra-attention Gating on Values\"},{\"level\":3,\"text\":\"Improving Transformer Models by Reordering their Sublayers\",\"anchor\":\"improving-transformer-models-by-reordering-their-sublayers\",\"htmlText\":\"Improving Transformer Models by Reordering their Sublayers\"},{\"level\":3,\"text\":\"Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View\",\"anchor\":\"understanding-and-improving-transformer-from-a-multi-particle-dynamic-system-point-of-view\",\"htmlText\":\"Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View\"},{\"level\":3,\"text\":\"T5's Simplified Relative Positional Encoding\",\"anchor\":\"t5s-simplified-relative-positional-encoding\",\"htmlText\":\"T5's Simplified Relative Positional Encoding\"},{\"level\":3,\"text\":\"Residual Attention\",\"anchor\":\"residual-attention\",\"htmlText\":\"Residual Attention\"},{\"level\":3,\"text\":\"Transformer-XL recurrence\",\"anchor\":\"transformer-xl-recurrence\",\"htmlText\":\"Transformer-XL recurrence\"},{\"level\":3,\"text\":\"Enhanced recurrence\",\"anchor\":\"enhanced-recurrence\",\"htmlText\":\"Enhanced recurrence\"},{\"level\":3,\"text\":\"Gated residual\",\"anchor\":\"gated-residual\",\"htmlText\":\"Gated residual\"},{\"level\":3,\"text\":\"Rotary Positional Embeddings\",\"anchor\":\"rotary-positional-embeddings\",\"htmlText\":\"Rotary Positional Embeddings\"},{\"level\":3,\"text\":\"Dynamic Positional Bias\",\"anchor\":\"dynamic-positional-bias\",\"htmlText\":\"Dynamic Positional Bias\"},{\"level\":3,\"text\":\"ALiBi Positional Embedding\",\"anchor\":\"alibi-positional-embedding\",\"htmlText\":\"ALiBi Positional Embedding\"},{\"level\":3,\"text\":\"Shifted Tokens\",\"anchor\":\"shifted-tokens\",\"htmlText\":\"Shifted Tokens\"},{\"level\":3,\"text\":\"Sandwich Norm\",\"anchor\":\"sandwich-norm\",\"htmlText\":\"Sandwich Norm\"},{\"level\":3,\"text\":\"ResiDual\",\"anchor\":\"residual\",\"htmlText\":\"ResiDual\"},{\"level\":3,\"text\":\"Normformer\",\"anchor\":\"normformer\",\"htmlText\":\"Normformer\"},{\"level\":3,\"text\":\"Cosine Sim Attention\",\"anchor\":\"cosine-sim-attention\",\"htmlText\":\"Cosine Sim Attention\"},{\"level\":3,\"text\":\"QK RMSNorm\",\"anchor\":\"qk-rmsnorm\",\"htmlText\":\"QK RMSNorm\"},{\"level\":3,\"text\":\"Turning off absolute positional embedding\",\"anchor\":\"turning-off-absolute-positional-embedding\",\"htmlText\":\"Turning off absolute positional embedding\"},{\"level\":3,\"text\":\"Forgetful Causal Mask\",\"anchor\":\"forgetful-causal-mask\",\"htmlText\":\"Forgetful Causal Mask\"},{\"level\":2,\"text\":\"Miscellaneous\",\"anchor\":\"miscellaneous\",\"htmlText\":\"Miscellaneous\"},{\"level\":3,\"text\":\"Cross Attention\",\"anchor\":\"cross-attention\",\"htmlText\":\"Cross Attention\"},{\"level\":3,\"text\":\"Continuous Embeddings\",\"anchor\":\"continuous-embeddings\",\"htmlText\":\"Continuous Embeddings\"},{\"level\":3,\"text\":\"xVal - Continuous and Discrete\",\"anchor\":\"xval---continuous-and-discrete\",\"htmlText\":\"xVal - Continuous and Discrete\"},{\"level\":2,\"text\":\"Citations\",\"anchor\":\"citations\",\"htmlText\":\"Citations\"}],\"siteNavLoginPath\":\"/login?return_to=https%3A%2F%2Fgithub.com%2Flucidrains%2Fx-transformers\"}},{\"displayName\":\"LICENSE\",\"repoName\":\"x-transformers\",\"refName\":\"main\",\"path\":\"LICENSE\",\"preferredFileType\":\"license\",\"tabName\":\"MIT\",\"richText\":null,\"loaded\":false,\"timedOut\":false,\"errorMessage\":null,\"headerInfo\":{\"toc\":null,\"siteNavLoginPath\":\"/login?return_to=https%3A%2F%2Fgithub.com%2Flucidrains%2Fx-transformers\"}}],\"overviewFilesProcessingTime\":258.84709}},\"appPayload\":{\"helpUrl\":\"https://docs.github.com\",\"findFileWorkerPath\":\"/assets-cdn/worker/find-file-worker-32bb159cc57c.js\",\"findInFileWorkerPath\":\"/assets-cdn/worker/find-in-file-worker-c6704d501c10.js\",\"githubDevUrl\":null,\"enabled_features\":{\"code_nav_ui_events\":false,\"copilot_conversational_ux\":false,\"copilot_conversational_ux_embedding_update\":false,\"copilot_popover_file_editor_header\":false,\"copilot_smell_icebreaker_ux\":true,\"copilot_workspace\":false,\"codeview_firefox_inert\":true}}}}</script>  <div data-target=\"react-partial.reactRoot\"><style data-styled=\"true\" data-styled-version=\"5.3.6\">.cgQnMS{font-weight:600;font-size:32px;margin:0;}/*!sc*/data-styled.g1[id=\"Heading__StyledHeading-sc-1c1dgg0-0\"]{content:\"cgQnMS,\"}/*!sc*/.izjvBm{margin-top:16px;margin-bottom:16px;}/*!sc*/.rPQgy{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/.eUMEDg{margin-bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;row-gap:16px;}/*!sc*/.eLcVee{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;padding-bottom:16px;padding-top:8px;}/*!sc*/.hsfLlq{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;gap:8px;}/*!sc*/@media screen and (max-width:320px){.hsfLlq{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}}/*!sc*/.gpKoUz{position:relative;}/*!sc*/@media screen and (max-width:380px){.gpKoUz .ref-selector-button-text-container{max-width:80px;}}/*!sc*/@media screen and (max-width:320px){.gpKoUz{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}.gpKoUz .overview-ref-selector{width:100%;}.gpKoUz .overview-ref-selector > span{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;}.gpKoUz .overview-ref-selector > span > span[data-component=\"text\"]{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}}/*!sc*/.kkrdEu{-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}/*!sc*/.bKgizp{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;}/*!sc*/.iPGYsi{margin-right:4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.dKmYfk{font-size:14px;min-width:0;max-width:125px;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}/*!sc*/.trpoQ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;pointer-events:none;}/*!sc*/.laYubZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (max-width:1079px){.laYubZ{display:none;}}/*!sc*/.swnaL{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (min-width:1080px){.swnaL{display:none;}}/*!sc*/@media screen and (max-width:543px){.swnaL{display:none;}}/*!sc*/.bWpuBf{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-left:8px;gap:8px;}/*!sc*/.grHjNb{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:8px;}/*!sc*/@media screen and (max-width:543px){.grHjNb{display:none;}}/*!sc*/.dXTsqj{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (max-width:1011px){.dXTsqj{display:none;}}/*!sc*/.dCOrmu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (min-width:1012px){.dCOrmu{display:none;}}/*!sc*/@media screen and (max-width:544px){.bVvbgP{display:none;}}/*!sc*/.bNDvfp{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (min-width:544px){.bNDvfp{display:none;}}/*!sc*/.yfPnm{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;gap:16px;}/*!sc*/.cAQuiW{width:100%;border-collapse:separate;border-spacing:0;border:1px solid;border-color:var(--borderColor-default,var(--color-border-default,#d0d7de));border-radius:6px;table-layout:fixed;overflow:unset;}/*!sc*/.iiUlLN{height:0px;line-height:0px;}/*!sc*/.iiUlLN tr{height:0px;font-size:0px;}/*!sc*/.jmggSN{padding:16px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));font-size:12px;text-align:left;height:40px;}/*!sc*/.jmggSN th{padding-left:16px;background-color:var(--bgColor-muted,var(--color-canvas-subtle,#f6f8fa));}/*!sc*/.kvYunM{width:100%;border-top-left-radius:6px;}/*!sc*/@media screen and (min-width:544px){.kvYunM{display:none;}}/*!sc*/.hrLuxA{width:40%;border-top-left-radius:6px;}/*!sc*/@media screen and (max-width:543px){.hrLuxA{display:none;}}/*!sc*/@media screen and (max-width:543px){.ePjhhA{display:none;}}/*!sc*/.cuEKae{text-align:right;padding-right:16px;width:136px;border-top-right-radius:6px;}/*!sc*/.jEbBOT{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));font-size:12px;height:40px;}/*!sc*/.bTxCvM{background-color:var(--bgColor-muted,var(--color-canvas-subtle,#f6f8fa));padding:4px;border-top-left-radius:6px;border-top-right-radius:6px;}/*!sc*/.eYedVD{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;gap:8px;min-width:273px;padding-right:8px;padding-left:16px;padding-top:8px;padding-bottom:8px;}/*!sc*/.jGfYmh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:8px;}/*!sc*/.lhFvfi{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/.bqgLjk{display:inherit;}/*!sc*/@media screen and (min-width:544px){.bqgLjk{display:none;}}/*!sc*/@media screen and (min-width:768px){.bqgLjk{display:none;}}/*!sc*/.epsqEd{text-align:center;vertical-align:center;height:40px;border-top:1px solid;border-color:var(--borderColor-default,var(--color-border-default,#d0d7de));}/*!sc*/.ldpruc{border-top:1px solid var(--borderColor-default,var(--color-border-default));cursor:pointer;}/*!sc*/.ehcSsh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;gap:16px;}/*!sc*/.iGmlUb{border:1px solid;border-color:var(--borderColor-default,var(--color-border-default,#d0d7de));border-radius:6px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}/*!sc*/@media screen and (max-width:543px){.iGmlUb{margin-left:-16px;margin-right:-16px;max-width:calc(100% + 32px);}}/*!sc*/@media screen and (min-width:544px){.iGmlUb{max-width:100%;}}/*!sc*/.iRQGXA{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;border-bottom:1px solid;border-bottom-color:var(--borderColor-default,var(--color-border-default,#d0d7de));-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-right:8px;position:-webkit-sticky;position:sticky;top:0;background-color:var(--bgColor-default,var(--color-canvas-default,#ffffff));z-index:1;border-top-left-radius:6px;border-top-right-radius:6px;}/*!sc*/.dvTdPK{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-left:8px;padding-right:8px;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;border-bottom:none;border-bottom-color:var(--borderColor-muted,var(--color-border-muted,hsla(210,18%,87%,1)));align:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-height:48px;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;max-width:100%;}/*!sc*/.gwuIGu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/.kOxwQs{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;margin-right:8px;}/*!sc*/.kOgeFj{font-weight:600;}/*!sc*/.bJMeLZ{padding:32px;overflow:auto;}/*!sc*/data-styled.g2[id=\"Box-sc-g0xbh4-0\"]{content:\"izjvBm,rPQgy,eUMEDg,eLcVee,hsfLlq,gpKoUz,kkrdEu,bKgizp,iPGYsi,dKmYfk,trpoQ,laYubZ,swnaL,bWpuBf,grHjNb,dXTsqj,dCOrmu,bVvbgP,bNDvfp,yfPnm,cAQuiW,iiUlLN,jmggSN,kvYunM,hrLuxA,ePjhhA,cuEKae,jEbBOT,bTxCvM,eYedVD,jGfYmh,lhFvfi,bqgLjk,epsqEd,ldpruc,ehcSsh,iGmlUb,iRQGXA,dvTdPK,gwuIGu,kOxwQs,kOgeFj,bJMeLZ,\"}/*!sc*/.bOMzPg{min-width:0;}/*!sc*/.eUGNHp{font-weight:600;}/*!sc*/.dALsKK{color:var(--fgColor-default,var(--color-fg-default,#1F2328));}/*!sc*/data-styled.g6[id=\"Text-sc-17v1xeu-0\"]{content:\"bOMzPg,eUGNHp,dALsKK,\"}/*!sc*/.dheQRw{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));-webkit-text-decoration:none;text-decoration:none;}/*!sc*/[data-a11y-link-underlines='true'] .Link__StyledLink-sc-14289xe-0[data-inline='true']{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.dheQRw:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.dheQRw:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/.vLMkZ{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));-webkit-text-decoration:none;text-decoration:none;position:relative;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;color:var(--fgColor-default,var(--color-fg-default,#1F2328));text-align:center;-webkit-text-decoration:none;text-decoration:none;line-height:calc(20/14);border-radius:6px;font-size:14px;padding-left:8px;padding-right:8px;padding-top:calc((2rem - 1.25rem) / 2);padding-bottom:calc((2rem - 1.25rem) / 2);}/*!sc*/[data-a11y-link-underlines='true'] .Link__StyledLink-sc-14289xe-0[data-inline='true']{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.vLMkZ:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.vLMkZ:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/.vLMkZ span[data-component=\"icon\"]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/@media (hover:hover){.vLMkZ:hover{background-color:var(--bgColor-neutral-muted,var(--color-neutral-muted,rgba(175,184,193,0.2)));-webkit-transition:background .12s ease-out;transition:background .12s ease-out;-webkit-text-decoration:none;text-decoration:none;}}/*!sc*/.vLMkZ:focus{outline:2px solid transparent;}/*!sc*/.vLMkZ:focus{box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.vLMkZ:focus:not(:focus-visible){box-shadow:none;}/*!sc*/.vLMkZ:focus-visible{outline:2px solid transparent;box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.vLMkZ span[data-content]::before{content:attr(data-content);display:block;height:0;font-weight:600;visibility:hidden;white-space:nowrap;}/*!sc*/.vLMkZ::after{position:absolute;right:50%;bottom:calc(50% - 25px);width:100%;height:2px;content:\"\";background-color:var(--underlineNav-borderColor-active,var(--color-primer-border-active,#fd8c73));border-radius:0;-webkit-transform:translate(50%,-50%);-ms-transform:translate(50%,-50%);transform:translate(50%,-50%);}/*!sc*/@media (forced-colors:active){.vLMkZ::after{background-color:LinkText;}}/*!sc*/.bhqztV{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));-webkit-text-decoration:none;text-decoration:none;position:relative;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;color:var(--fgColor-default,var(--color-fg-default,#1F2328));text-align:center;-webkit-text-decoration:none;text-decoration:none;line-height:calc(20/14);border-radius:6px;font-size:14px;padding-left:8px;padding-right:8px;padding-top:calc((2rem - 1.25rem) / 2);padding-bottom:calc((2rem - 1.25rem) / 2);}/*!sc*/[data-a11y-link-underlines='true'] .Link__StyledLink-sc-14289xe-0[data-inline='true']{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.bhqztV:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.bhqztV:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/.bhqztV span[data-component=\"icon\"]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/@media (hover:hover){.bhqztV:hover{background-color:var(--bgColor-neutral-muted,var(--color-neutral-muted,rgba(175,184,193,0.2)));-webkit-transition:background .12s ease-out;transition:background .12s ease-out;-webkit-text-decoration:none;text-decoration:none;}}/*!sc*/.bhqztV:focus{outline:2px solid transparent;}/*!sc*/.bhqztV:focus{box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.bhqztV:focus:not(:focus-visible){box-shadow:none;}/*!sc*/.bhqztV:focus-visible{outline:2px solid transparent;box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.bhqztV span[data-content]::before{content:attr(data-content);display:block;height:0;font-weight:600;visibility:hidden;white-space:nowrap;}/*!sc*/.bhqztV::after{position:absolute;right:50%;bottom:calc(50% - 25px);width:100%;height:2px;content:\"\";background-color:transparent;border-radius:0;-webkit-transform:translate(50%,-50%);-ms-transform:translate(50%,-50%);transform:translate(50%,-50%);}/*!sc*/@media (forced-colors:active){.bhqztV::after{background-color:transparent;}}/*!sc*/data-styled.g8[id=\"Link__StyledLink-sc-14289xe-0\"]{content:\"dheQRw,vLMkZ,bhqztV,\"}/*!sc*/.izDscS{border-radius:6px;border:1px solid;border-color:var(--button-default-borderColor-rest,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:var(--button-default-bgColor-rest,var(--color-btn-bg,#f6f8fa));box-shadow:var(--button-default-shadow-resting,var(--color-btn-shadow,0 1px 0 rgba(31,35,40,0.04))),var(--button-default-shadow-inset,var(--color-btn-inset-shadow,inset 0 1px 0 rgba(255,255,255,0.25)));}/*!sc*/.izDscS:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.izDscS:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.izDscS:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.izDscS[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.izDscS[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.izDscS:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.izDscS:active{-webkit-transition:none;transition:none;}/*!sc*/.izDscS[data-inactive]{cursor:auto;}/*!sc*/.izDscS:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));border-color:var(--button-default-borderColor-disabled,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));}/*!sc*/.izDscS:disabled [data-component=ButtonCounter]{color:inherit;}/*!sc*/@media (forced-colors:active){.izDscS:focus{outline:solid 1px transparent;}}/*!sc*/.izDscS [data-component=ButtonCounter]{font-size:12px;background-color:var(--buttonCounter-default-bgColor-rest,var(--color-btn-counter-bg,rgba(31,35,40,0.08)));}/*!sc*/.izDscS[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.izDscS[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.izDscS[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.izDscS[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.izDscS[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.izDscS[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.izDscS[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.izDscS[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.izDscS[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.izDscS[data-block=\"block\"]{width:100%;}/*!sc*/.izDscS[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.izDscS[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.izDscS [data-component=\"leadingVisual\"]{grid-area:leadingVisual;}/*!sc*/.izDscS [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.izDscS [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.izDscS [data-component=\"trailingAction\"]{margin-right:-4px;}/*!sc*/.izDscS [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.izDscS [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.izDscS:hover:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-hover,var(--color-btn-hover-bg,#f3f4f6));border-color:var(--button-default-borderColor-hover,var(--button-default-borderColor-hover,var(--color-btn-hover-border,rgba(31,35,40,0.15))));}/*!sc*/.izDscS:active:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/.izDscS[aria-expanded=true]{background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/.izDscS [data-component=\"leadingVisual\"],.izDscS [data-component=\"trailingVisual\"],.izDscS [data-component=\"trailingAction\"]{color:var(--button-color,var(--fgColor-muted,var(--color-fg-muted,#656d76)));}/*!sc*/.izDscS{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/.izDscS svg{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.izDscS > span{width:inherit;}/*!sc*/.cuOWTR{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:transparent;box-shadow:none;}/*!sc*/.cuOWTR:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.cuOWTR:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.cuOWTR:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.cuOWTR[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.cuOWTR[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.cuOWTR:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.cuOWTR:active{-webkit-transition:none;transition:none;}/*!sc*/.cuOWTR[data-inactive]{cursor:auto;}/*!sc*/.cuOWTR:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.cuOWTR:disabled [data-component=ButtonCounter],.cuOWTR:disabled [data-component=\"leadingVisual\"],.cuOWTR:disabled [data-component=\"trailingAction\"]{color:inherit;}/*!sc*/@media (forced-colors:active){.cuOWTR:focus{outline:solid 1px transparent;}}/*!sc*/.cuOWTR [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.cuOWTR[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.cuOWTR[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.cuOWTR[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.cuOWTR[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.cuOWTR[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.cuOWTR[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.cuOWTR[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.cuOWTR[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.cuOWTR[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.cuOWTR[data-block=\"block\"]{width:100%;}/*!sc*/.cuOWTR[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.cuOWTR[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.cuOWTR [data-component=\"leadingVisual\"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.cuOWTR [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.cuOWTR [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.cuOWTR [data-component=\"trailingAction\"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.cuOWTR [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.cuOWTR [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.cuOWTR:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));}/*!sc*/.cuOWTR:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));}/*!sc*/.cuOWTR[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/.cuOWTR[data-component=\"IconButton\"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.cuOWTR[data-no-visuals]{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.cuOWTR:has([data-component=\"ButtonCounter\"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/.cuOWTR:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.cuOWTR:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/.cuOWTR{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));padding-left:4px;padding-right:4px;}/*!sc*/.cuOWTR span[data-component=\"leadingVisual\"]{margin-right:4px !important;}/*!sc*/.tDSzd{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:transparent;box-shadow:none;}/*!sc*/.tDSzd:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.tDSzd:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.tDSzd:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.tDSzd[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.tDSzd[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.tDSzd:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.tDSzd:active{-webkit-transition:none;transition:none;}/*!sc*/.tDSzd[data-inactive]{cursor:auto;}/*!sc*/.tDSzd:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.tDSzd:disabled [data-component=ButtonCounter],.tDSzd:disabled [data-component=\"leadingVisual\"],.tDSzd:disabled [data-component=\"trailingAction\"]{color:inherit;}/*!sc*/@media (forced-colors:active){.tDSzd:focus{outline:solid 1px transparent;}}/*!sc*/.tDSzd [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.tDSzd[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.tDSzd[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.tDSzd[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.tDSzd[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.tDSzd[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.tDSzd[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.tDSzd[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.tDSzd[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.tDSzd[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.tDSzd[data-block=\"block\"]{width:100%;}/*!sc*/.tDSzd[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.tDSzd[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.tDSzd [data-component=\"leadingVisual\"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.tDSzd [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.tDSzd [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.tDSzd [data-component=\"trailingAction\"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.tDSzd [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.tDSzd [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.tDSzd:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));}/*!sc*/.tDSzd:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));}/*!sc*/.tDSzd[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/.tDSzd[data-component=\"IconButton\"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.tDSzd[data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.tDSzd:has([data-component=\"ButtonCounter\"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/.tDSzd:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.tDSzd:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/.ftZGca{border-radius:6px;border:1px solid;border-color:var(--button-default-borderColor-rest,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:var(--button-default-bgColor-rest,var(--color-btn-bg,#f6f8fa));box-shadow:var(--button-default-shadow-resting,var(--color-btn-shadow,0 1px 0 rgba(31,35,40,0.04))),var(--button-default-shadow-inset,var(--color-btn-inset-shadow,inset 0 1px 0 rgba(255,255,255,0.25)));}/*!sc*/.ftZGca:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.ftZGca:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.ftZGca:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.ftZGca[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.ftZGca[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.ftZGca:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.ftZGca:active{-webkit-transition:none;transition:none;}/*!sc*/.ftZGca[data-inactive]{cursor:auto;}/*!sc*/.ftZGca:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));border-color:var(--button-default-borderColor-disabled,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));}/*!sc*/.ftZGca:disabled [data-component=ButtonCounter]{color:inherit;}/*!sc*/@media (forced-colors:active){.ftZGca:focus{outline:solid 1px transparent;}}/*!sc*/.ftZGca [data-component=ButtonCounter]{font-size:12px;background-color:var(--buttonCounter-default-bgColor-rest,var(--color-btn-counter-bg,rgba(31,35,40,0.08)));}/*!sc*/.ftZGca[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.ftZGca[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.ftZGca[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.ftZGca[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.ftZGca[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.ftZGca[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.ftZGca[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.ftZGca[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.ftZGca[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.ftZGca[data-block=\"block\"]{width:100%;}/*!sc*/.ftZGca[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.ftZGca[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.ftZGca [data-component=\"leadingVisual\"]{grid-area:leadingVisual;}/*!sc*/.ftZGca [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.ftZGca [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.ftZGca [data-component=\"trailingAction\"]{margin-right:-4px;}/*!sc*/.ftZGca [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.ftZGca [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.ftZGca:hover:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-hover,var(--color-btn-hover-bg,#f3f4f6));border-color:var(--button-default-borderColor-hover,var(--button-default-borderColor-hover,var(--color-btn-hover-border,rgba(31,35,40,0.15))));}/*!sc*/.ftZGca:active:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/.ftZGca[aria-expanded=true]{background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/.ftZGca [data-component=\"leadingVisual\"],.ftZGca [data-component=\"trailingVisual\"],.ftZGca [data-component=\"trailingAction\"]{color:var(--button-color,var(--fgColor-muted,var(--color-fg-muted,#656d76)));}/*!sc*/.gYvpXq{border-radius:6px;border:1px solid;border-color:var(--button-primary-borderColor-rest,var(--color-btn-primary-border,rgba(31,35,40,0.15)));font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-primary-fgColor-rest,var(--color-btn-primary-text,#ffffff));background-color:var(--button-primary-bgColor-rest,var(--color-btn-primary-bg,#1f883d));box-shadow:var(--shadow-resting-small,var(--color-btn-primary-shadow,0 1px 0 rgba(31,35,40,0.1)));}/*!sc*/.gYvpXq:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.gYvpXq:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.gYvpXq:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.gYvpXq[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.gYvpXq[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.gYvpXq:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.gYvpXq:active{-webkit-transition:none;transition:none;}/*!sc*/.gYvpXq[data-inactive]{cursor:auto;}/*!sc*/.gYvpXq:disabled{cursor:not-allowed;box-shadow:none;color:var(--button-primary-fgColor-disabled,var(--color-btn-primary-disabled-text,rgba(255,255,255,0.8)));background-color:var(--button-primary-bgColor-disabled,var(--color-btn-primary-disabled-bg,#94d3a2));border-color:var(--button-primary-borderColor-disabled,var(--color-btn-primary-disabled-border,rgba(31,35,40,0.15)));}/*!sc*/.gYvpXq:disabled [data-component=ButtonCounter]{color:inherit;}/*!sc*/@media (forced-colors:active){.gYvpXq:focus{outline:solid 1px transparent;}}/*!sc*/.gYvpXq [data-component=ButtonCounter]{font-size:12px;background-color:var(--buttonCounter-primary-bgColor-rest,var(--color-btn-primary-counter-bg,rgba(0,45,17,0.2)));color:var(--button-primary-fgColor-rest,var(--color-btn-primary-text,#ffffff));}/*!sc*/.gYvpXq[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.gYvpXq[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.gYvpXq[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.gYvpXq[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.gYvpXq[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.gYvpXq[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.gYvpXq[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.gYvpXq[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.gYvpXq[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.gYvpXq[data-block=\"block\"]{width:100%;}/*!sc*/.gYvpXq[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.gYvpXq[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.gYvpXq [data-component=\"leadingVisual\"]{grid-area:leadingVisual;}/*!sc*/.gYvpXq [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.gYvpXq [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.gYvpXq [data-component=\"trailingAction\"]{margin-right:-4px;}/*!sc*/.gYvpXq [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.gYvpXq [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.gYvpXq:hover:not([disabled]):not([data-inactive]){color:btn.primary.hoverText;background-color:var(--button-primary-bgColor-hover,var(--color-btn-primary-hover-bg,#1a7f37));}/*!sc*/.gYvpXq:focus:not([disabled]){box-shadow:inset 0 0 0 3px;}/*!sc*/.gYvpXq:focus-visible:not([disabled]){box-shadow:inset 0 0 0 3px;}/*!sc*/.gYvpXq:active:not([disabled]):not([data-inactive]){background-color:var(--button-primary-bgColor-active,var(--color-btn-primary-selected-bg,hsla(137,66%,28%,1)));box-shadow:var(--button-primary-shadow-selected,var(--color-btn-primary-selected-shadow,inset 0 1px 0 rgba(0,45,17,0.2)));}/*!sc*/.gYvpXq[aria-expanded=true]{background-color:var(--button-primary-bgColor-active,var(--color-btn-primary-selected-bg,hsla(137,66%,28%,1)));box-shadow:var(--button-primary-shadow-selected,var(--color-btn-primary-selected-shadow,inset 0 1px 0 rgba(0,45,17,0.2)));}/*!sc*/.gYvpXq svg{color:fg.primary;}/*!sc*/.fAkXQN{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--fgColor-default,var(--color-fg-default,#1F2328));background-color:transparent;box-shadow:none;}/*!sc*/.fAkXQN:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.fAkXQN:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.fAkXQN:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.fAkXQN[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.fAkXQN[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.fAkXQN:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.fAkXQN:active{-webkit-transition:none;transition:none;}/*!sc*/.fAkXQN[data-inactive]{cursor:auto;}/*!sc*/.fAkXQN:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.fAkXQN:disabled [data-component=ButtonCounter],.fAkXQN:disabled [data-component=\"leadingVisual\"],.fAkXQN:disabled [data-component=\"trailingAction\"]{color:inherit;}/*!sc*/@media (forced-colors:active){.fAkXQN:focus{outline:solid 1px transparent;}}/*!sc*/.fAkXQN [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.fAkXQN[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.fAkXQN[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.fAkXQN[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.fAkXQN[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.fAkXQN[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.fAkXQN[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.fAkXQN[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.fAkXQN[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.fAkXQN[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.fAkXQN[data-block=\"block\"]{width:100%;}/*!sc*/.fAkXQN[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.fAkXQN[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.fAkXQN [data-component=\"leadingVisual\"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.fAkXQN [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.fAkXQN [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.fAkXQN [data-component=\"trailingAction\"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.fAkXQN [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.fAkXQN [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.fAkXQN:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.fAkXQN:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.fAkXQN[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/.fAkXQN[data-component=\"IconButton\"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.fAkXQN[data-no-visuals]{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.fAkXQN:has([data-component=\"ButtonCounter\"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/.fAkXQN:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.fAkXQN:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/.fAkXQN:focus:not([disabled]){-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.jPraEl{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:transparent;box-shadow:none;}/*!sc*/.jPraEl:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.jPraEl:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.jPraEl:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.jPraEl[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.jPraEl[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.jPraEl:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.jPraEl:active{-webkit-transition:none;transition:none;}/*!sc*/.jPraEl[data-inactive]{cursor:auto;}/*!sc*/.jPraEl:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.jPraEl:disabled [data-component=ButtonCounter],.jPraEl:disabled [data-component=\"leadingVisual\"],.jPraEl:disabled [data-component=\"trailingAction\"]{color:inherit;}/*!sc*/@media (forced-colors:active){.jPraEl:focus{outline:solid 1px transparent;}}/*!sc*/.jPraEl [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.jPraEl[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.jPraEl[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.jPraEl[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.jPraEl[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.jPraEl[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.jPraEl[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.jPraEl[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.jPraEl[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.jPraEl[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.jPraEl[data-block=\"block\"]{width:100%;}/*!sc*/.jPraEl[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.jPraEl[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.jPraEl [data-component=\"leadingVisual\"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.jPraEl [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.jPraEl [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.jPraEl [data-component=\"trailingAction\"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.jPraEl [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.jPraEl [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.jPraEl:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));}/*!sc*/.jPraEl:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));}/*!sc*/.jPraEl[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/.jPraEl[data-component=\"IconButton\"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.jPraEl[data-no-visuals]{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.jPraEl:has([data-component=\"ButtonCounter\"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/.jPraEl:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.jPraEl:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/.jPraEl{color:var(--fgColor-muted,var(--color-fg-subtle,#6e7781));padding-left:8px;padding-right:8px;}/*!sc*/data-styled.g9[id=\"types__StyledButton-sc-ws60qy-0\"]{content:\"izDscS,cuOWTR,tDSzd,ftZGca,gYvpXq,fAkXQN,jPraEl,\"}/*!sc*/.rTZSs{position:absolute;width:1px;height:1px;padding:0;margin:-1px;overflow:hidden;-webkit-clip:rect(0,0,0,0);clip:rect(0,0,0,0);white-space:nowrap;border-width:0;}/*!sc*/data-styled.g10[id=\"_VisuallyHidden__VisuallyHidden-sc-11jhm7a-0\"]{content:\"rTZSs,\"}/*!sc*/.fUpWeN{display:inline-block;overflow:hidden;text-overflow:ellipsis;vertical-align:top;white-space:nowrap;max-width:125px;max-width:100%;}/*!sc*/data-styled.g15[id=\"Truncate__StyledTruncate-sc-23o1d2-0\"]{content:\"fUpWeN,\"}/*!sc*/.dMjscx{position:relative;display:inline-block;}/*!sc*/.dMjscx::before{position:absolute;z-index:1000001;display:none;width:0px;height:0px;color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));pointer-events:none;content:'';border:6px solid transparent;opacity:0;}/*!sc*/.dMjscx::after{position:absolute;z-index:1000000;display:none;padding:0.5em 0.75em;font:normal normal 11px/1.5 -apple-system,BlinkMacSystemFont,\"Segoe UI\",\"Noto Sans\",Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\";-webkit-font-smoothing:subpixel-antialiased;color:var(--fgColor-onEmphasis,var(--color-fg-on-emphasis,#ffffff));text-align:center;-webkit-text-decoration:none;text-decoration:none;text-shadow:none;text-transform:none;-webkit-letter-spacing:normal;-moz-letter-spacing:normal;-ms-letter-spacing:normal;letter-spacing:normal;word-wrap:break-word;white-space:pre;pointer-events:none;content:attr(aria-label);background:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));border-radius:6px;opacity:0;}/*!sc*/@-webkit-keyframes tooltip-appear{from{opacity:0;}to{opacity:1;}}/*!sc*/@keyframes tooltip-appear{from{opacity:0;}to{opacity:1;}}/*!sc*/.dMjscx:hover::before,.dMjscx:active::before,.dMjscx:focus::before,.dMjscx:focus-within::before,.dMjscx:hover::after,.dMjscx:active::after,.dMjscx:focus::after,.dMjscx:focus-within::after{display:inline-block;-webkit-text-decoration:none;text-decoration:none;-webkit-animation-name:tooltip-appear;animation-name:tooltip-appear;-webkit-animation-duration:0.1s;animation-duration:0.1s;-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-timing-function:ease-in;animation-timing-function:ease-in;-webkit-animation-delay:0.4s;animation-delay:0.4s;}/*!sc*/.dMjscx.tooltipped-no-delay:hover::before,.dMjscx.tooltipped-no-delay:active::before,.dMjscx.tooltipped-no-delay:focus::before,.dMjscx.tooltipped-no-delay:focus-within::before,.dMjscx.tooltipped-no-delay:hover::after,.dMjscx.tooltipped-no-delay:active::after,.dMjscx.tooltipped-no-delay:focus::after,.dMjscx.tooltipped-no-delay:focus-within::after{-webkit-animation-delay:0s;animation-delay:0s;}/*!sc*/.dMjscx.tooltipped-multiline:hover::after,.dMjscx.tooltipped-multiline:active::after,.dMjscx.tooltipped-multiline:focus::after,.dMjscx.tooltipped-multiline:focus-within::after{display:table-cell;}/*!sc*/.dMjscx.tooltipped-s::after,.dMjscx.tooltipped-se::after,.dMjscx.tooltipped-sw::after{top:100%;right:50%;margin-top:6px;}/*!sc*/.dMjscx.tooltipped-s::before,.dMjscx.tooltipped-se::before,.dMjscx.tooltipped-sw::before{top:auto;right:50%;bottom:-7px;margin-right:-6px;border-bottom-color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));}/*!sc*/.dMjscx.tooltipped-se::after{right:auto;left:50%;margin-left:-16px;}/*!sc*/.dMjscx.tooltipped-sw::after{margin-right:-16px;}/*!sc*/.dMjscx.tooltipped-n::after,.dMjscx.tooltipped-ne::after,.dMjscx.tooltipped-nw::after{right:50%;bottom:100%;margin-bottom:6px;}/*!sc*/.dMjscx.tooltipped-n::before,.dMjscx.tooltipped-ne::before,.dMjscx.tooltipped-nw::before{top:-7px;right:50%;bottom:auto;margin-right:-6px;border-top-color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));}/*!sc*/.dMjscx.tooltipped-ne::after{right:auto;left:50%;margin-left:-16px;}/*!sc*/.dMjscx.tooltipped-nw::after{margin-right:-16px;}/*!sc*/.dMjscx.tooltipped-s::after,.dMjscx.tooltipped-n::after{-webkit-transform:translateX(50%);-ms-transform:translateX(50%);transform:translateX(50%);}/*!sc*/.dMjscx.tooltipped-w::after{right:100%;bottom:50%;margin-right:6px;-webkit-transform:translateY(50%);-ms-transform:translateY(50%);transform:translateY(50%);}/*!sc*/.dMjscx.tooltipped-w::before{top:50%;bottom:50%;left:-7px;margin-top:-6px;border-left-color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));}/*!sc*/.dMjscx.tooltipped-e::after{bottom:50%;left:100%;margin-left:6px;-webkit-transform:translateY(50%);-ms-transform:translateY(50%);transform:translateY(50%);}/*!sc*/.dMjscx.tooltipped-e::before{top:50%;right:-7px;bottom:50%;margin-top:-6px;border-right-color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));}/*!sc*/.dMjscx.tooltipped-multiline::after{width:-webkit-max-content;width:-moz-max-content;width:max-content;max-width:250px;word-wrap:break-word;white-space:pre-line;border-collapse:separate;}/*!sc*/.dMjscx.tooltipped-multiline.tooltipped-s::after,.dMjscx.tooltipped-multiline.tooltipped-n::after{right:auto;left:50%;-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%);transform:translateX(-50%);}/*!sc*/.dMjscx.tooltipped-multiline.tooltipped-w::after,.dMjscx.tooltipped-multiline.tooltipped-e::after{right:100%;}/*!sc*/.dMjscx.tooltipped-align-right-2::after{right:0;margin-right:0;}/*!sc*/.dMjscx.tooltipped-align-right-2::before{right:15px;}/*!sc*/.dMjscx.tooltipped-align-left-2::after{left:0;margin-left:0;}/*!sc*/.dMjscx.tooltipped-align-left-2::before{left:10px;}/*!sc*/data-styled.g18[id=\"Tooltip__TooltipBase-sc-17tf59c-0\"]{content:\"dMjscx,\"}/*!sc*/.bPgibo{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;list-style:none;white-space:nowrap;padding-top:0;padding-bottom:0;padding-left:0;padding-right:0;margin:0;margin-bottom:-1px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;gap:8px;position:relative;}/*!sc*/data-styled.g103[id=\"UnderlineNav__NavigationList-sc-1jfr31k-0\"]{content:\"bPgibo,\"}/*!sc*/</style> <!-- --> <!-- --> <div class=\"Box-sc-g0xbh4-0 izjvBm\"><div class=\"Box-sc-g0xbh4-0 rPQgy\"><div class=\"Box-sc-g0xbh4-0 eUMEDg\"></div></div><div class=\"Box-sc-g0xbh4-0 eLcVee\"><div class=\"Box-sc-g0xbh4-0 hsfLlq\"><div class=\"Box-sc-g0xbh4-0 gpKoUz\"><button type=\"button\" id=\"branch-picker-repos-header-ref-selector\" aria-haspopup=\"true\" tabindex=\"0\" aria-label=\"main branch\" data-testid=\"anchor-button\" class=\"types__StyledButton-sc-ws60qy-0 izDscS overview-ref-selector\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"text\"><div class=\"Box-sc-g0xbh4-0 bKgizp\"><div class=\"Box-sc-g0xbh4-0 iPGYsi\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-git-branch\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z\"></path></svg></div><div class=\"Box-sc-g0xbh4-0 dKmYfk ref-selector-button-text-container\"><span class=\"Text-sc-17v1xeu-0 bOMzPg\">\u00a0<!-- -->main</span></div></div></span><span data-component=\"trailingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-triangle-down\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z\"></path></svg></span></span></button><button hidden=\"\" data-hotkey-scope=\"read-only-cursor-text-area\"></button></div><div class=\"Box-sc-g0xbh4-0 laYubZ\"><a style=\"--button-color:fg.muted\" type=\"button\" href=\"/lucidrains/x-transformers/branches\" class=\"types__StyledButton-sc-ws60qy-0 cuOWTR\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-git-branch\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z\"></path></svg></span><span data-component=\"text\">Branches</span></span></a><a style=\"--button-color:fg.muted\" type=\"button\" href=\"/lucidrains/x-transformers/tags\" class=\"types__StyledButton-sc-ws60qy-0 cuOWTR\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-tag\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z\"></path></svg></span><span data-component=\"text\">Tags</span></span></a></div><div class=\"Box-sc-g0xbh4-0 swnaL\"><a style=\"--button-color:fg.muted\" type=\"button\" aria-label=\"Go to Branches page\" href=\"/lucidrains/x-transformers/branches\" data-no-visuals=\"true\" class=\"types__StyledButton-sc-ws60qy-0 tDSzd\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-git-branch\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z\"></path></svg></a><a style=\"--button-color:fg.muted\" type=\"button\" aria-label=\"Go to Tags page\" href=\"/lucidrains/x-transformers/tags\" data-no-visuals=\"true\" class=\"types__StyledButton-sc-ws60qy-0 tDSzd\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-tag\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z\"></path></svg></a></div></div><div class=\"Box-sc-g0xbh4-0 bWpuBf\"><div class=\"Box-sc-g0xbh4-0 grHjNb\"><div class=\"Box-sc-g0xbh4-0 dXTsqj\"><!--$!--><template></template><!--/$--></div><div class=\"Box-sc-g0xbh4-0 dCOrmu\"><button type=\"button\" data-no-visuals=\"true\" class=\"types__StyledButton-sc-ws60qy-0 ftZGca\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"text\">Go to file</span></span></button></div><div class=\"react-directory-add-file-icon\"></div><div class=\"react-directory-remove-file-icon\"></div></div><button type=\"button\" id=\":R2il5:\" aria-haspopup=\"true\" tabindex=\"0\" class=\"types__StyledButton-sc-ws60qy-0 gYvpXq\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><div class=\"Box-sc-g0xbh4-0 bVvbgP\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-code\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z\"></path></svg></div></span><span data-component=\"text\">Code</span></span><span data-component=\"trailingAction\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-triangle-down\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z\"></path></svg></span></button><div class=\"Box-sc-g0xbh4-0 bNDvfp\"><button data-component=\"IconButton\" type=\"button\" aria-label=\"Open more actions menu\" id=\":R3il5:\" aria-haspopup=\"true\" tabindex=\"0\" data-no-visuals=\"true\" class=\"types__StyledButton-sc-ws60qy-0 ftZGca\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-kebab-horizontal\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path></svg></button></div></div></div><div class=\"Box-sc-g0xbh4-0 yfPnm\"><div data-hpc=\"true\" class=\"Box-sc-g0xbh4-0\"><button hidden=\"\" data-testid=\"focus-next-element-button\" data-hotkey=\"j\" data-hotkey-scope=\"read-only-cursor-text-area\"></button><button hidden=\"\" data-hotkey=\"j\"></button><button hidden=\"\" data-testid=\"focus-previous-element-button\" data-hotkey=\"k\" data-hotkey-scope=\"read-only-cursor-text-area\"></button><button hidden=\"\" data-hotkey=\"k\"></button><h2 class=\"Heading__StyledHeading-sc-1c1dgg0-0 cgQnMS sr-only\" data-testid=\"screen-reader-heading\" id=\"folders-and-files\">Folders and files</h2><table aria-labelledby=\"folders-and-files\" class=\"Box-sc-g0xbh4-0 cAQuiW\"><thead class=\"Box-sc-g0xbh4-0 iiUlLN\"><tr class=\"Box-sc-g0xbh4-0 jmggSN\"><th colSpan=\"2\" class=\"Box-sc-g0xbh4-0 kvYunM\"><span class=\"Text-sc-17v1xeu-0 eUGNHp\">Name</span></th><th colSpan=\"1\" class=\"Box-sc-g0xbh4-0 hrLuxA\"><span class=\"Text-sc-17v1xeu-0 eUGNHp\">Name</span></th><th class=\"Box-sc-g0xbh4-0 ePjhhA\"><div title=\"Last commit message\" class=\"Truncate__StyledTruncate-sc-23o1d2-0 fUpWeN\"><span class=\"Text-sc-17v1xeu-0 eUGNHp\">Last commit message</span></div></th><th colSpan=\"1\" class=\"Box-sc-g0xbh4-0 cuEKae\"><div title=\"Last commit date\" class=\"Truncate__StyledTruncate-sc-23o1d2-0 fUpWeN\"><span class=\"Text-sc-17v1xeu-0 eUGNHp\">Last commit date</span></div></th></tr></thead><tbody><tr class=\"Box-sc-g0xbh4-0 jEbBOT\"><td colSpan=\"3\" class=\"Box-sc-g0xbh4-0 bTxCvM\"><div class=\"Box-sc-g0xbh4-0 eYedVD\"><h2 class=\"Heading__StyledHeading-sc-1c1dgg0-0 cgQnMS sr-only\" data-testid=\"screen-reader-heading\">Latest commit</h2><div style=\"width:120px\" class=\"Skeleton Skeleton--text\" data-testid=\"loading\">\u00a0</div><div class=\"Box-sc-g0xbh4-0 jGfYmh\"><div data-testid=\"latest-commit-details\" class=\"Box-sc-g0xbh4-0 lhFvfi\"></div><h2 class=\"Heading__StyledHeading-sc-1c1dgg0-0 cgQnMS sr-only\" data-testid=\"screen-reader-heading\">History</h2><a class=\"types__StyledButton-sc-ws60qy-0 fAkXQN react-last-commit-history-group\" href=\"/lucidrains/x-transformers/commits/main/\" data-size=\"small\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-history\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z\"></path></svg></span><span data-component=\"text\"><span class=\"Text-sc-17v1xeu-0 dALsKK\">493 Commits</span></span></span></a><div class=\"Box-sc-g0xbh4-0 bqgLjk\"></div><span role=\"tooltip\" aria-label=\"Commit history\" class=\"Tooltip__TooltipBase-sc-17tf59c-0 dMjscx tooltipped-n\"><a class=\"types__StyledButton-sc-ws60qy-0 fAkXQN react-last-commit-history-icon\" href=\"/lucidrains/x-transformers/commits/main/\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-history\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z\"></path></svg></span></span></a></span></div></div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-0\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\".github\" aria-label=\".github, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/x-transformers/tree/main/.github\">.github</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\".github\" aria-label=\".github, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/x-transformers/tree/main/.github\">.github</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-1\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"examples\" aria-label=\"examples, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/x-transformers/tree/main/examples\">examples</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"examples\" aria-label=\"examples, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/x-transformers/tree/main/examples\">examples</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-2\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"images\" aria-label=\"images, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/x-transformers/tree/main/images\">images</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"images\" aria-label=\"images, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/x-transformers/tree/main/images\">images</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-3\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"x_transformers\" aria-label=\"x_transformers, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/x-transformers/tree/main/x_transformers\">x_transformers</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"x_transformers\" aria-label=\"x_transformers, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/x-transformers/tree/main/x_transformers\">x_transformers</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-4\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\".gitignore\" aria-label=\".gitignore, (File)\" class=\"Link--primary\" href=\"/lucidrains/x-transformers/blob/main/.gitignore\">.gitignore</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\".gitignore\" aria-label=\".gitignore, (File)\" class=\"Link--primary\" href=\"/lucidrains/x-transformers/blob/main/.gitignore\">.gitignore</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-5\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"LICENSE\" aria-label=\"LICENSE, (File)\" class=\"Link--primary\" href=\"/lucidrains/x-transformers/blob/main/LICENSE\">LICENSE</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"LICENSE\" aria-label=\"LICENSE, (File)\" class=\"Link--primary\" href=\"/lucidrains/x-transformers/blob/main/LICENSE\">LICENSE</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-6\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"README.md\" aria-label=\"README.md, (File)\" class=\"Link--primary\" href=\"/lucidrains/x-transformers/blob/main/README.md\">README.md</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"README.md\" aria-label=\"README.md, (File)\" class=\"Link--primary\" href=\"/lucidrains/x-transformers/blob/main/README.md\">README.md</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-7\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"setup.py\" aria-label=\"setup.py, (File)\" class=\"Link--primary\" href=\"/lucidrains/x-transformers/blob/main/setup.py\">setup.py</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"setup.py\" aria-label=\"setup.py, (File)\" class=\"Link--primary\" href=\"/lucidrains/x-transformers/blob/main/setup.py\">setup.py</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"Box-sc-g0xbh4-0 epsqEd d-none\" data-testid=\"view-all-files-row\"><td colSpan=\"3\" class=\"Box-sc-g0xbh4-0 ldpruc\"><div><button class=\"Link__StyledLink-sc-14289xe-0 dheQRw\">View all files</button></div></td></tr></tbody></table></div><div class=\"Box-sc-g0xbh4-0 ehcSsh\"><div class=\"Box-sc-g0xbh4-0 iGmlUb\"><div class=\"Box-sc-g0xbh4-0 iRQGXA\"><h2 class=\"_VisuallyHidden__VisuallyHidden-sc-11jhm7a-0 rTZSs\">Repository files navigation</h2><nav aria-label=\"Repository files\" class=\"Box-sc-g0xbh4-0 dvTdPK\"><ul role=\"list\" class=\"UnderlineNav__NavigationList-sc-1jfr31k-0 bPgibo\"><li class=\"Box-sc-g0xbh4-0 gwuIGu\"><a href=\"#\" aria-current=\"page\" class=\"Link__StyledLink-sc-14289xe-0 vLMkZ\"><span data-component=\"icon\" class=\"Box-sc-g0xbh4-0 kOxwQs\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-book\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z\"></path></svg></span><span data-component=\"text\" data-content=\"README\" class=\"Box-sc-g0xbh4-0 kOgeFj\">README</span></a></li><li class=\"Box-sc-g0xbh4-0 gwuIGu\"><a href=\"#\" class=\"Link__StyledLink-sc-14289xe-0 bhqztV\"><span data-component=\"icon\" class=\"Box-sc-g0xbh4-0 kOxwQs\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-law\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M8.75.75V2h.985c.304 0 .603.08.867.231l1.29.736c.038.022.08.033.124.033h2.234a.75.75 0 0 1 0 1.5h-.427l2.111 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.006.005-.01.01-.045.04c-.21.176-.441.327-.686.45C14.556 10.78 13.88 11 13 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L12.178 4.5h-.162c-.305 0-.604-.079-.868-.231l-1.29-.736a.245.245 0 0 0-.124-.033H8.75V13h2.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1 0-1.5h2.5V3.5h-.984a.245.245 0 0 0-.124.033l-1.289.737c-.265.15-.564.23-.869.23h-.162l2.112 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.016.015-.045.04c-.21.176-.441.327-.686.45C4.556 10.78 3.88 11 3 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L2.178 4.5H1.75a.75.75 0 0 1 0-1.5h2.234a.249.249 0 0 0 .125-.033l1.288-.737c.265-.15.564-.23.869-.23h.984V.75a.75.75 0 0 1 1.5 0Zm2.945 8.477c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L13 6.327Zm-10 0c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L3 6.327Z\"></path></svg></span><span data-component=\"text\" data-content=\"MIT license\" class=\"Box-sc-g0xbh4-0\">MIT license</span></a></li></ul></nav><button style=\"--button-color:fg.subtle\" type=\"button\" aria-label=\"Outline\" id=\":Rdkl5:\" aria-haspopup=\"true\" tabindex=\"0\" class=\"types__StyledButton-sc-ws60qy-0 jPraEl\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-list-unordered\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z\"></path></svg></button></div><div class=\"Box-sc-g0xbh4-0 bJMeLZ js-snippet-clipboard-copy-unpositioned\" data-hpc=\"true\"><article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">x-transformers</h2><a id=\"user-content-x-transformers\" class=\"anchor-element\" aria-label=\"Permalink: x-transformers\" href=\"#x-transformers\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a href=\"https://badge.fury.io/py/x-transformers\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/6978d11285aa67ea58e35a57f96d02e2bf03aa6912d2aa6459e84be7d06bb999/68747470733a2f2f62616467652e667572792e696f2f70792f782d7472616e73666f726d6572732e737667\" alt=\"PyPI version\" data-canonical-src=\"https://badge.fury.io/py/x-transformers.svg\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">A concise but fully-featured transformer, complete with a set of promising e<strong>x</strong>perimental features from various papers.</p><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Install</h2><a id=\"user-content-install\" class=\"anchor-element\" aria-label=\"Permalink: Install\" href=\"#install\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"$ pip install x-transformers\"><pre>$ pip install x-transformers</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Usage</h2><a id=\"user-content-usage\" class=\"anchor-element\" aria-label=\"Permalink: Usage\" href=\"#usage\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">Full encoder / decoder</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import XTransformermodel = XTransformer(    dim = 512,    enc_num_tokens = 256,    enc_depth = 6,    enc_heads = 8,    enc_max_seq_len = 1024,    dec_num_tokens = 256,    dec_depth = 6,    dec_heads = 8,    dec_max_seq_len = 1024,    tie_token_emb = True      # tie embeddings of encoder and decoder)src = torch.randint(0, 256, (1, 1024))src_mask = torch.ones_like(src).bool()tgt = torch.randint(0, 256, (1, 1024))loss = model(src, tgt, mask = src_mask) # (1, 1024, 512)loss.backward()\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">XTransformer</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">XTransformer</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,    <span class=\"pl-s1\">enc_num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">enc_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">enc_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">enc_max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">dec_num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">dec_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">dec_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">dec_max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">tie_token_emb</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>      <span class=\"pl-c\"># tie embeddings of encoder and decoder</span>)<span class=\"pl-s1\">src</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">256</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>))<span class=\"pl-s1\">src_mask</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">ones_like</span>(<span class=\"pl-s1\">src</span>).<span class=\"pl-en\">bool</span>()<span class=\"pl-s1\">tgt</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">256</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>))<span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">model</span>(<span class=\"pl-s1\">src</span>, <span class=\"pl-s1\">tgt</span>, <span class=\"pl-s1\">mask</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">src_mask</span>) <span class=\"pl-c\"># (1, 1024, 512)</span><span class=\"pl-s1\">loss</span>.<span class=\"pl-en\">backward</span>()</pre></div><p dir=\"auto\">Decoder-only (GPT-like)</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 12,        heads = 8    )).cuda()x = torch.randint(0, 256, (1, 1024)).cuda()model(x) # (1, 1024, 20000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">12</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>    )).<span class=\"pl-en\">cuda</span>()<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">256</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>)).<span class=\"pl-en\">cuda</span>()<span class=\"pl-en\">model</span>(<span class=\"pl-s1\">x</span>) <span class=\"pl-c\"># (1, 1024, 20000)</span></pre></div><p dir=\"auto\">GPT3 would be approximately the following (but you wouldn't be able to run it anyways)</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"gpt3 = TransformerWrapper(    num_tokens = 50000,    max_seq_len = 2048,    attn_layers = Decoder(        dim = 12288,        depth = 96,        heads = 96,        attn_dim_head = 128    )).cuda()\"><pre><span class=\"pl-s1\">gpt3</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">50000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">12288</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">96</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">96</span>,        <span class=\"pl-s1\">attn_dim_head</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">128</span>    )).<span class=\"pl-en\">cuda</span>()</pre></div><p dir=\"auto\">Encoder-only (BERT-like)</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Encoder(        dim = 512,        depth = 12,        heads = 8    )).cuda()x = torch.randint(0, 256, (1, 1024)).cuda()mask = torch.ones_like(x).bool()model(x, mask = mask) # (1, 1024, 20000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Encoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">12</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>    )).<span class=\"pl-en\">cuda</span>()<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">256</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>)).<span class=\"pl-en\">cuda</span>()<span class=\"pl-s1\">mask</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">ones_like</span>(<span class=\"pl-s1\">x</span>).<span class=\"pl-en\">bool</span>()<span class=\"pl-en\">model</span>(<span class=\"pl-s1\">x</span>, <span class=\"pl-s1\">mask</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">mask</span>) <span class=\"pl-c\"># (1, 1024, 20000)</span></pre></div><p dir=\"auto\">State of the art image classification (<a href=\"https://arxiv.org/abs/2205.01580\" rel=\"nofollow\">SimpleViT</a>)</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import ViTransformerWrapper, Encodermodel = ViTransformerWrapper(    image_size = 256,    patch_size = 32,    num_classes = 1000,    attn_layers = Encoder(        dim = 512,        depth = 6,        heads = 8,    ))img = torch.randn(1, 3, 256, 256)model(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViTransformerWrapper</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViTransformerWrapper</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Encoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    ))<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-en\">model</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><p dir=\"auto\">Image -&gt; caption</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import ViTransformerWrapper, TransformerWrapper, Encoder, Decoderencoder = ViTransformerWrapper(    image_size = 256,    patch_size = 32,    attn_layers = Encoder(        dim = 512,        depth = 6,        heads = 8    ))decoder = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        cross_attend = True    ))img = torch.randn(1, 3, 256, 256)caption = torch.randint(0, 20000, (1, 1024))encoded = encoder(img, return_embeddings = True)decoder(caption, context = encoded) # (1, 1024, 20000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViTransformerWrapper</span>, <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Encoder</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">encoder</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViTransformerWrapper</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Encoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>    ))<span class=\"pl-s1\">decoder</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">cross_attend</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>    ))<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">caption</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>))<span class=\"pl-s1\">encoded</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">encoder</span>(<span class=\"pl-s1\">img</span>, <span class=\"pl-s1\">return_embeddings</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>)<span class=\"pl-en\">decoder</span>(<span class=\"pl-s1\">caption</span>, <span class=\"pl-s1\">context</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">encoded</span>) <span class=\"pl-c\"># (1, 1024, 20000)</span></pre></div><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2209.06794\" rel=\"nofollow\">PaLI</a>, state of the art language-vision model</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import ViTransformerWrapper, XTransformer, Encoder# PaLI composes of# 1. vision transformer (ViTransformerWrapper) +# 2. encoder-decoder transformer (XTransformer)vit = ViTransformerWrapper(    image_size = 256,    patch_size = 32,    attn_layers = Encoder(        dim = 512,        depth = 6,        heads = 8    ))pali = XTransformer(    dim = 512,    enc_num_tokens = 256,    enc_depth = 6,    enc_heads = 8,    enc_max_seq_len = 1024,    dec_num_tokens = 256,    dec_depth = 6,    dec_heads = 8,    dec_max_seq_len = 1024)# training dataimg = torch.randn(1, 3, 256, 256)               # imagesprompt = torch.randint(0, 256, (1, 1024))       # promptprompt_mask = torch.ones(1, 1024).bool()        # prompt text maskoutput_text = torch.randint(0, 256, (1, 1024))  # target output text# trainimg_embeds = vit(    img,    return_embeddings = True)loss = pali(    prompt,    output_text,    mask = prompt_mask,    src_prepend_embeds = img_embeds             # will preprend image embeddings to encoder text embeddings before attention)loss.backward()# do the above for many steps on a 17B parameter model# attention is all you need\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViTransformerWrapper</span>, <span class=\"pl-v\">XTransformer</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-c\"># PaLI composes of</span><span class=\"pl-c\"># 1. vision transformer (ViTransformerWrapper) +</span><span class=\"pl-c\"># 2. encoder-decoder transformer (XTransformer)</span><span class=\"pl-s1\">vit</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViTransformerWrapper</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Encoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>    ))<span class=\"pl-s1\">pali</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">XTransformer</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,    <span class=\"pl-s1\">enc_num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">enc_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">enc_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">enc_max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">dec_num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">dec_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">dec_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">dec_max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>)<span class=\"pl-c\"># training data</span><span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)               <span class=\"pl-c\"># images</span><span class=\"pl-s1\">prompt</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">256</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>))       <span class=\"pl-c\"># prompt</span><span class=\"pl-s1\">prompt_mask</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">ones</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>).<span class=\"pl-en\">bool</span>()        <span class=\"pl-c\"># prompt text mask</span><span class=\"pl-s1\">output_text</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">256</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>))  <span class=\"pl-c\"># target output text</span><span class=\"pl-c\"># train</span><span class=\"pl-s1\">img_embeds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">vit</span>(    <span class=\"pl-s1\">img</span>,    <span class=\"pl-s1\">return_embeddings</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>)<span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">pali</span>(    <span class=\"pl-s1\">prompt</span>,    <span class=\"pl-s1\">output_text</span>,    <span class=\"pl-s1\">mask</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">prompt_mask</span>,    <span class=\"pl-s1\">src_prepend_embeds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">img_embeds</span>             <span class=\"pl-c\"># will preprend image embeddings to encoder text embeddings before attention</span>)<span class=\"pl-s1\">loss</span>.<span class=\"pl-en\">backward</span>()<span class=\"pl-c\"># do the above for many steps on a 17B parameter model</span><span class=\"pl-c\"># attention is all you need</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Dropouts</h2><a id=\"user-content-dropouts\" class=\"anchor-element\" aria-label=\"Permalink: Dropouts\" href=\"#dropouts\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decoder, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    emb_dropout = 0.1,         # dropout after embedding    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        layer_dropout = 0.1,   # stochastic depth - dropout entire layer        attn_dropout = 0.1,    # dropout post-attention        ff_dropout = 0.1       # feedforward dropout    ))x = torch.randint(0, 20000, (1, 1024))model(x)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,         <span class=\"pl-c\"># dropout after embedding</span>    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">layer_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,   <span class=\"pl-c\"># stochastic depth - dropout entire layer</span>        <span class=\"pl-s1\">attn_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-c\"># dropout post-attention</span>        <span class=\"pl-s1\">ff_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>       <span class=\"pl-c\"># feedforward dropout</span>    ))<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>))<span class=\"pl-en\">model</span>(<span class=\"pl-s1\">x</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Features</h2><a id=\"user-content-features\" class=\"anchor-element\" aria-label=\"Permalink: Features\" href=\"#features\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Flash Attention</h3><a id=\"user-content-flash-attention\" class=\"anchor-element\" aria-label=\"Permalink: Flash Attention\" href=\"#flash-attention\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/flash-attention.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/flash-attention.png\" width=\"500px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">What originally started off as <a href=\"https://arxiv.org/abs/2112.05682\" rel=\"nofollow\">a short paper</a> from Markus Rabe culminated as a practical fused attention CUDA kernel, named <a href=\"https://arxiv.org/abs/2205.14135\" rel=\"nofollow\">Flash Attention</a> by <a href=\"https://tridao.me/\" rel=\"nofollow\">Tri Dao</a>.</p><p dir=\"auto\">The technique processes the attention matrix in tiles, only keeping track of the running softmax and exponentiated weighted sums. By recomputing on the backwards pass in a tiled fashion, one is able to keep the memory linear with respect to sequence length. This allows a lot of recent models  to be able to reach for longer context lengths without worrying about the memory bottleneck.</p><p dir=\"auto\">Other engineering decisions made by Tri Dao led to its enormous success, namely minimizing HBM accesses so that both the forwards and backwards outperform naive attention. In other words, flash attention is not only more memory efficient, but faster as well, making it a necessity for training transformers.</p><p dir=\"auto\">MetaAI has recently added the ability to use <a href=\"https://github.com/hazyresearch/flash-attention\">Tri Dao's CUDA kernel</a> through the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\" rel=\"nofollow\">scaled_dot_product_attention</a> function in Pytorch 2.0. (They also have a <code>mem_efficient</code> attention, which is identical to flash attention design, just that the tiles are traversed differently)</p><p dir=\"auto\"><a href=\"https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\" rel=\"nofollow\">Llama</a> was trained using Flash Attention. The only reason to avoid it is if you require operating on the attention matrix (dynamic positional bias, talking heads, residual attention).</p><p dir=\"auto\">You can use it in this repository by setting <code>attn_flash</code> to <code>True</code> and enjoy the immediate memory savings and increase in speed.</p><p dir=\"auto\">ex.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decoder, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        attn_flash = True # just set this to True if you have pytorch 2.0 installed    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">attn_flash</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span> <span class=\"pl-c\"># just set this to True if you have pytorch 2.0 installed</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Augmenting Self-attention with Persistent Memory</h3><a id=\"user-content-augmenting-self-attention-with-persistent-memory\" class=\"anchor-element\" aria-label=\"Permalink: Augmenting Self-attention with Persistent Memory\" href=\"#augmenting-self-attention-with-persistent-memory\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/all-attention.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/all-attention.png\" width=\"500px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/1907.01470\" rel=\"nofollow\">https://arxiv.org/abs/1907.01470</a></p><p dir=\"auto\">Proposes adding learned memory key / values prior to attention. They were able to remove feedforwards altogether and attain similar performance to the original transformers. I have found that keeping the feedforwards and adding the memory key / values leads to even better performance.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"from x_transformers import Decoder, Encoderenc = Encoder(    dim = 512,    depth = 6,    heads = 8,    attn_num_mem_kv = 16 # 16 memory key / values)\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Decoder</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">enc</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Encoder</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">attn_num_mem_kv</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span> <span class=\"pl-c\"># 16 memory key / values</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Memory Transformers</h3><a id=\"user-content-memory-transformers\" class=\"anchor-element\" aria-label=\"Permalink: Memory Transformers\" href=\"#memory-transformers\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/memory-transformer.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/memory-transformer.png\" width=\"500px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2006.11527\" rel=\"nofollow\">https://arxiv.org/abs/2006.11527</a></p><p dir=\"auto\">Proposes adding learned tokens, akin to CLS tokens, named memory tokens, that is passed through the attention layers alongside the input tokens. This setting is compatible with both encoder and decoder training.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decoder, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    num_memory_tokens = 20, # 20 memory tokens    attn_layers = Encoder(        dim = 512,        depth = 6,        heads = 8    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">num_memory_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20</span>, <span class=\"pl-c\"># 20 memory tokens</span>    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Encoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>    ))</pre></div><p dir=\"auto\">Update: MetaAI researchers <a href=\"https://arxiv.org/abs/2309.16588\" rel=\"nofollow\">have found</a> that adding memory tokens (they call them register tokens), alleviates outliers (which is suspected now to be a pathology of attention networks unable to <a href=\"https://arxiv.org/abs/2306.12929\" rel=\"nofollow\">attend to nothing</a>).</p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Transformers Without Tears</h3><a id=\"user-content-transformers-without-tears\" class=\"anchor-element\" aria-label=\"Permalink: Transformers Without Tears\" href=\"#transformers-without-tears\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/scalenorm.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/scalenorm.png\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/1910.05895\" rel=\"nofollow\">https://arxiv.org/abs/1910.05895</a></p><p dir=\"auto\">They experiment with alternatives to Layer normalization and found one that is both effective and simpler. Researchers have shared with me this leads to faster convergence.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decoder, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        use_scalenorm = True # set to True to use for all layers    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">use_scalenorm</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span> <span class=\"pl-c\"># set to True to use for all layers</span>    ))</pre></div><p dir=\"auto\">You can also use the l2 normalized embeddings proposed as part of <code>fixnorm</code>. I have found it leads to improved convergence, when paired with small initialization (proposed by <a href=\"https://github.com/BlinkDL\">BlinkDL</a>). The small initialization will be taken care of as long as <code>l2norm_embed</code> is set to <code>True</code></p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decoder, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    l2norm_embed = True,    # set this to True for l2 normalized embedding + small init    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">l2norm_embed</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>,    <span class=\"pl-c\"># set this to True for l2 normalized embedding + small init</span>    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>    ))</pre></div><p dir=\"auto\">Along the same lines of l2 normalized embeddings, Huggingface's <a href=\"https://huggingface.co/bigscience/bloom\" rel=\"nofollow\">175B parameter BLOOM</a> also places a layernorm right after the embeddings and just before the tokens enter the attention layers. This was corroborated by Yandex's <a href=\"https://github.com/yandex/YaLM-100B\">100B parameter YaLM</a> to stabilize training.</p><p dir=\"auto\">It is recommended you either have either <code>l2norm_embed</code> or <code>post_emb_norm</code> set to <code>True</code> but not both, as they probably serve the same purpose.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decoder, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    post_emb_norm = True,    # set this to True to layernorm summed token + pos embeddings    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">post_emb_norm</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>,    <span class=\"pl-c\"># set this to True to layernorm summed token + pos embeddings</span>    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Root Mean Square Layer Normalization</h3><a id=\"user-content-root-mean-square-layer-normalization\" class=\"anchor-element\" aria-label=\"Permalink: Root Mean Square Layer Normalization\" href=\"#root-mean-square-layer-normalization\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a href=\"https://arxiv.org/abs/1910.07467\" rel=\"nofollow\">https://arxiv.org/abs/1910.07467</a></p><p dir=\"auto\">The authors propose to replace layer normalization with a simpler alternative, without mean centering and the learned bias. An investigative paper found this to be the <a href=\"https://arxiv.org/abs/2102.11972\" rel=\"nofollow\">best performing normalization variant</a>. It was also used in Deepmind's latest large language models, <a href=\"https://deepmind.com/research/publications/2021/improving-language-models-by-retrieving-from-trillions-of-tokens\" rel=\"nofollow\">Retro</a> and <a href=\"https://arxiv.org/abs/2112.11446\" rel=\"nofollow\">Gopher</a>.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decoder, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        use_rmsnorm = True # set to true to use for all layers    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">use_rmsnorm</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span> <span class=\"pl-c\"># set to true to use for all layers</span>    ))</pre></div><p dir=\"auto\"><em>July 2023</em> <a href=\"https://arxiv.org/abs/2307.14995\" rel=\"nofollow\">A linear attention paper</a> has experiments to show that removing the learned multiplicative gamma led to no performance degradation. This simplifies the RMS normalization to a satisfying <code>l2norm(x) * sqrt(dim)</code>.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decoder, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        use_simple_rmsnorm = True # set to true to use for all layers    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">use_simple_rmsnorm</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span> <span class=\"pl-c\"># set to true to use for all layers</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">GLU Variants Improve Transformer</h3><a id=\"user-content-glu-variants-improve-transformer\" class=\"anchor-element\" aria-label=\"Permalink: GLU Variants Improve Transformer\" href=\"#glu-variants-improve-transformer\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/ffglu.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/ffglu.png\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2002.05202\" rel=\"nofollow\">https://arxiv.org/abs/2002.05202</a></p><p dir=\"auto\">Noam Shazeer paper that explores gating in the feedforward, finding that simple gating with GELU leads to significant improvements. This variant also showed up in the latest mT5 architecture. You should always turn this on (I may eventually turn it on by default).</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decoder, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        ff_glu = True # set to true to use for all feedforwards    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">ff_glu</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span> <span class=\"pl-c\"># set to true to use for all feedforwards</span>    ))</pre></div><p dir=\"auto\">The <a href=\"https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\" rel=\"nofollow\">PaLM</a> language model also chose to use the Swish GLU variant. You can turn this on by setting two flags</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decoder, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        ff_swish = True, # set this to True        ff_glu = True    # set to true to use for all feedforwards    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">ff_swish</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>, <span class=\"pl-c\"># set this to True</span>        <span class=\"pl-s1\">ff_glu</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>    <span class=\"pl-c\"># set to true to use for all feedforwards</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">No Bias in Feedforward</h3><a id=\"user-content-no-bias-in-feedforward\" class=\"anchor-element\" aria-label=\"Permalink: No Bias in Feedforward\" href=\"#no-bias-in-feedforward\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">Starting with <a href=\"https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\" rel=\"nofollow\">PaLM</a>, there begun a trend to remove biases from the transformer all together. <a href=\"https://github.com/borisdayma\">Boris Dayma</a> has run a number of experiments that showed removing biases from feedforwards led to increased throughput without any loss of accuracy. This was corroborated by <a href=\"https://arxiv.org/abs/2212.14034\" rel=\"nofollow\">yet another paper</a> investigating transformer architecture variants.</p><p dir=\"auto\">You can turn off the feedforward bias as follows</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decoder, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        ff_no_bias = True  # set this to True    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">ff_no_bias</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>  <span class=\"pl-c\"># set this to True</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">ReLU\u00b2</h3><a id=\"user-content-relu\" class=\"anchor-element\" aria-label=\"Permalink: ReLU\u00b2\" href=\"#relu\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2109.08668\" rel=\"nofollow\">https://arxiv.org/abs/2109.08668</a></p><p dir=\"auto\">This paper used neural architecture search and found an activation, Relu Squared, that is both simpler and performs better than GELU, in the autoregressive language model setting. I have confirmed this in my independent experiments. However, if one were using the GLU variant from above, GELU still performs better. Pending further corroboration.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decoder, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        ff_relu_squared = True    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">ff_relu_squared</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection</h3><a id=\"user-content-explicit-sparse-transformer-concentrated-attention-through-explicit-selection\" class=\"anchor-element\" aria-label=\"Permalink: Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection\" href=\"#explicit-sparse-transformer-concentrated-attention-through-explicit-selection\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/topk-attention.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/topk-attention.png\" width=\"500px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/1912.11637\" rel=\"nofollow\">https://arxiv.org/abs/1912.11637</a></p><p dir=\"auto\">This paper proposes an efficient way to sparsify attention by zeroing all dot-product query/key values not within the top k values. The show that this cheap method was as effective as other more expensive operations like sparsemax or entmax15. This technique comes with the cost of an extra hyperparameter (the top k values to keep). The paper recommends a value of <code>k = 8</code></p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        attn_sparse_topk = 8 # keep only the top 8 values before attention (softmax)    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">attn_sparse_topk</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span> <span class=\"pl-c\"># keep only the top 8 values before attention (softmax)</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Talking-Heads Attention</h3><a id=\"user-content-talking-heads-attention\" class=\"anchor-element\" aria-label=\"Permalink: Talking-Heads Attention\" href=\"#talking-heads-attention\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/talking-heads.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/talking-heads.png\" width=\"500px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2003.02436\" rel=\"nofollow\">https://arxiv.org/abs/2003.02436</a></p><p dir=\"auto\">A Noam Shazeer paper that proposes mixing information between heads pre and post attention (softmax). This comes with the cost of extra memory and compute.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        attn_talking_heads = True  # turn on information exchange between attention heads    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">attn_talking_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>  <span class=\"pl-c\"># turn on information exchange between attention heads</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">One Write-Head Is All You Need</h3><a id=\"user-content-one-write-head-is-all-you-need\" class=\"anchor-element\" aria-label=\"Permalink: One Write-Head Is All You Need\" href=\"#one-write-head-is-all-you-need\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a href=\"https://arxiv.org/abs/1911.02150\" rel=\"nofollow\">https://arxiv.org/abs/1911.02150</a></p><p dir=\"auto\">Yet another Noam Shazeer paper (he's a legend) that proposes to only have one head for the key / values, but multi-headed queries. This paper was largely ignored for a while, but recently validated at scale in <a href=\"https://arxiv.org/abs/2203.07814\" rel=\"nofollow\">AlphaCode</a> as well as <a href=\"https://arxiv.org/abs/2204.02311\" rel=\"nofollow\">PaLM</a>. It has the property of being memory efficient when decoding extremely large language models. You can use it with one keyword argument as shown below.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        attn_one_kv_head = True    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">attn_one_kv_head</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>    ))</pre></div><p dir=\"auto\">This has been further generalized in <a href=\"https://arxiv.org/abs/2305.13245\" rel=\"nofollow\">a recent paper</a> to allow for groups of query heads to attend to a single key / value head. You can use this by specifying the <code>attn_kv_heads</code></p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 12,        heads = 8,        attn_kv_heads = 2 # say you want 4 query heads to attend to 1 key / value head    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">12</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">attn_kv_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span> <span class=\"pl-c\"># say you want 4 query heads to attend to 1 key / value head</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Attention on Attention for Image Captioning</h3><a id=\"user-content-attention-on-attention-for-image-captioning\" class=\"anchor-element\" aria-label=\"Permalink: Attention on Attention for Image Captioning\" href=\"#attention-on-attention-for-image-captioning\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/attention-on-attention.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/attention-on-attention.png\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/1908.06954\" rel=\"nofollow\">https://arxiv.org/abs/1908.06954</a></p><p dir=\"auto\">This paper proposes to add a gated linear unit at the end of the attention layer, further gated by the original queries. Although this is not widely used outside of visual question / answering, I suspect it should lead to improvements after seeing the success of the feedforward GLU variant.</p><p dir=\"auto\">Update: After some experimentation, I found this variant actually performs worse, but if it were to be modified to not concatenate the queries before gating, it performs much better. That is what we will be using in this repository.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Encoder(        dim = 512,        depth = 6,        heads = 8,        attn_on_attn = True  # gate output of attention layer, by queries    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Encoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">attn_on_attn</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>  <span class=\"pl-c\"># gate output of attention layer, by queries</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Intra-attention Gating on Values</h3><a id=\"user-content-intra-attention-gating-on-values\" class=\"anchor-element\" aria-label=\"Permalink: Intra-attention Gating on Values\" href=\"#intra-attention-gating-on-values\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/gate_values.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/gate_values.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://github.com/deepmind/alphafold\">Alphafold2</a> had a peculiar variant of attention where they gate the aggregated values with the input, presumably to have the block have more control over the update.</p><p dir=\"auto\">A quick test shows a small but noticeable improvement, on about the same order as attention on attention.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Encoder(        dim = 512,        depth = 6,        heads = 8,        attn_gate_values = True  # gate aggregated values with the input    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Encoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">attn_gate_values</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>  <span class=\"pl-c\"># gate aggregated values with the input</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Improving Transformer Models by Reordering their Sublayers</h3><a id=\"user-content-improving-transformer-models-by-reordering-their-sublayers\" class=\"anchor-element\" aria-label=\"Permalink: Improving Transformer Models by Reordering their Sublayers\" href=\"#improving-transformer-models-by-reordering-their-sublayers\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/sandwich.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/sandwich.png\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/sandwich-2.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/sandwich-2.png\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/1911.03864\" rel=\"nofollow\">https://arxiv.org/abs/1911.03864</a></p><p dir=\"auto\">This paper proposes to break from the normal fixed pattern of alternating attention and feedforwards, but to have blocks of only attention at the beginning followed by blocks of feedforwards at the end. This was further corroborated by a paper by Nvidia that reduces the number of attention layers to be 1/3rd of the feedforwards without loss in performance.</p><p dir=\"auto\">The amount of interleaving is controlled by a \"sandwich coefficient\", which they found to be optimal at a value of <code>6</code>.</p><p dir=\"auto\">You can experiment with this feature as shown below</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Encoder(        dim = 512,        depth = 6,        heads = 8,        sandwich_coef = 6  # interleave attention and feedforwards with sandwich coefficient of 6    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Encoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">sandwich_coef</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>  <span class=\"pl-c\"># interleave attention and feedforwards with sandwich coefficient of 6</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View</h3><a id=\"user-content-understanding-and-improving-transformer-from-a-multi-particle-dynamic-system-point-of-view\" class=\"anchor-element\" aria-label=\"Permalink: Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View\" href=\"#understanding-and-improving-transformer-from-a-multi-particle-dynamic-system-point-of-view\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/macaron-1.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/macaron-1.png\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/macaron-2.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/macaron-2.png\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/1906.02762\" rel=\"nofollow\">https://arxiv.org/abs/1906.02762</a></p><p dir=\"auto\">The authors propose to view the success of transformers from a dynamical systems point of view, and then proposes an improvement based on mathematics of that POV. Specifically, they propose to place the attention layer in between two feedforward layers. This was adopted by a paper using transformers for speech recognition, the <a href=\"https://arxiv.org/abs/2005.08100\" rel=\"nofollow\">Conformer</a>.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Encoder(        dim = 512,        depth = 6,        heads = 8,        macaron = True  # use macaron configuration    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Encoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">macaron</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>  <span class=\"pl-c\"># use macaron configuration</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">T5's Simplified Relative Positional Encoding</h3><a id=\"user-content-t5s-simplified-relative-positional-encoding\" class=\"anchor-element\" aria-label=\"Permalink: T5's Simplified Relative Positional Encoding\" href=\"#t5s-simplified-relative-positional-encoding\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a href=\"https://arxiv.org/abs/1910.10683\" rel=\"nofollow\">https://arxiv.org/abs/1910.10683</a></p><p dir=\"auto\">T5 is one of the most successful encoder / decoder transformer architectures trained to date. They invented a new simplified relative positional encoding based on learned bias values that are added to the attention matrix pre-softmax. This bias is shared and injected into each attention layer. I have decided to include this because it offers a cheap way to have relative positional encoding (superior to absolute positional), and I have read papers that suggest having positional encoding added to each layer (vs only before the first) is beneficial.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        rel_pos_bias = True  # adds relative positional bias to all attention layers, a la T5    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">rel_pos_bias</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>  <span class=\"pl-c\"># adds relative positional bias to all attention layers, a la T5</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Residual Attention</h3><a id=\"user-content-residual-attention\" class=\"anchor-element\" aria-label=\"Permalink: Residual Attention\" href=\"#residual-attention\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/residual_attn.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/residual_attn.png\" width=\"500px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2012.11747\" rel=\"nofollow\">https://arxiv.org/abs/2012.11747</a></p><p dir=\"auto\">This paper from Google proposes residualizing the pre-attention scores across all layers. At the cost of no extra parameters, they show improvement on top of regular attention networks. If you turn on this setting, be aware that the best results in the paper used post-normalization, in which case a learning warmup will be needed. The authors also reported that they could use a higher learning rate and get even better gains in the same amount of steps. (In the paper they use <code>2e-4</code> vs <code>1e-4</code> for vanilla transformer)</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Encodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Encoder(        dim = 512,        depth = 6,        heads = 8,        pre_norm = False,       # in the paper, residual attention had best results with post-layernorm        residual_attn = True    # add residual attention    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Encoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">pre_norm</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">False</span>,       <span class=\"pl-c\"># in the paper, residual attention had best results with post-layernorm</span>        <span class=\"pl-s1\">residual_attn</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>    <span class=\"pl-c\"># add residual attention</span>    ))</pre></div><p dir=\"auto\">I also tried residualizing cross attention and may have noticed an improvement in convergence. You can try it by setting the <code>cross_residual_attn</code> keyword to <code>True</code></p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import XTransformermodel = XTransformer(    dim = 512,    enc_num_tokens = 256,    enc_depth = 6,    enc_heads = 8,    enc_max_seq_len = 1024,    dec_num_tokens = 256,    dec_depth = 6,    dec_heads = 8,    dec_max_seq_len = 1024,    dec_cross_residual_attn = True     # residualize cross attention)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">XTransformer</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">XTransformer</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,    <span class=\"pl-s1\">enc_num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">enc_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">enc_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">enc_max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">dec_num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">dec_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">dec_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">dec_max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">dec_cross_residual_attn</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>     <span class=\"pl-c\"># residualize cross attention</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Transformer-XL recurrence</h3><a id=\"user-content-transformer-xl-recurrence\" class=\"anchor-element\" aria-label=\"Permalink: Transformer-XL recurrence\" href=\"#transformer-xl-recurrence\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">You can also do Transformer-XL recurrence, by simply passing in a <code>max_mem_len</code> in the <code>TransformerWrapper</code> class, and then making sure your <code>Decoder</code> has <code>rel_pos_bias</code> (or <code>rotary_pos_emb</code>) set to <code>True</code>.</p><p dir=\"auto\">Then, you can retrieve the memories at each step with the <code>return_mems</code> keyword and pass it to the next iteration.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel_xl = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 512,    max_mem_len = 2048,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        rel_pos_bias = True    ))seg1 = torch.randint(0, 20000, (1, 512))seg2 = torch.randint(0, 20000, (1, 512))seg3 = torch.randint(0, 20000, (1, 512))logits1, mems1  = model_xl(seg1, return_mems = True)logits2, mems2  = model_xl(seg2, mems = mems1, return_mems = True)logits3, mems3  = model_xl(seg3, mems = mems2, return_mems = True)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model_xl</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,    <span class=\"pl-s1\">max_mem_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">rel_pos_bias</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>    ))<span class=\"pl-s1\">seg1</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">512</span>))<span class=\"pl-s1\">seg2</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">512</span>))<span class=\"pl-s1\">seg3</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">512</span>))<span class=\"pl-s1\">logits1</span>, <span class=\"pl-s1\">mems1</span>  <span class=\"pl-c1\">=</span> <span class=\"pl-en\">model_xl</span>(<span class=\"pl-s1\">seg1</span>, <span class=\"pl-s1\">return_mems</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>)<span class=\"pl-s1\">logits2</span>, <span class=\"pl-s1\">mems2</span>  <span class=\"pl-c1\">=</span> <span class=\"pl-en\">model_xl</span>(<span class=\"pl-s1\">seg2</span>, <span class=\"pl-s1\">mems</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">mems1</span>, <span class=\"pl-s1\">return_mems</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>)<span class=\"pl-s1\">logits3</span>, <span class=\"pl-s1\">mems3</span>  <span class=\"pl-c1\">=</span> <span class=\"pl-en\">model_xl</span>(<span class=\"pl-s1\">seg3</span>, <span class=\"pl-s1\">mems</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">mems2</span>, <span class=\"pl-s1\">return_mems</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>)</pre></div><p dir=\"auto\">Setting up the logic for training and sampling from transformer xl can be a bit overwhelming. This repository offers a simple wrapper that should make this easy, with the <code>XLAutoregressiveWrapper</code>.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"# pass in the above model_xlxl_wrapper = XLAutoregressiveWrapper(model_xl)seg = torch.randint(0, 20000, (1, 4096)).cuda()  # sequence exceeding max length, automatically segmented and memory managedloss = xl_wrapper(seg)loss.backward()# then, after much trainingprime = seg[:, :1024]   # if prime exceeds max length, memory will be caught up before generatinggenerated = xl_wrapper.generate(prime, 4096)  # (1, 4096)\"><pre><span class=\"pl-c\"># pass in the above model_xl</span><span class=\"pl-s1\">xl_wrapper</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">XLAutoregressiveWrapper</span>(<span class=\"pl-s1\">model_xl</span>)<span class=\"pl-s1\">seg</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">4096</span>)).<span class=\"pl-en\">cuda</span>()  <span class=\"pl-c\"># sequence exceeding max length, automatically segmented and memory managed</span><span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">xl_wrapper</span>(<span class=\"pl-s1\">seg</span>)<span class=\"pl-s1\">loss</span>.<span class=\"pl-en\">backward</span>()<span class=\"pl-c\"># then, after much training</span><span class=\"pl-s1\">prime</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">seg</span>[:, :<span class=\"pl-c1\">1024</span>]   <span class=\"pl-c\"># if prime exceeds max length, memory will be caught up before generating</span><span class=\"pl-s1\">generated</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">xl_wrapper</span>.<span class=\"pl-en\">generate</span>(<span class=\"pl-s1\">prime</span>, <span class=\"pl-c1\">4096</span>)  <span class=\"pl-c\"># (1, 4096)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Enhanced recurrence</h3><a id=\"user-content-enhanced-recurrence\" class=\"anchor-element\" aria-label=\"Permalink: Enhanced recurrence\" href=\"#enhanced-recurrence\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/enhanced-recurrence.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/enhanced-recurrence.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2012.15688\" rel=\"nofollow\">This paper</a> proposes a simple technique to enhance the range of Transformer-XL. They simply route the memory segment of a layer to the layer below it, for the next recurrent step. You can enable this by setting <code>shift_mem_down = 1</code>. You can also shift down arbitrary number of layers by setting this value to <code>&gt; 1</code>.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel_xl = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 512,    max_mem_len = 2048,    shift_mem_down = 1,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        rotary_pos_emb = True    ))seg1 = torch.randint(0, 20000, (1, 512))seg2 = torch.randint(0, 20000, (1, 512))seg3 = torch.randint(0, 20000, (1, 512))logits1, mems1  = model_xl(seg1, return_mems = True)logits2, mems2  = model_xl(seg2, mems = mems1, return_mems = True) # mems1 of layer N are automatically routed to the layer N-1\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model_xl</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,    <span class=\"pl-s1\">max_mem_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">shift_mem_down</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">rotary_pos_emb</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>    ))<span class=\"pl-s1\">seg1</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">512</span>))<span class=\"pl-s1\">seg2</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">512</span>))<span class=\"pl-s1\">seg3</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">512</span>))<span class=\"pl-s1\">logits1</span>, <span class=\"pl-s1\">mems1</span>  <span class=\"pl-c1\">=</span> <span class=\"pl-en\">model_xl</span>(<span class=\"pl-s1\">seg1</span>, <span class=\"pl-s1\">return_mems</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>)<span class=\"pl-s1\">logits2</span>, <span class=\"pl-s1\">mems2</span>  <span class=\"pl-c1\">=</span> <span class=\"pl-en\">model_xl</span>(<span class=\"pl-s1\">seg2</span>, <span class=\"pl-s1\">mems</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">mems1</span>, <span class=\"pl-s1\">return_mems</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>) <span class=\"pl-c\"># mems1 of layer N are automatically routed to the layer N-1</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Gated residual</h3><a id=\"user-content-gated-residual\" class=\"anchor-element\" aria-label=\"Permalink: Gated residual\" href=\"#gated-residual\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/gating.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/gating.png\" width=\"500px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/1910.06764\" rel=\"nofollow\">https://arxiv.org/abs/1910.06764</a></p><p dir=\"auto\">The authors propose gating the residual connections in the transformer network and demonstrate increased stability and performance for Transformer-XL in a variety of reinforcement learning tasks.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    max_mem_len = 2048,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 16,        gate_residual = True    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">max_mem_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,        <span class=\"pl-s1\">gate_residual</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Rotary Positional Embeddings</h3><a id=\"user-content-rotary-positional-embeddings\" class=\"anchor-element\" aria-label=\"Permalink: Rotary Positional Embeddings\" href=\"#rotary-positional-embeddings\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/rotary.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/rotary.png\" width=\"500px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">Developed in Beijing, this new technique quickly gained interest in the NLP circles. In short, it allows you to endow the transformer with relative positional embeddings at the cost of no learned parameters. You apply a rotary operation to the queries and keys prior to their dot product in attention. The big idea is injecting positions through rotations.</p><p dir=\"auto\">Highly recommend that you have this turned on whenever you are working on an ordered sequence.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        rotary_pos_emb = True  # turns on rotary positional embeddings    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">rotary_pos_emb</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>  <span class=\"pl-c\"># turns on rotary positional embeddings</span>    ))</pre></div><p dir=\"auto\">Update (12/2022): Rotary embedding has since been hugely successful, widely adopted in many large language models, including the largest in the world, PaLM. However, it has been uncovered in the ALiBi paper that rotary embeddings cannot length extrapolate well. This was recently addressed in <a href=\"https://arxiv.org/abs/2212.10554v1\" rel=\"nofollow\">a Microsoft research paper</a>. They propose a way to unobtrusively add the same decay as in ALiBi, and found that this resolves the extrapolation problem. You can use it in this repository by setting <code>rotary_xpos = True</code>. Like ALiBi, it would enforce the attention to be local. You can set the receptive field with <code>rotary_xpos_scale_base</code> value, which defaults to <code>512</code></p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        rotary_xpos = True   # modified rotary to extrapolate well beyond length at which it was trained    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">rotary_xpos</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>   <span class=\"pl-c\"># modified rotary to extrapolate well beyond length at which it was trained</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Dynamic Positional Bias</h3><a id=\"user-content-dynamic-positional-bias\" class=\"anchor-element\" aria-label=\"Permalink: Dynamic Positional Bias\" href=\"#dynamic-positional-bias\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/dynamic-pos-bias.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/dynamic-pos-bias.png\" width=\"150px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This technique bears roots from the field of vision transformers, where researchers are trying to have relative positions generalize to larger resolutions (without having to retrain the entire network). It was used in two recent papers, <a href=\"https://arxiv.org/abs/2108.00154\" rel=\"nofollow\">CrossFormer</a>, as well as <a href=\"https://arxiv.org/abs/2111.09883\" rel=\"nofollow\">SwinV2</a>.</p><p dir=\"auto\"><a href=\"https://github.com/cfoster0\">Charles Foster</a> first tried this for a language model, and found that it works. Later on <a href=\"https://github.com/bob80333\">Eric Engelhart</a> produced experimental results that show the same type of extrapolation holds, even for 1d sequences.</p><p dir=\"auto\">Eric trained at sequence lengths of 128, and showed that it generalized well to 1024. In addition, he showed that linear positions was better than log (used in SwinV2), for language.</p><p dir=\"auto\">Linear distances</p><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/dynamic-pos-bias-linear.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/dynamic-pos-bias-linear.png\" width=\"600px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">Log distances</p><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/dynamic-pos-bias-log.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/dynamic-pos-bias-log.png\" width=\"600px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">Negative control - Sinusoidal</p><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/dynamic-pos-bias-sinusoidal.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/dynamic-pos-bias-sinusoidal.png\" width=\"600px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">More of Eric's experimental results can be found <a href=\"https://github.com/bob80333/investigating_extrapolation\">here</a></p><p dir=\"auto\">You can use this type of relative position if you wish to train at smaller sequence lengths and have it generalize to longer ones, for both autoregressive and bidirectional models.</p><p dir=\"auto\">Update: <a href=\"https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding/discussion/460121\" rel=\"nofollow\">First place RNA folding using dynamic positional bias</a></p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 256,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        dynamic_pos_bias = True,                # set this to True        dynamic_pos_bias_log_distance = False   # whether to use log distance, as in SwinV2    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">dynamic_pos_bias</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>,                <span class=\"pl-c\"># set this to True</span>        <span class=\"pl-s1\">dynamic_pos_bias_log_distance</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">False</span>   <span class=\"pl-c\"># whether to use log distance, as in SwinV2</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">ALiBi Positional Embedding</h3><a id=\"user-content-alibi-positional-embedding\" class=\"anchor-element\" aria-label=\"Permalink: ALiBi Positional Embedding\" href=\"#alibi-positional-embedding\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a href=\"https://ofir.io/train_short_test_long.pdf\" rel=\"nofollow\">This paper</a> proposes to simply apply a static linear bias to the attention matrix. The authors show this is not only effective as a relative positional encoding, but also allows the attention net to extrapolate to greater sequences length than what it was trained on, for autoregressive language models.</p><p dir=\"auto\">This repository also offers a bidirectional variant (nonsymmetric), proposed by the authors <a href=\"https://github.com/ofirpress/attention_with_linear_biases/issues/5\" data-hovercard-type=\"issue\" data-hovercard-url=\"/ofirpress/attention_with_linear_biases/issues/5/hovercard\">here</a>. However, this is untested. If you need bidirectional length extrapolation, the safest option would be Dynamic Position Bias</p><p dir=\"auto\">Update: It may be that ALiBi enforces a strong local attention across the heads, and may hinder it from attending at distances greater than 1k. To avoid any issues with global message passing, I've decided to introduce another hyperparameter <code>alibi_num_heads</code>, so one can specify less heads for the ALiBi bias</p><p dir=\"auto\">Update: There are reports that ALiBi outperform Rotary embeddings for pretraining and downstream fine-tuning.</p><p dir=\"auto\">Update: <a href=\"https://arxiv.org/abs/2305.19466\" rel=\"nofollow\">New paper</a> shows that no positional embedding can length extrapolate even than explicit ones</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        alibi_pos_bias = True, # turns on ALiBi positional embedding        alibi_num_heads = 4    # only use ALiBi for 4 out of the 8 heads, so other 4 heads can still attend far distances    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">alibi_pos_bias</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>, <span class=\"pl-c\"># turns on ALiBi positional embedding</span>        <span class=\"pl-s1\">alibi_num_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>    <span class=\"pl-c\"># only use ALiBi for 4 out of the 8 heads, so other 4 heads can still attend far distances</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Shifted Tokens</h3><a id=\"user-content-shifted-tokens\" class=\"anchor-element\" aria-label=\"Permalink: Shifted Tokens\" href=\"#shifted-tokens\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">An <a href=\"https://github.com/BlinkDL\">independent researcher</a> has found that shifting a subset of the feature dimension along the sequence dimension by 1 token helps with convergence (<a href=\"https://zhuanlan.zhihu.com/p/191393788\" rel=\"nofollow\">Time-mixing</a>). I have tested this for the autoregressive case and can confirm that it leads to greatly improved convergence. This also lines up with <a href=\"https://arxiv.org/abs/2106.07477\" rel=\"nofollow\">the results</a> of some papers in the vision domain.</p><p dir=\"auto\">To use it, simply set <code>shift_tokens = 1</code> (or to whatever number of shifts you desire). The feature dimension will be divided by <code>shift_tokens + 1</code> and then each chunk will be shifted <code>[0, shift_tokens]</code> respectively</p><p dir=\"auto\">Update: new experiments by @sdtblck suggests this may only work for character-level training</p><p dir=\"auto\">Update: after more experiments, it seems that in the context of BPE encoding, with rotary turned on, there is no benefit to shifting. for character-level training, shifting may still improve a tiny bit</p><p dir=\"auto\">Update: When doing BPE encoded tokens, it seems that shift of 2 will bottleneck the dimensions (divided by 5). It is recommended you always do a shift of 1, unless if you are working with character level.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        shift_tokens = 1    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">shift_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1</span>    ))</pre></div><p dir=\"auto\">If you want finer control over how much is shifted per block (whether attention or feedforward), simply pass in a tuple of size that is equal to the number of layers.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        shift_tokens = (1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0) # 12 blocks, attention and feedforward alternating, with progressively less shifting    ))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">shift_tokens</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>) <span class=\"pl-c\"># 12 blocks, attention and feedforward alternating, with progressively less shifting</span>    ))</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Sandwich Norm</h3><a id=\"user-content-sandwich-norm\" class=\"anchor-element\" aria-label=\"Permalink: Sandwich Norm\" href=\"#sandwich-norm\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/sandwich_norm.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/sandwich_norm.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This technique first made an appearance in <a href=\"https://arxiv.org/abs/2105.13290\" rel=\"nofollow\">the CoqView paper</a>, a Chinese version of the famous text-to-image transformer DALL-E. They propose, when using pre-layernorm, to add an extra layernorm to all the branch outputs. I have found this to be very effective for a number of projects, when facing instability during training.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        sandwich_norm = True # set this to True    ))x = torch.randint(0, 20000, (1, 1024))model(x)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">sandwich_norm</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span> <span class=\"pl-c\"># set this to True</span>    ))<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>))<span class=\"pl-en\">model</span>(<span class=\"pl-s1\">x</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">ResiDual</h3><a id=\"user-content-residual\" class=\"anchor-element\" aria-label=\"Permalink: ResiDual\" href=\"#residual\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/resi_dual.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/resi_dual.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2304.14802\" rel=\"nofollow\">This Microsoft paper</a> proposes yet another normalization configuration, combining both pre and post layernorm. They claim this hybridization reduces representation collapse (known to be an issue with pre-layernorm with increasing depth), while maintaining stability and reducing vanishing gradients (issues with post-layernorm). Initial experiments on my end show it to work no worse than pre-layernorm or sandwich norm. More study needed by the public to see if this is actually a winning technique.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        resi_dual = True,               # set this to True        resi_dual_scale = 0.1           # in appendix, they said on fp16 the prenorm residual is prone to overflow. they claim by scaling it at each layer by a factor, it would prevent the overflow, and keep results the same (as layernorms are invariant to scaling of the input)    ))x = torch.randint(0, 20000, (1, 1024))model(x)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">resi_dual</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>,               <span class=\"pl-c\"># set this to True</span>        <span class=\"pl-s1\">resi_dual_scale</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>           <span class=\"pl-c\"># in appendix, they said on fp16 the prenorm residual is prone to overflow. they claim by scaling it at each layer by a factor, it would prevent the overflow, and keep results the same (as layernorms are invariant to scaling of the input)</span>    ))<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>))<span class=\"pl-en\">model</span>(<span class=\"pl-s1\">x</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Normformer</h3><a id=\"user-content-normformer\" class=\"anchor-element\" aria-label=\"Permalink: Normformer\" href=\"#normformer\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/normformer.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/normformer.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This <a href=\"https://openreview.net/forum?id=GMYWzWztDx5\" rel=\"nofollow\">paper</a> uncovers an issue with pre-norm transformers where gradients are mismatched between the early and later layers. They propose 4 changes, of which I will be offering 3.</p><p dir=\"auto\">The first change is to offer per head scaling after aggregating the values in attention. My experiments show a slight improvement in convergence.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        attn_head_scale = True  # set this to True    ))x = torch.randint(0, 20000, (1, 1024))model(x)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">attn_head_scale</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>  <span class=\"pl-c\"># set this to True</span>    ))<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>))<span class=\"pl-en\">model</span>(<span class=\"pl-s1\">x</span>)</pre></div><p dir=\"auto\">The second change is an extra layernorm right after the activation in the feedforward. I have also verified a slight improvement, at the cost of extra compute.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        ff_post_act_ln = True # set this to True    ))x = torch.randint(0, 20000, (1, 1024))model(x)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">ff_post_act_ln</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span> <span class=\"pl-c\"># set this to True</span>    ))<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>))<span class=\"pl-en\">model</span>(<span class=\"pl-s1\">x</span>)</pre></div><p dir=\"auto\">For the residual scaling, you simply have to set <code>scale_residual = True</code>. I have noticed slight improvements, but occasional instability as well, so use with caution.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        scale_residual = True # set this to True    ))x = torch.randint(0, 20000, (1, 1024))model(x)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">scale_residual</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span> <span class=\"pl-c\"># set this to True</span>    ))<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>))<span class=\"pl-en\">model</span>(<span class=\"pl-s1\">x</span>)</pre></div><p dir=\"auto\">The last change is a layernorm right after the outwards projection in attention. This is actually identical to the sandwich norm proposed by the Coqview paper, so you can use this by simply setting <code>sandwich_norm = True</code>, although it would also add it to the feedforward layer.</p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Cosine Sim Attention</h3><a id=\"user-content-cosine-sim-attention\" class=\"anchor-element\" aria-label=\"Permalink: Cosine Sim Attention\" href=\"#cosine-sim-attention\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/cosine-sim-attention.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/cosine-sim-attention.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This <a href=\"https://arxiv.org/abs/2010.04245\" rel=\"nofollow\">paper</a> proposes to l2 normalize the queries and keys along the head dimension before the dot product (cosine similarity), with the additional change of the scale being learned rather than static. The normalization prevents the attention operation from overflowing, and removes any need for numerical stability measures prior to softmax. Both are perennial problems when training transformers.</p><p dir=\"auto\">This was validated at scale recently by the training of <a href=\"https://arxiv.org/abs/2111.09883\" rel=\"nofollow\">a 3B parameter vision transformer</a>. The SwinV2 paper also proposes to change the pre-layernorm to a post-layernorm for further stability.</p><p dir=\"auto\">I have validated that this works just as well as dot product attention in an autoregressive setting, if one were to initialize the temperature as proposed in the QK-norm paper (as a function of the sequence length).</p><p dir=\"auto\">This flavor of attention also has <a href=\"https://arxiv.org/abs/2111.05498\" rel=\"nofollow\">a connection</a> to sparse distributed memory. <a href=\"https://www.youtube.com/watch?v=THIIk7LR9_8\" rel=\"nofollow\">[youtube talk]</a></p><p dir=\"auto\">Update: I have discovered a way to remove the learned temperature altogether, by grouping the feature dimension and doing l2-normalization on each group. This allows the queries and keys to have a similarity that is upper bounded by the number of groups. A group size of 8 or 16 was sufficient in my tests. Decided to name this technique \"Grouped QK Normalization\". The drawback is that I believe an attention head dimension 32 is too small to use this tactic (a dimension often used in vision)</p><p dir=\"auto\">Update 2: Tero Karras has successfully used cosine sim attention in <a href=\"https://arxiv.org/abs/2312.02696\" rel=\"nofollow\">a new paper</a>.</p><p dir=\"auto\">You can use it as follows</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        attn_qk_norm = True,       # set this to True        attn_qk_norm_groups = 8    # number of groups in the feature dimension for l2norm, similarity scores will be bounded between [-group, group]. determines how sharp the attention can be    ))x = torch.randint(0, 20000, (1, 1024))model(x)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">attn_qk_norm</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>,       <span class=\"pl-c\"># set this to True</span>        <span class=\"pl-s1\">attn_qk_norm_groups</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>    <span class=\"pl-c\"># number of groups in the feature dimension for l2norm, similarity scores will be bounded between [-group, group]. determines how sharp the attention can be</span>    ))<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>))<span class=\"pl-en\">model</span>(<span class=\"pl-s1\">x</span>)</pre></div><p dir=\"auto\">Another update: Simply scaling the cosine similarity (group of 1) with a fixed constant (10) may work too</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,        attn_qk_norm = True,       # set to True        attn_qk_norm_scale = 10    # new scale on the similarity, with groups of 1    ))x = torch.randint(0, 20000, (1, 1024))model(x)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">attn_qk_norm</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>,       <span class=\"pl-c\"># set to True</span>        <span class=\"pl-s1\">attn_qk_norm_scale</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">10</span>    <span class=\"pl-c\"># new scale on the similarity, with groups of 1</span>    ))<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>))<span class=\"pl-en\">model</span>(<span class=\"pl-s1\">x</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">QK RMSNorm</h3><a id=\"user-content-qk-rmsnorm\" class=\"anchor-element\" aria-label=\"Permalink: QK RMSNorm\" href=\"#qk-rmsnorm\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/qknorm-analysis.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/qknorm-analysis.png\" width=\"450px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">Update: Google Brain has proven out something similar to cosine sim attention in <a href=\"https://arxiv.org/abs/2302.05442\" rel=\"nofollow\">a 22B parameter model</a>. In their papers, they have analysis showing that the normalization resulted in not only extra stability, but also better results in the end (due to less need to adjust learning rate when increasing parameter count).</p><p dir=\"auto\">We are nearing the point of wiping out a source of transformer training instability with one simple intervention, in my opinion. The only slight difference in the paper is that they still have a learned scale across the feature dimension (per use of rmsnorm). Not sure how critical this is, but just to make sure we don't miss anything, I will include this here. You can use this by setting <code>qk_norm_dim_scale = True</code></p><p dir=\"auto\">Update: <a href=\"https://twitter.com/Tim_Dettmers/status/1625531080513306627\" rel=\"nofollow\">Counterpoint from Tim Dettmers</a></p><p dir=\"auto\">Update 2: <a href=\"https://arxiv.org/abs/2305.19268\" rel=\"nofollow\">Counter</a> to Tim's assertion that outliers are needed, and potentially even <a href=\"https://arxiv.org/abs/2306.12929\" rel=\"nofollow\">some solutions</a></p><p dir=\"auto\">Update 3: Used by <a href=\"https://www.adept.ai/blog/persimmon-8b\" rel=\"nofollow\">8B parameter LLM</a> successfully</p><p dir=\"auto\">Update 4: a MetaAI group found that they can <a href=\"https://arxiv.org/abs/2309.16588\" rel=\"nofollow\">alleviate outliers</a> by adding <code>register tokens</code>, also known as <code>memory tokens</code> from earlier literature (Burtsev et al). Perhaps what should be tried next is see if qk norm can be improved in the presence of memory tokens.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 12,        heads = 8,        attn_qk_norm = True,        attn_qk_norm_dim_scale = True # set this to True, in addition to `attn_qk_norm = True`    ))x = torch.randint(0, 256, (1, 1024))model(x)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">12</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">attn_qk_norm</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>,        <span class=\"pl-s1\">attn_qk_norm_dim_scale</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span> <span class=\"pl-c\"># set this to True, in addition to `attn_qk_norm = True`</span>    ))<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">256</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>))<span class=\"pl-en\">model</span>(<span class=\"pl-s1\">x</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Turning off absolute positional embedding</h3><a id=\"user-content-turning-off-absolute-positional-embedding\" class=\"anchor-element\" aria-label=\"Permalink: Turning off absolute positional embedding\" href=\"#turning-off-absolute-positional-embedding\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">A number of papers have hinted that causal transformers (<code>Decoder</code>) can learn absolute positions in the absence of added embeddings of any sort. This was recently thoroughly investigated <a href=\"https://arxiv.org/abs/2203.16634\" rel=\"nofollow\">here</a>. You can turn off the absolute positional embedding by setting <code>use_abs_pos_emb = False</code> in the <code>TransformerWrapper</code></p><p dir=\"auto\">Given <a href=\"https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\" rel=\"nofollow\">PaLM</a>, the trend going forward may be to forgo absolute positional embedding (again, for causal transformers only), and add relative positional embeddings with RoPE, ALiBi, etc.</p><p dir=\"auto\">Update: <a href=\"https://arxiv.org/abs/2305.19466\" rel=\"nofollow\">This paper</a> shows that in the absence of any engineered absolute or relative positional embeddings, decoders can generate implicit positions, and even length generalize better than solutions of the past. They were unaware of dynamic positional bias, however.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decodermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    use_abs_pos_emb = False,   # set this to False    attn_layers = Decoder(        dim = 512,        depth = 6,        heads = 8,    ))x = torch.randint(0, 20000, (1, 1024))model(x)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">use_abs_pos_emb</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">False</span>,   <span class=\"pl-c\"># set this to False</span>    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    ))<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>))<span class=\"pl-en\">model</span>(<span class=\"pl-s1\">x</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Forgetful Causal Mask</h3><a id=\"user-content-forgetful-causal-mask\" class=\"anchor-element\" aria-label=\"Permalink: Forgetful Causal Mask\" href=\"#forgetful-causal-mask\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/fcm.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/fcm.png\" width=\"450px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2210.13432\" rel=\"nofollow\">This paper</a> shows convincing results that one can combine masking (from masked language modeling) with autoregressive training, leading to significantly better results.</p><p dir=\"auto\">You can use this by setting the <code>mask_prob</code> on the <code>AutoregressiveWrapper</code> class</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import TransformerWrapper, Decoder, AutoregressiveWrappermodel = TransformerWrapper(    num_tokens = 20000,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 12,        heads = 8    ))model = AutoregressiveWrapper(    model,    mask_prob = 0.15  # in paper, they use 15%, same as BERT).cuda()# mock datax = torch.randint(0, 20000, (1, 1024)).cuda()# derive cross entropy loss, masking all taken care ofloss = model(x)loss.backward()\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TransformerWrapper</span>, <span class=\"pl-v\">Decoder</span>, <span class=\"pl-v\">AutoregressiveWrapper</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">20000</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">12</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>    ))<span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">AutoregressiveWrapper</span>(    <span class=\"pl-s1\">model</span>,    <span class=\"pl-s1\">mask_prob</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.15</span>  <span class=\"pl-c\"># in paper, they use 15%, same as BERT</span>).<span class=\"pl-en\">cuda</span>()<span class=\"pl-c\"># mock data</span><span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20000</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>)).<span class=\"pl-en\">cuda</span>()<span class=\"pl-c\"># derive cross entropy loss, masking all taken care of</span><span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">model</span>(<span class=\"pl-s1\">x</span>)<span class=\"pl-s1\">loss</span>.<span class=\"pl-en\">backward</span>()</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Miscellaneous</h2><a id=\"user-content-miscellaneous\" class=\"anchor-element\" aria-label=\"Permalink: Miscellaneous\" href=\"#miscellaneous\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Cross Attention</h3><a id=\"user-content-cross-attention\" class=\"anchor-element\" aria-label=\"Permalink: Cross Attention\" href=\"#cross-attention\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import Encoder, CrossAttenderenc = Encoder(dim = 512, depth = 6)model = CrossAttender(dim = 512, depth = 6)nodes = torch.randn(1, 1, 512)node_masks = torch.ones(1, 1).bool()neighbors = torch.randn(1, 5, 512)neighbor_masks = torch.ones(1, 5).bool()encoded_neighbors = enc(neighbors, mask = neighbor_masks)model(nodes, context = encoded_neighbors, mask = node_masks, context_mask = neighbor_masks) # (1, 1, 512)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Encoder</span>, <span class=\"pl-v\">CrossAttender</span><span class=\"pl-s1\">enc</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Encoder</span>(<span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>, <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>)<span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">CrossAttender</span>(<span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>, <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>)<span class=\"pl-s1\">nodes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">512</span>)<span class=\"pl-s1\">node_masks</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">ones</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>).<span class=\"pl-en\">bool</span>()<span class=\"pl-s1\">neighbors</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">512</span>)<span class=\"pl-s1\">neighbor_masks</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">ones</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">5</span>).<span class=\"pl-en\">bool</span>()<span class=\"pl-s1\">encoded_neighbors</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">enc</span>(<span class=\"pl-s1\">neighbors</span>, <span class=\"pl-s1\">mask</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">neighbor_masks</span>)<span class=\"pl-en\">model</span>(<span class=\"pl-s1\">nodes</span>, <span class=\"pl-s1\">context</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">encoded_neighbors</span>, <span class=\"pl-s1\">mask</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">node_masks</span>, <span class=\"pl-s1\">context_mask</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">neighbor_masks</span>) <span class=\"pl-c\"># (1, 1, 512)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Continuous Embeddings</h3><a id=\"user-content-continuous-embeddings\" class=\"anchor-element\" aria-label=\"Permalink: Continuous Embeddings\" href=\"#continuous-embeddings\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import ContinuousTransformerWrapper, Decodermodel = ContinuousTransformerWrapper(    dim_in = 32,    dim_out = 100,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 12,        heads = 8    ))x = torch.randn((1, 1024, 32))model(x) # (1, 1024, 100)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ContinuousTransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ContinuousTransformerWrapper</span>(    <span class=\"pl-s1\">dim_in</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">dim_out</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">100</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">12</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>    ))<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">32</span>))<span class=\"pl-en\">model</span>(<span class=\"pl-s1\">x</span>) <span class=\"pl-c\"># (1, 1024, 100)</span></pre></div><p dir=\"auto\">You can also train a transformer that accepts continuous values autoregressively easily, in the same scheme as done successfully in <a href=\"https://arxiv.org/abs/2112.05329\" rel=\"nofollow\">this paper</a></p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import ContinuousTransformerWrapper, Decoderfrom x_transformers import ContinuousAutoregressiveWrappermodel = ContinuousTransformerWrapper(    dim_in = 777,    dim_out = 777,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 12,        heads = 8    ))# wrap it with the continuous autoregressive wrappermodel = ContinuousAutoregressiveWrapper(model)# mock datax = torch.randn((1, 1024, 777))mask = torch.ones(1, 1024).bool()# train on a lot of data aboveloss = model(x, mask = mask)loss.backward# then generatestart_emb = torch.randn(1, 777)generated = model.generate(start_emb, 17) # (17, 777)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ContinuousTransformerWrapper</span>, <span class=\"pl-v\">Decoder</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ContinuousAutoregressiveWrapper</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ContinuousTransformerWrapper</span>(    <span class=\"pl-s1\">dim_in</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">777</span>,    <span class=\"pl-s1\">dim_out</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">777</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">12</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>    ))<span class=\"pl-c\"># wrap it with the continuous autoregressive wrapper</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ContinuousAutoregressiveWrapper</span>(<span class=\"pl-s1\">model</span>)<span class=\"pl-c\"># mock data</span><span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">777</span>))<span class=\"pl-s1\">mask</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">ones</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>).<span class=\"pl-en\">bool</span>()<span class=\"pl-c\"># train on a lot of data above</span><span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">model</span>(<span class=\"pl-s1\">x</span>, <span class=\"pl-s1\">mask</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">mask</span>)<span class=\"pl-s1\">loss</span>.<span class=\"pl-s1\">backward</span><span class=\"pl-c\"># then generate</span><span class=\"pl-s1\">start_emb</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">777</span>)<span class=\"pl-s1\">generated</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">model</span>.<span class=\"pl-en\">generate</span>(<span class=\"pl-s1\">start_emb</span>, <span class=\"pl-c1\">17</span>) <span class=\"pl-c\"># (17, 777)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">xVal - Continuous and Discrete</h3><a id=\"user-content-xval---continuous-and-discrete\" class=\"anchor-element\" aria-label=\"Permalink: xVal - Continuous and Discrete\" href=\"#xval---continuous-and-discrete\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/x-transformers/blob/main/images/xval.png\"><img src=\"/lucidrains/x-transformers/raw/main/images/xval.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This is promising work that resulted from the collaboration across many institutes (collectively known as Polymathic AI). They found that by offering a continuously scaled number token to the transformer, the transformer was able to generalize arithmetic and forecasting tasks better than the alternative encoding schemes.</p><p dir=\"auto\">This is corroborated by some <a href=\"https://github.com/lucidrains/tab-transformer-pytorch#ft-transformer\">prior work</a></p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom x_transformers import (    Decoder,    XValTransformerWrapper,    XValAutoregressiveWrapper)model = XValTransformerWrapper(    num_tokens = 4,    numerical_token_id = 3,    max_seq_len = 1024,    attn_layers = Decoder(        dim = 512,        depth = 12,        heads = 8    ))# wrap it with the xval autoregressive wrappermodel = XValAutoregressiveWrapper(model)# mock dataids = torch.randint(0, 4, (1, 777))nums = torch.randn(1, 777)mask = torch.ones(1, 777).bool()# train on a lot of data aboveloss = model(ids, nums, mask = mask)loss.backward()# then generatestart_ids = torch.randint(0, 4, (1, 1))start_nums = torch.randn(1, 1)ids_out, num_out, is_number_mask = model.generate(start_ids, start_nums, 17)# (1, 17), (1, 17), (1, 17)# discrete, continuous, mask for discrete / continuous\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> (    <span class=\"pl-v\">Decoder</span>,    <span class=\"pl-v\">XValTransformerWrapper</span>,    <span class=\"pl-v\">XValAutoregressiveWrapper</span>)<span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">XValTransformerWrapper</span>(    <span class=\"pl-s1\">num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,    <span class=\"pl-s1\">numerical_token_id</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">attn_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Decoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">12</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>    ))<span class=\"pl-c\"># wrap it with the xval autoregressive wrapper</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">XValAutoregressiveWrapper</span>(<span class=\"pl-s1\">model</span>)<span class=\"pl-c\"># mock data</span><span class=\"pl-s1\">ids</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">4</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">777</span>))<span class=\"pl-s1\">nums</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">777</span>)<span class=\"pl-s1\">mask</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">ones</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">777</span>).<span class=\"pl-en\">bool</span>()<span class=\"pl-c\"># train on a lot of data above</span><span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">model</span>(<span class=\"pl-s1\">ids</span>, <span class=\"pl-s1\">nums</span>, <span class=\"pl-s1\">mask</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">mask</span>)<span class=\"pl-s1\">loss</span>.<span class=\"pl-en\">backward</span>()<span class=\"pl-c\"># then generate</span><span class=\"pl-s1\">start_ids</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">4</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>))<span class=\"pl-s1\">start_nums</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>)<span class=\"pl-s1\">ids_out</span>, <span class=\"pl-s1\">num_out</span>, <span class=\"pl-s1\">is_number_mask</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">model</span>.<span class=\"pl-en\">generate</span>(<span class=\"pl-s1\">start_ids</span>, <span class=\"pl-s1\">start_nums</span>, <span class=\"pl-c1\">17</span>)<span class=\"pl-c\"># (1, 17), (1, 17), (1, 17)</span><span class=\"pl-c\"># discrete, continuous, mask for discrete / continuous</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Citations</h2><a id=\"user-content-citations\" class=\"anchor-element\" aria-label=\"Permalink: Citations\" href=\"#citations\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{vaswani2017attention,    title   = {Attention Is All You Need},    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},    year    = {2017},    eprint  = {1706.03762},    archivePrefix = {arXiv},    primaryClass = {cs.CL}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">vaswani2017attention</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Attention Is All You Need<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2017<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>1706.03762<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CL<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{DBLP:journals/corr/abs-1907-01470,    author    = {Sainbayar Sukhbaatar and               Edouard Grave and               Guillaume Lample and               Herv{\\'{e}} J{\\'{e}}gou and               Armand Joulin},    title     = {Augmenting Self-attention with Persistent Memory},    journal   = {CoRR},    volume    = {abs/1907.01470},    year      = {2019},    url       = {http://arxiv.org/abs/1907.01470}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">DBLP:journals/corr/abs-1907-01470</span>,    <span class=\"pl-s\">author</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Sainbayar Sukhbaatar and</span><span class=\"pl-s\">               Edouard Grave and</span><span class=\"pl-s\">               Guillaume Lample and</span><span class=\"pl-s\">               Herv{\\'{e}} J{\\'{e}}gou and</span><span class=\"pl-s\">               Armand Joulin<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">title</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Augmenting Self-attention with Persistent Memory<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>CoRR<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">volume</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>abs/1907.01470<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>      = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2019<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>       = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>http://arxiv.org/abs/1907.01470<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{1910.05895,    author  = {Toan Q. Nguyen and Julian Salazar},    title   = {Transformers without Tears: Improving the Normalization of Self-Attention},    year    = {2019},    eprint  = {arXiv:1910.05895},    doi     = {10.5281/zenodo.3525484},}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">1910.05895</span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Toan Q. Nguyen and Julian Salazar<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Transformers without Tears: Improving the Normalization of Self-Attention<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2019<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv:1910.05895<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">doi</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>10.5281/zenodo.3525484<span class=\"pl-pds\">}</span></span>,}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{shazeer2020glu,    title   = {GLU Variants Improve Transformer},    author  = {Noam Shazeer},    year    = {2020},    url     = {https://arxiv.org/abs/2002.05202}    }\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">shazeer2020glu</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>GLU Variants Improve Transformer<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Noam Shazeer<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2020<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://arxiv.org/abs/2002.05202<span class=\"pl-pds\">}</span></span>    }</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Zoph2022STMoEDS,    title   = {ST-MoE: Designing Stable and Transferable Sparse Expert Models},    author  = {Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam M. Shazeer and William Fedus},    year    = {2022}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Zoph2022STMoEDS</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ST-MoE: Designing Stable and Transferable Sparse Expert Models<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam M. Shazeer and William Fedus<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{bhojanapalli2020lowrank,    title   = {Low-Rank Bottleneck in Multi-head Attention Models},    author  = {Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},    year    = {2020},    eprint  = {2002.07028}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">bhojanapalli2020lowrank</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Low-Rank Bottleneck in Multi-head Attention Models<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2020<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2002.07028<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{burtsev2020memory,    title   = {Memory Transformer},     author  = {Mikhail S. Burtsev and Grigory V. Sapunov},    year    = {2020},    eprint  = {2006.11527},    archivePrefix = {arXiv},    primaryClass = {cs.CL}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">burtsev2020memory</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Memory Transformer<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Mikhail S. Burtsev and Grigory V. Sapunov<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2020<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2006.11527<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CL<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{zhao2019explicit,    title   = {Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection},     author  = {Guangxiang Zhao and Junyang Lin and Zhiyuan Zhang and Xuancheng Ren and Qi Su and Xu Sun},    year    = {2019},    eprint  = {1912.11637},    archivePrefix = {arXiv},    primaryClass = {cs.CL}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">zhao2019explicit</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Guangxiang Zhao and Junyang Lin and Zhiyuan Zhang and Xuancheng Ren and Qi Su and Xu Sun<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2019<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>1912.11637<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CL<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{correia2019adaptively,    title   = {Adaptively Sparse Transformers},    author  = {Gon\u00e7alo M. Correia and Vlad Niculae and Andr\u00e9 F. T. Martins},    year    = {2019},    eprint  = {1909.00015},    archivePrefix = {arXiv},    primaryClass = {cs.CL}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">correia2019adaptively</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Adaptively Sparse Transformers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Gon\u00e7alo M. Correia and Vlad Niculae and Andr\u00e9 F. T. Martins<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2019<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>1909.00015<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CL<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{shazeer2020talkingheads,    title   = {Talking-Heads Attention},     author  = {Noam Shazeer and Zhenzhong Lan and Youlong Cheng and Nan Ding and Le Hou},    year    = {2020},    eprint  = {2003.02436},    archivePrefix = {arXiv},    primaryClass = {cs.LG}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">shazeer2020talkingheads</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Talking-Heads Attention<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Noam Shazeer and Zhenzhong Lan and Youlong Cheng and Nan Ding and Le Hou<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2020<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2003.02436<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.LG<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{press2020improving,    title   = {Improving Transformer Models by Reordering their Sublayers},     author  = {Ofir Press and Noah A. Smith and Omer Levy},    year    = {2020},    eprint  = {1911.03864},    archivePrefix = {arXiv},    primaryClass = {cs.CL}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">press2020improving</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Improving Transformer Models by Reordering their Sublayers<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Ofir Press and Noah A. Smith and Omer Levy<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2020<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>1911.03864<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CL<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{lu2019understanding,    title   = {Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View},     author  = {Yiping Lu and Zhuohan Li and Di He and Zhiqing Sun and Bin Dong and Tao Qin and Liwei Wang and Tie-Yan Liu},    year    = {2019},    eprint  = {1906.02762},    archivePrefix = {arXiv},    primaryClass = {cs.LG}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">lu2019understanding</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Yiping Lu and Zhuohan Li and Di He and Zhiqing Sun and Bin Dong and Tao Qin and Liwei Wang and Tie-Yan Liu<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2019<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>1906.02762<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.LG<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{ke2020rethinking,    title     = {Rethinking Positional Encoding in Language Pre-training},    author    = {Guolin Ke and Di He and Tie-Yan Liu},    year      = {2020},    eprint    = {2006.15595},    archivePrefix = {arXiv},    primaryClass = {cs.CL}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">ke2020rethinking</span>,    <span class=\"pl-s\">title</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Rethinking Positional Encoding in Language Pre-training<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Guolin Ke and Di He and Tie-Yan Liu<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>      = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2020<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2006.15595<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CL<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{dosovitskiy2020image,    title   = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},    author  = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},    year    = {2020},    eprint  = {2010.11929},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">dosovitskiy2020image</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2020<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2010.11929<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{huang2019attention,    title   = {Attention on Attention for Image Captioning},    author  = {Lun Huang and Wenmin Wang and Jie Chen and Xiao-Yong Wei},    year    = {2019},    eprint  = {1908.06954},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">huang2019attention</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Attention on Attention for Image Captioning<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Lun Huang and Wenmin Wang and Jie Chen and Xiao-Yong Wei<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2019<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>1908.06954<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{raffel2020exploring,    title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},     author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},    year    = {2020},    eprint  = {1910.10683},    archivePrefix = {arXiv},    primaryClass = {cs.LG}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">raffel2020exploring</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2020<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>1910.10683<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.LG<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{martins-etal-2020-sparse,    title   = &quot;Sparse Text Generation&quot;,    author  = &quot;Martins, Pedro Henrique  and        Marinho, Zita  and        Martins, Andr{\\'e} F. T.&quot;,    booktitle = &quot;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)&quot;,    month   = nov,    year    = &quot;2020&quot;,    address = &quot;Online&quot;,    publisher = &quot;Association for Computational Linguistics&quot;,    url     = &quot;https://www.aclweb.org/anthology/2020.emnlp-main.348&quot;}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">martins-etal-2020-sparse</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Sparse Text Generation<span class=\"pl-pds\">\"</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Martins, Pedro Henrique  and</span><span class=\"pl-s\">        Marinho, Zita  and</span><span class=\"pl-s\">        Martins, Andr{\\'e} F. T.<span class=\"pl-pds\">\"</span></span>,    <span class=\"pl-s\">booktitle</span> = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<span class=\"pl-pds\">\"</span></span>,    <span class=\"pl-s\">month</span>   = nov,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2020<span class=\"pl-pds\">\"</span></span>,    <span class=\"pl-s\">address</span> = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Online<span class=\"pl-pds\">\"</span></span>,    <span class=\"pl-s\">publisher</span> = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Association for Computational Linguistics<span class=\"pl-pds\">\"</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>https://www.aclweb.org/anthology/2020.emnlp-main.348<span class=\"pl-pds\">\"</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{he2020realformer,    title   = {RealFormer: Transformer Likes Residual Attention},    author  = {Ruining He and Anirudh Ravula and Bhargav Kanagal and Joshua Ainslie},    year    = {2020},    eprint  = {2012.11747},    archivePrefix = {arXiv},    primaryClass = {cs.LG}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">he2020realformer</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>RealFormer: Transformer Likes Residual Attention<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Ruining He and Anirudh Ravula and Bhargav Kanagal and Joshua Ainslie<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2020<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2012.11747<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.LG<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{carion2020endtoend,    title   = {End-to-End Object Detection with Transformers},    author  = {Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko},    year    = {2020},    eprint  = {2005.12872},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">carion2020endtoend</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>End-to-End Object Detection with Transformers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2020<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2005.12872<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{press2021ALiBi,    title   = {Train Short, Test Long: Attention with Linear Biases Enable Input Length Extrapolation},    author  = {Ofir Press and Noah A. Smith and Mike Lewis},    year    = {2021},    url     = {https://ofir.io/train_short_test_long.pdf}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">press2021ALiBi</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Train Short, Test Long: Attention with Linear Biases Enable Input Length Extrapolation<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Ofir Press and Noah A. Smith and Mike Lewis<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://ofir.io/train_short_test_long.pdf<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{parisotto2019stabilizing,    title     = {Stabilizing Transformers for Reinforcement Learning},    author    = {Emilio Parisotto and H. Francis Song and Jack W. Rae and Razvan Pascanu and Caglar Gulcehre and Siddhant M. Jayakumar and Max Jaderberg and Raphael Lopez Kaufman and Aidan Clark and Seb Noury and Matthew M. Botvinick and Nicolas Heess and Raia Hadsell},    year      = {2019},    eprint    = {1910.06764},    archivePrefix = {arXiv},    primaryClass = {cs.LG}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">parisotto2019stabilizing</span>,    <span class=\"pl-s\">title</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Stabilizing Transformers for Reinforcement Learning<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Emilio Parisotto and H. Francis Song and Jack W. Rae and Razvan Pascanu and Caglar Gulcehre and Siddhant M. Jayakumar and Max Jaderberg and Raphael Lopez Kaufman and Aidan Clark and Seb Noury and Matthew M. Botvinick and Nicolas Heess and Raia Hadsell<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>      = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2019<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>1910.06764<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.LG<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{narang2021transformer,    title       = {Do Transformer Modifications Transfer Across Implementations and Applications?},    author      = {Sharan Narang and Hyung Won Chung and Yi Tay and William Fedus and Thibault Fevry and Michael Matena and Karishma Malkan and Noah Fiedel and Noam Shazeer and Zhenzhong Lan and Yanqi Zhou and Wei Li and Nan Ding and Jake Marcus and Adam Roberts and Colin Raffel},    year        = {2021},    eprint      = {2102.11972},    archivePrefix = {arXiv},    primaryClass = {cs.LG}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">narang2021transformer</span>,    <span class=\"pl-s\">title</span>       = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Do Transformer Modifications Transfer Across Implementations and Applications?<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>      = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Sharan Narang and Hyung Won Chung and Yi Tay and William Fedus and Thibault Fevry and Michael Matena and Karishma Malkan and Noah Fiedel and Noam Shazeer and Zhenzhong Lan and Yanqi Zhou and Wei Li and Nan Ding and Jake Marcus and Adam Roberts and Colin Raffel<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>        = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>      = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2102.11972<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.LG<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{zhang2019root,    title   = {Root Mean Square Layer Normalization},    author  = {Biao Zhang and Rico Sennrich},    year    = {2019},    eprint  = {1910.07467},    archivePrefix = {arXiv},    primaryClass = {cs.LG}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">zhang2019root</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Root Mean Square Layer Normalization<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Biao Zhang and Rico Sennrich<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2019<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>1910.07467<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.LG<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Qin2023ScalingTT,    title   = {Scaling TransNormer to 175 Billion Parameters},    author  = {Zhen Qin and Dong Li and Weigao Sun and Weixuan Sun and Xuyang Shen and Xiaodong Han and Yunshen Wei and Baohong Lv and Fei Yuan and Xiao Luo and Y. Qiao and Yiran Zhong},    year    = {2023},    url     = {https://api.semanticscholar.org/CorpusID:260203124}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Qin2023ScalingTT</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Scaling TransNormer to 175 Billion Parameters<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Zhen Qin and Dong Li and Weigao Sun and Weixuan Sun and Xuyang Shen and Xiaodong Han and Yunshen Wei and Baohong Lv and Fei Yuan and Xiao Luo and Y. Qiao and Yiran Zhong<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://api.semanticscholar.org/CorpusID:260203124<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{su2021roformer,    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},    year    = {2021},    eprint  = {2104.09864},    archivePrefix = {arXiv},    primaryClass = {cs.CL}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">su2021roformer</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>RoFormer: Enhanced Transformer with Rotary Position Embedding<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2104.09864<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CL<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Chen2023ExtendingCW,    title   = {Extending Context Window of Large Language Models via Positional Interpolation},    author  = {Shouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian},    year    = {2023}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Chen2023ExtendingCW</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Extending Context Window of Large Language Models via Positional Interpolation<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Shouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Sun2022ALT,  title     = {A Length-Extrapolatable Transformer},  author    = {Yutao Sun and Li Dong and Barun Patra and Shuming Ma and Shaohan Huang and Alon Benhaim and Vishrav Chaudhary and Xia Song and Furu Wei},  year      = {2022}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Sun2022ALT</span>,  <span class=\"pl-s\">title</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>A Length-Extrapolatable Transformer<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">author</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Yutao Sun and Li Dong and Barun Patra and Shuming Ma and Shaohan Huang and Alon Benhaim and Vishrav Chaudhary and Xia Song and Furu Wei<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">year</span>      = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@Article{AlphaFold2021,    author  = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\\v{Z}}{\\'\\i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A A and Ballard, Andrew J and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},    journal = {Nature},    title   = {Highly accurate protein structure prediction with {AlphaFold}},    year    = {2021},    doi     = {10.1038/s41586-021-03819-2},    note    = {(Accelerated article preview)},}\"><pre><span class=\"pl-k\">@Article</span>{<span class=\"pl-en\">AlphaFold2021</span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\\v{Z}}{\\'\\i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A A and Ballard, Andrew J and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Nature<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Highly accurate protein structure prediction with {AlphaFold}<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">doi</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>10.1038/s41586-021-03819-2<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">note</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>(Accelerated article preview)<span class=\"pl-pds\">}</span></span>,}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@software{peng_bo_2021_5196578,    author       = {PENG Bo},    title        = {BlinkDL/RWKV-LM: 0.01},    month        = {aug},    year         = {2021},    publisher    = {Zenodo},    version      = {0.01},    doi          = {10.5281/zenodo.5196578},    url          = {https://doi.org/10.5281/zenodo.5196578}}\"><pre><span class=\"pl-k\">@software</span>{<span class=\"pl-en\">peng_bo_2021_5196578</span>,    <span class=\"pl-s\">author</span>       = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>PENG Bo<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">title</span>        = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>BlinkDL/RWKV-LM: 0.01<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">month</span>        = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>aug<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>         = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">publisher</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Zenodo<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">version</span>      = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>0.01<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">doi</span>          = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>10.5281/zenodo.5196578<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>          = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://doi.org/10.5281/zenodo.5196578<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{csord\u00e1s2021devil,    title   = {The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers},    author  = {R\u00f3bert Csord\u00e1s and Kazuki Irie and J\u00fcrgen Schmidhuber},    year    = {2021},    eprint  = {2108.12284},    archivePrefix = {arXiv},    primaryClass = {cs.LG}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">csord\u00e1s2021devil</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>R\u00f3bert Csord\u00e1s and Kazuki Irie and J\u00fcrgen Schmidhuber<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2108.12284<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.LG<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{so2021primer,    title   = {Primer: Searching for Efficient Transformers for Language Modeling},     author  = {David R. So and Wojciech Ma\u0144ke and Hanxiao Liu and Zihang Dai and Noam Shazeer and Quoc V. Le},    year    = {2021},    eprint  = {2109.08668},    archivePrefix = {arXiv},    primaryClass = {cs.LG}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">so2021primer</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Primer: Searching for Efficient Transformers for Language Modeling<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>David R. So and Wojciech Ma\u0144ke and Hanxiao Liu and Zihang Dai and Noam Shazeer and Quoc V. Le<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2109.08668<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.LG<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{ding2021erniedoc,    title   = {ERNIE-Doc: A Retrospective Long-Document Modeling Transformer},     author  = {Siyu Ding and Junyuan Shang and Shuohuan Wang and Yu Sun and Hao Tian and Hua Wu and Haifeng Wang},    year    = {2021},    eprint  = {2012.15688},    archivePrefix = {arXiv},    primaryClass = {cs.CL}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">ding2021erniedoc</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ERNIE-Doc: A Retrospective Long-Document Modeling Transformer<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Siyu Ding and Junyuan Shang and Shuohuan Wang and Yu Sun and Hao Tian and Hua Wu and Haifeng Wang<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2012.15688<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CL<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{ding2021cogview,    title   = {CogView: Mastering Text-to-Image Generation via Transformers},    author  = {Ming Ding and Zhuoyi Yang and Wenyi Hong and Wendi Zheng and Chang Zhou and Da Yin and Junyang Lin and Xu Zou and Zhou Shao and Hongxia Yang and Jie Tang},    year    = {2021},    eprint  = {2105.13290},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">ding2021cogview</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>CogView: Mastering Text-to-Image Generation via Transformers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Ming Ding and Zhuoyi Yang and Wenyi Hong and Wendi Zheng and Chang Zhou and Da Yin and Junyang Lin and Xu Zou and Zhou Shao and Hongxia Yang and Jie Tang<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2105.13290<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{anonymous2022normformer,    title   = {NormFormer: Improved Transformer Pretraining with Extra Normalization},    author  = {Anonymous},    booktitle = {Submitted to The Tenth International Conference on Learning Representations },    year    = {2022},    url     = {https://openreview.net/forum?id=GMYWzWztDx5},    note    = {under review}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">anonymous2022normformer</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>NormFormer: Improved Transformer Pretraining with Extra Normalization<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Anonymous<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">booktitle</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Submitted to The Tenth International Conference on Learning Representations <span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://openreview.net/forum?id=GMYWzWztDx5<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">note</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>under review<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{henry2020querykey,    title   = {Query-Key Normalization for Transformers},    author  = {Alex Henry and Prudhvi Raj Dachapally and Shubham Pawar and Yuxuan Chen},    year    = {2020},    eprint  = {2010.04245},    archivePrefix = {arXiv},    primaryClass = {cs.CL}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">henry2020querykey</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Query-Key Normalization for Transformers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Alex Henry and Prudhvi Raj Dachapally and Shubham Pawar and Yuxuan Chen<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2020<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2010.04245<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CL<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{liu2021swin,    title   = {Swin Transformer V2: Scaling Up Capacity and Resolution},    author  = {Ze Liu and Han Hu and Yutong Lin and Zhuliang Yao and Zhenda Xie and Yixuan Wei and Jia Ning and Yue Cao and Zheng Zhang and Li Dong and Furu Wei and Baining Guo},    year    = {2021},    eprint  = {2111.09883},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">liu2021swin</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Swin Transformer V2: Scaling Up Capacity and Resolution<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Ze Liu and Han Hu and Yutong Lin and Zhuliang Yao and Zhenda Xie and Yixuan Wei and Jia Ning and Yue Cao and Zheng Zhang and Li Dong and Furu Wei and Baining Guo<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2111.09883<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{Haviv2022TransformerLM,    title   = {Transformer Language Models without Positional Encodings Still Learn Positional Information},    author  = {Adi Haviv and Ori Ram and Ofir Press and Peter Izsak and Omer Levy},    journal = {ArXiv},    year    = {2022},    volume  = {abs/2203.16634}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">Haviv2022TransformerLM</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Transformer Language Models without Positional Encodings Still Learn Positional Information<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Adi Haviv and Ori Ram and Ofir Press and Peter Izsak and Omer Levy<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ArXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">volume</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>abs/2203.16634<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{chowdhery2022PaLM,    title   = {PaLM: Scaling Language Modeling with Pathways},    author  = {Chowdhery, Aakanksha et al},    year    = {2022}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">chowdhery2022PaLM</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>PaLM: Scaling Language Modeling with Pathways<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Chowdhery, Aakanksha et al<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{Shazeer2019FastTD,    title   = {Fast Transformer Decoding: One Write-Head is All You Need},    author  = {Noam M. Shazeer},    journal = {ArXiv},    year    = {2019},    volume  = {abs/1911.02150}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">Shazeer2019FastTD</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Fast Transformer Decoding: One Write-Head is All You Need<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Noam M. Shazeer<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ArXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2019<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">volume</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>abs/1911.02150<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{Ainslie2023GQATG,    title   = {GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},    author  = {Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebr'on and Sumit K. Sanghai},    journal = {ArXiv},    year    = {2023},    volume  = {abs/2305.13245},    url     = {https://api.semanticscholar.org/CorpusID:258833177}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">Ainslie2023GQATG</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebr'on and Sumit K. Sanghai<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ArXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">volume</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>abs/2305.13245<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://api.semanticscholar.org/CorpusID:258833177<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{schlag2020enhancing,    title   = {Enhancing the Transformer with explicit relational encoding for math problem solving},    author  = {Imanol Schlag and Paul Smolensky and Roland Fernandez and Nebojsa Jojic and J{\\&quot;u}rgen Schmidhuber and Jianfeng Gao},    year    = {2020},    url     = {https://openreview.net/forum?id=B1xfElrKPr}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">schlag2020enhancing</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Enhancing the Transformer with explicit relational encoding for math problem solving<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Imanol Schlag and Paul Smolensky and Roland Fernandez and Nebojsa Jojic and J{\\\"u}rgen Schmidhuber and Jianfeng Gao<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2020<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://openreview.net/forum?id=B1xfElrKPr<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{Liu2022FCMFC,    title   = {FCM: Forgetful Causal Masking Makes Causal Language Models Better Zero-Shot Learners},    author  = {Hao Liu and Xinyang Geng and Lisa Lee and Igor Mordatch and Sergey Levine and Sharan Narang and P. Abbeel},    journal = {ArXiv},    year    = {2022},    volume  = {abs/2210.13432}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">Liu2022FCMFC</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>FCM: Forgetful Causal Masking Makes Causal Language Models Better Zero-Shot Learners<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Hao Liu and Xinyang Geng and Lisa Lee and Igor Mordatch and Sergey Levine and Sharan Narang and P. Abbeel<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ArXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">volume</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>abs/2210.13432<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Huang2016DeepNW,    title   = {Deep Networks with Stochastic Depth},    author  = {Gao Huang and Yu Sun and Zhuang Liu and Daniel Sedra and Kilian Q. Weinberger},    booktitle = {European Conference on Computer Vision},    year    = {2016}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Huang2016DeepNW</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Deep Networks with Stochastic Depth<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Gao Huang and Yu Sun and Zhuang Liu and Daniel Sedra and Kilian Q. Weinberger<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">booktitle</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>European Conference on Computer Vision<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2016<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Hua2022TransformerQI,    title   = {Transformer Quality in Linear Time},    author  = {Weizhe Hua and Zihang Dai and Hanxiao Liu and Quoc V. Le},    booktitle = {International Conference on Machine Learning},    year    = {2022}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Hua2022TransformerQI</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Transformer Quality in Linear Time<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Weizhe Hua and Zihang Dai and Hanxiao Liu and Quoc V. Le<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">booktitle</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>International Conference on Machine Learning<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{Chang2022MaskGITMG,    title   = {MaskGIT: Masked Generative Image Transformer},    author  = {Huiwen Chang and Han Zhang and Lu Jiang and Ce Liu and William T. Freeman},    journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    year    = {2022},    pages   = {11305-11315}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">Chang2022MaskGITMG</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>MaskGIT: Masked Generative Image Transformer<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Huiwen Chang and Han Zhang and Lu Jiang and Ce Liu and William T. Freeman<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">pages</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>11305-11315<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{Lezama2022ImprovedMI,    title   = {Improved Masked Image Generation with Token-Critic},    author  = {Jos{\\'e} Lezama and Huiwen Chang and Lu Jiang and Irfan Essa},    journal = {ArXiv},    year    = {2022},    volume  = {abs/2209.04439}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">Lezama2022ImprovedMI</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Improved Masked Image Generation with Token-Critic<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Jos{\\'e} Lezama and Huiwen Chang and Lu Jiang and Irfan Essa<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ArXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">volume</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>abs/2209.04439<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{https://doi.org/10.48550/arxiv.2302.01327,    doi     = {10.48550/ARXIV.2302.01327},    url     = {https://arxiv.org/abs/2302.01327},    author  = {Kumar, Manoj and Dehghani, Mostafa and Houlsby, Neil},    title   = {Dual PatchNorm},    publisher = {arXiv},    year    = {2023},    copyright = {Creative Commons Attribution 4.0 International}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">https://doi.org/10.48550/arxiv.2302.01327</span>,    <span class=\"pl-s\">doi</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>10.48550/ARXIV.2302.01327<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://arxiv.org/abs/2302.01327<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Kumar, Manoj and Dehghani, Mostafa and Houlsby, Neil<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Dual PatchNorm<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">publisher</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">copyright</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Creative Commons Attribution 4.0 International<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{dao2022flashattention,    title   = {Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},    author  = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\\'e}, Christopher},    booktitle = {Advances in Neural Information Processing Systems},    year    = {2022}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">dao2022flashattention</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\\'e}, Christopher<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">booktitle</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Advances in Neural Information Processing Systems<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{Xie2023ResiDualTW,  title     = {ResiDual: Transformer with Dual Residual Connections},  author    = {Shufang Xie and Huishuai Zhang and Junliang Guo and Xu Tan and Jiang Bian and Hany Hassan Awadalla and Arul Menezes and Tao Qin and Rui Yan},  journal   = {ArXiv},  year      = {2023},  volume    = {abs/2304.14802}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">Xie2023ResiDualTW</span>,  <span class=\"pl-s\">title</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ResiDual: Transformer with Dual Residual Connections<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">author</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Shufang Xie and Huishuai Zhang and Junliang Guo and Xu Tan and Jiang Bian and Hany Hassan Awadalla and Arul Menezes and Tao Qin and Rui Yan<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">journal</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ArXiv<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">year</span>      = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">volume</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>abs/2304.14802<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Dehghani2023ScalingVT,    title   = {Scaling Vision Transformers to 22 Billion Parameters},    author  = {Mostafa Dehghani and Josip Djolonga and Basil Mustafa and Piotr Padlewski and Jonathan Heek and Justin Gilmer and Andreas Steiner and Mathilde Caron and Robert Geirhos and Ibrahim M. Alabdulmohsin and Rodolphe Jenatton and Lucas Beyer and Michael Tschannen and Anurag Arnab and Xiao Wang and Carlos Riquelme and Matthias Minderer and Joan Puigcerver and Utku Evci and Manoj Kumar and Sjoerd van Steenkiste and Gamaleldin F. Elsayed and Aravindh Mahendran and Fisher Yu and Avital Oliver and Fantine Huot and Jasmijn Bastings and Mark Collier and Alexey A. Gritsenko and Vighnesh Birodkar and Cristina Nader Vasconcelos and Yi Tay and Thomas Mensink and Alexander Kolesnikov and Filip Paveti'c and Dustin Tran and Thomas Kipf and Mario Luvci'c and Xiaohua Zhai and Daniel Keysers and Jeremiah Harmsen and Neil Houlsby},    year    = {2023}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Dehghani2023ScalingVT</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Scaling Vision Transformers to 22 Billion Parameters<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Mostafa Dehghani and Josip Djolonga and Basil Mustafa and Piotr Padlewski and Jonathan Heek and Justin Gilmer and Andreas Steiner and Mathilde Caron and Robert Geirhos and Ibrahim M. Alabdulmohsin and Rodolphe Jenatton and Lucas Beyer and Michael Tschannen and Anurag Arnab and Xiao Wang and Carlos Riquelme and Matthias Minderer and Joan Puigcerver and Utku Evci and Manoj Kumar and Sjoerd van Steenkiste and Gamaleldin F. Elsayed and Aravindh Mahendran and Fisher Yu and Avital Oliver and Fantine Huot and Jasmijn Bastings and Mark Collier and Alexey A. Gritsenko and Vighnesh Birodkar and Cristina Nader Vasconcelos and Yi Tay and Thomas Mensink and Alexander Kolesnikov and Filip Paveti'c and Dustin Tran and Thomas Kipf and Mario Luvci'c and Xiaohua Zhai and Daniel Keysers and Jeremiah Harmsen and Neil Houlsby<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{Beyer2022BetterPV,    title   = {Better plain ViT baselines for ImageNet-1k},    author  = {Lucas Beyer and Xiaohua Zhai and Alexander Kolesnikov},    journal = {ArXiv},    year    = {2022},    volume  = {abs/2205.01580}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">Beyer2022BetterPV</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Better plain ViT baselines for ImageNet-1k<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Lucas Beyer and Xiaohua Zhai and Alexander Kolesnikov<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ArXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">volume</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>abs/2205.01580<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{Kazemnejad2023TheIO,    title   = {The Impact of Positional Encoding on Length Generalization in Transformers},    author  = {Amirhossein Kazemnejad and Inkit Padhi and Karthikeyan Natesan Ramamurthy and Payel Das and Siva Reddy},    journal = {ArXiv},    year    = {2023},    volume  = {abs/2305.19466}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">Kazemnejad2023TheIO</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>The Impact of Positional Encoding on Length Generalization in Transformers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Amirhossein Kazemnejad and Inkit Padhi and Karthikeyan Natesan Ramamurthy and Payel Das and Siva Reddy<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ArXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">volume</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>abs/2305.19466<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{bloc97-2023    title   = {NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.},    author  = {/u/bloc97},    url     = {https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">bloc97-2023</span>    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>/u/bloc97<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Zoph2022STMoEDS,    title   = {ST-MoE: Designing Stable and Transferable Sparse Expert Models},    author  = {Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam M. Shazeer and William Fedus},    year    = {2022}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Zoph2022STMoEDS</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ST-MoE: Designing Stable and Transferable Sparse Expert Models<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam M. Shazeer and William Fedus<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{Lan2019ALBERTAL,    title   = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},    author  = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},    journal = {ArXiv},    year    = {2019},    volume  = {abs/1909.11942},    url     = {https://api.semanticscholar.org/CorpusID:202888986}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">Lan2019ALBERTAL</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ArXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2019<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">volume</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>abs/1909.11942<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://api.semanticscholar.org/CorpusID:202888986<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Li2022ContrastiveDO,    title   = {Contrastive Decoding: Open-ended Text Generation as Optimization},    author  = {Xiang Lisa Li and Ari Holtzman and Daniel Fried and Percy Liang and Jason Eisner and Tatsunori Hashimoto and Luke Zettlemoyer and Mike Lewis},    booktitle = {Annual Meeting of the Association for Computational Linguistics},    year    = {2022},    url     = {https://api.semanticscholar.org/CorpusID:253157949}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Li2022ContrastiveDO</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Contrastive Decoding: Open-ended Text Generation as Optimization<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Xiang Lisa Li and Ari Holtzman and Daniel Fried and Percy Liang and Jason Eisner and Tatsunori Hashimoto and Luke Zettlemoyer and Mike Lewis<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">booktitle</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Annual Meeting of the Association for Computational Linguistics<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://api.semanticscholar.org/CorpusID:253157949<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{OBrien2023ContrastiveDI,    title   = {Contrastive Decoding Improves Reasoning in Large Language Models},    author  = {Sean O'Brien and Mike Lewis},    year    = {2023},    url     = {https://api.semanticscholar.org/CorpusID:261884427}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">OBrien2023ContrastiveDI</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Contrastive Decoding Improves Reasoning in Large Language Models<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Sean O'Brien and Mike Lewis<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://api.semanticscholar.org/CorpusID:261884427<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Darcet2023VisionTN,    title   = {Vision Transformers Need Registers},    author  = {Timoth'ee Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski},    year    = {2023},    url     = {https://api.semanticscholar.org/CorpusID:263134283}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Darcet2023VisionTN</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Vision Transformers Need Registers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Timoth'ee Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://api.semanticscholar.org/CorpusID:263134283<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{Bondarenko2023QuantizableTR,    title   = {Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing},    author  = {Yelysei Bondarenko and Markus Nagel and Tijmen Blankevoort},    journal = {ArXiv},    year    = {2023},    volume  = {abs/2306.12929},    url     = {https://api.semanticscholar.org/CorpusID:259224568}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">Bondarenko2023QuantizableTR</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Yelysei Bondarenko and Markus Nagel and Tijmen Blankevoort<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ArXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">volume</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>abs/2306.12929<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://api.semanticscholar.org/CorpusID:259224568<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Golkar2023xValAC,    title   = {xVal: A Continuous Number Encoding for Large Language Models},    author  = {Siavash Golkar and Mariel Pettee and Michael Eickenberg and Alberto Bietti and M. Cranmer and G{\\'e}raud Krawezik and Francois Lanusse and Michael McCabe and Ruben Ohana and Liam Parker and Bruno R{\\'e}galdo-Saint Blancard and Tiberiu Te\u015fileanu and Kyunghyun Cho and Shirley Ho},    year    = {2023},    url     = {https://api.semanticscholar.org/CorpusID:263622222}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Golkar2023xValAC</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>xVal: A Continuous Number Encoding for Large Language Models<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Siavash Golkar and Mariel Pettee and Michael Eickenberg and Alberto Bietti and M. Cranmer and G{\\'e}raud Krawezik and Francois Lanusse and Michael McCabe and Ruben Ohana and Liam Parker and Bruno R{\\'e}galdo-Saint Blancard and Tiberiu Te\u015fileanu and Kyunghyun Cho and Shirley Ho<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://api.semanticscholar.org/CorpusID:263622222<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{Rafailov2023DirectPO,    title   = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},    author  = {Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},    journal = {ArXiv},    year    = {2023},    volume  = {abs/2305.18290},    url     = {https://api.semanticscholar.org/CorpusID:258959321}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">Rafailov2023DirectPO</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Direct Preference Optimization: Your Language Model is Secretly a Reward Model<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ArXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">volume</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>abs/2305.18290<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://api.semanticscholar.org/CorpusID:258959321<span class=\"pl-pds\">}</span></span>}</pre></div><p dir=\"auto\"><em>solve intelligence... then use that to solve everything else.</em> - Demis Hassabis</p></article></div></div></div></div></div> <!-- --> <!-- --> <script type=\"application/json\" id=\"__PRIMER_DATA__\">{\"resolvedServerColorMode\":\"day\"}</script></div></react-partial>        <input type=\"hidden\" data-csrf=\"true\" value=\"QHgqjnGtD/QTPsAW2FyAdVQqjWHb720vzUOo+o4NB3nIGt2rwad7uAWoFzSinhsdWGK+XTCUo3uo4JkC7SO7hA==\" /></div>  <div data-view-component=\"true\" class=\"Layout-sidebar\">            <div class=\"BorderGrid about-margin\" data-pjax>        <div class=\"BorderGrid-row\">          <div class=\"BorderGrid-cell\">            <div class=\"hide-sm hide-md\">  <h2 class=\"mb-3 h4\">About</h2>      <p class=\"f4 my-3\">        A simple but complete full-attention transformer with a set of promising experimental features from various papers      </p>    <h3 class=\"sr-only\">Topics</h3>    <div class=\"my-3\">        <div class=\"f6\">      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:deep-learning\" href=\"/topics/deep-learning\" title=\"Topic: deep-learning\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  deep-learning</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:transformers\" href=\"/topics/transformers\" title=\"Topic: transformers\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  transformers</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:artificial-intelligence\" href=\"/topics/artificial-intelligence\" title=\"Topic: artificial-intelligence\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  artificial-intelligence</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:attention-mechanism\" href=\"/topics/attention-mechanism\" title=\"Topic: attention-mechanism\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  attention-mechanism</a>  </div>    </div>    <h3 class=\"sr-only\">Resources</h3>    <div class=\"mt-2\">      <a class=\"Link--muted\" data-analytics-event=\"{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:readme&quot;}\" href=\"#readme-ov-file\">        <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-book mr-2\">    <path d=\"M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z\"></path></svg>        Readme</a>    </div>      <h3 class=\"sr-only\">License</h3>  <div class=\"mt-2\">    <a href=\"#MIT-1-ov-file\"      class=\"Link--muted\"            data-analytics-event=\"{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:license&quot;}\"    >      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-law mr-2\">    <path d=\"M8.75.75V2h.985c.304 0 .603.08.867.231l1.29.736c.038.022.08.033.124.033h2.234a.75.75 0 0 1 0 1.5h-.427l2.111 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.006.005-.01.01-.045.04c-.21.176-.441.327-.686.45C14.556 10.78 13.88 11 13 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L12.178 4.5h-.162c-.305 0-.604-.079-.868-.231l-1.29-.736a.245.245 0 0 0-.124-.033H8.75V13h2.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1 0-1.5h2.5V3.5h-.984a.245.245 0 0 0-.124.033l-1.289.737c-.265.15-.564.23-.869.23h-.162l2.112 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.016.015-.045.04c-.21.176-.441.327-.686.45C4.556 10.78 3.88 11 3 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L2.178 4.5H1.75a.75.75 0 0 1 0-1.5h2.234a.249.249 0 0 0 .125-.033l1.288-.737c.265-.15.564-.23.869-.23h.984V.75a.75.75 0 0 1 1.5 0Zm2.945 8.477c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L13 6.327Zm-10 0c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L3 6.327Z\"></path></svg>     MIT license    </a>  </div>  <include-fragment  src=\"/lucidrains/x-transformers/hovercards/citation/sidebar_partial?tree_name=main\">  </include-fragment>  <div class=\"mt-2\">    <a href=\"/lucidrains/x-transformers/activity\" data-view-component=\"true\" class=\"Link Link--muted\">      <svg text=\"gray\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-pulse mr-2\">    <path d=\"M6 2c.306 0 .582.187.696.471L10 10.731l1.304-3.26A.751.751 0 0 1 12 7h3.25a.75.75 0 0 1 0 1.5h-2.742l-1.812 4.528a.751.751 0 0 1-1.392 0L6 4.77 4.696 8.03A.75.75 0 0 1 4 8.5H.75a.75.75 0 0 1 0-1.5h2.742l1.812-4.529A.751.751 0 0 1 6 2Z\"></path></svg>      <span class=\"color-fg-muted\">Activity</span></a>  </div>  <h3 class=\"sr-only\">Stars</h3>  <div class=\"mt-2\">    <a href=\"/lucidrains/x-transformers/stargazers\" data-view-component=\"true\" class=\"Link Link--muted\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-star mr-2\">    <path d=\"M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z\"></path></svg>      <strong>3.9k</strong>      stars</a>  </div>  <h3 class=\"sr-only\">Watchers</h3>  <div class=\"mt-2\">    <a href=\"/lucidrains/x-transformers/watchers\" data-view-component=\"true\" class=\"Link Link--muted\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-eye mr-2\">    <path d=\"M8 2c1.981 0 3.671.992 4.933 2.078 1.27 1.091 2.187 2.345 2.637 3.023a1.62 1.62 0 0 1 0 1.798c-.45.678-1.367 1.932-2.637 3.023C11.67 13.008 9.981 14 8 14c-1.981 0-3.671-.992-4.933-2.078C1.797 10.83.88 9.576.43 8.898a1.62 1.62 0 0 1 0-1.798c.45-.677 1.367-1.931 2.637-3.022C4.33 2.992 6.019 2 8 2ZM1.679 7.932a.12.12 0 0 0 0 .136c.411.622 1.241 1.75 2.366 2.717C5.176 11.758 6.527 12.5 8 12.5c1.473 0 2.825-.742 3.955-1.715 1.124-.967 1.954-2.096 2.366-2.717a.12.12 0 0 0 0-.136c-.412-.621-1.242-1.75-2.366-2.717C10.824 4.242 9.473 3.5 8 3.5c-1.473 0-2.825.742-3.955 1.715-1.124.967-1.954 2.096-2.366 2.717ZM8 10a2 2 0 1 1-.001-3.999A2 2 0 0 1 8 10Z\"></path></svg>      <strong>52</strong>      watching</a>  </div>  <h3 class=\"sr-only\">Forks</h3>  <div class=\"mt-2\">    <a href=\"/lucidrains/x-transformers/forks\" data-view-component=\"true\" class=\"Link Link--muted\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo-forked mr-2\">    <path d=\"M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z\"></path></svg>      <strong>343</strong>      forks</a>  </div>    <div class=\"mt-2\">      <a class=\"Link--muted\" href=\"/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Flucidrains%2Fx-transformers&amp;report=lucidrains+%28user%29\">          Report repository</a>    </div></div>          </div>        </div>                    <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\">  <a href=\"/lucidrains/x-transformers/releases\" data-view-component=\"true\" class=\"Link--primary no-underline Link\">    Releases      <span title=\"341\" data-view-component=\"true\" class=\"Counter\">341</span></a></h2>  <a class=\"Link--primary d-flex no-underline\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" href=\"/lucidrains/x-transformers/releases/tag/1.27.19\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-tag flex-shrink-0 mt-1 color-fg-success\">    <path d=\"M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z\"></path></svg>    <div class=\"ml-2 min-width-0\">      <div class=\"d-flex\">        <span class=\"css-truncate css-truncate-target text-bold mr-2\" style=\"max-width: none;\">1.27.19</span>        <span title=\"Label: Latest\" data-view-component=\"true\" class=\"Label Label--success flex-shrink-0\">          Latest</span>      </div>      <div class=\"text-small color-fg-muted\"><relative-time datetime=\"2024-02-27T22:16:18Z\" class=\"no-wrap\">Feb 27, 2024</relative-time></div>    </div></a>    <div data-view-component=\"true\" class=\"mt-3\">      <a text=\"small\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" href=\"/lucidrains/x-transformers/releases\" data-view-component=\"true\" class=\"Link\">        + 340 releases</a></div>              </div>            </div>                    <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\">Sponsor this project</h2><include-fragment src=\"/lucidrains/x-transformers/sponsors_list?block_button=false&amp;current_repository=x-transformers\" aria-busy=\"true\" aria-label=\"Loading sponsorable links\">        <ul class=\"list-style-none\">        </ul></include-fragment>              </div>            </div>                    <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\">  <a href=\"/users/lucidrains/packages?repo_name=x-transformers\" data-view-component=\"true\" class=\"Link--primary no-underline Link d-flex flex-items-center\">    Packages      <span title=\"0\" hidden=\"hidden\" data-view-component=\"true\" class=\"Counter ml-1\">0</span></a></h2>      <div class=\"text-small color-fg-muted\">        No packages published <br>      </div>              </div>            </div>                    <div class=\"BorderGrid-row\" >              <div class=\"BorderGrid-cell\">                  <h2 class=\"h4 mb-3\">    <a href=\"/lucidrains/x-transformers/network/dependents\" data-view-component=\"true\" class=\"Link--primary no-underline Link\">      Used by <span title=\"224\" data-view-component=\"true\" class=\"Counter\">224</span></a>  </h2>  <a class=\"d-flex flex-items-center\" href=\"/lucidrains/x-transformers/network/dependents\">    <ul class=\"hx_flex-avatar-stack list-style-none min-width-0\">          <li class=\"hx_flex-avatar-stack-item\">            <img class=\"avatar avatar-user\" src=\"https://avatars.githubusercontent.com/u/80318998?s=64&amp;v=4\" width=\"32\" height=\"32\" alt=\"@erlendlokna\" />          </li>          <li class=\"hx_flex-avatar-stack-item\">            <img class=\"avatar\" src=\"https://avatars.githubusercontent.com/u/43243757?s=64&amp;v=4\" width=\"32\" height=\"32\" alt=\"@PKU-RL\" />          </li>          <li class=\"hx_flex-avatar-stack-item\">            <img class=\"avatar avatar-user\" src=\"https://avatars.githubusercontent.com/u/159537625?s=64&amp;v=4\" width=\"32\" height=\"32\" alt=\"@mausset\" />          </li>          <li class=\"hx_flex-avatar-stack-item\">            <img class=\"avatar avatar-user\" src=\"https://avatars.githubusercontent.com/u/65861156?s=64&amp;v=4\" width=\"32\" height=\"32\" alt=\"@blueputty01\" />          </li>          <li class=\"hx_flex-avatar-stack-item\">            <img class=\"avatar avatar-user\" src=\"https://avatars.githubusercontent.com/u/55674301?s=64&amp;v=4\" width=\"32\" height=\"32\" alt=\"@SergioLeonardoMendes\" />          </li>          <li class=\"hx_flex-avatar-stack-item\">            <img class=\"avatar avatar-user\" src=\"https://avatars.githubusercontent.com/u/71316838?s=64&amp;v=4\" width=\"32\" height=\"32\" alt=\"@samanthajiang\" />          </li>          <li class=\"hx_flex-avatar-stack-item\">            <img class=\"avatar avatar-user\" src=\"https://avatars.githubusercontent.com/u/4533248?s=64&amp;v=4\" width=\"32\" height=\"32\" alt=\"@APN-Pucky\" />          </li>          <li class=\"hx_flex-avatar-stack-item\">            <img class=\"avatar\" src=\"https://avatars.githubusercontent.com/u/4370142?s=64&amp;v=4\" width=\"32\" height=\"32\" alt=\"@DasLab\" />          </li>    </ul>      <span class=\"px-2 text-bold text-small no-wrap\">        + 216      </span>  </a>              </div>            </div>                    <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\">  <a href=\"/lucidrains/x-transformers/graphs/contributors\" data-view-component=\"true\" class=\"Link--primary no-underline Link d-flex flex-items-center\">    Contributors      <span title=\"21\" data-view-component=\"true\" class=\"Counter ml-1\">21</span></a></h2>      <ul class=\"list-style-none d-flex flex-wrap mb-n2\">    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/lucidrains\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/lucidrains/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/108653?s=64&amp;v=4\" alt=\"@lucidrains\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/gurvindersingh\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/gurvindersingh/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/186077?s=64&amp;v=4\" alt=\"@gurvindersingh\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/jbcdnr\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/jbcdnr/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/6209990?s=64&amp;v=4\" alt=\"@jbcdnr\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/taemincho\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/taemincho/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/22582839?s=64&amp;v=4\" alt=\"@taemincho\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/tmphex\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/tmphex/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/84812863?s=64&amp;v=4\" alt=\"@tmphex\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/jstjohn\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/jstjohn/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/459014?s=64&amp;v=4\" alt=\"@jstjohn\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/cifkao\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/cifkao/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/8046580?s=64&amp;v=4\" alt=\"@cifkao\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/ilya16\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/ilya16/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/22874472?s=64&amp;v=4\" alt=\"@ilya16\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/pfeatherstone\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/pfeatherstone/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/45853521?s=64&amp;v=4\" alt=\"@pfeatherstone\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/kcarnold\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/kcarnold/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/21072?s=64&amp;v=4\" alt=\"@kcarnold\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/apage43\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/apage43/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/169252?s=64&amp;v=4\" alt=\"@apage43\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/stas-sl\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/stas-sl/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/4602302?s=64&amp;v=4\" alt=\"@stas-sl\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/RameshArvind\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/RameshArvind/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/5190935?s=64&amp;v=4\" alt=\"@RameshArvind\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/ncoop57\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/ncoop57/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/7613470?s=64&amp;v=4\" alt=\"@ncoop57\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li></ul>  <div data-view-component=\"true\" class=\"mt-3\">    <a text=\"small\" href=\"/lucidrains/x-transformers/graphs/contributors\" data-view-component=\"true\" class=\"Link\">      + 7 contributors</a></div>              </div>            </div>                            <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\">Languages</h2><div class=\"mb-2\">  <span data-view-component=\"true\" class=\"Progress\">    <span style=\"background-color:#3572A5 !important;;width: 100.0%;\" itemprop=\"keywords\" aria-label=\"Python 100.0\" data-view-component=\"true\" class=\"Progress-item color-bg-success-emphasis\"></span></span></div><ul class=\"list-style-none\">    <li class=\"d-inline\">        <a class=\"d-inline-flex flex-items-center flex-nowrap Link--secondary no-underline text-small mr-3\" href=\"/lucidrains/x-transformers/search?l=python\"  data-ga-click=\"Repository, language stats search click, location:repo overview\">          <svg style=\"color:#3572A5;\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-dot-fill mr-2\">    <path d=\"M8 4a4 4 0 1 1 0 8 4 4 0 0 1 0-8Z\"></path></svg>          <span class=\"color-fg-default text-bold mr-1\">Python</span>          <span>100.0%</span>        </a>    </li></ul>              </div>            </div>              </div></div>  </div></div>  </div>  </div></turbo-frame>    </main>  </div>  </div>          <footer class=\"footer pt-8 pb-6 f6 color-fg-muted p-responsive\" role=\"contentinfo\" >  <h2 class='sr-only'>Footer</h2>    <div class=\"d-flex flex-justify-center flex-items-center flex-column-reverse flex-lg-row flex-wrap flex-lg-nowrap\">    <div class=\"d-flex flex-items-center flex-shrink-0 mx-2\">      <a aria-label=\"Homepage\" title=\"GitHub\" class=\"footer-octicon mr-2\" href=\"https://github.com\">        <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-mark-github\">    <path d=\"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z\"></path></svg></a>      <span>        &copy; 2024 GitHub,&nbsp;Inc.      </span>    </div>    <nav aria-label=\"Footer\">      <h3 class=\"sr-only\" id=\"sr-footer-heading\">Footer navigation</h3>      <ul class=\"list-style-none d-flex flex-justify-center flex-wrap mb-2 mb-lg-0\" aria-labelledby=\"sr-footer-heading\">          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to Terms&quot;,&quot;label&quot;:&quot;text:terms&quot;}\" href=\"https://docs.github.com/site-policy/github-terms/github-terms-of-service\" data-view-component=\"true\" class=\"Link--secondary Link\">Terms</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to privacy&quot;,&quot;label&quot;:&quot;text:privacy&quot;}\" href=\"https://docs.github.com/site-policy/privacy-policies/github-privacy-statement\" data-view-component=\"true\" class=\"Link--secondary Link\">Privacy</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to security&quot;,&quot;label&quot;:&quot;text:security&quot;}\" href=\"/security\" data-view-component=\"true\" class=\"Link--secondary Link\">Security</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to status&quot;,&quot;label&quot;:&quot;text:status&quot;}\" href=\"https://www.githubstatus.com/\" data-view-component=\"true\" class=\"Link--secondary Link\">Status</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to docs&quot;,&quot;label&quot;:&quot;text:docs&quot;}\" href=\"https://docs.github.com\" data-view-component=\"true\" class=\"Link--secondary Link\">Docs</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to contact&quot;,&quot;label&quot;:&quot;text:contact&quot;}\" href=\"https://support.github.com?tags=dotcom-footer\" data-view-component=\"true\" class=\"Link--secondary Link\">Contact</a>          </li>          <li class=\"mx-2\" >  <cookie-consent-link>    <button type=\"button\" class=\"Link--secondary underline-on-hover border-0 p-0 color-bg-transparent\" data-action=\"click:cookie-consent-link#showConsentManagement\">      Manage cookies    </button>  </cookie-consent-link></li><li class=\"mx-2\">  <cookie-consent-link>    <button type=\"button\" class=\"Link--secondary underline-on-hover border-0 p-0 color-bg-transparent\" data-action=\"click:cookie-consent-link#showConsentManagement\">      Do not share my personal information    </button>  </cookie-consent-link></li>      </ul>    </nav>  </div></footer>    <cookie-consent id=\"cookie-consent-banner\" class=\"position-fixed bottom-0 left-0\" style=\"z-index: 999999\" data-initial-cookie-consent-allowed=\"\" data-cookie-consent-required=\"false\"></cookie-consent>  <div id=\"ajax-error-message\" class=\"ajax-error-message flash flash-error\" hidden>    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-alert\">    <path d=\"M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>    <button type=\"button\" class=\"flash-close js-ajax-error-dismiss\" aria-label=\"Dismiss error\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg>    </button>    You can\u2019t perform that action at this time.  </div>    <template id=\"site-details-dialog\">  <details class=\"details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm\" open>    <summary role=\"button\" aria-label=\"Close dialog\"></summary>    <details-dialog class=\"Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal\">      <button class=\"Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0\" type=\"button\" aria-label=\"Close dialog\" data-close-dialog>        <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg>      </button>      <div class=\"octocat-spinner my-6 js-details-dialog-spinner\"></div>    </details-dialog>  </details></template>    <div class=\"Popover js-hovercard-content position-absolute\" style=\"display: none; outline: none;\" tabindex=\"0\">  <div class=\"Popover-message Popover-message--bottom-left Popover-message--large Box color-shadow-large\" style=\"width:360px;\">  </div></div>    <template id=\"snippet-clipboard-copy-button\">  <div class=\"zeroclipboard-container position-absolute right-0 top-0\">    <clipboard-copy aria-label=\"Copy\" class=\"ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay\" data-copy-feedback=\"Copied!\" data-tooltip-direction=\"w\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copy js-clipboard-copy-icon m-2\">    <path d=\"M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z\"></path><path d=\"M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path></svg>      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2\">    <path d=\"M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z\"></path></svg>    </clipboard-copy>  </div></template><template id=\"snippet-clipboard-copy-button-unpositioned\">  <div class=\"zeroclipboard-container\">    <clipboard-copy aria-label=\"Copy\" class=\"ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center\" data-copy-feedback=\"Copied!\" data-tooltip-direction=\"w\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copy js-clipboard-copy-icon\">    <path d=\"M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z\"></path><path d=\"M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path></svg>      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-check js-clipboard-check-icon color-fg-success d-none\">    <path d=\"M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z\"></path></svg>    </clipboard-copy>  </div></template>    </div>    <div id=\"js-global-screen-reader-notice\" class=\"sr-only\" aria-live=\"polite\" aria-atomic=\"true\" ></div>    <div id=\"js-global-screen-reader-notice-assertive\" class=\"sr-only\" aria-live=\"assertive\" aria-atomic=\"true\"></div>  </body></html>",
  "embeddings": []
}