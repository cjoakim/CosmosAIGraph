{
  "libtype": "pypi",
  "libname": "open-clip-torch",
  "url": "https://github.com/mlfoundations/open_clip",
  "html": "<!DOCTYPE html><html  lang=\"en\"    data-color-mode=\"auto\" data-light-theme=\"light\" data-dark-theme=\"dark\"  data-a11y-animated-images=\"system\" data-a11y-link-underlines=\"true\"  >  <head>    <meta charset=\"utf-8\">  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">  <link rel=\"dns-prefetch\" href=\"https://avatars.githubusercontent.com\">  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">  <link rel=\"dns-prefetch\" href=\"https://user-images.githubusercontent.com/\">  <link rel=\"preconnect\" href=\"https://github.githubassets.com\" crossorigin>  <link rel=\"preconnect\" href=\"https://avatars.githubusercontent.com\">    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/light-0eace2597ca3.css\" /><link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/dark-a167e256da9c.css\" /><link data-color-theme=\"dark_dimmed\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/dark_dimmed-d11f2cf8009b.css\" /><link data-color-theme=\"dark_high_contrast\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/dark_high_contrast-ea7373db06c8.css\" /><link data-color-theme=\"dark_colorblind\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/dark_colorblind-afa99dcf40f7.css\" /><link data-color-theme=\"light_colorblind\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/light_colorblind-af6c685139ba.css\" /><link data-color-theme=\"light_high_contrast\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/light_high_contrast-578cdbc8a5a9.css\" /><link data-color-theme=\"light_tritanopia\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/light_tritanopia-5cb699a7e247.css\" /><link data-color-theme=\"dark_tritanopia\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/dark_tritanopia-9b32204967c6.css\" />    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/primer-primitives-2ef2a46b27ee.css\" />    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/primer-711f412bb361.css\" />    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/global-4803cd254267.css\" />    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/github-f4d857cbc96a.css\" />  <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/repository-6247ca238fd4.css\" /><link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/code-6d7b4ef0ea51.css\" />    <script type=\"application/json\" id=\"client-env\">{\"locale\":\"en\",\"featureFlags\":[\"code_vulnerability_scanning\",\"copilot_conversational_ux_history_refs\",\"copilot_chat_attach_knowledge\",\"copilot_chat_knowledge_base_copy\",\"copilot_smell_icebreaker_ux\",\"copilot_implicit_context\",\"docset_management_ui\",\"copilot_chat_settings\",\"failbot_handle_non_errors\",\"geojson_azure_maps\",\"image_metric_tracking\",\"marketing_forms_api_integration_contact_request\",\"marketing_pages_search_explore_provider\",\"turbo_experiment_risky\",\"sample_network_conn_type\",\"no_character_key_shortcuts_in_inputs\",\"custom_inp\",\"remove_child_patch\"]}</script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/wp-runtime-47578fb192fd.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_dompurify_dist_purify_js-6890e890956f.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_stacktrace-parser_dist_stack-trace-parser_esm_js-node_modules_github_bro-a4c183-79f9611c275b.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_hydro-analytics-client_dist_analytics-client_js-node_modules_gith-6a10dd-e66ebda625fb.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_failbot_failbot_ts-479802999bcc.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/environment-fe7570f3bc38.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_selector-observer_dist_index_esm_js-9f960d9b217c.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_behaviors_dist_esm_focus-zone_js-086f7a27bac0.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_relative-time-element_dist_index_js-c76945c5961a.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_delegated-events_dist_index_js-node_modules_github_details-dialog-elemen-29dc30-a2a71f11a507.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_auto-complete-element_dist_index_js-12366198e7a5.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_text-expander-element_dist_index_js-8a621df59e80.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_filter-input-element_dist_index_js-node_modules_github_remote-inp-b7d8f4-654130b7cde5.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_file-attachment-element_dist_index_js-node_modules_primer_view-co-5dccdf-e5e2b9fa3c0c.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/github-elements-e4eda4896b4e.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/element-registry-b99c9d8fad1d.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_catalyst_lib_index_js-node_modules_github_hydro-analytics-client_-978abc0-add939c751ce.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_lit-html_lit-html_js-5b376145beff.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_mini-throttle_dist_index_js-node_modules_github_alive-client_dist-bf5aa2-1b562c29ab8e.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_morphdom_dist_morphdom-esm_js-5bff297a06de.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_turbo_dist_turbo_es2017-esm_js-c91f4ad18b62.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_color-convert_index_js-72c9fbde5ad4.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_scroll-anchoring_dist_scro-231ccf-aa129238d13b.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_behaviors_dist_esm_dimensions_js-node_modules_github_jtml_lib_index_js-95b84ee6bc34.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_session-resume_dist_index_js-node_modules_primer_behaviors_dist_e-da6ec6-3f39339c9d98.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_paste-markdown_dist_index_esm_js-node_modules_github_quote-select-67e0dc-1aa35af077a4.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_updatable-content_ts-ee3fc84d7fb0.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_behaviors_task-list_ts-app_assets_modules_github_onfocus_ts-app_ass-421cec-9de4213015af.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_sticky-scroll-into-view_ts-94209c43e6af.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_behaviors_ajax-error_ts-app_assets_modules_github_behaviors_include-467754-f9bd433e9591.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_behaviors_commenting_edit_ts-app_assets_modules_github_behaviors_ht-83c235-9285faa0e011.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_blob-anchor_ts-app_assets_modules_github_filter-sort_ts-app_assets_-c96432-da3733f430b8.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/behaviors-1fb9e5061509.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_delegated-events_dist_index_js-node_modules_github_catalyst_lib_index_js-d0256ebff5cd.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/notifications-global-352d84c6cc82.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_virtualized-list_es_index_js-node_modules_github_template-parts_lib_index_js-878844713bc9.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_delegated-events_dist_inde-c537341-c7f6a41a084c.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_ref-selector_ts-b593b93f23f5.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/codespaces-1a8626dd714a.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_filter-input-element_dist_index_js-node_modules_github_mini-throt-08ab15-3e0517baca99.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_file-attachment-element_dist_index_js-node_modules_github_mini-th-55cf52-e14cb4b719b4.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/repositories-69068e0899f9.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/code-menu-614feb194539.js\"></script>    <title>GitHub - mlfoundations/open_clip: An open source implementation of CLIP.</title>  <meta name=\"route-pattern\" content=\"/:user_id/:repository\" data-turbo-transient>  <meta name=\"route-controller\" content=\"files\" data-turbo-transient>  <meta name=\"route-action\" content=\"disambiguate\" data-turbo-transient>      <meta name=\"current-catalog-service-hash\" content=\"82c569b93da5c18ed649ebd4c2c79437db4611a6a1373e805a3cb001c64130b7\">  <meta name=\"request-id\" content=\"D019:4525:C971F3:12964EB:65E796B4\" data-pjax-transient=\"true\"/><meta name=\"html-safe-nonce\" content=\"19cb42d491dc1b554d7063f3f255b481dc00a9a6a14f12399d9389807cbc3447\" data-pjax-transient=\"true\"/><meta name=\"visitor-payload\" content=\"eyJyZWZlcnJlciI6IiIsInJlcXVlc3RfaWQiOiJEMDE5OjQ1MjU6Qzk3MUYzOjEyOTY0RUI6NjVFNzk2QjQiLCJ2aXNpdG9yX2lkIjoiNzU2NjI2NjYzMzc3MjU3MDI5MiIsInJlZ2lvbl9lZGdlIjoiaWFkIiwicmVnaW9uX3JlbmRlciI6ImlhZCJ9\" data-pjax-transient=\"true\"/><meta name=\"visitor-hmac\" content=\"11ebe107a9ec5320065578be7ead0010a56560438af41315a11f5e20fdca841b\" data-pjax-transient=\"true\"/>    <meta name=\"hovercard-subject-tag\" content=\"repository:390536799\" data-turbo-transient>  <meta name=\"github-keyboard-shortcuts\" content=\"repository,copilot\" data-turbo-transient=\"true\" />    <meta name=\"selected-link\" value=\"repo_source\" data-turbo-transient>  <link rel=\"assets\" href=\"https://github.githubassets.com/\">    <meta name=\"google-site-verification\" content=\"c1kuD-K2HIVF635lypcsWPoD4kilo5-jA_wBFyT4uMY\">  <meta name=\"google-site-verification\" content=\"KT5gs8h0wvaagLKAVWq8bbeNwnZZK1r1XQysX3xurLU\">  <meta name=\"google-site-verification\" content=\"ZzhVyEFwb7w3e0-uOTltm8Jsck2F5StVihD0exw2fsA\">  <meta name=\"google-site-verification\" content=\"GXs5KoUUkNCoaAZn7wPN-t01Pywp9M3sEjnt_3_ZWPc\">  <meta name=\"google-site-verification\" content=\"Apib7-x98H0j5cPqHWwSMm6dNU4GmODRoqxLiDzdx9I\"><meta name=\"octolytics-url\" content=\"https://collector.github.com/github/collect\" />  <meta name=\"analytics-location\" content=\"/&lt;user-name&gt;/&lt;repo-name&gt;\" data-turbo-transient=\"true\" />        <meta name=\"user-login\" content=\"\">      <meta name=\"viewport\" content=\"width=device-width\">          <meta name=\"description\" content=\"An open source implementation of CLIP. Contribute to mlfoundations/open_clip development by creating an account on GitHub.\">      <link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"/opensearch.xml\" title=\"GitHub\">    <link rel=\"fluid-icon\" href=\"https://github.com/fluidicon.png\" title=\"GitHub\">    <meta property=\"fb:app_id\" content=\"1401488693436528\">    <meta name=\"apple-itunes-app\" content=\"app-id=1477376905, app-argument=https://github.com/mlfoundations/open_clip\" />      <meta name=\"twitter:image:src\" content=\"https://opengraph.githubassets.com/022e54f230902c5f6fd3871815de0ad66f7518c009676fa154f86ec0b1d8a9ab/mlfoundations/open_clip\" /><meta name=\"twitter:site\" content=\"@github\" /><meta name=\"twitter:card\" content=\"summary_large_image\" /><meta name=\"twitter:title\" content=\"GitHub - mlfoundations/open_clip: An open source implementation of CLIP.\" /><meta name=\"twitter:description\" content=\"An open source implementation of CLIP. Contribute to mlfoundations/open_clip development by creating an account on GitHub.\" />      <meta property=\"og:image\" content=\"https://opengraph.githubassets.com/022e54f230902c5f6fd3871815de0ad66f7518c009676fa154f86ec0b1d8a9ab/mlfoundations/open_clip\" /><meta property=\"og:image:alt\" content=\"An open source implementation of CLIP. Contribute to mlfoundations/open_clip development by creating an account on GitHub.\" /><meta property=\"og:image:width\" content=\"1200\" /><meta property=\"og:image:height\" content=\"600\" /><meta property=\"og:site_name\" content=\"GitHub\" /><meta property=\"og:type\" content=\"object\" /><meta property=\"og:title\" content=\"GitHub - mlfoundations/open_clip: An open source implementation of CLIP.\" /><meta property=\"og:url\" content=\"https://github.com/mlfoundations/open_clip\" /><meta property=\"og:description\" content=\"An open source implementation of CLIP. Contribute to mlfoundations/open_clip development by creating an account on GitHub.\" />              <meta name=\"hostname\" content=\"github.com\">        <meta name=\"expected-hostname\" content=\"github.com\">  <meta http-equiv=\"x-pjax-version\" content=\"b9fa4cafade57d606c6dcfafff1d08bd597980af7b9837ed473fdf0cdea8a3bc\" data-turbo-track=\"reload\">  <meta http-equiv=\"x-pjax-csp-version\" content=\"5dcfbec3488c5fd5a334e287ce6a17058b7d4beb91db2d4d184e4d55bbf1d7d7\" data-turbo-track=\"reload\">  <meta http-equiv=\"x-pjax-css-version\" content=\"d33c7c2fcff40783f3002896023f41e2c17ec62b12ddbe7434e2001d743fb853\" data-turbo-track=\"reload\">  <meta http-equiv=\"x-pjax-js-version\" content=\"4ba4a7cc07194c8d5f24291dea4fbc790ffd83ba40beacaf8d0117187b571b4d\" data-turbo-track=\"reload\">  <meta name=\"turbo-cache-control\" content=\"no-preview\" data-turbo-transient=\"\">      <meta data-hydrostats=\"publish\">  <meta name=\"go-import\" content=\"github.com/mlfoundations/open_clip git https://github.com/mlfoundations/open_clip.git\">  <meta name=\"octolytics-dimension-user_id\" content=\"87461581\" /><meta name=\"octolytics-dimension-user_login\" content=\"mlfoundations\" /><meta name=\"octolytics-dimension-repository_id\" content=\"390536799\" /><meta name=\"octolytics-dimension-repository_nwo\" content=\"mlfoundations/open_clip\" /><meta name=\"octolytics-dimension-repository_public\" content=\"true\" /><meta name=\"octolytics-dimension-repository_is_fork\" content=\"false\" /><meta name=\"octolytics-dimension-repository_network_root_id\" content=\"390536799\" /><meta name=\"octolytics-dimension-repository_network_root_nwo\" content=\"mlfoundations/open_clip\" />    <link rel=\"canonical\" href=\"https://github.com/mlfoundations/open_clip\" data-turbo-transient>  <meta name=\"turbo-body-classes\" content=\"logged-out env-production page-responsive\">  <meta name=\"browser-stats-url\" content=\"https://api.github.com/_private/browser/stats\">  <meta name=\"browser-errors-url\" content=\"https://api.github.com/_private/browser/errors\">  <link rel=\"mask-icon\" href=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" color=\"#000000\">  <link rel=\"alternate icon\" class=\"js-site-favicon\" type=\"image/png\" href=\"https://github.githubassets.com/favicons/favicon.png\">  <link rel=\"icon\" class=\"js-site-favicon\" type=\"image/svg+xml\" href=\"https://github.githubassets.com/favicons/favicon.svg\"><meta name=\"theme-color\" content=\"#1e2327\"><meta name=\"color-scheme\" content=\"light dark\" />  <link rel=\"manifest\" href=\"/manifest.json\" crossOrigin=\"use-credentials\">  </head>  <body class=\"logged-out env-production page-responsive\" style=\"word-wrap: break-word;\">    <div data-turbo-body class=\"logged-out env-production page-responsive\" style=\"word-wrap: break-word;\">          <div class=\"position-relative js-header-wrapper \">      <a href=\"#start-of-content\" class=\"px-2 py-4 color-bg-accent-emphasis color-fg-on-emphasis show-on-focus js-skip-to-content\">Skip to content</a>      <span data-view-component=\"true\" class=\"progress-pjax-loader Progress position-fixed width-full\">    <span style=\"width: 0%;\" data-view-component=\"true\" class=\"Progress-item progress-pjax-loader-bar left-0 top-0 color-bg-accent-emphasis\"></span></span>              <script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Button_IconButton_js-node_modules_primer_react_lib--23bcad-a89698f38643.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/keyboard-shortcuts-dialog-a23eda2bcf8d.js\"></script><react-partial  partial-name=\"keyboard-shortcuts-dialog\"  data-ssr=\"false\">    <script type=\"application/json\" data-target=\"react-partial.embeddedData\">{\"props\":{}}</script>  <div data-target=\"react-partial.reactRoot\"></div></react-partial>                          <script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_delegated-events_dist_inde-94fd67-99519581d0f8.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/sessions-585a7232e50a.js\"></script><header class=\"Header-old header-logged-out js-details-container Details position-relative f4 py-3\" role=\"banner\" data-color-mode=light data-light-theme=light data-dark-theme=dark>  <button type=\"button\" class=\"Header-backdrop d-lg-none border-0 position-fixed top-0 left-0 width-full height-full js-details-target\" aria-label=\"Toggle navigation\">    <span class=\"d-none\">Toggle navigation</span>  </button>  <div class=\" d-flex flex-column flex-lg-row flex-items-center p-responsive height-full position-relative z-1\">    <div class=\"d-flex flex-justify-between flex-items-center width-full width-lg-auto\">      <a class=\"mr-lg-3 color-fg-inherit flex-order-2\" href=\"https://github.com/\" aria-label=\"Homepage\" data-ga-click=\"(Logged out) Header, go to homepage, icon:logo-wordmark\">        <svg height=\"32\" aria-hidden=\"true\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"32\" data-view-component=\"true\" class=\"octicon octicon-mark-github\">    <path d=\"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z\"></path></svg>      </a>      <div class=\"flex-1\">        <a href=\"/login?return_to=https%3A%2F%2Fgithub.com%2Fmlfoundations%2Fopen_clip\"          class=\"d-inline-block d-lg-none flex-order-1 f5 no-underline border color-border-default rounded-2 px-2 py-1 color-fg-inherit\"          data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/mlfoundations/open_clip&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"dc360152aa35af6b9b9e9fd1b1a5fd2598094817ff28da4cf8468da138b15cc7\"          data-ga-click=\"(Logged out) Header, clicked Sign in, text:sign-in\">          Sign in        </a>      </div>      <div class=\"flex-1 flex-order-2 text-right\">        <button aria-label=\"Toggle navigation\" aria-expanded=\"false\" type=\"button\" data-view-component=\"true\" class=\"js-details-target Button--link Button--medium Button d-lg-none color-fg-inherit p-1\">  <span class=\"Button-content\">    <span class=\"Button-label\"><div class=\"HeaderMenu-toggle-bar rounded my-1\"></div>            <div class=\"HeaderMenu-toggle-bar rounded my-1\"></div>            <div class=\"HeaderMenu-toggle-bar rounded my-1\"></div></span>  </span></button>      </div>    </div>    <div class=\"HeaderMenu--logged-out p-responsive height-fit position-lg-relative d-lg-flex flex-column flex-auto pt-7 pb-4 top-0\">      <div class=\"header-menu-wrapper d-flex flex-column flex-self-end flex-lg-row flex-justify-between flex-auto p-3 p-lg-0 rounded rounded-lg-0 mt-3 mt-lg-0\">          <nav class=\"mt-0 px-3 px-lg-0 mb-3 mb-lg-0\" aria-label=\"Global\">            <ul class=\"d-lg-flex list-style-none\">                <li class=\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\">      <button type=\"button\" class=\"HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target\" aria-expanded=\"false\">        Product        <svg opacity=\"0.5\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-chevron-down HeaderMenu-icon ml-1\">    <path d=\"M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z\"></path></svg>      </button>      <div class=\"HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 d-lg-flex dropdown-menu-wide\">          <div class=\"px-lg-4 border-lg-right mb-4 mb-lg-0 pr-lg-7\">            <ul class=\"list-style-none f5\" >                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}\" href=\"/features/actions\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-workflow color-fg-subtle mr-3\">    <path d=\"M1 3a2 2 0 0 1 2-2h6.5a2 2 0 0 1 2 2v6.5a2 2 0 0 1-2 2H7v4.063C7 16.355 7.644 17 8.438 17H12.5v-2.5a2 2 0 0 1 2-2H21a2 2 0 0 1 2 2V21a2 2 0 0 1-2 2h-6.5a2 2 0 0 1-2-2v-2.5H8.437A2.939 2.939 0 0 1 5.5 15.562V11.5H3a2 2 0 0 1-2-2Zm2-.5a.5.5 0 0 0-.5.5v6.5a.5.5 0 0 0 .5.5h6.5a.5.5 0 0 0 .5-.5V3a.5.5 0 0 0-.5-.5ZM14.5 14a.5.5 0 0 0-.5.5V21a.5.5 0 0 0 .5.5H21a.5.5 0 0 0 .5-.5v-6.5a.5.5 0 0 0-.5-.5Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Actions</div>        Automate any workflow      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}\" href=\"/features/packages\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-package color-fg-subtle mr-3\">    <path d=\"M12.876.64V.639l8.25 4.763c.541.313.875.89.875 1.515v9.525a1.75 1.75 0 0 1-.875 1.516l-8.25 4.762a1.748 1.748 0 0 1-1.75 0l-8.25-4.763a1.75 1.75 0 0 1-.875-1.515V6.917c0-.625.334-1.202.875-1.515L11.126.64a1.748 1.748 0 0 1 1.75 0Zm-1 1.298L4.251 6.34l7.75 4.474 7.75-4.474-7.625-4.402a.248.248 0 0 0-.25 0Zm.875 19.123 7.625-4.402a.25.25 0 0 0 .125-.216V7.639l-7.75 4.474ZM3.501 7.64v8.803c0 .09.048.172.125.216l7.625 4.402v-8.947Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Packages</div>        Host and manage packages      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}\" href=\"/features/security\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-shield-check color-fg-subtle mr-3\">    <path d=\"M16.53 9.78a.75.75 0 0 0-1.06-1.06L11 13.19l-1.97-1.97a.75.75 0 0 0-1.06 1.06l2.5 2.5a.75.75 0 0 0 1.06 0l5-5Z\"></path><path d=\"m12.54.637 8.25 2.675A1.75 1.75 0 0 1 22 4.976V10c0 6.19-3.771 10.704-9.401 12.83a1.704 1.704 0 0 1-1.198 0C5.77 20.705 2 16.19 2 10V4.976c0-.758.489-1.43 1.21-1.664L11.46.637a1.748 1.748 0 0 1 1.08 0Zm-.617 1.426-8.25 2.676a.249.249 0 0 0-.173.237V10c0 5.46 3.28 9.483 8.43 11.426a.199.199 0 0 0 .14 0C17.22 19.483 20.5 15.461 20.5 10V4.976a.25.25 0 0 0-.173-.237l-8.25-2.676a.253.253 0 0 0-.154 0Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Security</div>        Find and fix vulnerabilities      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}\" href=\"/features/codespaces\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-codespaces color-fg-subtle mr-3\">    <path d=\"M3.5 3.75C3.5 2.784 4.284 2 5.25 2h13.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 18.75 13H5.25a1.75 1.75 0 0 1-1.75-1.75Zm-2 12c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v4a1.75 1.75 0 0 1-1.75 1.75H3.25a1.75 1.75 0 0 1-1.75-1.75ZM5.25 3.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h13.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Zm-2 12a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h17.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25Z\"></path><path d=\"M10 17.75a.75.75 0 0 1 .75-.75h6.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Codespaces</div>        Instant dev environments      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}\" href=\"/features/copilot\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-copilot color-fg-subtle mr-3\">    <path d=\"M23.922 16.992c-.861 1.495-5.859 5.023-11.922 5.023-6.063 0-11.061-3.528-11.922-5.023A.641.641 0 0 1 0 16.736v-2.869a.841.841 0 0 1 .053-.22c.372-.935 1.347-2.292 2.605-2.656.167-.429.414-1.055.644-1.517a10.195 10.195 0 0 1-.052-1.086c0-1.331.282-2.499 1.132-3.368.397-.406.89-.717 1.474-.952 1.399-1.136 3.392-2.093 6.122-2.093 2.731 0 4.767.957 6.166 2.093.584.235 1.077.546 1.474.952.85.869 1.132 2.037 1.132 3.368 0 .368-.014.733-.052 1.086.23.462.477 1.088.644 1.517 1.258.364 2.233 1.721 2.605 2.656a.832.832 0 0 1 .053.22v2.869a.641.641 0 0 1-.078.256ZM12.172 11h-.344a4.323 4.323 0 0 1-.355.508C10.703 12.455 9.555 13 7.965 13c-1.725 0-2.989-.359-3.782-1.259a2.005 2.005 0 0 1-.085-.104L4 11.741v6.585c1.435.779 4.514 2.179 8 2.179 3.486 0 6.565-1.4 8-2.179v-6.585l-.098-.104s-.033.045-.085.104c-.793.9-2.057 1.259-3.782 1.259-1.59 0-2.738-.545-3.508-1.492a4.323 4.323 0 0 1-.355-.508h-.016.016Zm.641-2.935c.136 1.057.403 1.913.878 2.497.442.544 1.134.938 2.344.938 1.573 0 2.292-.337 2.657-.751.384-.435.558-1.15.558-2.361 0-1.14-.243-1.847-.705-2.319-.477-.488-1.319-.862-2.824-1.025-1.487-.161-2.192.138-2.533.529-.269.307-.437.808-.438 1.578v.021c0 .265.021.562.063.893Zm-1.626 0c.042-.331.063-.628.063-.894v-.02c-.001-.77-.169-1.271-.438-1.578-.341-.391-1.046-.69-2.533-.529-1.505.163-2.347.537-2.824 1.025-.462.472-.705 1.179-.705 2.319 0 1.211.175 1.926.558 2.361.365.414 1.084.751 2.657.751 1.21 0 1.902-.394 2.344-.938.475-.584.742-1.44.878-2.497Z\"></path><path d=\"M14.5 14.25a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Zm-5 0a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Copilot</div>        Write better code with AI      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}\" href=\"/features/code-review\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-code-review color-fg-subtle mr-3\">    <path d=\"M10.3 6.74a.75.75 0 0 1-.04 1.06l-2.908 2.7 2.908 2.7a.75.75 0 1 1-1.02 1.1l-3.5-3.25a.75.75 0 0 1 0-1.1l3.5-3.25a.75.75 0 0 1 1.06.04Zm3.44 1.06a.75.75 0 1 1 1.02-1.1l3.5 3.25a.75.75 0 0 1 0 1.1l-3.5 3.25a.75.75 0 1 1-1.02-1.1l2.908-2.7-2.908-2.7Z\"></path><path d=\"M1.5 4.25c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v12.5a1.75 1.75 0 0 1-1.75 1.75h-9.69l-3.573 3.573A1.458 1.458 0 0 1 5 21.043V18.5H3.25a1.75 1.75 0 0 1-1.75-1.75ZM3.25 4a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h2.5a.75.75 0 0 1 .75.75v3.19l3.72-3.72a.749.749 0 0 1 .53-.22h10a.25.25 0 0 0 .25-.25V4.25a.25.25 0 0 0-.25-.25Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Code review</div>        Manage code changes      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}\" href=\"/features/issues\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-issue-opened color-fg-subtle mr-3\">    <path d=\"M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1ZM2.5 12a9.5 9.5 0 0 0 9.5 9.5 9.5 9.5 0 0 0 9.5-9.5A9.5 9.5 0 0 0 12 2.5 9.5 9.5 0 0 0 2.5 12Zm9.5 2a2 2 0 1 1-.001-3.999A2 2 0 0 1 12 14Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Issues</div>        Plan and track work      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}\" href=\"/features/discussions\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-comment-discussion color-fg-subtle mr-3\">    <path d=\"M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 14.25 14H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 15.543V14H1.75A1.75 1.75 0 0 1 0 12.25v-9.5C0 1.784.784 1 1.75 1ZM1.5 2.75v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Z\"></path><path d=\"M22.5 8.75a.25.25 0 0 0-.25-.25h-3.5a.75.75 0 0 1 0-1.5h3.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 22.25 20H21v1.543a1.457 1.457 0 0 1-2.487 1.03L15.939 20H10.75A1.75 1.75 0 0 1 9 18.25v-1.465a.75.75 0 0 1 1.5 0v1.465c0 .138.112.25.25.25h5.5a.75.75 0 0 1 .53.22l2.72 2.72v-2.19a.75.75 0 0 1 .75-.75h2a.25.25 0 0 0 .25-.25v-9.5Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Discussions</div>        Collaborate outside of code      </div>    </a></li>            </ul>          </div>          <div class=\"px-lg-4\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"product-explore-heading\">Explore</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"product-explore-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to All features&quot;,&quot;label&quot;:&quot;ref_cta:All features;&quot;}\" href=\"/features\">      All features    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Documentation&quot;,&quot;label&quot;:&quot;ref_cta:Documentation;&quot;}\" href=\"https://docs.github.com\">      Documentation    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to GitHub Skills&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Skills;&quot;}\" href=\"https://skills.github.com/\">      GitHub Skills    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Blog&quot;,&quot;label&quot;:&quot;ref_cta:Blog;&quot;}\" href=\"https://github.blog\">      Blog    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>            </ul>          </div>      </div></li>                <li class=\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\">      <button type=\"button\" class=\"HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target\" aria-expanded=\"false\">        Solutions        <svg opacity=\"0.5\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-chevron-down HeaderMenu-icon ml-1\">    <path d=\"M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z\"></path></svg>      </button>      <div class=\"HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 px-lg-4\">          <div class=\"border-bottom pb-3 mb-3\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"solutions-for-heading\">For</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"solutions-for-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Enterprise&quot;,&quot;label&quot;:&quot;ref_cta:Enterprise;&quot;}\" href=\"/enterprise\">      Enterprise    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Teams&quot;,&quot;label&quot;:&quot;ref_cta:Teams;&quot;}\" href=\"/team\">      Teams    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Startups&quot;,&quot;label&quot;:&quot;ref_cta:Startups;&quot;}\" href=\"/enterprise/startups\">      Startups    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Education&quot;,&quot;label&quot;:&quot;ref_cta:Education;&quot;}\" href=\"https://education.github.com\">      Education    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>            </ul>          </div>          <div class=\"border-bottom pb-3 mb-3\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"solutions-by-solution-heading\">By Solution</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"solutions-by-solution-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to CI/CD &amp;amp; Automation&quot;,&quot;label&quot;:&quot;ref_cta:CI/CD &amp;amp; Automation;&quot;}\" href=\"/solutions/ci-cd/\">      CI/CD &amp; Automation    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to DevOps&quot;,&quot;label&quot;:&quot;ref_cta:DevOps;&quot;}\" href=\"/solutions/devops/\">      DevOps    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to DevSecOps&quot;,&quot;label&quot;:&quot;ref_cta:DevSecOps;&quot;}\" href=\"https://resources.github.com/devops/fundamentals/devsecops/\">      DevSecOps    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>            </ul>          </div>          <div class=\"\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"solutions-resources-heading\">Resources</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"solutions-resources-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Learning Pathways&quot;,&quot;label&quot;:&quot;ref_cta:Learning Pathways;&quot;}\" href=\"https://resources.github.com/learn/pathways/\">      Learning Pathways    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to White papers, Ebooks, Webinars&quot;,&quot;label&quot;:&quot;ref_cta:White papers, Ebooks, Webinars;&quot;}\" href=\"https://resources.github.com/\">      White papers, Ebooks, Webinars    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Customer Stories&quot;,&quot;label&quot;:&quot;ref_cta:Customer Stories;&quot;}\" href=\"/customer-stories\">      Customer Stories    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Partners&quot;,&quot;label&quot;:&quot;ref_cta:Partners;&quot;}\" href=\"https://partner.github.com/\">      Partners    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>            </ul>          </div>      </div></li>                <li class=\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\">      <button type=\"button\" class=\"HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target\" aria-expanded=\"false\">        Open Source        <svg opacity=\"0.5\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-chevron-down HeaderMenu-icon ml-1\">    <path d=\"M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z\"></path></svg>      </button>      <div class=\"HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 px-lg-4\">          <div class=\"border-bottom pb-3 mb-3\">            <ul class=\"list-style-none f5\" >                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}\" href=\"/sponsors\">            <div>        <div class=\"color-fg-default h4\">GitHub Sponsors</div>        Fund open source developers      </div>    </a></li>            </ul>          </div>          <div class=\"border-bottom pb-3 mb-3\">            <ul class=\"list-style-none f5\" >                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}\" href=\"/readme\">            <div>        <div class=\"color-fg-default h4\">The ReadME Project</div>        GitHub community articles      </div>    </a></li>            </ul>          </div>          <div class=\"\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"open-source-repositories-heading\">Repositories</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"open-source-repositories-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Topics&quot;,&quot;label&quot;:&quot;ref_cta:Topics;&quot;}\" href=\"/topics\">      Topics    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Trending&quot;,&quot;label&quot;:&quot;ref_cta:Trending;&quot;}\" href=\"/trending\">      Trending    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Collections&quot;,&quot;label&quot;:&quot;ref_cta:Collections;&quot;}\" href=\"/collections\">      Collections    </a></li>            </ul>          </div>      </div></li>                <li class=\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\">    <a class=\"HeaderMenu-link no-underline px-0 px-lg-2 py-3 py-lg-2 d-block d-lg-inline-block\" data-analytics-event=\"{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}\" href=\"/pricing\">Pricing</a></li>            </ul>          </nav>        <div class=\"d-lg-flex flex-items-center mb-3 mb-lg-0 text-center text-lg-left ml-3\" style=\"\">                <qbsearch-input class=\"search-input\" data-scope=\"repo:mlfoundations/open_clip\" data-custom-scopes-path=\"/search/custom_scopes\" data-delete-custom-scopes-csrf=\"lO8RA4CyTCHL7b7jiaVZcKrEUkYSYTS8j_oE-9iDiCOc8C699-NTc0lBfKo9iyRtYiB_yyeASoCiF1MKFRWTfA\" data-max-custom-scopes=\"10\" data-header-redesign-enabled=\"false\" data-initial-value=\"\" data-blackbird-suggestions-path=\"/search/suggestions\" data-jump-to-suggestions-path=\"/_graphql/GetSuggestedNavigationDestinations\" data-current-repository=\"mlfoundations/open_clip\" data-current-org=\"mlfoundations\" data-current-owner=\"\" data-logged-in=\"false\" data-copilot-chat-enabled=\"false\" data-blackbird-indexed-repo-csrf=\"<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=E%2FlslubYtoa4oBG1n74mNU2L4NBXEa3Nj%2BMjmN5qfujj5c2d54XP%2F4zq5E9P037tMj%2FEHKRblu9qqLkessLVfGfLswCwZUMtG5q9EXR%2FQzVfbDTK7Nr97ScqXNwlXicGQ0gDWBPg%2FPbGXyFzf7XumRkZXrGJlzWfKcH9%2FArhTwGGT7nKnyWeB0axSjTMxL1r8a9SxBCBjq3EF3SnM4QsFfk%2Fy5gqD8qbagU2TPCpmEcsxro%2BEr0DoK%2B5Q2u%2F7SJvmecVbOf4TKbTSoDecwR5%2FPuNCpdhDqVc9%2Bt58k78JVuhoUqTs%2Ft1CGh4d%2BC2cNpBCKgTUICWpRzgI9Pl3Q6FI12kqjckfdrRF66C0K2R5m%2FwS5%2BDSavqBPjjaLmusHeFSvnklsDJuSTIKBrTogBmAZ8S%2Fy31gghaxv7W7KP4Zv%2FErsG4o8Q0ivGV4sRQI0Lfy9O5YNOA5oiUdPTIOqKM8Ux14FzseNi%2F3FBZQpSZQMKNHgEo%2FxbxA%2FcapFsYl2t8SO3BZomTMeoV8cojaIEqzF4xGkrAYw%3D%3D--9xiTjY%2FAp0b4ij9I--aMnozxtpBBF0Fzdn%2BBij8Q%3D%3D&quot; />\">  <div    class=\"search-input-container search-with-dialog position-relative d-flex flex-row flex-items-center mr-4 rounded\"    data-action=\"click:qbsearch-input#searchInputContainerClicked\"  >      <button        type=\"button\"        class=\"header-search-button placeholder  input-button form-control d-flex flex-1 flex-self-stretch flex-items-center no-wrap width-full py-0 pl-2 pr-0 text-left border-0 box-shadow-none\"        data-target=\"qbsearch-input.inputButton\"        placeholder=\"Search or jump to...\"        data-hotkey=s,/        autocapitalize=\"off\"        data-action=\"click:qbsearch-input#handleExpand\"      >        <div class=\"mr-2 color-fg-muted\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-search\">    <path d=\"M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z\"></path></svg>        </div>        <span class=\"flex-1\" data-target=\"qbsearch-input.inputButtonText\">Search or jump to...</span>          <div class=\"d-flex\" data-target=\"qbsearch-input.hotkeyIndicator\">            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"22\" height=\"20\" aria-hidden=\"true\" class=\"mr-1\"><path fill=\"none\" stroke=\"#979A9C\" opacity=\".4\" d=\"M3.5.5h12c1.7 0 3 1.3 3 3v13c0 1.7-1.3 3-3 3h-12c-1.7 0-3-1.3-3-3v-13c0-1.7 1.3-3 3-3z\"></path><path fill=\"#979A9C\" d=\"M11.8 6L8 15.1h-.9L10.8 6h1z\"></path></svg>          </div>      </button>    <input type=\"hidden\" name=\"type\" class=\"js-site-search-type-field\">    <div class=\"Overlay--hidden \" data-modal-dialog-overlay>  <modal-dialog data-action=\"close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose\" data-target=\"qbsearch-input.searchSuggestionsDialog\" role=\"dialog\" id=\"search-suggestions-dialog\" aria-modal=\"true\" aria-labelledby=\"search-suggestions-dialog-header\" data-view-component=\"true\" class=\"Overlay Overlay--width-large Overlay--height-auto\">      <h1 id=\"search-suggestions-dialog-header\" class=\"sr-only\">Search code, repositories, users, issues, pull requests...</h1>    <div class=\"Overlay-body Overlay-body--paddingNone\">                <div data-view-component=\"true\">        <div class=\"search-suggestions position-fixed width-full color-shadow-large border color-fg-default color-bg-default overflow-hidden d-flex flex-column query-builder-container\"          style=\"border-radius: 12px;\"          data-target=\"qbsearch-input.queryBuilderContainer\"          hidden        >          <!-- '\"` --><!-- </textarea></xmp> --></option></form><form id=\"query-builder-test-form\" action=\"\" accept-charset=\"UTF-8\" method=\"get\">  <query-builder data-target=\"qbsearch-input.queryBuilder\" id=\"query-builder-query-builder-test\" data-filter-key=\":\" data-view-component=\"true\" class=\"QueryBuilder search-query-builder\">    <div class=\"FormControl FormControl--fullWidth\">      <label id=\"query-builder-test-label\" for=\"query-builder-test\" class=\"FormControl-label sr-only\">        Search      </label>      <div        class=\"QueryBuilder-StyledInput width-fit \"        data-target=\"query-builder.styledInput\"      >          <span id=\"query-builder-test-leadingvisual-wrap\" class=\"FormControl-input-leadingVisualWrap QueryBuilder-leadingVisualWrap\">            <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-search FormControl-input-leadingVisual\">    <path d=\"M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z\"></path></svg>          </span>        <div data-target=\"query-builder.styledInputContainer\" class=\"QueryBuilder-StyledInputContainer\">          <div            aria-hidden=\"true\"            class=\"QueryBuilder-StyledInputContent\"            data-target=\"query-builder.styledInputContent\"          ></div>          <div class=\"QueryBuilder-InputWrapper\">            <div aria-hidden=\"true\" class=\"QueryBuilder-Sizer\" data-target=\"query-builder.sizer\"></div>            <input id=\"query-builder-test\" name=\"query-builder-test\" value=\"\" autocomplete=\"off\" type=\"text\" role=\"combobox\" spellcheck=\"false\" aria-expanded=\"false\" aria-describedby=\"validation-8c79fbd3-5a0b-4c4d-b937-e0cb5b9792c1\" data-target=\"query-builder.input\" data-action=\"          input:query-builder#inputChange          blur:query-builder#inputBlur          keydown:query-builder#inputKeydown          focus:query-builder#inputFocus        \" data-view-component=\"true\" class=\"FormControl-input QueryBuilder-Input FormControl-medium\" />          </div>        </div>          <span class=\"sr-only\" id=\"query-builder-test-clear\">Clear</span>          <button role=\"button\" id=\"query-builder-test-clear-button\" aria-labelledby=\"query-builder-test-clear query-builder-test-label\" data-target=\"query-builder.clearButton\" data-action=\"                click:query-builder#clear                focus:query-builder#clearButtonFocus                blur:query-builder#clearButtonBlur              \" variant=\"small\" hidden=\"hidden\" type=\"button\" data-view-component=\"true\" class=\"Button Button--iconOnly Button--invisible Button--medium mr-1 px-2 py-0 d-flex flex-items-center rounded-1 color-fg-muted\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x-circle-fill Button-visual\">    <path d=\"M2.343 13.657A8 8 0 1 1 13.658 2.343 8 8 0 0 1 2.343 13.657ZM6.03 4.97a.751.751 0 0 0-1.042.018.751.751 0 0 0-.018 1.042L6.94 8 4.97 9.97a.749.749 0 0 0 .326 1.275.749.749 0 0 0 .734-.215L8 9.06l1.97 1.97a.749.749 0 0 0 1.275-.326.749.749 0 0 0-.215-.734L9.06 8l1.97-1.97a.749.749 0 0 0-.326-1.275.749.749 0 0 0-.734.215L8 6.94Z\"></path></svg></button>      </div>      <template id=\"search-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-search\">    <path d=\"M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z\"></path></svg></template><template id=\"code-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code\">    <path d=\"m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z\"></path></svg></template><template id=\"file-code-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-file-code\">    <path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0 1 14.25 15h-9a.75.75 0 0 1 0-1.5h9a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 10 4.25V1.5H5.75a.25.25 0 0 0-.25.25v2.5a.75.75 0 0 1-1.5 0Zm1.72 4.97a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734l1.47-1.47-1.47-1.47a.75.75 0 0 1 0-1.06ZM3.28 7.78 1.81 9.25l1.47 1.47a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018l-2-2a.75.75 0 0 1 0-1.06l2-2a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Zm8.22-6.218V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg></template><template id=\"history-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-history\">    <path d=\"m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z\"></path></svg></template><template id=\"repo-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo\">    <path d=\"M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z\"></path></svg></template><template id=\"bookmark-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-bookmark\">    <path d=\"M3 2.75C3 1.784 3.784 1 4.75 1h6.5c.966 0 1.75.784 1.75 1.75v11.5a.75.75 0 0 1-1.227.579L8 11.722l-3.773 3.107A.751.751 0 0 1 3 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.91l3.023-2.489a.75.75 0 0 1 .954 0l3.023 2.49V2.75a.25.25 0 0 0-.25-.25Z\"></path></svg></template><template id=\"plus-circle-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-plus-circle\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm7.25-3.25v2.5h2.5a.75.75 0 0 1 0 1.5h-2.5v2.5a.75.75 0 0 1-1.5 0v-2.5h-2.5a.75.75 0 0 1 0-1.5h2.5v-2.5a.75.75 0 0 1 1.5 0Z\"></path></svg></template><template id=\"circle-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-dot-fill\">    <path d=\"M8 4a4 4 0 1 1 0 8 4 4 0 0 1 0-8Z\"></path></svg></template><template id=\"trash-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-trash\">    <path d=\"M11 1.75V3h2.25a.75.75 0 0 1 0 1.5H2.75a.75.75 0 0 1 0-1.5H5V1.75C5 .784 5.784 0 6.75 0h2.5C10.216 0 11 .784 11 1.75ZM4.496 6.675l.66 6.6a.25.25 0 0 0 .249.225h5.19a.25.25 0 0 0 .249-.225l.66-6.6a.75.75 0 0 1 1.492.149l-.66 6.6A1.748 1.748 0 0 1 10.595 15h-5.19a1.75 1.75 0 0 1-1.741-1.575l-.66-6.6a.75.75 0 1 1 1.492-.15ZM6.5 1.75V3h3V1.75a.25.25 0 0 0-.25-.25h-2.5a.25.25 0 0 0-.25.25Z\"></path></svg></template><template id=\"team-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-people\">    <path d=\"M2 5.5a3.5 3.5 0 1 1 5.898 2.549 5.508 5.508 0 0 1 3.034 4.084.75.75 0 1 1-1.482.235 4 4 0 0 0-7.9 0 .75.75 0 0 1-1.482-.236A5.507 5.507 0 0 1 3.102 8.05 3.493 3.493 0 0 1 2 5.5ZM11 4a3.001 3.001 0 0 1 2.22 5.018 5.01 5.01 0 0 1 2.56 3.012.749.749 0 0 1-.885.954.752.752 0 0 1-.549-.514 3.507 3.507 0 0 0-2.522-2.372.75.75 0 0 1-.574-.73v-.352a.75.75 0 0 1 .416-.672A1.5 1.5 0 0 0 11 5.5.75.75 0 0 1 11 4Zm-5.5-.5a2 2 0 1 0-.001 3.999A2 2 0 0 0 5.5 3.5Z\"></path></svg></template><template id=\"project-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-project\">    <path d=\"M1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0ZM1.5 1.75v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25ZM11.75 3a.75.75 0 0 1 .75.75v7.5a.75.75 0 0 1-1.5 0v-7.5a.75.75 0 0 1 .75-.75Zm-8.25.75a.75.75 0 0 1 1.5 0v5.5a.75.75 0 0 1-1.5 0ZM8 3a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 3Z\"></path></svg></template><template id=\"pencil-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-pencil\">    <path d=\"M11.013 1.427a1.75 1.75 0 0 1 2.474 0l1.086 1.086a1.75 1.75 0 0 1 0 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 0 1-.927-.928l.929-3.25c.081-.286.235-.547.445-.758l8.61-8.61Zm.176 4.823L9.75 4.81l-6.286 6.287a.253.253 0 0 0-.064.108l-.558 1.953 1.953-.558a.253.253 0 0 0 .108-.064Zm1.238-3.763a.25.25 0 0 0-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 0 0 0-.354Z\"></path></svg></template><template id=\"copilot-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copilot\">    <path d=\"M7.998 15.035c-4.562 0-7.873-2.914-7.998-3.749V9.338c.085-.628.677-1.686 1.588-2.065.013-.07.024-.143.036-.218.029-.183.06-.384.126-.612-.201-.508-.254-1.084-.254-1.656 0-.87.128-1.769.693-2.484.579-.733 1.494-1.124 2.724-1.261 1.206-.134 2.262.034 2.944.765.05.053.096.108.139.165.044-.057.094-.112.143-.165.682-.731 1.738-.899 2.944-.765 1.23.137 2.145.528 2.724 1.261.566.715.693 1.614.693 2.484 0 .572-.053 1.148-.254 1.656.066.228.098.429.126.612.012.076.024.148.037.218.924.385 1.522 1.471 1.591 2.095v1.872c0 .766-3.351 3.795-8.002 3.795Zm0-1.485c2.28 0 4.584-1.11 5.002-1.433V7.862l-.023-.116c-.49.21-1.075.291-1.727.291-1.146 0-2.059-.327-2.71-.991A3.222 3.222 0 0 1 8 6.303a3.24 3.24 0 0 1-.544.743c-.65.664-1.563.991-2.71.991-.652 0-1.236-.081-1.727-.291l-.023.116v4.255c.419.323 2.722 1.433 5.002 1.433ZM6.762 2.83c-.193-.206-.637-.413-1.682-.297-1.019.113-1.479.404-1.713.7-.247.312-.369.789-.369 1.554 0 .793.129 1.171.308 1.371.162.181.519.379 1.442.379.853 0 1.339-.235 1.638-.54.315-.322.527-.827.617-1.553.117-.935-.037-1.395-.241-1.614Zm4.155-.297c-1.044-.116-1.488.091-1.681.297-.204.219-.359.679-.242 1.614.091.726.303 1.231.618 1.553.299.305.784.54 1.638.54.922 0 1.28-.198 1.442-.379.179-.2.308-.578.308-1.371 0-.765-.123-1.242-.37-1.554-.233-.296-.693-.587-1.713-.7Z\"></path><path d=\"M6.25 9.037a.75.75 0 0 1 .75.75v1.501a.75.75 0 0 1-1.5 0V9.787a.75.75 0 0 1 .75-.75Zm4.25.75v1.501a.75.75 0 0 1-1.5 0V9.787a.75.75 0 0 1 1.5 0Z\"></path></svg></template><template id=\"workflow-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-workflow\">    <path d=\"M0 1.75C0 .784.784 0 1.75 0h3.5C6.216 0 7 .784 7 1.75v3.5A1.75 1.75 0 0 1 5.25 7H4v4a1 1 0 0 0 1 1h4v-1.25C9 9.784 9.784 9 10.75 9h3.5c.966 0 1.75.784 1.75 1.75v3.5A1.75 1.75 0 0 1 14.25 16h-3.5A1.75 1.75 0 0 1 9 14.25v-.75H5A2.5 2.5 0 0 1 2.5 11V7h-.75A1.75 1.75 0 0 1 0 5.25Zm1.75-.25a.25.25 0 0 0-.25.25v3.5c0 .138.112.25.25.25h3.5a.25.25 0 0 0 .25-.25v-3.5a.25.25 0 0 0-.25-.25Zm9 9a.25.25 0 0 0-.25.25v3.5c0 .138.112.25.25.25h3.5a.25.25 0 0 0 .25-.25v-3.5a.25.25 0 0 0-.25-.25Z\"></path></svg></template><template id=\"book-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-book\">    <path d=\"M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z\"></path></svg></template><template id=\"code-review-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code-review\">    <path d=\"M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 13H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25v-8.5C0 1.784.784 1 1.75 1ZM1.5 2.75v8.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-8.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Zm5.28 1.72a.75.75 0 0 1 0 1.06L5.31 7l1.47 1.47a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018l-2-2a.75.75 0 0 1 0-1.06l2-2a.75.75 0 0 1 1.06 0Zm2.44 0a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L10.69 7 9.22 5.53a.75.75 0 0 1 0-1.06Z\"></path></svg></template><template id=\"codespaces-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-codespaces\">    <path d=\"M0 11.25c0-.966.784-1.75 1.75-1.75h12.5c.966 0 1.75.784 1.75 1.75v3A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25Zm2-9.5C2 .784 2.784 0 3.75 0h8.5C13.216 0 14 .784 14 1.75v5a1.75 1.75 0 0 1-1.75 1.75h-8.5A1.75 1.75 0 0 1 2 6.75Zm1.75-.25a.25.25 0 0 0-.25.25v5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-5a.25.25 0 0 0-.25-.25Zm-2 9.5a.25.25 0 0 0-.25.25v3c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-3a.25.25 0 0 0-.25-.25Z\"></path><path d=\"M7 12.75a.75.75 0 0 1 .75-.75h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z\"></path></svg></template><template id=\"comment-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-comment\">    <path d=\"M1 2.75C1 1.784 1.784 1 2.75 1h10.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 13.25 12H9.06l-2.573 2.573A1.458 1.458 0 0 1 4 13.543V12H2.75A1.75 1.75 0 0 1 1 10.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h4.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path></svg></template><template id=\"comment-discussion-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-comment-discussion\">    <path d=\"M1.75 1h8.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 10.25 10H7.061l-2.574 2.573A1.458 1.458 0 0 1 2 11.543V10h-.25A1.75 1.75 0 0 1 0 8.25v-5.5C0 1.784.784 1 1.75 1ZM1.5 2.75v5.5c0 .138.112.25.25.25h1a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h3.5a.25.25 0 0 0 .25-.25v-5.5a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25Zm13 2a.25.25 0 0 0-.25-.25h-.5a.75.75 0 0 1 0-1.5h.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 14.25 12H14v1.543a1.458 1.458 0 0 1-2.487 1.03L9.22 12.28a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l2.22 2.22v-2.19a.75.75 0 0 1 .75-.75h1a.25.25 0 0 0 .25-.25Z\"></path></svg></template><template id=\"organization-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-organization\">    <path d=\"M1.75 16A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0h8.5C11.216 0 12 .784 12 1.75v12.5c0 .085-.006.168-.018.25h2.268a.25.25 0 0 0 .25-.25V8.285a.25.25 0 0 0-.111-.208l-1.055-.703a.749.749 0 1 1 .832-1.248l1.055.703c.487.325.779.871.779 1.456v5.965A1.75 1.75 0 0 1 14.25 16h-3.5a.766.766 0 0 1-.197-.026c-.099.017-.2.026-.303.026h-3a.75.75 0 0 1-.75-.75V14h-1v1.25a.75.75 0 0 1-.75.75Zm-.25-1.75c0 .138.112.25.25.25H4v-1.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 .75.75v1.25h2.25a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25ZM3.75 6h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5ZM3 3.75A.75.75 0 0 1 3.75 3h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 3.75Zm4 3A.75.75 0 0 1 7.75 6h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 7 6.75ZM7.75 3h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5ZM3 9.75A.75.75 0 0 1 3.75 9h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 9.75ZM7.75 9h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5Z\"></path></svg></template><template id=\"rocket-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-rocket\">    <path d=\"M14.064 0h.186C15.216 0 16 .784 16 1.75v.186a8.752 8.752 0 0 1-2.564 6.186l-.458.459c-.314.314-.641.616-.979.904v3.207c0 .608-.315 1.172-.833 1.49l-2.774 1.707a.749.749 0 0 1-1.11-.418l-.954-3.102a1.214 1.214 0 0 1-.145-.125L3.754 9.816a1.218 1.218 0 0 1-.124-.145L.528 8.717a.749.749 0 0 1-.418-1.11l1.71-2.774A1.748 1.748 0 0 1 3.31 4h3.204c.288-.338.59-.665.904-.979l.459-.458A8.749 8.749 0 0 1 14.064 0ZM8.938 3.623h-.002l-.458.458c-.76.76-1.437 1.598-2.02 2.5l-1.5 2.317 2.143 2.143 2.317-1.5c.902-.583 1.74-1.26 2.499-2.02l.459-.458a7.25 7.25 0 0 0 2.123-5.127V1.75a.25.25 0 0 0-.25-.25h-.186a7.249 7.249 0 0 0-5.125 2.123ZM3.56 14.56c-.732.732-2.334 1.045-3.005 1.148a.234.234 0 0 1-.201-.064.234.234 0 0 1-.064-.201c.103-.671.416-2.273 1.15-3.003a1.502 1.502 0 1 1 2.12 2.12Zm6.94-3.935c-.088.06-.177.118-.266.175l-2.35 1.521.548 1.783 1.949-1.2a.25.25 0 0 0 .119-.213ZM3.678 8.116 5.2 5.766c.058-.09.117-.178.176-.266H3.309a.25.25 0 0 0-.213.119l-1.2 1.95ZM12 5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg></template><template id=\"shield-check-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-shield-check\">    <path d=\"m8.533.133 5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667l5.25-1.68a1.748 1.748 0 0 1 1.066 0Zm-.61 1.429.001.001-5.25 1.68a.251.251 0 0 0-.174.237V7c0 1.36.275 2.666 1.057 3.859.784 1.194 2.121 2.342 4.366 3.298a.196.196 0 0 0 .154 0c2.245-.957 3.582-2.103 4.366-3.297C13.225 9.666 13.5 8.358 13.5 7V3.48a.25.25 0 0 0-.174-.238l-5.25-1.68a.25.25 0 0 0-.153 0ZM11.28 6.28l-3.5 3.5a.75.75 0 0 1-1.06 0l-1.5-1.5a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l.97.97 2.97-2.97a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z\"></path></svg></template><template id=\"heart-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-heart\">    <path d=\"m8 14.25.345.666a.75.75 0 0 1-.69 0l-.008-.004-.018-.01a7.152 7.152 0 0 1-.31-.17 22.055 22.055 0 0 1-3.434-2.414C2.045 10.731 0 8.35 0 5.5 0 2.836 2.086 1 4.25 1 5.797 1 7.153 1.802 8 3.02 8.847 1.802 10.203 1 11.75 1 13.914 1 16 2.836 16 5.5c0 2.85-2.045 5.231-3.885 6.818a22.066 22.066 0 0 1-3.744 2.584l-.018.01-.006.003h-.002ZM4.25 2.5c-1.336 0-2.75 1.164-2.75 3 0 2.15 1.58 4.144 3.365 5.682A20.58 20.58 0 0 0 8 13.393a20.58 20.58 0 0 0 3.135-2.211C12.92 9.644 14.5 7.65 14.5 5.5c0-1.836-1.414-3-2.75-3-1.373 0-2.609.986-3.029 2.456a.749.749 0 0 1-1.442 0C6.859 3.486 5.623 2.5 4.25 2.5Z\"></path></svg></template><template id=\"server-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-server\">    <path d=\"M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v4c0 .372-.116.717-.314 1 .198.283.314.628.314 1v4a1.75 1.75 0 0 1-1.75 1.75H1.75A1.75 1.75 0 0 1 0 12.75v-4c0-.358.109-.707.314-1a1.739 1.739 0 0 1-.314-1v-4C0 1.784.784 1 1.75 1ZM1.5 2.75v4c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Zm.25 5.75a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25ZM7 4.75A.75.75 0 0 1 7.75 4h4.5a.75.75 0 0 1 0 1.5h-4.5A.75.75 0 0 1 7 4.75ZM7.75 10h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM3 4.75A.75.75 0 0 1 3.75 4h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 4.75ZM3.75 10h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5Z\"></path></svg></template><template id=\"globe-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-globe\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM5.78 8.75a9.64 9.64 0 0 0 1.363 4.177c.255.426.542.832.857 1.215.245-.296.551-.705.857-1.215A9.64 9.64 0 0 0 10.22 8.75Zm4.44-1.5a9.64 9.64 0 0 0-1.363-4.177c-.307-.51-.612-.919-.857-1.215a9.927 9.927 0 0 0-.857 1.215A9.64 9.64 0 0 0 5.78 7.25Zm-5.944 1.5H1.543a6.507 6.507 0 0 0 4.666 5.5c-.123-.181-.24-.365-.352-.552-.715-1.192-1.437-2.874-1.581-4.948Zm-2.733-1.5h2.733c.144-2.074.866-3.756 1.58-4.948.12-.197.237-.381.353-.552a6.507 6.507 0 0 0-4.666 5.5Zm10.181 1.5c-.144 2.074-.866 3.756-1.58 4.948-.12.197-.237.381-.353.552a6.507 6.507 0 0 0 4.666-5.5Zm2.733-1.5a6.507 6.507 0 0 0-4.666-5.5c.123.181.24.365.353.552.714 1.192 1.436 2.874 1.58 4.948Z\"></path></svg></template><template id=\"issue-opened-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-issue-opened\">    <path d=\"M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path><path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z\"></path></svg></template><template id=\"device-mobile-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-device-mobile\">    <path d=\"M3.75 0h8.5C13.216 0 14 .784 14 1.75v12.5A1.75 1.75 0 0 1 12.25 16h-8.5A1.75 1.75 0 0 1 2 14.25V1.75C2 .784 2.784 0 3.75 0ZM3.5 1.75v12.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25ZM8 13a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z\"></path></svg></template><template id=\"package-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-package\">    <path d=\"m8.878.392 5.25 3.045c.54.314.872.89.872 1.514v6.098a1.75 1.75 0 0 1-.872 1.514l-5.25 3.045a1.75 1.75 0 0 1-1.756 0l-5.25-3.045A1.75 1.75 0 0 1 1 11.049V4.951c0-.624.332-1.201.872-1.514L7.122.392a1.75 1.75 0 0 1 1.756 0ZM7.875 1.69l-4.63 2.685L8 7.133l4.755-2.758-4.63-2.685a.248.248 0 0 0-.25 0ZM2.5 5.677v5.372c0 .09.047.171.125.216l4.625 2.683V8.432Zm6.25 8.271 4.625-2.683a.25.25 0 0 0 .125-.216V5.677L8.75 8.432Z\"></path></svg></template><template id=\"credit-card-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-credit-card\">    <path d=\"M10.75 9a.75.75 0 0 0 0 1.5h1.5a.75.75 0 0 0 0-1.5h-1.5Z\"></path><path d=\"M0 3.75C0 2.784.784 2 1.75 2h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 14H1.75A1.75 1.75 0 0 1 0 12.25ZM14.5 6.5h-13v5.75c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25Zm0-2.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25V5h13Z\"></path></svg></template><template id=\"play-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-play\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z\"></path></svg></template><template id=\"gift-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-gift\">    <path d=\"M2 2.75A2.75 2.75 0 0 1 4.75 0c.983 0 1.873.42 2.57 1.232.268.318.497.668.68 1.042.183-.375.411-.725.68-1.044C9.376.42 10.266 0 11.25 0a2.75 2.75 0 0 1 2.45 4h.55c.966 0 1.75.784 1.75 1.75v2c0 .698-.409 1.301-1 1.582v4.918A1.75 1.75 0 0 1 13.25 16H2.75A1.75 1.75 0 0 1 1 14.25V9.332C.409 9.05 0 8.448 0 7.75v-2C0 4.784.784 4 1.75 4h.55c-.192-.375-.3-.8-.3-1.25ZM7.25 9.5H2.5v4.75c0 .138.112.25.25.25h4.5Zm1.5 0v5h4.5a.25.25 0 0 0 .25-.25V9.5Zm0-4V8h5.5a.25.25 0 0 0 .25-.25v-2a.25.25 0 0 0-.25-.25Zm-7 0a.25.25 0 0 0-.25.25v2c0 .138.112.25.25.25h5.5V5.5h-5.5Zm3-4a1.25 1.25 0 0 0 0 2.5h2.309c-.233-.818-.542-1.401-.878-1.793-.43-.502-.915-.707-1.431-.707ZM8.941 4h2.309a1.25 1.25 0 0 0 0-2.5c-.516 0-1 .205-1.43.707-.337.392-.646.975-.879 1.793Z\"></path></svg></template><template id=\"code-square-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code-square\">    <path d=\"M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25Zm7.47 3.97a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L10.69 8 9.22 6.53a.75.75 0 0 1 0-1.06ZM6.78 6.53 5.31 8l1.47 1.47a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215l-2-2a.75.75 0 0 1 0-1.06l2-2a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z\"></path></svg></template><template id=\"device-desktop-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-device-desktop\">    <path d=\"M14.25 1c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 14.25 12h-3.727c.099 1.041.52 1.872 1.292 2.757A.752.752 0 0 1 11.25 16h-6.5a.75.75 0 0 1-.565-1.243c.772-.885 1.192-1.716 1.292-2.757H1.75A1.75 1.75 0 0 1 0 10.25v-7.5C0 1.784.784 1 1.75 1ZM1.75 2.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25ZM9.018 12H6.982a5.72 5.72 0 0 1-.765 2.5h3.566a5.72 5.72 0 0 1-.765-2.5Z\"></path></svg></template>        <div class=\"position-relative\">                <ul                  role=\"listbox\"                  class=\"ActionListWrap QueryBuilder-ListWrap\"                  aria-label=\"Suggestions\"                  data-action=\"                    combobox-commit:query-builder#comboboxCommit                    mousedown:query-builder#resultsMousedown                  \"                  data-target=\"query-builder.resultsList\"                  data-persist-list=false                  id=\"query-builder-test-results\"                ></ul>        </div>      <div class=\"FormControl-inlineValidation\" id=\"validation-8c79fbd3-5a0b-4c4d-b937-e0cb5b9792c1\" hidden=\"hidden\">        <span class=\"FormControl-inlineValidation--visual\">          <svg aria-hidden=\"true\" height=\"12\" viewBox=\"0 0 12 12\" version=\"1.1\" width=\"12\" data-view-component=\"true\" class=\"octicon octicon-alert-fill\">    <path d=\"M4.855.708c.5-.896 1.79-.896 2.29 0l4.675 8.351a1.312 1.312 0 0 1-1.146 1.954H1.33A1.313 1.313 0 0 1 .183 9.058ZM7 7V3H5v4Zm-1 3a1 1 0 1 0 0-2 1 1 0 0 0 0 2Z\"></path></svg>        </span>        <span></span></div>    </div>    <div data-target=\"query-builder.screenReaderFeedback\" aria-live=\"polite\" aria-atomic=\"true\" class=\"sr-only\"></div></query-builder></form>          <div class=\"d-flex flex-row color-fg-muted px-3 text-small color-bg-default search-feedback-prompt\">            <a target=\"_blank\" href=\"https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax\" data-view-component=\"true\" class=\"Link color-fg-accent text-normal ml-2\">              Search syntax tips</a>            <div class=\"d-flex flex-1\"></div>          </div>        </div></div>    </div></modal-dialog></div>  </div>  <div data-action=\"click:qbsearch-input#retract\" class=\"dark-backdrop position-fixed\" hidden data-target=\"qbsearch-input.darkBackdrop\"></div>  <div class=\"color-fg-default\">    <dialog-helper>  <dialog data-target=\"qbsearch-input.feedbackDialog\" data-action=\"close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose\" id=\"feedback-dialog\" aria-modal=\"true\" aria-labelledby=\"feedback-dialog-title\" aria-describedby=\"feedback-dialog-description\" data-view-component=\"true\" class=\"Overlay Overlay-whenNarrow Overlay--size-medium Overlay--motion-scaleFade\">    <div data-view-component=\"true\" class=\"Overlay-header\">  <div class=\"Overlay-headerContentWrap\">    <div class=\"Overlay-titleWrap\">      <h1 class=\"Overlay-title \" id=\"feedback-dialog-title\">        Provide feedback      </h1>    </div>    <div class=\"Overlay-actionWrap\">      <button data-close-dialog-id=\"feedback-dialog\" aria-label=\"Close\" type=\"button\" data-view-component=\"true\" class=\"close-button Overlay-closeButton\"><svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg></button>    </div>  </div></div>      <scrollable-region data-labelled-by=\"feedback-dialog-title\">        <div data-view-component=\"true\" class=\"Overlay-body\">        <!-- '\"` --><!-- </textarea></xmp> --></option></form><form id=\"code-search-feedback-form\" data-turbo=\"false\" action=\"/search/feedback\" accept-charset=\"UTF-8\" method=\"post\"><input type=\"hidden\" data-csrf=\"true\" name=\"authenticity_token\" value=\"ydHrcEAnNvgWrPaNn/zr/8UqmTFGNiR3POMZYB3sp/Qm4UMWCmkdNbfNZk0EuoWgoUvuzka8mKSsqIpuOYnycg==\" />          <p>We read every piece of feedback, and take your input very seriously.</p>          <textarea name=\"feedback\" class=\"form-control width-full mb-2\" style=\"height: 120px\" id=\"feedback\"></textarea>          <input name=\"include_email\" id=\"include_email\" aria-label=\"Include my email address so I can be contacted\" class=\"form-control mr-2\" type=\"checkbox\">          <label for=\"include_email\" style=\"font-weight: normal\">Include my email address so I can be contacted</label></form></div>      </scrollable-region>      <div data-view-component=\"true\" class=\"Overlay-footer Overlay-footer--alignEnd\">          <button data-close-dialog-id=\"feedback-dialog\" type=\"button\" data-view-component=\"true\" class=\"btn\">    Cancel</button>          <button form=\"code-search-feedback-form\" data-action=\"click:qbsearch-input#submitFeedback\" type=\"submit\" data-view-component=\"true\" class=\"btn-primary btn\">    Submit feedback</button></div></dialog></dialog-helper>    <custom-scopes data-target=\"qbsearch-input.customScopesManager\">    <dialog-helper>  <dialog data-target=\"custom-scopes.customScopesModalDialog\" data-action=\"close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose\" id=\"custom-scopes-dialog\" aria-modal=\"true\" aria-labelledby=\"custom-scopes-dialog-title\" aria-describedby=\"custom-scopes-dialog-description\" data-view-component=\"true\" class=\"Overlay Overlay-whenNarrow Overlay--size-medium Overlay--motion-scaleFade\">    <div data-view-component=\"true\" class=\"Overlay-header Overlay-header--divided\">  <div class=\"Overlay-headerContentWrap\">    <div class=\"Overlay-titleWrap\">      <h1 class=\"Overlay-title \" id=\"custom-scopes-dialog-title\">        Saved searches      </h1>        <h2 id=\"custom-scopes-dialog-description\" class=\"Overlay-description\">Use saved searches to filter your results more quickly</h2>    </div>    <div class=\"Overlay-actionWrap\">      <button data-close-dialog-id=\"custom-scopes-dialog\" aria-label=\"Close\" type=\"button\" data-view-component=\"true\" class=\"close-button Overlay-closeButton\"><svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg></button>    </div>  </div></div>      <scrollable-region data-labelled-by=\"custom-scopes-dialog-title\">        <div data-view-component=\"true\" class=\"Overlay-body\">        <div data-target=\"custom-scopes.customScopesModalDialogFlash\"></div>        <div hidden class=\"create-custom-scope-form\" data-target=\"custom-scopes.createCustomScopeForm\">        <!-- '\"` --><!-- </textarea></xmp> --></option></form><form id=\"custom-scopes-dialog-form\" data-turbo=\"false\" action=\"/search/custom_scopes\" accept-charset=\"UTF-8\" method=\"post\"><input type=\"hidden\" data-csrf=\"true\" name=\"authenticity_token\" value=\"TsjyMtzq9cDVSHIugULLnijNB0LNaxDEkP/0b7bpq/sCZ/fj6WURuuc2dvSaOFjcEnrON8yMaCBgrjnGoYlu/Q==\" />          <div data-target=\"custom-scopes.customScopesModalDialogFlash\"></div>          <input type=\"hidden\" id=\"custom_scope_id\" name=\"custom_scope_id\" data-target=\"custom-scopes.customScopesIdField\">          <div class=\"form-group\">            <label for=\"custom_scope_name\">Name</label>            <auto-check src=\"/search/custom_scopes/check_name\" required>              <input                type=\"text\"                name=\"custom_scope_name\"                id=\"custom_scope_name\"                data-target=\"custom-scopes.customScopesNameField\"                class=\"form-control\"                autocomplete=\"off\"                placeholder=\"github-ruby\"                required                maxlength=\"50\">              <input type=\"hidden\" data-csrf=\"true\" value=\"WSXFCQdpbVYNKFgd9X68W4iWTwYMTLmeU3bTO5r8prdlDPWhiFLhRGLXsa0VeoXogxqKd3298k/nhvguYkylNg==\" />            </auto-check>          </div>          <div class=\"form-group\">            <label for=\"custom_scope_query\">Query</label>            <input              type=\"text\"              name=\"custom_scope_query\"              id=\"custom_scope_query\"              data-target=\"custom-scopes.customScopesQueryField\"              class=\"form-control\"              autocomplete=\"off\"              placeholder=\"(repo:mona/a OR repo:mona/b) AND lang:python\"              required              maxlength=\"500\">          </div>          <p class=\"text-small color-fg-muted\">            To see all available qualifiers, see our <a class=\"Link--inTextBlock\" href=\"https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax\">documentation</a>.          </p></form>        </div>        <div data-target=\"custom-scopes.manageCustomScopesForm\">          <div data-target=\"custom-scopes.list\"></div>        </div></div>      </scrollable-region>      <div data-view-component=\"true\" class=\"Overlay-footer Overlay-footer--alignEnd Overlay-footer--divided\">          <button data-action=\"click:custom-scopes#customScopesCancel\" type=\"button\" data-view-component=\"true\" class=\"btn\">    Cancel</button>          <button form=\"custom-scopes-dialog-form\" data-action=\"click:custom-scopes#customScopesSubmit\" data-target=\"custom-scopes.customScopesSubmitButton\" type=\"submit\" data-view-component=\"true\" class=\"btn-primary btn\">    Create saved search</button></div></dialog></dialog-helper>    </custom-scopes>  </div></qbsearch-input><input type=\"hidden\" data-csrf=\"true\" class=\"js-data-jump-to-suggestions-path-csrf\" value=\"C6aCNRYlR45CBhzGZu75Bqw8RihBP7ycuu8Y9me1QId9u/sT0swsa5LTRQHqYvgnnrKCV0tek6ajo3PmJroz8A==\" />          <div class=\"position-relative mr-lg-3 d-lg-inline-block\">            <a href=\"/login?return_to=https%3A%2F%2Fgithub.com%2Fmlfoundations%2Fopen_clip\"              class=\"HeaderMenu-link HeaderMenu-link--sign-in flex-shrink-0 no-underline d-block d-lg-inline-block border border-lg-0 rounded rounded-lg-0 p-2 p-lg-0\"              data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/mlfoundations/open_clip&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"dc360152aa35af6b9b9e9fd1b1a5fd2598094817ff28da4cf8468da138b15cc7\"              data-ga-click=\"(Logged out) Header, clicked Sign in, text:sign-in\">              Sign in            </a>          </div>            <a href=\"/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=mlfoundations%2Fopen_clip\"              class=\"HeaderMenu-link HeaderMenu-link--sign-up flex-shrink-0 d-none d-lg-inline-block no-underline border color-border-default rounded px-2 py-1\"              data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/mlfoundations/open_clip&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"dc360152aa35af6b9b9e9fd1b1a5fd2598094817ff28da4cf8468da138b15cc7\"              data-analytics-event=\"{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/&lt;user-name&gt;/&lt;repo-name&gt;;ref_cta:Sign up;ref_loc:header logged out&quot;}\"            >              Sign up            </a>        </div>      </div>    </div>  </div></header>      <div hidden=\"hidden\" data-view-component=\"true\" class=\"js-stale-session-flash stale-session-flash flash flash-warn flash-full mb-3\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-alert\">    <path d=\"M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>        <span class=\"js-stale-session-flash-signed-in\" hidden>You signed in with another tab or window. <a class=\"Link--inTextBlock\" href=\"\">Reload</a> to refresh your session.</span>        <span class=\"js-stale-session-flash-signed-out\" hidden>You signed out in another tab or window. <a class=\"Link--inTextBlock\" href=\"\">Reload</a> to refresh your session.</span>        <span class=\"js-stale-session-flash-switched\" hidden>You switched accounts on another tab or window. <a class=\"Link--inTextBlock\" href=\"\">Reload</a> to refresh your session.</span>    <button id=\"icon-button-8e38585c-3ea4-4d84-bef2-adf5983b9694\" aria-labelledby=\"tooltip-6270d361-da43-41e9-9bff-c3b1e2553b85\" type=\"button\" data-view-component=\"true\" class=\"Button Button--iconOnly Button--invisible Button--medium flash-close js-flash-close\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x Button-visual\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg></button><tool-tip id=\"tooltip-6270d361-da43-41e9-9bff-c3b1e2553b85\" for=\"icon-button-8e38585c-3ea4-4d84-bef2-adf5983b9694\" popover=\"manual\" data-direction=\"s\" data-type=\"label\" data-view-component=\"true\" class=\"sr-only position-absolute\">Dismiss alert</tool-tip>  </div>    </div>  <div id=\"start-of-content\" class=\"show-on-focus\"></div>    <div id=\"js-flash-container\" data-turbo-replace>  <template class=\"js-flash-template\">    <div class=\"flash flash-full   {{ className }}\">  <div class=\"px-2\" >    <button autofocus class=\"flash-close js-flash-close\" type=\"button\" aria-label=\"Dismiss this message\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg>    </button>    <div aria-atomic=\"true\" role=\"alert\" class=\"js-flash-alert\">            <div>{{ message }}</div>    </div>  </div></div>  </template></div>        <include-fragment class=\"js-notification-shelf-include-fragment\" data-base-src=\"https://github.com/notifications/beta/shelf\"></include-fragment>  <div    class=\"application-main \"    data-commit-hovercards-enabled    data-discussion-hovercards-enabled    data-issue-and-pr-hovercards-enabled  >        <div itemscope itemtype=\"http://schema.org/SoftwareSourceCode\" class=\"\">    <main id=\"js-repo-pjax-container\" >                        <div id=\"repository-container-header\"  class=\"pt-3 hide-full-screen\" style=\"background-color: var(--page-header-bgColor, var(--color-page-header-bg));\" data-turbo-replace>      <div class=\"d-flex flex-wrap flex-justify-end mb-3  px-3 px-md-4 px-lg-5\" style=\"gap: 1rem;\">        <div class=\"flex-auto min-width-0 width-fit mr-3\">              <div class=\" d-flex flex-wrap flex-items-center wb-break-word f3 text-normal\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo color-fg-muted mr-2\">    <path d=\"M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z\"></path></svg>        <span class=\"author flex-self-stretch\" itemprop=\"author\">      <a class=\"url fn\" rel=\"author\" data-hovercard-type=\"organization\" data-hovercard-url=\"/orgs/mlfoundations/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"/mlfoundations\">        mlfoundations</a>    </span>    <span class=\"mx-1 flex-self-stretch color-fg-muted\">/</span>    <strong itemprop=\"name\" class=\"mr-2 flex-self-stretch\">      <a data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" href=\"/mlfoundations/open_clip\">open_clip</a>    </strong>    <span></span><span class=\"Label Label--secondary v-align-middle mr-1\">Public</span>  </div>        </div>        <div id=\"repository-details-container\" data-turbo-replace>            <ul class=\"pagehead-actions flex-shrink-0 d-none d-md-inline\" style=\"padding: 2px 0;\">            <li>            <a href=\"/login?return_to=%2Fmlfoundations%2Fopen_clip\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/mlfoundations/open_clip&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"074af8a5c13a5981aa20285bd75731cd3a9fd3b374417289df88a61e3850b9ea\" aria-label=\"You must be signed in to change notification settings\" data-view-component=\"true\" class=\"tooltipped tooltipped-s btn-sm btn\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-bell mr-2\">    <path d=\"M8 16a2 2 0 0 0 1.985-1.75c.017-.137-.097-.25-.235-.25h-3.5c-.138 0-.252.113-.235.25A2 2 0 0 0 8 16ZM3 5a5 5 0 0 1 10 0v2.947c0 .05.015.098.042.139l1.703 2.555A1.519 1.519 0 0 1 13.482 13H2.518a1.516 1.516 0 0 1-1.263-2.36l1.703-2.554A.255.255 0 0 0 3 7.947Zm5-3.5A3.5 3.5 0 0 0 4.5 5v2.947c0 .346-.102.683-.294.97l-1.703 2.556a.017.017 0 0 0-.003.01l.001.006c0 .002.002.004.004.006l.006.004.007.001h10.964l.007-.001.006-.004.004-.006.001-.007a.017.017 0 0 0-.003-.01l-1.703-2.554a1.745 1.745 0 0 1-.294-.97V5A3.5 3.5 0 0 0 8 1.5Z\"></path></svg>Notifications</a>  </li>  <li>          <a icon=\"repo-forked\" id=\"fork-button\" href=\"/login?return_to=%2Fmlfoundations%2Fopen_clip\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;repo details fork button&quot;,&quot;repository_id&quot;:390536799,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/mlfoundations/open_clip&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"659668c680eb64f40361fdf1bbe0aa2340d32ce1ac7520e2b1f220be917fed00\" data-view-component=\"true\" class=\"btn-sm btn\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo-forked mr-2\">    <path d=\"M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z\"></path></svg>Fork    <span id=\"repo-network-counter\" data-pjax-replace=\"true\" data-turbo-replace=\"true\" title=\"786\" data-view-component=\"true\" class=\"Counter\">786</span></a>  </li>  <li>        <div data-view-component=\"true\" class=\"BtnGroup d-flex\">        <a href=\"/login?return_to=%2Fmlfoundations%2Fopen_clip\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:390536799,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/mlfoundations/open_clip&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"3eeaacbbefc5ff7197a2a16f01f83495f5cc8f022b77ea185a66b67bb8779fd8\" aria-label=\"You must be signed in to star a repository\" data-view-component=\"true\" class=\"tooltipped tooltipped-s btn-sm btn BtnGroup-item\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-star v-align-text-bottom d-inline-block mr-2\">    <path d=\"M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z\"></path></svg><span data-view-component=\"true\" class=\"d-inline\">          Star</span>          <span id=\"repo-stars-counter-star\" aria-label=\"7864 users starred this repository\" data-singular-suffix=\"user starred this repository\" data-plural-suffix=\"users starred this repository\" data-turbo-replace=\"true\" title=\"7,864\" data-view-component=\"true\" class=\"Counter js-social-count\">7.9k</span></a>        <button aria-label=\"You must be signed in to add this repository to a list\" type=\"button\" disabled=\"disabled\" data-view-component=\"true\" class=\"btn-sm btn BtnGroup-item px-2\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-triangle-down\">    <path d=\"m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z\"></path></svg></button></div>  </li>    <li>            </li></ul>        </div>      </div>        <div id=\"responsive-meta-container\" data-turbo-replace>      <div class=\"d-block d-md-none mb-2 px-3 px-md-4 px-lg-5\">      <p class=\"f4 mb-3 \">        An open source implementation of CLIP.      </p>          <h3 class=\"sr-only\">License</h3>  <div class=\"mb-2\">    <a href=\"/mlfoundations/open_clip/blob/main/LICENSE\"      class=\"Link--muted\"            data-analytics-event=\"{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:license&quot;}\"    >      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-law mr-2\">    <path d=\"M8.75.75V2h.985c.304 0 .603.08.867.231l1.29.736c.038.022.08.033.124.033h2.234a.75.75 0 0 1 0 1.5h-.427l2.111 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.006.005-.01.01-.045.04c-.21.176-.441.327-.686.45C14.556 10.78 13.88 11 13 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L12.178 4.5h-.162c-.305 0-.604-.079-.868-.231l-1.29-.736a.245.245 0 0 0-.124-.033H8.75V13h2.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1 0-1.5h2.5V3.5h-.984a.245.245 0 0 0-.124.033l-1.289.737c-.265.15-.564.23-.869.23h-.162l2.112 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.016.015-.045.04c-.21.176-.441.327-.686.45C4.556 10.78 3.88 11 3 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L2.178 4.5H1.75a.75.75 0 0 1 0-1.5h2.234a.249.249 0 0 0 .125-.033l1.288-.737c.265-.15.564-.23.869-.23h.984V.75a.75.75 0 0 1 1.5 0Zm2.945 8.477c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L13 6.327Zm-10 0c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L3 6.327Z\"></path></svg>     View license    </a>  </div>    <div class=\"mb-3\">        <a class=\"Link--secondary no-underline mr-3\" href=\"/mlfoundations/open_clip/stargazers\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-star mr-1\">    <path d=\"M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z\"></path></svg>          <span class=\"text-bold\">7.9k</span>          stars</a>        <a class=\"Link--secondary no-underline mr-3\" href=\"/mlfoundations/open_clip/forks\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo-forked mr-1\">    <path d=\"M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z\"></path></svg>          <span class=\"text-bold\">786</span>          forks</a>          <a class=\"Link--secondary no-underline mr-3 d-inline-block\" href=\"/mlfoundations/open_clip/branches\">            <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-git-branch mr-1\">    <path d=\"M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z\"></path></svg>            <span>Branches</span></a>          <a class=\"Link--secondary no-underline d-inline-block\" href=\"/mlfoundations/open_clip/tags\">            <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-tag mr-1\">    <path d=\"M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z\"></path></svg>            <span>Tags</span></a>        <a class=\"Link--secondary no-underline d-inline-block\" href=\"/mlfoundations/open_clip/activity\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-pulse mr-1\">    <path d=\"M6 2c.306 0 .582.187.696.471L10 10.731l1.304-3.26A.751.751 0 0 1 12 7h3.25a.75.75 0 0 1 0 1.5h-2.742l-1.812 4.528a.751.751 0 0 1-1.392 0L6 4.77 4.696 8.03A.75.75 0 0 1 4 8.5H.75a.75.75 0 0 1 0-1.5h2.742l1.812-4.529A.751.751 0 0 1 6 2Z\"></path></svg>          <span>Activity</span></a>    </div>      <div class=\"d-flex flex-wrap gap-2\">        <div class=\"flex-1\">            <div data-view-component=\"true\" class=\"BtnGroup d-flex\">        <a href=\"/login?return_to=%2Fmlfoundations%2Fopen_clip\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:390536799,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/mlfoundations/open_clip&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"3eeaacbbefc5ff7197a2a16f01f83495f5cc8f022b77ea185a66b67bb8779fd8\" aria-label=\"You must be signed in to star a repository\" data-view-component=\"true\" class=\"tooltipped tooltipped-s btn-sm btn btn-block BtnGroup-item\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-star v-align-text-bottom d-inline-block mr-2\">    <path d=\"M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z\"></path></svg><span data-view-component=\"true\" class=\"d-inline\">          Star</span></a>        <button aria-label=\"You must be signed in to add this repository to a list\" type=\"button\" disabled=\"disabled\" data-view-component=\"true\" class=\"btn-sm btn BtnGroup-item px-2\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-triangle-down\">    <path d=\"m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z\"></path></svg></button></div>        </div>        <div class=\"flex-1\">                <a href=\"/login?return_to=%2Fmlfoundations%2Fopen_clip\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/mlfoundations/open_clip&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"074af8a5c13a5981aa20285bd75731cd3a9fd3b374417289df88a61e3850b9ea\" aria-label=\"You must be signed in to change notification settings\" data-view-component=\"true\" class=\"tooltipped tooltipped-s btn-sm btn btn-block\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-bell mr-2\">    <path d=\"M8 16a2 2 0 0 0 1.985-1.75c.017-.137-.097-.25-.235-.25h-3.5c-.138 0-.252.113-.235.25A2 2 0 0 0 8 16ZM3 5a5 5 0 0 1 10 0v2.947c0 .05.015.098.042.139l1.703 2.555A1.519 1.519 0 0 1 13.482 13H2.518a1.516 1.516 0 0 1-1.263-2.36l1.703-2.554A.255.255 0 0 0 3 7.947Zm5-3.5A3.5 3.5 0 0 0 4.5 5v2.947c0 .346-.102.683-.294.97l-1.703 2.556a.017.017 0 0 0-.003.01l.001.006c0 .002.002.004.004.006l.006.004.007.001h10.964l.007-.001.006-.004.004-.006.001-.007a.017.017 0 0 0-.003-.01l-1.703-2.554a1.745 1.745 0 0 1-.294-.97V5A3.5 3.5 0 0 0 8 1.5Z\"></path></svg>Notifications</a>        </div>          <span>                      </span>      </div>  </div></div>          <nav data-pjax=\"#js-repo-pjax-container\" aria-label=\"Repository\" data-view-component=\"true\" class=\"js-repo-nav js-sidenav-container-pjax js-responsive-underlinenav overflow-hidden UnderlineNav px-3 px-md-4 px-lg-5\">  <ul data-view-component=\"true\" class=\"UnderlineNav-body list-style-none\">      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"code-tab\" href=\"/mlfoundations/open_clip\" data-tab-item=\"i0code-tab\" data-selected-links=\"repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches repo_packages repo_deployments repo_attestations /mlfoundations/open_clip\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g c\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Code&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" aria-current=\"page\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item selected\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z\"></path></svg>        <span data-content=\"Code\">Code</span>          <span id=\"code-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"Not available\" data-view-component=\"true\" class=\"Counter\"></span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"issues-tab\" href=\"/mlfoundations/open_clip/issues\" data-tab-item=\"i1issues-tab\" data-selected-links=\"repo_issues repo_labels repo_milestones /mlfoundations/open_clip/issues\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g i\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Issues&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-issue-opened UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path><path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z\"></path></svg>        <span data-content=\"Issues\">Issues</span>          <span id=\"issues-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"104\" data-view-component=\"true\" class=\"Counter\">104</span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"pull-requests-tab\" href=\"/mlfoundations/open_clip/pulls\" data-tab-item=\"i2pull-requests-tab\" data-selected-links=\"repo_pulls checks /mlfoundations/open_clip/pulls\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g p\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Pull requests&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-git-pull-request UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M1.5 3.25a2.25 2.25 0 1 1 3 2.122v5.256a2.251 2.251 0 1 1-1.5 0V5.372A2.25 2.25 0 0 1 1.5 3.25Zm5.677-.177L9.573.677A.25.25 0 0 1 10 .854V2.5h1A2.5 2.5 0 0 1 13.5 5v5.628a2.251 2.251 0 1 1-1.5 0V5a1 1 0 0 0-1-1h-1v1.646a.25.25 0 0 1-.427.177L7.177 3.427a.25.25 0 0 1 0-.354ZM3.75 2.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm0 9.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm8.25.75a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Z\"></path></svg>        <span data-content=\"Pull requests\">Pull requests</span>          <span id=\"pull-requests-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"39\" data-view-component=\"true\" class=\"Counter\">39</span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"discussions-tab\" href=\"/mlfoundations/open_clip/discussions\" data-tab-item=\"i3discussions-tab\" data-selected-links=\"repo_discussions /mlfoundations/open_clip/discussions\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g g\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Discussions&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-comment-discussion UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M1.75 1h8.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 10.25 10H7.061l-2.574 2.573A1.458 1.458 0 0 1 2 11.543V10h-.25A1.75 1.75 0 0 1 0 8.25v-5.5C0 1.784.784 1 1.75 1ZM1.5 2.75v5.5c0 .138.112.25.25.25h1a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h3.5a.25.25 0 0 0 .25-.25v-5.5a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25Zm13 2a.25.25 0 0 0-.25-.25h-.5a.75.75 0 0 1 0-1.5h.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 14.25 12H14v1.543a1.458 1.458 0 0 1-2.487 1.03L9.22 12.28a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l2.22 2.22v-2.19a.75.75 0 0 1 .75-.75h1a.25.25 0 0 0 .25-.25Z\"></path></svg>        <span data-content=\"Discussions\">Discussions</span>          <span id=\"discussions-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"Not available\" data-view-component=\"true\" class=\"Counter\"></span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"actions-tab\" href=\"/mlfoundations/open_clip/actions\" data-tab-item=\"i4actions-tab\" data-selected-links=\"repo_actions /mlfoundations/open_clip/actions\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g a\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Actions&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-play UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z\"></path></svg>        <span data-content=\"Actions\">Actions</span>          <span id=\"actions-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"Not available\" data-view-component=\"true\" class=\"Counter\"></span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"projects-tab\" href=\"/mlfoundations/open_clip/projects\" data-tab-item=\"i5projects-tab\" data-selected-links=\"repo_projects new_repo_project repo_project /mlfoundations/open_clip/projects\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g b\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Projects&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-table UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25ZM6.5 6.5v8h7.75a.25.25 0 0 0 .25-.25V6.5Zm8-1.5V1.75a.25.25 0 0 0-.25-.25H6.5V5Zm-13 1.5v7.75c0 .138.112.25.25.25H5v-8ZM5 5V1.5H1.75a.25.25 0 0 0-.25.25V5Z\"></path></svg>        <span data-content=\"Projects\">Projects</span>          <span id=\"projects-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"0\" hidden=\"hidden\" data-view-component=\"true\" class=\"Counter\">0</span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"security-tab\" href=\"/mlfoundations/open_clip/security\" data-tab-item=\"i6security-tab\" data-selected-links=\"security overview alerts policy token_scanning code_scanning /mlfoundations/open_clip/security\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g s\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Security&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-shield UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M7.467.133a1.748 1.748 0 0 1 1.066 0l5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667Zm.61 1.429a.25.25 0 0 0-.153 0l-5.25 1.68a.25.25 0 0 0-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.196.196 0 0 0 .154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.251.251 0 0 0-.174-.237l-5.25-1.68ZM8.75 4.75v3a.75.75 0 0 1-1.5 0v-3a.75.75 0 0 1 1.5 0ZM9 10.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>        <span data-content=\"Security\">Security</span>          <include-fragment src=\"/mlfoundations/open_clip/security/overall-count\" accept=\"text/fragment+html\"></include-fragment>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"insights-tab\" href=\"/mlfoundations/open_clip/pulse\" data-tab-item=\"i7insights-tab\" data-selected-links=\"repo_graphs repo_contributors dependency_graph dependabot_updates pulse people community /mlfoundations/open_clip/pulse\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Insights&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-graph UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M1.5 1.75V13.5h13.75a.75.75 0 0 1 0 1.5H.75a.75.75 0 0 1-.75-.75V1.75a.75.75 0 0 1 1.5 0Zm14.28 2.53-5.25 5.25a.75.75 0 0 1-1.06 0L7 7.06 4.28 9.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.25-3.25a.75.75 0 0 1 1.06 0L10 7.94l4.72-4.72a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z\"></path></svg>        <span data-content=\"Insights\">Insights</span>          <span id=\"insights-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"Not available\" data-view-component=\"true\" class=\"Counter\"></span>    </a></li></ul>    <div style=\"visibility:hidden;\" data-view-component=\"true\" class=\"UnderlineNav-actions js-responsive-underlinenav-overflow position-absolute pr-3 pr-md-4 pr-lg-5 right-0\">      <action-menu data-select-variant=\"none\" data-view-component=\"true\">  <focus-group direction=\"vertical\" mnemonics retain>    <button id=\"action-menu-bf478e2d-72e4-4e0f-a182-17e610fb0df6-button\" popovertarget=\"action-menu-bf478e2d-72e4-4e0f-a182-17e610fb0df6-overlay\" aria-controls=\"action-menu-bf478e2d-72e4-4e0f-a182-17e610fb0df6-list\" aria-haspopup=\"true\" aria-labelledby=\"tooltip-72eaa35f-2308-4610-a598-26788205e44a\" type=\"button\" data-view-component=\"true\" class=\"Button Button--iconOnly Button--secondary Button--medium UnderlineNav-item\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-kebab-horizontal Button-visual\">    <path d=\"M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path></svg></button><tool-tip id=\"tooltip-72eaa35f-2308-4610-a598-26788205e44a\" for=\"action-menu-bf478e2d-72e4-4e0f-a182-17e610fb0df6-button\" popover=\"manual\" data-direction=\"s\" data-type=\"label\" data-view-component=\"true\" class=\"sr-only position-absolute\">Additional navigation options</tool-tip><anchored-position id=\"action-menu-bf478e2d-72e4-4e0f-a182-17e610fb0df6-overlay\" anchor=\"action-menu-bf478e2d-72e4-4e0f-a182-17e610fb0df6-button\" align=\"start\" side=\"outside-bottom\" anchor-offset=\"normal\" popover=\"auto\" data-view-component=\"true\">  <div data-view-component=\"true\" class=\"Overlay Overlay--size-auto\">          <div data-view-component=\"true\" class=\"Overlay-body Overlay-body--paddingNone\">          <div data-view-component=\"true\">  <ul aria-labelledby=\"action-menu-bf478e2d-72e4-4e0f-a182-17e610fb0df6-button\" id=\"action-menu-bf478e2d-72e4-4e0f-a182-17e610fb0df6-list\" role=\"menu\" data-view-component=\"true\" class=\"ActionListWrap--inset ActionListWrap\">      <li hidden=\"hidden\" data-menu-item=\"i0code-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-b0bca179-7dc6-41f0-8e7a-ea1041134673\" href=\"/mlfoundations/open_clip\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code\">    <path d=\"m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Code</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i1issues-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-198d23d0-7cb5-4292-8d04-323c8fb6f66d\" href=\"/mlfoundations/open_clip/issues\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-issue-opened\">    <path d=\"M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path><path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Issues</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i2pull-requests-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-2d490b9c-cfc0-40a9-ba4c-5668034e907d\" href=\"/mlfoundations/open_clip/pulls\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-git-pull-request\">    <path d=\"M1.5 3.25a2.25 2.25 0 1 1 3 2.122v5.256a2.251 2.251 0 1 1-1.5 0V5.372A2.25 2.25 0 0 1 1.5 3.25Zm5.677-.177L9.573.677A.25.25 0 0 1 10 .854V2.5h1A2.5 2.5 0 0 1 13.5 5v5.628a2.251 2.251 0 1 1-1.5 0V5a1 1 0 0 0-1-1h-1v1.646a.25.25 0 0 1-.427.177L7.177 3.427a.25.25 0 0 1 0-.354ZM3.75 2.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm0 9.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm8.25.75a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Pull requests</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i3discussions-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-8533dcca-50af-48b1-8d18-9eea16eb61d5\" href=\"/mlfoundations/open_clip/discussions\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-comment-discussion\">    <path d=\"M1.75 1h8.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 10.25 10H7.061l-2.574 2.573A1.458 1.458 0 0 1 2 11.543V10h-.25A1.75 1.75 0 0 1 0 8.25v-5.5C0 1.784.784 1 1.75 1ZM1.5 2.75v5.5c0 .138.112.25.25.25h1a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h3.5a.25.25 0 0 0 .25-.25v-5.5a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25Zm13 2a.25.25 0 0 0-.25-.25h-.5a.75.75 0 0 1 0-1.5h.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 14.25 12H14v1.543a1.458 1.458 0 0 1-2.487 1.03L9.22 12.28a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l2.22 2.22v-2.19a.75.75 0 0 1 .75-.75h1a.25.25 0 0 0 .25-.25Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Discussions</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i4actions-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-51de4c81-fe89-4a41-9aa6-f15d692f776f\" href=\"/mlfoundations/open_clip/actions\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-play\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Actions</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i5projects-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-768bb51b-91b3-4392-9db7-0ebfc6b09e8a\" href=\"/mlfoundations/open_clip/projects\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-table\">    <path d=\"M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25ZM6.5 6.5v8h7.75a.25.25 0 0 0 .25-.25V6.5Zm8-1.5V1.75a.25.25 0 0 0-.25-.25H6.5V5Zm-13 1.5v7.75c0 .138.112.25.25.25H5v-8ZM5 5V1.5H1.75a.25.25 0 0 0-.25.25V5Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Projects</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i6security-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-4d4c8e63-2988-420a-aaa6-2e4b4b7c2999\" href=\"/mlfoundations/open_clip/security\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-shield\">    <path d=\"M7.467.133a1.748 1.748 0 0 1 1.066 0l5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667Zm.61 1.429a.25.25 0 0 0-.153 0l-5.25 1.68a.25.25 0 0 0-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.196.196 0 0 0 .154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.251.251 0 0 0-.174-.237l-5.25-1.68ZM8.75 4.75v3a.75.75 0 0 1-1.5 0v-3a.75.75 0 0 1 1.5 0ZM9 10.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Security</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i7insights-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-0661bfb8-6052-42d3-b819-cce38250a6bc\" href=\"/mlfoundations/open_clip/pulse\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-graph\">    <path d=\"M1.5 1.75V13.5h13.75a.75.75 0 0 1 0 1.5H.75a.75.75 0 0 1-.75-.75V1.75a.75.75 0 0 1 1.5 0Zm14.28 2.53-5.25 5.25a.75.75 0 0 1-1.06 0L7 7.06 4.28 9.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.25-3.25a.75.75 0 0 1 1.06 0L10 7.94l4.72-4.72a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Insights</span></a>    </li></ul>  </div></div>      </div></anchored-position>  </focus-group></action-menu></div></nav>  </div>  <turbo-frame id=\"repo-content-turbo-frame\" target=\"_top\" data-turbo-action=\"advance\" class=\"\">    <div id=\"repo-content-pjax-container\" class=\"repository-content \" >                <h1 class='sr-only'>mlfoundations/open_clip</h1>  <div class=\"clearfix container-xl px-md-4 px-lg-5 px-3\">    <div>  <div id=\"spoof-warning\" class=\"mt-0 pb-3\" hidden aria-hidden>  <div data-view-component=\"true\" class=\"flash flash-warn mt-0 clearfix\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-alert float-left mt-1\">    <path d=\"M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>      <div class=\"overflow-hidden\">This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.</div>  </div></div>  <include-fragment src=\"/mlfoundations/open_clip/spoofed_commit_check/73fa7f03a33da53653f61841eb6d69aef161e521\" data-test-selector=\"spoofed-commit-check\"></include-fragment>  <div style=\"max-width: 100%\" data-view-component=\"true\" class=\"Layout Layout--flowRow-until-md react-repos-overview-margin Layout--sidebarPosition-end Layout--sidebarPosition-flowRow-end\">  <div data-view-component=\"true\" class=\"Layout-main\">        <script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/react-lib-1fbfc5be2c18.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_octicons-react_dist_index_esm_js-node_modules_primer_react_lib-es-2e8e7c-adc8451a70cf.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Box_Box_js-8f8c5e2a2cbf.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Button_Button_js-67fe00b5266a.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_ActionList_index_js-2dd4d13d3ae6.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Overlay_Overlay_js-node_modules_primer_react_lib-es-fa1130-829932cf63db.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Text_Text_js-node_modules_primer_react_lib-esm_Text-85a14b-236dc9716ad0.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_ActionMenu_ActionMenu_js-eaf74522e470.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_react-router-dom_dist_index_js-3b41341d50fe.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Dialog_js-node_modules_primer_react_lib-esm_Label_L-857e1c-77794958a54a.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_UnderlineNav_index_js-89fa5806aa3c.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_AvatarStack_AvatarStack_js-node_modules_primer_reac-e445e7-175b51e43dcc.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_react-core_create-browser-history_ts-ui_packages_react-core_AppContextProvider_ts-809ab9-bf008735d0bb.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_paths_index_ts-7137b25aa38b.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_ref-selector_RefSelector_tsx-dbbdef4348e2.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_commit-attribution_index_ts-ui_packages_commit-checks-status_index_ts-ui_packages-ffbe33-4c4ddf7d268d.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_react-code-view_components_directory_DirectoryContent_index_ts-app_assets_-1fd1f5-c96303590595.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/repos-overview-523b8f59ec33.js\"></script><react-partial  partial-name=\"repos-overview\"  data-ssr=\"true\">    <script type=\"application/json\" data-target=\"react-partial.embeddedData\">{\"props\":{\"initialPayload\":{\"allShortcutsEnabled\":false,\"path\":\"/\",\"repo\":{\"id\":390536799,\"defaultBranch\":\"main\",\"name\":\"open_clip\",\"ownerLogin\":\"mlfoundations\",\"currentUserCanPush\":false,\"isFork\":false,\"isEmpty\":false,\"createdAt\":\"2021-07-28T23:24:39.000Z\",\"ownerAvatar\":\"https://avatars.githubusercontent.com/u/87461581?v=4\",\"public\":true,\"private\":false,\"isOrgOwned\":true},\"currentUser\":null,\"refInfo\":{\"name\":\"main\",\"listCacheKey\":\"v0:1704710150.0\",\"canEdit\":false,\"refType\":\"branch\",\"currentOid\":\"73fa7f03a33da53653f61841eb6d69aef161e521\"},\"tree\":{\"items\":[{\"name\":\".github/workflows\",\"path\":\".github/workflows\",\"contentType\":\"directory\",\"hasSimplifiedPath\":true},{\"name\":\"docs\",\"path\":\"docs\",\"contentType\":\"directory\"},{\"name\":\"scripts\",\"path\":\"scripts\",\"contentType\":\"directory\"},{\"name\":\"src\",\"path\":\"src\",\"contentType\":\"directory\"},{\"name\":\"tests\",\"path\":\"tests\",\"contentType\":\"directory\"},{\"name\":\"tutorials\",\"path\":\"tutorials\",\"contentType\":\"directory\"},{\"name\":\".gitignore\",\"path\":\".gitignore\",\"contentType\":\"file\"},{\"name\":\"CITATION.cff\",\"path\":\"CITATION.cff\",\"contentType\":\"file\"},{\"name\":\"HISTORY.md\",\"path\":\"HISTORY.md\",\"contentType\":\"file\"},{\"name\":\"LICENSE\",\"path\":\"LICENSE\",\"contentType\":\"file\"},{\"name\":\"MANIFEST.in\",\"path\":\"MANIFEST.in\",\"contentType\":\"file\"},{\"name\":\"Makefile\",\"path\":\"Makefile\",\"contentType\":\"file\"},{\"name\":\"README.md\",\"path\":\"README.md\",\"contentType\":\"file\"},{\"name\":\"pytest.ini\",\"path\":\"pytest.ini\",\"contentType\":\"file\"},{\"name\":\"requirements-test.txt\",\"path\":\"requirements-test.txt\",\"contentType\":\"file\"},{\"name\":\"requirements-training.txt\",\"path\":\"requirements-training.txt\",\"contentType\":\"file\"},{\"name\":\"requirements.txt\",\"path\":\"requirements.txt\",\"contentType\":\"file\"},{\"name\":\"setup.py\",\"path\":\"setup.py\",\"contentType\":\"file\"}],\"templateDirectorySuggestionUrl\":null,\"readme\":null,\"totalCount\":18,\"showBranchInfobar\":false},\"fileTree\":null,\"fileTreeProcessingTime\":null,\"foldersToFetch\":[],\"treeExpanded\":false,\"symbolsExpanded\":false,\"isOverview\":true,\"overview\":{\"banners\":{\"shouldRecommendReadme\":false,\"isPersonalRepo\":false,\"showUseActionBanner\":false,\"actionSlug\":null,\"actionId\":null,\"showProtectBranchBanner\":false,\"recentlyTouchedDataChannel\":null,\"publishBannersInfo\":{\"dismissActionNoticePath\":\"/settings/dismiss-notice/publish_action_from_repo\",\"releasePath\":\"/mlfoundations/open_clip/releases/new?marketplace=true\",\"showPublishActionBanner\":false},\"interactionLimitBanner\":null,\"showInvitationBanner\":false,\"inviterName\":null},\"codeButton\":{\"contactPath\":\"/contact\",\"isEnterprise\":false,\"local\":{\"protocolInfo\":{\"httpAvailable\":true,\"sshAvailable\":null,\"httpUrl\":\"https://github.com/mlfoundations/open_clip.git\",\"showCloneWarning\":null,\"sshUrl\":null,\"sshCertificatesRequired\":null,\"sshCertificatesAvailable\":null,\"ghCliUrl\":\"gh repo clone mlfoundations/open_clip\",\"defaultProtocol\":\"http\",\"newSshKeyUrl\":\"/settings/ssh/new\",\"setProtocolPath\":\"/users/set_protocol\"},\"platformInfo\":{\"cloneUrl\":\"https://desktop.github.com\",\"showVisualStudioCloneButton\":false,\"visualStudioCloneUrl\":\"https://windows.github.com\",\"showXcodeCloneButton\":false,\"xcodeCloneUrl\":\"https://developer.apple.com\",\"zipballUrl\":\"/mlfoundations/open_clip/archive/refs/heads/main.zip\"}},\"newCodespacePath\":\"/codespaces/new?hide_repo_select=true\\u0026repo=390536799\"},\"popovers\":{\"rename\":null,\"renamedParentRepo\":null},\"commitCount\":\"512\",\"overviewFiles\":[{\"displayName\":\"README.md\",\"repoName\":\"open_clip\",\"refName\":\"main\",\"path\":\"README.md\",\"preferredFileType\":\"readme\",\"tabName\":\"README\",\"richText\":\"\\u003carticle class=\\\"markdown-body entry-content container-lg\\\" itemprop=\\\"text\\\"\\u003e\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch1 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eOpenCLIP\\u003c/h1\\u003e\\u003ca id=\\\"user-content-openclip\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: OpenCLIP\\\" href=\\\"#openclip\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2212.07143\\\" rel=\\\"nofollow\\\"\\u003e[Paper]\\u003c/a\\u003e \\u003ca href=\\\"#citing\\\"\\u003e[Citations]\\u003c/a\\u003e \\u003ca href=\\\"https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb\\\" rel=\\\"nofollow\\\"\\u003e[Clip Colab]\\u003c/a\\u003e \\u003ca href=\\\"https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb\\\" rel=\\\"nofollow\\\"\\u003e[Coca Colab]\\u003c/a\\u003e\\n\\u003ca href=\\\"https://pypi.python.org/pypi/open_clip_torch\\\" rel=\\\"nofollow\\\"\\u003e\\u003cimg src=\\\"https://camo.githubusercontent.com/8d1068793d4518a7aa8d18b65b7d6f163e22445c3986b2a85834ce1d4c339501/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6f70656e5f636c69705f746f7263682e737667\\\" alt=\\\"pypi\\\" data-canonical-src=\\\"https://img.shields.io/pypi/v/open_clip_torch.svg\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eWelcome to an open source implementation of OpenAI's \\u003ca href=\\\"https://arxiv.org/abs/2103.00020\\\" rel=\\\"nofollow\\\"\\u003eCLIP\\u003c/a\\u003e (Contrastive Language-Image Pre-training).\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUsing this codebase, we have trained several models on a variety of data sources and compute budgets, ranging from \\u003ca href=\\\"/mlfoundations/open_clip/blob/main/docs/LOW_ACC.md\\\"\\u003esmall-scale experiments\\u003c/a\\u003e to larger runs including models trained on datasets such as \\u003ca href=\\\"https://arxiv.org/abs/2111.02114\\\" rel=\\\"nofollow\\\"\\u003eLAION-400M\\u003c/a\\u003e, \\u003ca href=\\\"https://arxiv.org/abs/2210.08402\\\" rel=\\\"nofollow\\\"\\u003eLAION-2B\\u003c/a\\u003e and \\u003ca href=\\\"https://arxiv.org/abs/2304.14108\\\" rel=\\\"nofollow\\\"\\u003eDataComp-1B\\u003c/a\\u003e.\\nMany of our models and their scaling properties are studied in detail in the paper \\u003ca href=\\\"https://arxiv.org/abs/2212.07143\\\" rel=\\\"nofollow\\\"\\u003ereproducible scaling laws for contrastive language-image learning\\u003c/a\\u003e.\\nSome of our best models and their zero-shot ImageNet-1k accuracy are shown below, along with the ViT-L model trained by OpenAI.\\nWe provide more details about our full collection of pretrained models \\u003ca href=\\\"/mlfoundations/open_clip/blob/main/docs/PRETRAINED.md\\\"\\u003ehere\\u003c/a\\u003e, and zero-shot results for 38 datasets \\u003ca href=\\\"/mlfoundations/open_clip/blob/main/docs/openclip_results.csv\\\"\\u003ehere\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003ctable\\u003e\\n\\u003cthead\\u003e\\n\\u003ctr\\u003e\\n\\u003cth\\u003eModel\\u003c/th\\u003e\\n\\u003cth\\u003eTraining data\\u003c/th\\u003e\\n\\u003cth\\u003eResolution\\u003c/th\\u003e\\n\\u003cth\\u003e# of samples seen\\u003c/th\\u003e\\n\\u003cth\\u003eImageNet zero-shot acc.\\u003c/th\\u003e\\n\\u003c/tr\\u003e\\n\\u003c/thead\\u003e\\n\\u003ctbody\\u003e\\n\\u003ctr\\u003e\\n\\u003ctd\\u003eConvNext-Base\\u003c/td\\u003e\\n\\u003ctd\\u003eLAION-2B\\u003c/td\\u003e\\n\\u003ctd\\u003e256px\\u003c/td\\u003e\\n\\u003ctd\\u003e13B\\u003c/td\\u003e\\n\\u003ctd\\u003e71.5%\\u003c/td\\u003e\\n\\u003c/tr\\u003e\\n\\u003ctr\\u003e\\n\\u003ctd\\u003eConvNext-Large\\u003c/td\\u003e\\n\\u003ctd\\u003eLAION-2B\\u003c/td\\u003e\\n\\u003ctd\\u003e320px\\u003c/td\\u003e\\n\\u003ctd\\u003e29B\\u003c/td\\u003e\\n\\u003ctd\\u003e76.9%\\u003c/td\\u003e\\n\\u003c/tr\\u003e\\n\\u003ctr\\u003e\\n\\u003ctd\\u003eConvNext-XXLarge\\u003c/td\\u003e\\n\\u003ctd\\u003eLAION-2B\\u003c/td\\u003e\\n\\u003ctd\\u003e256px\\u003c/td\\u003e\\n\\u003ctd\\u003e34B\\u003c/td\\u003e\\n\\u003ctd\\u003e79.5%\\u003c/td\\u003e\\n\\u003c/tr\\u003e\\n\\u003ctr\\u003e\\n\\u003ctd\\u003eViT-B/32\\u003c/td\\u003e\\n\\u003ctd\\u003eDataComp-1B\\u003c/td\\u003e\\n\\u003ctd\\u003e256px\\u003c/td\\u003e\\n\\u003ctd\\u003e34B\\u003c/td\\u003e\\n\\u003ctd\\u003e72.8%\\u003c/td\\u003e\\n\\u003c/tr\\u003e\\n\\u003ctr\\u003e\\n\\u003ctd\\u003eViT-B/16\\u003c/td\\u003e\\n\\u003ctd\\u003eDataComp-1B\\u003c/td\\u003e\\n\\u003ctd\\u003e224px\\u003c/td\\u003e\\n\\u003ctd\\u003e13B\\u003c/td\\u003e\\n\\u003ctd\\u003e73.5%\\u003c/td\\u003e\\n\\u003c/tr\\u003e\\n\\u003ctr\\u003e\\n\\u003ctd\\u003eViT-L/14\\u003c/td\\u003e\\n\\u003ctd\\u003eLAION-2B\\u003c/td\\u003e\\n\\u003ctd\\u003e224px\\u003c/td\\u003e\\n\\u003ctd\\u003e32B\\u003c/td\\u003e\\n\\u003ctd\\u003e75.3%\\u003c/td\\u003e\\n\\u003c/tr\\u003e\\n\\u003ctr\\u003e\\n\\u003ctd\\u003eViT-H/14\\u003c/td\\u003e\\n\\u003ctd\\u003eLAION-2B\\u003c/td\\u003e\\n\\u003ctd\\u003e224px\\u003c/td\\u003e\\n\\u003ctd\\u003e32B\\u003c/td\\u003e\\n\\u003ctd\\u003e78.0%\\u003c/td\\u003e\\n\\u003c/tr\\u003e\\n\\u003ctr\\u003e\\n\\u003ctd\\u003eViT-L/14\\u003c/td\\u003e\\n\\u003ctd\\u003eDataComp-1B\\u003c/td\\u003e\\n\\u003ctd\\u003e224px\\u003c/td\\u003e\\n\\u003ctd\\u003e13B\\u003c/td\\u003e\\n\\u003ctd\\u003e79.2%\\u003c/td\\u003e\\n\\u003c/tr\\u003e\\n\\u003ctr\\u003e\\n\\u003ctd\\u003eViT-G/14\\u003c/td\\u003e\\n\\u003ctd\\u003eLAION-2B\\u003c/td\\u003e\\n\\u003ctd\\u003e224px\\u003c/td\\u003e\\n\\u003ctd\\u003e34B\\u003c/td\\u003e\\n\\u003ctd\\u003e80.1%\\u003c/td\\u003e\\n\\u003c/tr\\u003e\\n\\u003ctr\\u003e\\n\\u003ctd\\u003e\\u003c/td\\u003e\\n\\u003ctd\\u003e\\u003c/td\\u003e\\n\\u003ctd\\u003e\\u003c/td\\u003e\\n\\u003ctd\\u003e\\u003c/td\\u003e\\n\\u003ctd\\u003e\\u003c/td\\u003e\\n\\u003c/tr\\u003e\\n\\u003ctr\\u003e\\n\\u003ctd\\u003eViT-L/14\\u003c/td\\u003e\\n\\u003ctd\\u003eOpenAI's WIT\\u003c/td\\u003e\\n\\u003ctd\\u003e224px\\u003c/td\\u003e\\n\\u003ctd\\u003e13B\\u003c/td\\u003e\\n\\u003ctd\\u003e75.5%\\u003c/td\\u003e\\n\\u003c/tr\\u003e\\n\\u003c/tbody\\u003e\\n\\u003c/table\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eModel cards with additional model specific details can be found on the Hugging Face Hub under the OpenCLIP library tag: \\u003ca href=\\\"https://huggingface.co/models?library=open_clip\\\" rel=\\\"nofollow\\\"\\u003ehttps://huggingface.co/models?library=open_clip\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eIf you found this repository useful, please consider \\u003ca href=\\\"#citing\\\"\\u003eciting\\u003c/a\\u003e.\\nWe welcome anyone to submit an issue or send an email if you have any other requests or suggestions.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eNote that portions of \\u003ccode\\u003esrc/open_clip/\\u003c/code\\u003e modelling and tokenizer code are adaptations of OpenAI's official \\u003ca href=\\\"https://github.com/openai/CLIP\\\"\\u003erepository\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eApproach\\u003c/h2\\u003e\\u003ca id=\\\"user-content-approach\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Approach\\\" href=\\\"#approach\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003ctable\\u003e\\n\\u003cthead\\u003e\\n\\u003ctr\\u003e\\n\\u003cth align=\\\"center\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer nofollow\\\" href=\\\"https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/CLIP.png\\\"\\u003e\\u003cimg src=\\\"https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/CLIP.png\\\" alt=\\\"CLIP\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/th\\u003e\\n\\u003c/tr\\u003e\\n\\u003c/thead\\u003e\\n\\u003ctbody\\u003e\\n\\u003ctr\\u003e\\n\\u003ctd align=\\\"center\\\"\\u003eImage Credit: \\u003ca href=\\\"https://github.com/openai/CLIP\\\"\\u003ehttps://github.com/openai/CLIP\\u003c/a\\u003e\\u003c/td\\u003e\\n\\u003c/tr\\u003e\\n\\u003c/tbody\\u003e\\n\\u003c/table\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eUsage\\u003c/h2\\u003e\\u003ca id=\\\"user-content-usage\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Usage\\\" href=\\\"#usage\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"snippet-clipboard-content notranslate position-relative overflow-auto\\\" data-snippet-clipboard-copy-content=\\\"pip install open_clip_torch\\\"\\u003e\\u003cpre class=\\\"notranslate\\\"\\u003e\\u003ccode\\u003epip install open_clip_torch\\n\\u003c/code\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom PIL import Image\\nimport open_clip\\n\\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\\ntokenizer = open_clip.get_tokenizer('ViT-B-32')\\n\\nimage = preprocess(Image.open(\\u0026quot;CLIP.png\\u0026quot;)).unsqueeze(0)\\ntext = tokenizer([\\u0026quot;a diagram\\u0026quot;, \\u0026quot;a dog\\u0026quot;, \\u0026quot;a cat\\u0026quot;])\\n\\nwith torch.no_grad(), torch.cuda.amp.autocast():\\n    image_features = model.encode_image(image)\\n    text_features = model.encode_text(text)\\n    image_features /= image_features.norm(dim=-1, keepdim=True)\\n    text_features /= text_features.norm(dim=-1, keepdim=True)\\n\\n    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\\n\\nprint(\\u0026quot;Label probs:\\u0026quot;, text_probs)  # prints: [[1., 0., 0.]]\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003ePIL\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eImage\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eopen_clip\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003e_\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003epreprocess\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eopen_clip\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ecreate_model_and_transforms\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s\\\"\\u003e'ViT-B-32'\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003epretrained\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-s\\\"\\u003e'laion2b_s34b_b79k'\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003etokenizer\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eopen_clip\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eget_tokenizer\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s\\\"\\u003e'ViT-B-32'\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimage\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003epreprocess\\u003c/span\\u003e(\\u003cspan class=\\\"pl-v\\\"\\u003eImage\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eopen\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"CLIP.png\\\"\\u003c/span\\u003e)).\\u003cspan class=\\\"pl-en\\\"\\u003eunsqueeze\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003etext\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003etokenizer\\u003c/span\\u003e([\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"a diagram\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s\\\"\\u003e\\\"a dog\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s\\\"\\u003e\\\"a cat\\\"\\u003c/span\\u003e])\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003ewith\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eno_grad\\u003c/span\\u003e(), \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003ecuda\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eamp\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eautocast\\u003c/span\\u003e():\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_features\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eencode_image\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimage\\u003c/span\\u003e)\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003etext_features\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eencode_text\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003etext\\u003c/span\\u003e)\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_features\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e/=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_features\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003enorm\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e-\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ekeepdim\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e)\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003etext_features\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e/=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etext_features\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003enorm\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e-\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ekeepdim\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e)\\n\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003etext_probs\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e100.0\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e*\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_features\\u003c/span\\u003e @ \\u003cspan class=\\\"pl-s1\\\"\\u003etext_features\\u003c/span\\u003e.\\u003cspan class=\\\"pl-v\\\"\\u003eT\\u003c/span\\u003e).\\u003cspan class=\\\"pl-en\\\"\\u003esoftmax\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e-\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-en\\\"\\u003eprint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"Label probs:\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003etext_probs\\u003c/span\\u003e)  \\u003cspan class=\\\"pl-c\\\"\\u003e# prints: [[1., 0., 0.]]\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eSee also this \\u003ca href=\\\"https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb\\\" rel=\\\"nofollow\\\"\\u003e[Clip Colab]\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eTo compute billions of embeddings efficiently, you can use \\u003ca href=\\\"https://github.com/rom1504/clip-retrieval\\\"\\u003eclip-retrieval\\u003c/a\\u003e which has openclip support.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003ePretrained models\\u003c/h3\\u003e\\u003ca id=\\\"user-content-pretrained-models\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Pretrained models\\\" href=\\\"#pretrained-models\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eWe offer a simple model interface to instantiate both pre-trained and untrained models.\\nTo see which pretrained models are available, use the following code snippet.\\nMore details about our pretrained models are available \\u003ca href=\\\"/mlfoundations/open_clip/blob/main/docs/PRETRAINED.md\\\"\\u003ehere\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"\\u0026gt;\\u0026gt;\\u0026gt; import open_clip\\n\\u0026gt;\\u0026gt;\\u0026gt; open_clip.list_pretrained()\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e\\u0026gt;\\u0026gt;\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e\\u0026gt;\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eopen_clip\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c1\\\"\\u003e\\u0026gt;\\u0026gt;\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e\\u0026gt;\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eopen_clip\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003elist_pretrained\\u003c/span\\u003e()\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can find more about the models we support (e.g. number of parameters, FLOPs) in \\u003ca href=\\\"/mlfoundations/open_clip/blob/main/docs/model_profile.csv\\\"\\u003ethis table\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eNOTE: Many existing checkpoints use the QuickGELU activation from the original OpenAI models. This activation is actually less efficient than native torch.nn.GELU in recent versions of PyTorch. The model defaults are now nn.GELU, so one should use model definitions with \\u003ccode\\u003e-quickgelu\\u003c/code\\u003e postfix for the OpenCLIP pretrained weights. All OpenAI pretrained weights will always default to QuickGELU. One can also use the non \\u003ccode\\u003e-quickgelu\\u003c/code\\u003e model definitions with pretrained weights using QuickGELU but there will be an accuracy drop, for fine-tune that will likely vanish for longer runs.\\nFuture trained models will use nn.GELU.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eLoading models\\u003c/h3\\u003e\\u003ca id=\\\"user-content-loading-models\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Loading models\\\" href=\\\"#loading-models\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eModels can be loaded with \\u003ccode\\u003eopen_clip.create_model_and_transforms\\u003c/code\\u003e, as shown in the example below. The model name and corresponding \\u003ccode\\u003epretrained\\u003c/code\\u003e keys are compatible with the outputs of \\u003ccode\\u003eopen_clip.list_pretrained()\\u003c/code\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe \\u003ccode\\u003epretrained\\u003c/code\\u003e argument also accepts local paths, for example \\u003ccode\\u003e/path/to/my/b32.pt\\u003c/code\\u003e.\\nYou can also load checkpoints from huggingface this way. To do so, download the \\u003ccode\\u003eopen_clip_pytorch_model.bin\\u003c/code\\u003e file (for example, \\u003ca href=\\\"https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/blob/main/open_clip_pytorch_model.bin\\\" rel=\\\"nofollow\\\"\\u003ehttps://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/tree/main\\u003c/a\\u003e), and use \\u003ccode\\u003epretrained=/path/to/open_clip_pytorch_model.bin\\u003c/code\\u003e.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"# pretrained also accepts local paths\\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k') \\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-c\\\"\\u003e# pretrained also accepts local paths\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003e_\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003epreprocess\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eopen_clip\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ecreate_model_and_transforms\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s\\\"\\u003e'ViT-B-32'\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003epretrained\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-s\\\"\\u003e'laion2b_s34b_b79k'\\u003c/span\\u003e) \\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eFine-tuning on classification tasks\\u003c/h2\\u003e\\u003ca id=\\\"user-content-fine-tuning-on-classification-tasks\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Fine-tuning on classification tasks\\\" href=\\\"#fine-tuning-on-classification-tasks\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis repository is focused on training CLIP models. To fine-tune a \\u003cem\\u003etrained\\u003c/em\\u003e zero-shot model on a downstream classification task such as ImageNet, please see \\u003ca href=\\\"https://github.com/mlfoundations/wise-ft\\\"\\u003eour other repository: WiSE-FT\\u003c/a\\u003e. The \\u003ca href=\\\"https://github.com/mlfoundations/wise-ft\\\"\\u003eWiSE-FT repository\\u003c/a\\u003e contains code for our paper on \\u003ca href=\\\"https://arxiv.org/abs/2109.01903\\\" rel=\\\"nofollow\\\"\\u003eRobust Fine-tuning of Zero-shot Models\\u003c/a\\u003e, in which we introduce a technique for fine-tuning zero-shot models while preserving robustness under distribution shift.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eData\\u003c/h2\\u003e\\u003ca id=\\\"user-content-data\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Data\\\" href=\\\"#data\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eTo download datasets as webdataset, we recommend \\u003ca href=\\\"https://github.com/rom1504/img2dataset\\\"\\u003eimg2dataset\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eConceptual Captions\\u003c/h3\\u003e\\u003ca id=\\\"user-content-conceptual-captions\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Conceptual Captions\\\" href=\\\"#conceptual-captions\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eSee \\u003ca href=\\\"https://github.com/rom1504/img2dataset/blob/main/dataset_examples/cc3m.md\\\"\\u003ecc3m img2dataset example\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eYFCC and other datasets\\u003c/h3\\u003e\\u003ca id=\\\"user-content-yfcc-and-other-datasets\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: YFCC and other datasets\\\" href=\\\"#yfcc-and-other-datasets\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eIn addition to specifying the training data via CSV files as mentioned above, our codebase also supports \\u003ca href=\\\"https://github.com/webdataset/webdataset\\\"\\u003ewebdataset\\u003c/a\\u003e, which is recommended for larger scale datasets. The expected format is a series of \\u003ccode\\u003e.tar\\u003c/code\\u003e files. Each of these \\u003ccode\\u003e.tar\\u003c/code\\u003e files should contain two files for each training example, one for the image and one for the corresponding text. Both files should have the same name but different extensions. For instance, \\u003ccode\\u003eshard_001.tar\\u003c/code\\u003e could contain files such as \\u003ccode\\u003eabc.jpg\\u003c/code\\u003e and \\u003ccode\\u003eabc.txt\\u003c/code\\u003e. You can learn more about \\u003ccode\\u003ewebdataset\\u003c/code\\u003e at \\u003ca href=\\\"https://github.com/webdataset/webdataset\\\"\\u003ehttps://github.com/webdataset/webdataset\\u003c/a\\u003e. We use \\u003ccode\\u003e.tar\\u003c/code\\u003e files with 1,000 data points each, which we create using \\u003ca href=\\\"https://github.com/webdataset/tarp\\\"\\u003etarp\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can download the YFCC dataset from \\u003ca href=\\\"http://mmcommons.org/\\\" rel=\\\"nofollow\\\"\\u003eMultimedia Commons\\u003c/a\\u003e.\\nSimilar to OpenAI, we used a subset of YFCC to reach the aforementioned accuracy numbers.\\nThe indices of images in this subset are in \\u003ca href=\\\"https://github.com/openai/CLIP/blob/main/data/yfcc100m.md\\\"\\u003eOpenAI's CLIP repository\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eTraining CLIP\\u003c/h2\\u003e\\u003ca id=\\\"user-content-training-clip\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Training CLIP\\\" href=\\\"#training-clip\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eInstall\\u003c/h3\\u003e\\u003ca id=\\\"user-content-install\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Install\\\" href=\\\"#install\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eWe advise you first create a virtual environment with:\\u003c/p\\u003e\\n\\u003cdiv class=\\\"snippet-clipboard-content notranslate position-relative overflow-auto\\\" data-snippet-clipboard-copy-content=\\\"python3 -m venv .env\\nsource .env/bin/activate\\npip install -U pip\\\"\\u003e\\u003cpre class=\\\"notranslate\\\"\\u003e\\u003ccode\\u003epython3 -m venv .env\\nsource .env/bin/activate\\npip install -U pip\\n\\u003c/code\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can then install openclip for training with \\u003ccode\\u003epip install 'open_clip_torch[training]'\\u003c/code\\u003e.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch4 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eDevelopment\\u003c/h4\\u003e\\u003ca id=\\\"user-content-development\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Development\\\" href=\\\"#development\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eIf you want to make changes to contribute code, you can clone openclip then run \\u003ccode\\u003emake install\\u003c/code\\u003e in openclip folder (after creating a virtualenv)\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eInstall pip PyTorch as per \\u003ca href=\\\"https://pytorch.org/get-started/locally/\\\" rel=\\\"nofollow\\\"\\u003ehttps://pytorch.org/get-started/locally/\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou may run \\u003ccode\\u003emake install-training\\u003c/code\\u003e to install training deps\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch4 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eTesting\\u003c/h4\\u003e\\u003ca id=\\\"user-content-testing\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Testing\\\" href=\\\"#testing\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eTest can be run with \\u003ccode\\u003emake install-test\\u003c/code\\u003e then \\u003ccode\\u003emake test\\u003c/code\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ccode\\u003epython -m pytest -x -s -v tests -k \\\"training\\\"\\u003c/code\\u003e to run a specific test\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eRunning regression tests against a specific git revision or tag:\\u003c/p\\u003e\\n\\u003col dir=\\\"auto\\\"\\u003e\\n\\u003cli\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eGenerate testing data\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"python tests/util_test.py --model RN50 RN101 --save_model_list models.txt --git_revision 9d31b2ec4df6d8228f370ff20c8267ec6ba39383\\\"\\u003e\\u003cpre\\u003epython tests/util_test.py --model RN50 RN101 --save_model_list models.txt --git_revision 9d31b2ec4df6d8228f370ff20c8267ec6ba39383\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003cstrong\\u003e\\u003cem\\u003eWARNING\\u003c/em\\u003e: This will invoke git and modify your working tree, but will reset it to the current state after data has been generated! \\u003cbr\\u003e\\nDon't modify your working tree while test data is being generated this way.\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\u003c/li\\u003e\\n\\u003cli\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eRun regression tests\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"OPEN_CLIP_TEST_REG_MODELS=models.txt python -m pytest -x -s -v -m regression_test\\\"\\u003e\\u003cpre\\u003eOPEN_CLIP_TEST_REG_MODELS=models.txt python -m pytest -x -s -v -m regression_test\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003c/li\\u003e\\n\\u003c/ol\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eSample single-process running code:\\u003c/h3\\u003e\\u003ca id=\\\"user-content-sample-single-process-running-code\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Sample single-process running code:\\\" href=\\\"#sample-single-process-running-code\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"python -m training.main \\\\\\n    --save-frequency 1 \\\\\\n    --zeroshot-frequency 1 \\\\\\n    --report-to tensorboard \\\\\\n    --train-data=\\u0026quot;/path/to/train_data.csv\\u0026quot;  \\\\\\n    --val-data=\\u0026quot;/path/to/validation_data.csv\\u0026quot;  \\\\\\n    --csv-img-key filepath \\\\\\n    --csv-caption-key title \\\\\\n    --imagenet-val=/path/to/imagenet/root/val/ \\\\\\n    --warmup 10000 \\\\\\n    --batch-size=128 \\\\\\n    --lr=1e-3 \\\\\\n    --wd=0.1 \\\\\\n    --epochs=30 \\\\\\n    --workers=8 \\\\\\n    --model RN50\\\"\\u003e\\u003cpre\\u003epython -m training.main \\\\\\n    --save-frequency 1 \\\\\\n    --zeroshot-frequency 1 \\\\\\n    --report-to tensorboard \\\\\\n    --train-data=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e/path/to/train_data.csv\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e  \\\\\\n    --val-data=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e/path/to/validation_data.csv\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e  \\\\\\n    --csv-img-key filepath \\\\\\n    --csv-caption-key title \\\\\\n    --imagenet-val=/path/to/imagenet/root/val/ \\\\\\n    --warmup 10000 \\\\\\n    --batch-size=128 \\\\\\n    --lr=1e-3 \\\\\\n    --wd=0.1 \\\\\\n    --epochs=30 \\\\\\n    --workers=8 \\\\\\n    --model RN50\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eNote: \\u003ccode\\u003eimagenet-val\\u003c/code\\u003e is the path to the \\u003cem\\u003evalidation\\u003c/em\\u003e set of ImageNet for zero-shot evaluation, not the training set!\\nYou can remove this argument if you do not want to perform zero-shot evaluation on ImageNet throughout training. Note that the \\u003ccode\\u003eval\\u003c/code\\u003e folder should contain subfolders. If it does not, please use \\u003ca href=\\\"https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh\\\" rel=\\\"nofollow\\\"\\u003ethis script\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eMulti-GPU and Beyond\\u003c/h3\\u003e\\u003ca id=\\\"user-content-multi-gpu-and-beyond\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Multi-GPU and Beyond\\\" href=\\\"#multi-gpu-and-beyond\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis code has been battle tested up to 1024 A100s and offers a variety of solutions\\nfor distributed training. We include native support for SLURM clusters.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eAs the number of devices used to train increases, so does the space complexity of\\nthe the logit matrix. Using a na\u00efve all-gather scheme, space complexity will be\\n\\u003ccode\\u003eO(n^2)\\u003c/code\\u003e. Instead, complexity may become effectively linear if the flags\\n\\u003ccode\\u003e--gather-with-grad\\u003c/code\\u003e and \\u003ccode\\u003e--local-loss\\u003c/code\\u003e are used. This alteration results in one-to-one\\nnumerical results as the na\u00efve method.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch4 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eEpochs\\u003c/h4\\u003e\\u003ca id=\\\"user-content-epochs\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Epochs\\\" href=\\\"#epochs\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eFor larger datasets (eg Laion2B), we recommend setting \\u003ccode\\u003e--train-num-samples\\u003c/code\\u003e to a lower value than the full epoch, for example \\u003ccode\\u003e--train-num-samples 135646078\\u003c/code\\u003e to 1/16 of an epoch in conjunction with \\u003ccode\\u003e--dataset-resampled\\u003c/code\\u003e to do sampling with replacement. This allows having frequent checkpoints to evaluate more often.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch4 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003ePatch Dropout\\u003c/h4\\u003e\\u003ca id=\\\"user-content-patch-dropout\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Patch Dropout\\\" href=\\\"#patch-dropout\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2212.00794\\\" rel=\\\"nofollow\\\"\\u003eRecent research\\u003c/a\\u003e has shown that one can dropout half to three-quarters of the visual tokens, leading to up to 2-3x training speeds without loss of accuracy.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can set this on your visual transformer config with the key \\u003ccode\\u003epatch_dropout\\u003c/code\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eIn the paper, they also finetuned without the patch dropout at the end. You can do this with the command-line argument \\u003ccode\\u003e--force-patch-dropout 0.\\u003c/code\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch4 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eMultiple data sources\\u003c/h4\\u003e\\u003ca id=\\\"user-content-multiple-data-sources\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Multiple data sources\\\" href=\\\"#multiple-data-sources\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eOpenCLIP supports using multiple data sources, by separating different data paths with \\u003ccode\\u003e::\\u003c/code\\u003e.\\nFor instance, to train on CC12M and on LAION, one might use \\u003ccode\\u003e--train-data \\\"/data/cc12m/cc12m-train-{0000..2175}.tar::/data/LAION-400M/{00000..41455}.tar\\\"\\u003c/code\\u003e.\\nUsing \\u003ccode\\u003e--dataset-resampled\\u003c/code\\u003e is recommended for these cases.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eBy default, on expectation the amount of times the model will see a sample from each source is proportional to the size of the source.\\nFor instance, when training on one data source with size 400M and one with size 10M, samples from the first source are 40x more likely to be seen in expectation.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eWe also support different weighting of the data sources, by using the \\u003ccode\\u003e--train-data-upsampling-factors\\u003c/code\\u003e flag.\\nFor instance, using \\u003ccode\\u003e--train-data-upsampling-factors=1::1\\u003c/code\\u003e in the above scenario is equivalent to not using the flag, and \\u003ccode\\u003e--train-data-upsampling-factors=1::2\\u003c/code\\u003e is equivalent to upsampling the second data source twice.\\nIf you want to sample from data sources with the same frequency, the upsampling factors should be inversely proportional to the sizes of the data sources.\\nFor instance, if dataset \\u003ccode\\u003eA\\u003c/code\\u003e has 1000 samples and dataset \\u003ccode\\u003eB\\u003c/code\\u003e has 100 samples, you can use \\u003ccode\\u003e--train-data-upsampling-factors=0.001::0.01\\u003c/code\\u003e (or analogously, \\u003ccode\\u003e--train-data-upsampling-factors=1::10\\u003c/code\\u003e).\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch4 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eSingle-Node\\u003c/h4\\u003e\\u003ca id=\\\"user-content-single-node\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Single-Node\\\" href=\\\"#single-node\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eWe make use of \\u003ccode\\u003etorchrun\\u003c/code\\u003e to launch distributed jobs. The following launches a\\na job on a node of 4 GPUs:\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"cd open_clip/src\\ntorchrun --nproc_per_node 4 -m training.main \\\\\\n    --train-data '/data/cc12m/cc12m-train-{0000..2175}.tar' \\\\\\n    --train-num-samples 10968539 \\\\\\n    --dataset-type webdataset \\\\\\n    --batch-size 320 \\\\\\n    --precision amp \\\\\\n    --workers 4 \\\\\\n    --imagenet-val /data/imagenet/validation/\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003ecd\\u003c/span\\u003e open_clip/src\\ntorchrun --nproc_per_node 4 -m training.main \\\\\\n    --train-data \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e'\\u003c/span\\u003e/data/cc12m/cc12m-train-{0000..2175}.tar\\u003cspan class=\\\"pl-pds\\\"\\u003e'\\u003c/span\\u003e\\u003c/span\\u003e \\\\\\n    --train-num-samples 10968539 \\\\\\n    --dataset-type webdataset \\\\\\n    --batch-size 320 \\\\\\n    --precision amp \\\\\\n    --workers 4 \\\\\\n    --imagenet-val /data/imagenet/validation/\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch4 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eMulti-Node\\u003c/h4\\u003e\\u003ca id=\\\"user-content-multi-node\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Multi-Node\\\" href=\\\"#multi-node\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe same script above works, so long as users include information about the number\\nof nodes and host node.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"cd open_clip/src\\ntorchrun --nproc_per_node=4 \\\\\\n    --rdzv_endpoint=$HOSTE_NODE_ADDR \\\\\\n    -m training.main \\\\\\n    --train-data '/data/cc12m/cc12m-train-{0000..2175}.tar' \\\\\\n    --train-num-samples 10968539 \\\\\\n    --dataset-type webdataset \\\\\\n    --batch-size 320 \\\\\\n    --precision amp \\\\\\n    --workers 4 \\\\\\n    --imagenet-val /data/imagenet/validation/\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003ecd\\u003c/span\\u003e open_clip/src\\ntorchrun --nproc_per_node=4 \\\\\\n    --rdzv_endpoint=\\u003cspan class=\\\"pl-smi\\\"\\u003e$HOSTE_NODE_ADDR\\u003c/span\\u003e \\\\\\n    -m training.main \\\\\\n    --train-data \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e'\\u003c/span\\u003e/data/cc12m/cc12m-train-{0000..2175}.tar\\u003cspan class=\\\"pl-pds\\\"\\u003e'\\u003c/span\\u003e\\u003c/span\\u003e \\\\\\n    --train-num-samples 10968539 \\\\\\n    --dataset-type webdataset \\\\\\n    --batch-size 320 \\\\\\n    --precision amp \\\\\\n    --workers 4 \\\\\\n    --imagenet-val /data/imagenet/validation/\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch4 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eSLURM\\u003c/h4\\u003e\\u003ca id=\\\"user-content-slurm\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: SLURM\\\" href=\\\"#slurm\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis is likely the easiest solution to utilize. The following script was used to\\ntrain our largest models:\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"#!/bin/bash -x\\n#SBATCH --nodes=32\\n#SBATCH --gres=gpu:4\\n#SBATCH --ntasks-per-node=4\\n#SBATCH --cpus-per-task=6\\n#SBATCH --wait-all-nodes=1\\n#SBATCH --job-name=open_clip\\n#SBATCH --account=ACCOUNT_NAME\\n#SBATCH --partition PARTITION_NAME\\n\\neval \\u0026quot;$(/path/to/conda/bin/conda shell.bash hook)\\u0026quot; # init conda\\nconda activate open_clip\\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\\nexport MASTER_PORT=12802\\n\\nmaster_addr=$(scontrol show hostnames \\u0026quot;$SLURM_JOB_NODELIST\\u0026quot; | head -n 1)\\nexport MASTER_ADDR=$master_addr\\n\\ncd /shared/open_clip\\nexport PYTHONPATH=\\u0026quot;$PYTHONPATH:$PWD/src\\u0026quot;\\nsrun --cpu_bind=v --accel-bind=gn python -u src/training/main.py \\\\\\n    --save-frequency 1 \\\\\\n    --report-to tensorboard \\\\\\n    --train-data=\\u0026quot;/data/LAION-400M/{00000..41455}.tar\\u0026quot; \\\\\\n    --warmup 2000 \\\\\\n    --batch-size=256 \\\\\\n    --epochs=32 \\\\\\n    --workers=8 \\\\\\n    --model ViT-B-32 \\\\\\n    --name \\u0026quot;ViT-B-32-Vanilla\\u0026quot; \\\\\\n    --seed 0 \\\\\\n    --local-loss \\\\\\n    --gather-with-grad\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-c\\\"\\u003e\\u003cspan class=\\\"pl-c\\\"\\u003e#!\\u003c/span\\u003e/bin/bash -x\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e\\u003cspan class=\\\"pl-c\\\"\\u003e#\\u003c/span\\u003eSBATCH --nodes=32\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e\\u003cspan class=\\\"pl-c\\\"\\u003e#\\u003c/span\\u003eSBATCH --gres=gpu:4\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e\\u003cspan class=\\\"pl-c\\\"\\u003e#\\u003c/span\\u003eSBATCH --ntasks-per-node=4\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e\\u003cspan class=\\\"pl-c\\\"\\u003e#\\u003c/span\\u003eSBATCH --cpus-per-task=6\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e\\u003cspan class=\\\"pl-c\\\"\\u003e#\\u003c/span\\u003eSBATCH --wait-all-nodes=1\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e\\u003cspan class=\\\"pl-c\\\"\\u003e#\\u003c/span\\u003eSBATCH --job-name=open_clip\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e\\u003cspan class=\\\"pl-c\\\"\\u003e#\\u003c/span\\u003eSBATCH --account=ACCOUNT_NAME\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e\\u003cspan class=\\\"pl-c\\\"\\u003e#\\u003c/span\\u003eSBATCH --partition PARTITION_NAME\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c1\\\"\\u003eeval\\u003c/span\\u003e \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e$(\\u003c/span\\u003e/path/to/conda/bin/conda shell.bash hook\\u003cspan class=\\\"pl-pds\\\"\\u003e)\\u003c/span\\u003e\\u003c/span\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e \\u003cspan class=\\\"pl-c\\\"\\u003e\\u003cspan class=\\\"pl-c\\\"\\u003e#\\u003c/span\\u003e init conda\\u003c/span\\u003e\\nconda activate open_clip\\n\\u003cspan class=\\\"pl-k\\\"\\u003eexport\\u003c/span\\u003e CUDA_VISIBLE_DEVICES=0,1,2,3\\n\\u003cspan class=\\\"pl-k\\\"\\u003eexport\\u003c/span\\u003e MASTER_PORT=12802\\n\\nmaster_addr=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e$(\\u003c/span\\u003escontrol show hostnames \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003cspan class=\\\"pl-smi\\\"\\u003e$SLURM_JOB_NODELIST\\u003c/span\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003e|\\u003c/span\\u003e head -n 1\\u003cspan class=\\\"pl-pds\\\"\\u003e)\\u003c/span\\u003e\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003eexport\\u003c/span\\u003e MASTER_ADDR=\\u003cspan class=\\\"pl-smi\\\"\\u003e$master_addr\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c1\\\"\\u003ecd\\u003c/span\\u003e /shared/open_clip\\n\\u003cspan class=\\\"pl-k\\\"\\u003eexport\\u003c/span\\u003e PYTHONPATH=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003cspan class=\\\"pl-smi\\\"\\u003e$PYTHONPATH\\u003c/span\\u003e:\\u003cspan class=\\\"pl-smi\\\"\\u003e$PWD\\u003c/span\\u003e/src\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e\\nsrun --cpu_bind=v --accel-bind=gn python -u src/training/main.py \\\\\\n    --save-frequency 1 \\\\\\n    --report-to tensorboard \\\\\\n    --train-data=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e/data/LAION-400M/{00000..41455}.tar\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e \\\\\\n    --warmup 2000 \\\\\\n    --batch-size=256 \\\\\\n    --epochs=32 \\\\\\n    --workers=8 \\\\\\n    --model ViT-B-32 \\\\\\n    --name \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003eViT-B-32-Vanilla\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e \\\\\\n    --seed 0 \\\\\\n    --local-loss \\\\\\n    --gather-with-grad\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eResuming from a checkpoint:\\u003c/h3\\u003e\\u003ca id=\\\"user-content-resuming-from-a-checkpoint\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Resuming from a checkpoint:\\\" href=\\\"#resuming-from-a-checkpoint\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"python -m training.main \\\\\\n    --train-data=\\u0026quot;/path/to/train_data.csv\\u0026quot; \\\\\\n    --val-data=\\u0026quot;/path/to/validation_data.csv\\u0026quot;  \\\\\\n    --resume /path/to/checkpoints/epoch_K.pt\\\"\\u003e\\u003cpre\\u003epython -m training.main \\\\\\n    --train-data=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e/path/to/train_data.csv\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e \\\\\\n    --val-data=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e/path/to/validation_data.csv\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e  \\\\\\n    --resume /path/to/checkpoints/epoch_K.pt\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eTraining CoCa:\\u003c/h3\\u003e\\u003ca id=\\\"user-content-training-coca\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Training CoCa:\\\" href=\\\"#training-coca\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eTraining \\u003ca href=\\\"https://arxiv.org/abs/2205.01917\\\" rel=\\\"nofollow\\\"\\u003eCoCa\\u003c/a\\u003e models is enabled through specifying a CoCa config using the \\u003ccode\\u003e--model\\u003c/code\\u003e parameter of the training script. Currently available configs are \\\"coca_base\\\", \\\"coca_ViT-B-32\\\", and \\\"coca_roberta-ViT-B-32\\\" (which uses RoBERTa as the text encoder). CoCa configs are different from CLIP configs because they have an additional \\\"multimodal_cfg\\\" component which specifies parameters for the multimodal text decoder. Here's an example from the coca_ViT-B-32 config:\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-json notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"\\u0026quot;multimodal_cfg\\u0026quot;: {\\n\\t\\u0026quot;context_length\\u0026quot;: 76,\\n\\t\\u0026quot;vocab_size\\u0026quot;: 49408,\\n\\t\\u0026quot;width\\u0026quot;: 512,\\n\\t\\u0026quot;heads\\u0026quot;: 8,\\n\\t\\u0026quot;layers\\u0026quot;: 12,\\n\\t\\u0026quot;latent_dim\\u0026quot;: 512,\\n\\t\\u0026quot;attn_pooler_heads\\u0026quot;: 8\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-ent\\\"\\u003e\\\"multimodal_cfg\\\"\\u003c/span\\u003e: {\\n\\t\\u003cspan class=\\\"pl-ent\\\"\\u003e\\\"context_length\\\"\\u003c/span\\u003e: \\u003cspan class=\\\"pl-c1\\\"\\u003e76\\u003c/span\\u003e,\\n\\t\\u003cspan class=\\\"pl-ent\\\"\\u003e\\\"vocab_size\\\"\\u003c/span\\u003e: \\u003cspan class=\\\"pl-c1\\\"\\u003e49408\\u003c/span\\u003e,\\n\\t\\u003cspan class=\\\"pl-ent\\\"\\u003e\\\"width\\\"\\u003c/span\\u003e: \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n\\t\\u003cspan class=\\\"pl-ent\\\"\\u003e\\\"heads\\\"\\u003c/span\\u003e: \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n\\t\\u003cspan class=\\\"pl-ent\\\"\\u003e\\\"layers\\\"\\u003c/span\\u003e: \\u003cspan class=\\\"pl-c1\\\"\\u003e12\\u003c/span\\u003e,\\n\\t\\u003cspan class=\\\"pl-ent\\\"\\u003e\\\"latent_dim\\\"\\u003c/span\\u003e: \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n\\t\\u003cspan class=\\\"pl-ent\\\"\\u003e\\\"attn_pooler_heads\\\"\\u003c/span\\u003e: \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eCredit to \\u003ca href=\\\"https://github.com/lucidrains\\\"\\u003elucidrains\\u003c/a\\u003e for \\u003ca href=\\\"https://github.com/lucidrains/CoCa-pytorch\\\"\\u003einitial code\\u003c/a\\u003e, \\u003ca href=\\\"https://github.com/gpucce\\\"\\u003egpucce\\u003c/a\\u003e for adapting the code to open_clip, and \\u003ca href=\\\"https://github.com/iejMac\\\"\\u003eiejMac\\u003c/a\\u003e for training the models.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eGenerating text with CoCa\\u003c/h3\\u003e\\u003ca id=\\\"user-content-generating-text-with-coca\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Generating text with CoCa\\\" href=\\\"#generating-text-with-coca\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import open_clip\\nimport torch\\nfrom PIL import Image\\n\\nmodel, _, transform = open_clip.create_model_and_transforms(\\n  model_name=\\u0026quot;coca_ViT-L-14\\u0026quot;,\\n  pretrained=\\u0026quot;mscoco_finetuned_laion2B-s13B-b90k\\u0026quot;\\n)\\n\\nim = Image.open(\\u0026quot;cat.jpg\\u0026quot;).convert(\\u0026quot;RGB\\u0026quot;)\\nim = transform(im).unsqueeze(0)\\n\\nwith torch.no_grad(), torch.cuda.amp.autocast():\\n  generated = model.generate(im)\\n\\nprint(open_clip.decode(generated[0]).split(\\u0026quot;\\u0026lt;end_of_text\\u0026gt;\\u0026quot;)[0].replace(\\u0026quot;\\u0026lt;start_of_text\\u0026gt;\\u0026quot;, \\u0026quot;\\u0026quot;))\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eopen_clip\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003ePIL\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eImage\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003e_\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003etransform\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eopen_clip\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ecreate_model_and_transforms\\u003c/span\\u003e(\\n  \\u003cspan class=\\\"pl-s1\\\"\\u003emodel_name\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"coca_ViT-L-14\\\"\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s1\\\"\\u003epretrained\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"mscoco_finetuned_laion2B-s13B-b90k\\\"\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eImage\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eopen\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"cat.jpg\\\"\\u003c/span\\u003e).\\u003cspan class=\\\"pl-en\\\"\\u003econvert\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"RGB\\\"\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003etransform\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eim\\u003c/span\\u003e).\\u003cspan class=\\\"pl-en\\\"\\u003eunsqueeze\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003ewith\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eno_grad\\u003c/span\\u003e(), \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003ecuda\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eamp\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eautocast\\u003c/span\\u003e():\\n  \\u003cspan class=\\\"pl-s1\\\"\\u003egenerated\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003egenerate\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eim\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-en\\\"\\u003eprint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eopen_clip\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003edecode\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003egenerated\\u003c/span\\u003e[\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e]).\\u003cspan class=\\\"pl-en\\\"\\u003esplit\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"\\u0026lt;end_of_text\\u0026gt;\\\"\\u003c/span\\u003e)[\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e].\\u003cspan class=\\\"pl-en\\\"\\u003ereplace\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"\\u0026lt;start_of_text\\u0026gt;\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s\\\"\\u003e\\\"\\\"\\u003c/span\\u003e))\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eSee also this \\u003ca href=\\\"https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb\\\" rel=\\\"nofollow\\\"\\u003e[Coca Colab]\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eFine Tuning CoCa\\u003c/h3\\u003e\\u003ca id=\\\"user-content-fine-tuning-coca\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Fine Tuning CoCa\\\" href=\\\"#fine-tuning-coca\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eTo fine-tune coca on mscoco, first create the dataset, one way is using a csvdataset and perhaps the simplest way to do it is using \\u003ca href=\\\"https://github.com/LAION-AI/CLIP_benchmark\\\"\\u003eCLIP_benchmark\\u003c/a\\u003e which in turn uses \\u003ca href=\\\"https://github.com/cocodataset/cocoapi\\\"\\u003epycocotools\\u003c/a\\u003e (that can be used also by itself).\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"from clip_benchmark.datasets.builder import build_dataset\\nimport pandas as pd\\nimport os\\n\\nroot_path = \\u0026quot;path/to/data/dir\\u0026quot; # set this to smth meaningful\\nds = build_dataset(\\u0026quot;mscoco_captions\\u0026quot;, root=root_path, split=\\u0026quot;train\\u0026quot;) # this downloads the dataset if it is not there already\\ncoco = ds.coco\\nimgs = coco.loadImgs(coco.getImgIds())\\nfuture_df = {\\u0026quot;filepath\\u0026quot;:[], \\u0026quot;title\\u0026quot;:[]}\\nfor img in imgs:\\n    caps = coco.imgToAnns[img[\\u0026quot;id\\u0026quot;]]\\n    for cap in caps:\\n        future_df[\\u0026quot;filepath\\u0026quot;].append(img[\\u0026quot;file_name\\u0026quot;])\\n        future_df[\\u0026quot;title\\u0026quot;].append(cap[\\u0026quot;caption\\u0026quot;])\\npd.DataFrame.from_dict(future_df).to_csv(\\n  os.path.join(root_path, \\u0026quot;train2014.csv\\u0026quot;), index=False, sep=\\u0026quot;\\\\t\\u0026quot;\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eclip_benchmark\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003edatasets\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003ebuilder\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ebuild_dataset\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003epandas\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eas\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003epd\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eos\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eroot_path\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s\\\"\\u003e\\\"path/to/data/dir\\\"\\u003c/span\\u003e \\u003cspan class=\\\"pl-c\\\"\\u003e# set this to smth meaningful\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ebuild_dataset\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"mscoco_captions\\\"\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eroot\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-s1\\\"\\u003eroot_path\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003esplit\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"train\\\"\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# this downloads the dataset if it is not there already\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ecoco\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eds\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003ecoco\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimgs\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ecoco\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eloadImgs\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ecoco\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003egetImgIds\\u003c/span\\u003e())\\n\\u003cspan class=\\\"pl-s1\\\"\\u003efuture_df\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e {\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"filepath\\\"\\u003c/span\\u003e:[], \\u003cspan class=\\\"pl-s\\\"\\u003e\\\"title\\\"\\u003c/span\\u003e:[]}\\n\\u003cspan class=\\\"pl-k\\\"\\u003efor\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003ein\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eimgs\\u003c/span\\u003e:\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecaps\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ecoco\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eimgToAnns\\u003c/span\\u003e[\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e[\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"id\\\"\\u003c/span\\u003e]]\\n    \\u003cspan class=\\\"pl-k\\\"\\u003efor\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ecap\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003ein\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ecaps\\u003c/span\\u003e:\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003efuture_df\\u003c/span\\u003e[\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"filepath\\\"\\u003c/span\\u003e].\\u003cspan class=\\\"pl-en\\\"\\u003eappend\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e[\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"file_name\\\"\\u003c/span\\u003e])\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003efuture_df\\u003c/span\\u003e[\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"title\\\"\\u003c/span\\u003e].\\u003cspan class=\\\"pl-en\\\"\\u003eappend\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ecap\\u003c/span\\u003e[\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"caption\\\"\\u003c/span\\u003e])\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epd\\u003c/span\\u003e.\\u003cspan class=\\\"pl-v\\\"\\u003eDataFrame\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003efrom_dict\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003efuture_df\\u003c/span\\u003e).\\u003cspan class=\\\"pl-en\\\"\\u003eto_csv\\u003c/span\\u003e(\\n  \\u003cspan class=\\\"pl-s1\\\"\\u003eos\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003epath\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ejoin\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eroot_path\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s\\\"\\u003e\\\"train2014.csv\\\"\\u003c/span\\u003e), \\u003cspan class=\\\"pl-s1\\\"\\u003eindex\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003eFalse\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003esep\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-s\\\"\\u003e\\\"\\u003cspan class=\\\"pl-cce\\\"\\u003e\\\\t\\u003c/span\\u003e\\\"\\u003c/span\\u003e\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis should create a csv dataset that one can use to fine-tune coca with open_clip\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"python -m training.main \\\\\\n    --dataset-type \\u0026quot;csv\\u0026quot; \\\\\\n    --train-data \\u0026quot;path/to/data/dir/train2014.csv\\u0026quot; \\\\\\n    --warmup 1000 \\\\\\n    --batch-size 128 \\\\\\n    --lr 1e-5 \\\\\\n    --wd 0.1 \\\\\\n    --epochs 1 \\\\\\n    --workers 3 \\\\\\n    --model \\u0026quot;coca_ViT-L-14\\u0026quot; \\\\\\n    --report-to \\u0026quot;wandb\\u0026quot; \\\\\\n    --coca-contrastive-loss-weight 0 \\\\\\n    --coca-caption-loss-weight 1 \\\\\\n    --log-every-n-steps 100\\\"\\u003e\\u003cpre\\u003epython -m training.main \\\\\\n    --dataset-type \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003ecsv\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e \\\\\\n    --train-data \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003epath/to/data/dir/train2014.csv\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e \\\\\\n    --warmup 1000 \\\\\\n    --batch-size 128 \\\\\\n    --lr 1e-5 \\\\\\n    --wd 0.1 \\\\\\n    --epochs 1 \\\\\\n    --workers 3 \\\\\\n    --model \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003ecoca_ViT-L-14\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e \\\\\\n    --report-to \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003ewandb\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e \\\\\\n    --coca-contrastive-loss-weight 0 \\\\\\n    --coca-caption-loss-weight 1 \\\\\\n    --log-every-n-steps 100\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis is a general setting, open_clip has very parameters that can be set, \\u003ccode\\u003epython -m training.main --help\\u003c/code\\u003e should show them. The only relevant change compared to pre-training are the two arguments\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"--coca-contrastive-loss-weight 0\\n--coca-caption-loss-weight 1\\\"\\u003e\\u003cpre\\u003e--coca-contrastive-loss-weight 0\\n--coca-caption-loss-weight 1\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003ewhich make the model only train the generative side.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eTraining with pre-trained language models as text encoder:\\u003c/h3\\u003e\\u003ca id=\\\"user-content-training-with-pre-trained-language-models-as-text-encoder\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Training with pre-trained language models as text encoder:\\\" href=\\\"#training-with-pre-trained-language-models-as-text-encoder\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eIf you wish to use different language models as the text encoder for CLIP you can do so by using one of the Hugging Face model configs in \\u003ccode\\u003esrc/open_clip/model_configs\\u003c/code\\u003e and passing in it's tokenizer as the \\u003ccode\\u003e--model\\u003c/code\\u003e and \\u003ccode\\u003e--hf-tokenizer-name\\u003c/code\\u003e parameters respectively. Currently we only support RoBERTa (\\\"test-roberta\\\" config), however adding new models should be trivial. You can also determine how many layers, from the end, to leave unfrozen with the \\u003ccode\\u003e--lock-text-unlocked-layers\\u003c/code\\u003e parameter. Here's an example command to train CLIP with the RoBERTa LM that has it's last 10 layers unfrozen:\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"python -m training.main \\\\\\n         --train-data=\\u0026quot;pipe:aws s3 cp s3://s-mas/cc3m/{00000..00329}.tar -\\u0026quot; \\\\\\n         --train-num-samples 3000000 \\\\\\n         --val-data=\\u0026quot;pipe:aws s3 cp s3://s-mas/cc3m/{00330..00331}.tar -\\u0026quot; \\\\\\n         --val-num-samples 10000 \\\\\\n         --dataset-type webdataset \\\\\\n         --batch-size 256 \\\\\\n         --warmup 2000 \\\\\\n         --epochs 10 \\\\\\n         --lr 5e-4 \\\\\\n         --precision amp \\\\\\n         --workers 6 \\\\\\n         --model \\u0026quot;roberta-ViT-B-32\\u0026quot; \\\\\\n         --lock-text \\\\\\n         --lock-text-unlocked-layers 10 \\\\\\n         --name \\u0026quot;10_unfrozen\\u0026quot; \\\\\\n         --report-to \\u0026quot;tensorboard\\u0026quot; \\\\\\\"\\u003e\\u003cpre\\u003epython -m training.main \\\\\\n         --train-data=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003epipe:aws s3 cp s3://s-mas/cc3m/{00000..00329}.tar -\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e \\\\\\n         --train-num-samples 3000000 \\\\\\n         --val-data=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003epipe:aws s3 cp s3://s-mas/cc3m/{00330..00331}.tar -\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e \\\\\\n         --val-num-samples 10000 \\\\\\n         --dataset-type webdataset \\\\\\n         --batch-size 256 \\\\\\n         --warmup 2000 \\\\\\n         --epochs 10 \\\\\\n         --lr 5e-4 \\\\\\n         --precision amp \\\\\\n         --workers 6 \\\\\\n         --model \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003eroberta-ViT-B-32\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e \\\\\\n         --lock-text \\\\\\n         --lock-text-unlocked-layers 10 \\\\\\n         --name \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e10_unfrozen\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e \\\\\\n         --report-to \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003etensorboard\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e \\\\\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eLoss Curves\\u003c/h3\\u003e\\u003ca id=\\\"user-content-loss-curves\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Loss Curves\\\" href=\\\"#loss-curves\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eWhen run on a machine with 8 GPUs the command should produce the following training curve for Conceptual Captions:\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer nofollow\\\" href=\\\"https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/clip_zeroshot.png\\\"\\u003e\\u003cimg src=\\\"https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/clip_zeroshot.png\\\" alt=\\\"CLIP zero shot training curve\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eMore detailed curves for Conceptual Captions are given at \\u003ca href=\\\"/mlfoundations/open_clip/blob/main/docs/clip_conceptual_captions.md\\\"\\u003e/docs/clip_conceptual_captions.md\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eWhen training a RN50 on YFCC the same hyperparameters as above are used, with the exception of \\u003ccode\\u003elr=5e-4\\u003c/code\\u003e and \\u003ccode\\u003eepochs=32\\u003c/code\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eNote that to use another model, like \\u003ccode\\u003eViT-B/32\\u003c/code\\u003e or \\u003ccode\\u003eRN50x4\\u003c/code\\u003e or \\u003ccode\\u003eRN50x16\\u003c/code\\u003e or \\u003ccode\\u003eViT-B/16\\u003c/code\\u003e, specify with \\u003ccode\\u003e--model RN50x4\\u003c/code\\u003e.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eLogging\\u003c/h3\\u003e\\u003ca id=\\\"user-content-logging\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Logging\\\" href=\\\"#logging\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eFor tensorboard logging, run:\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"tensorboard --logdir=logs/tensorboard/ --port=7777\\\"\\u003e\\u003cpre\\u003etensorboard --logdir=logs/tensorboard/ --port=7777\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eFor wandb logging, we recommend looking at the \\u003ccode\\u003estep\\u003c/code\\u003e variable instead of \\u003ccode\\u003eStep\\u003c/code\\u003e, since the later was not properly set in earlier versions of this codebase.\\nFor older runs with models trained before \\u003ca class=\\\"issue-link js-issue-link\\\" data-error-text=\\\"Failed to load title\\\" data-id=\\\"1870186659\\\" data-permission-text=\\\"Title is private\\\" data-url=\\\"https://github.com/mlfoundations/open_clip/issues/613\\\" data-hovercard-type=\\\"pull_request\\\" data-hovercard-url=\\\"/mlfoundations/open_clip/pull/613/hovercard\\\" href=\\\"https://github.com/mlfoundations/open_clip/pull/613\\\"\\u003e#613\\u003c/a\\u003e, the \\u003ccode\\u003eStep\\u003c/code\\u003e variable should be ignored.\\nFor newer runs, after that PR, the two variables are the same.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eEvaluation / Zero-Shot\\u003c/h2\\u003e\\u003ca id=\\\"user-content-evaluation--zero-shot\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Evaluation / Zero-Shot\\\" href=\\\"#evaluation--zero-shot\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eWe recommend \\u003ca href=\\\"https://github.com/LAION-AI/CLIP_benchmark#how-to-use\\\"\\u003ehttps://github.com/LAION-AI/CLIP_benchmark#how-to-use\\u003c/a\\u003e for systematic evaluation on 40 datasets.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eEvaluating local checkpoint:\\u003c/h3\\u003e\\u003ca id=\\\"user-content-evaluating-local-checkpoint\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Evaluating local checkpoint:\\\" href=\\\"#evaluating-local-checkpoint\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"python -m training.main \\\\\\n    --val-data=\\u0026quot;/path/to/validation_data.csv\\u0026quot;  \\\\\\n    --model RN101 \\\\\\n    --pretrained /path/to/checkpoints/epoch_K.pt\\\"\\u003e\\u003cpre\\u003epython -m training.main \\\\\\n    --val-data=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e/path/to/validation_data.csv\\u003cspan class=\\\"pl-pds\\\"\\u003e\\\"\\u003c/span\\u003e\\u003c/span\\u003e  \\\\\\n    --model RN101 \\\\\\n    --pretrained /path/to/checkpoints/epoch_K.pt\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eEvaluating hosted pretrained checkpoint on ImageNet zero-shot prediction:\\u003c/h3\\u003e\\u003ca id=\\\"user-content-evaluating-hosted-pretrained-checkpoint-on-imagenet-zero-shot-prediction\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Evaluating hosted pretrained checkpoint on ImageNet zero-shot prediction:\\\" href=\\\"#evaluating-hosted-pretrained-checkpoint-on-imagenet-zero-shot-prediction\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"python -m training.main \\\\\\n    --imagenet-val /path/to/imagenet/validation \\\\\\n    --model ViT-B-32-quickgelu \\\\\\n    --pretrained laion400m_e32\\\"\\u003e\\u003cpre\\u003epython -m training.main \\\\\\n    --imagenet-val /path/to/imagenet/validation \\\\\\n    --model ViT-B-32-quickgelu \\\\\\n    --pretrained laion400m_e32\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eModel distillation\\u003c/h3\\u003e\\u003ca id=\\\"user-content-model-distillation\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Model distillation\\\" href=\\\"#model-distillation\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can distill from a pre-trained by using \\u003ccode\\u003e--distill-model\\u003c/code\\u003e and \\u003ccode\\u003e--distill-pretrained\\u003c/code\\u003e to specify the model you'd like to distill from.\\nFor instance, to distill from OpenAI ViT-L/14 use \\u003ccode\\u003e--distill-model ViT-L-14 --distill-pretrained openai\\u003c/code\\u003e.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eGradient accumulation\\u003c/h3\\u003e\\u003ca id=\\\"user-content-gradient-accumulation\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Gradient accumulation\\\" href=\\\"#gradient-accumulation\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eTo simulate larger batches use \\u003ccode\\u003e--accum-freq k\\u003c/code\\u003e. If per gpu batch size, \\u003ccode\\u003e--batch-size\\u003c/code\\u003e, is \\u003ccode\\u003em\\u003c/code\\u003e, then the effective batch size will be \\u003ccode\\u003ek * m * num_gpus\\u003c/code\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eWhen increasing \\u003ccode\\u003e--accum-freq\\u003c/code\\u003e from its default of 1, samples/s will remain approximately constant (batch size will double, as will time-per-batch). It is recommended to use other features to reduce batch size such as \\u003ccode\\u003e--grad-checkpointing --local-loss --gather-with-grad\\u003c/code\\u003e before increasing \\u003ccode\\u003e--accum-freq\\u003c/code\\u003e. \\u003ccode\\u003e--accum-freq\\u003c/code\\u003e can be used in addition to these features.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eInstead of 1 forward pass per example, there are now 2 forward passes per-example. However, the first is done with \\u003ccode\\u003etorch.no_grad\\u003c/code\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThere is some additional GPU memory required --- the features and data from all \\u003ccode\\u003em\\u003c/code\\u003e batches are stored in memory.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThere are also \\u003ccode\\u003em\\u003c/code\\u003e loss computations instead of the usual 1.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eFor more information see Cui et al. (\\u003ca href=\\\"https://arxiv.org/abs/2112.09331\\\" rel=\\\"nofollow\\\"\\u003ehttps://arxiv.org/abs/2112.09331\\u003c/a\\u003e) or Pham et al. (\\u003ca href=\\\"https://arxiv.org/abs/2111.10050\\\" rel=\\\"nofollow\\\"\\u003ehttps://arxiv.org/abs/2111.10050\\u003c/a\\u003e).\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eInt8 Support\\u003c/h3\\u003e\\u003ca id=\\\"user-content-int8-support\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Int8 Support\\\" href=\\\"#int8-support\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eWe have beta support for int8 training and inference.\\nYou can enable int8 training with \\u003ccode\\u003e--use-bnb-linear SwitchBackLinearGlobal\\u003c/code\\u003e or \\u003ccode\\u003e--use-bnb-linear SwitchBackLinearGlobalMemEfficient\\u003c/code\\u003e.\\nPlease see the bitsandbytes library for definitions for these layers.\\nFor CLIP VIT-Huge this should currently correspond to a 10% training speedup with no accuracy loss.\\nMore speedups comin when the attention layer is refactored so that linear layers man be replaced there, too.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eSee the tutorial \\u003ca href=\\\"https://github.com/mlfoundations/open_clip/blob/main/tutorials/int8_tutorial.ipynb\\\"\\u003ehttps://github.com/mlfoundations/open_clip/blob/main/tutorials/int8_tutorial.ipynb\\u003c/a\\u003e or \\u003ca href=\\\"https://arxiv.org/abs/2304.13013\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eSupport for remote loading/training\\u003c/h3\\u003e\\u003ca id=\\\"user-content-support-for-remote-loadingtraining\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Support for remote loading/training\\\" href=\\\"#support-for-remote-loadingtraining\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eIt is always possible to resume directly from a remote file, e.g., a file in an s3 bucket. Just set \\u003ccode\\u003e--resume s3://\\u0026lt;path-to-checkpoint\\u0026gt; \\u003c/code\\u003e.\\nThis will work with any filesystem supported by \\u003ccode\\u003efsspec\\u003c/code\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eIt is also possible to train \\u003ccode\\u003eopen_clip\\u003c/code\\u003e models while continuously backing up to s3. This can help to avoid slow local file systems.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eSay that your node has a local ssd \\u003ccode\\u003e/scratch\\u003c/code\\u003e, an s3 bucket \\u003ccode\\u003es3://\\u0026lt;path-to-bucket\\u0026gt;\\u003c/code\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eIn that case, set \\u003ccode\\u003e--logs /scratch\\u003c/code\\u003e and \\u003ccode\\u003e--remote-sync s3://\\u0026lt;path-to-bucket\\u0026gt;\\u003c/code\\u003e. Then, a background process will sync \\u003ccode\\u003e/scratch/\\u0026lt;run-name\\u0026gt;\\u003c/code\\u003e to \\u003ccode\\u003es3://\\u0026lt;path-to-bucket\\u0026gt;/\\u0026lt;run-name\\u0026gt;\\u003c/code\\u003e. After syncing, the background process will sleep for \\u003ccode\\u003e--remote-sync-frequency\\u003c/code\\u003e seconds, which defaults to 5 minutes.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThere is also experimental support for syncing to other remote file systems, not just s3. To do so, specify \\u003ccode\\u003e--remote-sync-protocol fsspec\\u003c/code\\u003e. However, this is currently very slow and not recommended.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eAlso, to optionally avoid saving too many checkpoints locally when using these features, you can use \\u003ccode\\u003e--delete-previous-checkpoint\\u003c/code\\u003e which deletes the previous checkpoint after saving a new one.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eNote: if you are using this feature with \\u003ccode\\u003e--resume latest\\u003c/code\\u003e, there are a few warnings. First, use with \\u003ccode\\u003e--save-most-recent\\u003c/code\\u003e is not supported. Second, only \\u003ccode\\u003es3\\u003c/code\\u003e is supported. Finally, since the sync happens in the background, it is possible that the most recent checkpoint may not be finished syncing to the remote.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003ePushing Models to Hugging Face Hub\\u003c/h3\\u003e\\u003ca id=\\\"user-content-pushing-models-to-hugging-face-hub\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Pushing Models to Hugging Face Hub\\\" href=\\\"#pushing-models-to-hugging-face-hub\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe module \\u003ccode\\u003eopen_clip.push_to_hf_hub\\u003c/code\\u003e includes helpers for pushing models /w weights and config to the HF Hub.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe tool can be run from command line, ex:\\n\\u003ccode\\u003epython -m open_clip.push_to_hf_hub --model convnext_large_d_320 --pretrained /train/checkpoints/epoch_12.pt --repo-id laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft\\u003c/code\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eAcknowledgments\\u003c/h2\\u003e\\u003ca id=\\\"user-content-acknowledgments\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Acknowledgments\\\" href=\\\"#acknowledgments\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eWe gratefully acknowledge the Gauss Centre for Supercomputing e.V. (\\u003ca href=\\\"http://www.gauss-centre.eu\\\" rel=\\\"nofollow\\\"\\u003ewww.gauss-centre.eu\\u003c/a\\u003e) for funding this part of work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at J\u00fclich Supercomputing Centre (JSC).\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eThe Team\\u003c/h2\\u003e\\u003ca id=\\\"user-content-the-team\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: The Team\\\" href=\\\"#the-team\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eCurrent development of this repository is led by \\u003ca href=\\\"https://rwightman.com/\\\" rel=\\\"nofollow\\\"\\u003eRoss Wightman\\u003c/a\\u003e, \\u003ca href=\\\"https://github.com/rom1504\\\"\\u003eRomain Beaumont\\u003c/a\\u003e, \\u003ca href=\\\"http://cadegordon.io/\\\" rel=\\\"nofollow\\\"\\u003eCade Gordon\\u003c/a\\u003e, and \\u003ca href=\\\"http://vaishaal.com/\\\" rel=\\\"nofollow\\\"\\u003eVaishaal Shankar\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe original version of this repository is from a group of researchers at UW, Google, Stanford, Amazon, Columbia, and Berkeley.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"http://gabrielilharco.com/\\\" rel=\\\"nofollow\\\"\\u003eGabriel Ilharco*\\u003c/a\\u003e, \\u003ca href=\\\"https://mitchellnw.github.io/\\\" rel=\\\"nofollow\\\"\\u003eMitchell Wortsman*\\u003c/a\\u003e, \\u003ca href=\\\"https://nicholas.carlini.com/\\\" rel=\\\"nofollow\\\"\\u003eNicholas Carlini\\u003c/a\\u003e, \\u003ca href=\\\"https://www.rohantaori.com/\\\" rel=\\\"nofollow\\\"\\u003eRohan Taori\\u003c/a\\u003e, \\u003ca href=\\\"http://www.achaldave.com/\\\" rel=\\\"nofollow\\\"\\u003eAchal Dave\\u003c/a\\u003e, \\u003ca href=\\\"http://vaishaal.com/\\\" rel=\\\"nofollow\\\"\\u003eVaishaal Shankar\\u003c/a\\u003e, \\u003ca href=\\\"https://people.eecs.berkeley.edu/~miller_john/\\\" rel=\\\"nofollow\\\"\\u003eJohn Miller\\u003c/a\\u003e, \\u003ca href=\\\"https://hsnamkoong.github.io/\\\" rel=\\\"nofollow\\\"\\u003eHongseok Namkoong\\u003c/a\\u003e, \\u003ca href=\\\"https://homes.cs.washington.edu/~hannaneh/\\\" rel=\\\"nofollow\\\"\\u003eHannaneh Hajishirzi\\u003c/a\\u003e, \\u003ca href=\\\"https://homes.cs.washington.edu/~ali/\\\" rel=\\\"nofollow\\\"\\u003eAli Farhadi\\u003c/a\\u003e, \\u003ca href=\\\"https://people.csail.mit.edu/ludwigs/\\\" rel=\\\"nofollow\\\"\\u003eLudwig Schmidt\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eSpecial thanks to \\u003ca href=\\\"https://jongwook.kim/\\\" rel=\\\"nofollow\\\"\\u003eJong Wook Kim\\u003c/a\\u003e and \\u003ca href=\\\"https://github.com/Newmu\\\"\\u003eAlec Radford\\u003c/a\\u003e for help with reproducing CLIP!\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eCiting\\u003c/h2\\u003e\\u003ca id=\\\"user-content-citing\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Citing\\\" href=\\\"#citing\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eIf you found this repository useful, please consider citing:\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@software{ilharco_gabriel_2021_5143773,\\n  author       = {Ilharco, Gabriel and\\n                  Wortsman, Mitchell and\\n                  Wightman, Ross and\\n                  Gordon, Cade and\\n                  Carlini, Nicholas and\\n                  Taori, Rohan and\\n                  Dave, Achal and\\n                  Shankar, Vaishaal and\\n                  Namkoong, Hongseok and\\n                  Miller, John and\\n                  Hajishirzi, Hannaneh and\\n                  Farhadi, Ali and\\n                  Schmidt, Ludwig},\\n  title        = {OpenCLIP},\\n  month        = jul,\\n  year         = 2021,\\n  note         = {If you use this software, please cite it as below.},\\n  publisher    = {Zenodo},\\n  version      = {0.1},\\n  doi          = {10.5281/zenodo.5143773},\\n  url          = {https://doi.org/10.5281/zenodo.5143773}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@software\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eilharco_gabriel_2021_5143773\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e       = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eIlharco, Gabriel and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e                  Wortsman, Mitchell and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e                  Wightman, Ross and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e                  Gordon, Cade and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e                  Carlini, Nicholas and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e                  Taori, Rohan and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e                  Dave, Achal and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e                  Shankar, Vaishaal and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e                  Namkoong, Hongseok and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e                  Miller, John and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e                  Hajishirzi, Hannaneh and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e                  Farhadi, Ali and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e                  Schmidt, Ludwig\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e        = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eOpenCLIP\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003emonth\\u003c/span\\u003e        = jul,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e         = \\u003cspan class=\\\"pl-c1\\\"\\u003e2021\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003enote\\u003c/span\\u003e         = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eIf you use this software, please cite it as below.\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003epublisher\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eZenodo\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003eversion\\u003c/span\\u003e      = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e0.1\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003edoi\\u003c/span\\u003e          = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e10.5281/zenodo.5143773\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e          = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://doi.org/10.5281/zenodo.5143773\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{cherti2023reproducible,\\n  title={Reproducible scaling laws for contrastive language-image learning},\\n  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},\\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n  pages={2818--2829},\\n  year={2023}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003echerti2023reproducible\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eReproducible scaling laws for contrastive language-image learning\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eCherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003ebooktitle\\u003c/span\\u003e=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003epages\\u003c/span\\u003e=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2818--2829\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Radford2021LearningTV,\\n  title={Learning Transferable Visual Models From Natural Language Supervision},\\n  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},\\n  booktitle={ICML},\\n  year={2021}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eRadford2021LearningTV\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eLearning Transferable Visual Models From Natural Language Supervision\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAlec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003ebooktitle\\u003c/span\\u003e=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eICML\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{schuhmann2022laionb,\\n  title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},\\n  author={Christoph Schuhmann and\\n          Romain Beaumont and\\n          Richard Vencu and\\n          Cade W Gordon and\\n          Ross Wightman and\\n          Mehdi Cherti and\\n          Theo Coombes and\\n          Aarush Katta and\\n          Clayton Mullis and\\n          Mitchell Wortsman and\\n          Patrick Schramowski and\\n          Srivatsa R Kundurthy and\\n          Katherine Crowson and\\n          Ludwig Schmidt and\\n          Robert Kaczmarczyk and\\n          Jenia Jitsev},\\n  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\\n  year={2022},\\n  url={https://openreview.net/forum?id=M3Y74vmsMcY}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eschuhmann2022laionb\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e{LAION}-5B: An open large-scale dataset for training next generation image-text models\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eChristoph Schuhmann and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e          Romain Beaumont and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e          Richard Vencu and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e          Cade W Gordon and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e          Ross Wightman and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e          Mehdi Cherti and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e          Theo Coombes and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e          Aarush Katta and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e          Clayton Mullis and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e          Mitchell Wortsman and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e          Patrick Schramowski and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e          Srivatsa R Kundurthy and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e          Katherine Crowson and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e          Ludwig Schmidt and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e          Robert Kaczmarczyk and\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s\\\"\\u003e          Jenia Jitsev\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003ebooktitle\\u003c/span\\u003e=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eThirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n  \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e=\\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://openreview.net/forum?id=M3Y74vmsMcY\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://zenodo.org/badge/latestdoi/390536799\\\" rel=\\\"nofollow\\\"\\u003e\\u003cimg src=\\\"https://camo.githubusercontent.com/8b5a675f92c07e9595aa1843dbe843d3d1398f78c403dae68a82ea5bd586ab13/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3339303533363739392e737667\\\" alt=\\\"DOI\\\" data-canonical-src=\\\"https://zenodo.org/badge/390536799.svg\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003c/article\\u003e\",\"loaded\":true,\"timedOut\":false,\"errorMessage\":null,\"headerInfo\":{\"toc\":[{\"level\":1,\"text\":\"OpenCLIP\",\"anchor\":\"openclip\",\"htmlText\":\"OpenCLIP\"},{\"level\":2,\"text\":\"Approach\",\"anchor\":\"approach\",\"htmlText\":\"Approach\"},{\"level\":2,\"text\":\"Usage\",\"anchor\":\"usage\",\"htmlText\":\"Usage\"},{\"level\":3,\"text\":\"Pretrained models\",\"anchor\":\"pretrained-models\",\"htmlText\":\"Pretrained models\"},{\"level\":3,\"text\":\"Loading models\",\"anchor\":\"loading-models\",\"htmlText\":\"Loading models\"},{\"level\":2,\"text\":\"Fine-tuning on classification tasks\",\"anchor\":\"fine-tuning-on-classification-tasks\",\"htmlText\":\"Fine-tuning on classification tasks\"},{\"level\":2,\"text\":\"Data\",\"anchor\":\"data\",\"htmlText\":\"Data\"},{\"level\":3,\"text\":\"Conceptual Captions\",\"anchor\":\"conceptual-captions\",\"htmlText\":\"Conceptual Captions\"},{\"level\":3,\"text\":\"YFCC and other datasets\",\"anchor\":\"yfcc-and-other-datasets\",\"htmlText\":\"YFCC and other datasets\"},{\"level\":2,\"text\":\"Training CLIP\",\"anchor\":\"training-clip\",\"htmlText\":\"Training CLIP\"},{\"level\":3,\"text\":\"Install\",\"anchor\":\"install\",\"htmlText\":\"Install\"},{\"level\":4,\"text\":\"Development\",\"anchor\":\"development\",\"htmlText\":\"Development\"},{\"level\":4,\"text\":\"Testing\",\"anchor\":\"testing\",\"htmlText\":\"Testing\"},{\"level\":3,\"text\":\"Sample single-process running code:\",\"anchor\":\"sample-single-process-running-code\",\"htmlText\":\"Sample single-process running code:\"},{\"level\":3,\"text\":\"Multi-GPU and Beyond\",\"anchor\":\"multi-gpu-and-beyond\",\"htmlText\":\"Multi-GPU and Beyond\"},{\"level\":4,\"text\":\"Epochs\",\"anchor\":\"epochs\",\"htmlText\":\"Epochs\"},{\"level\":4,\"text\":\"Patch Dropout\",\"anchor\":\"patch-dropout\",\"htmlText\":\"Patch Dropout\"},{\"level\":4,\"text\":\"Multiple data sources\",\"anchor\":\"multiple-data-sources\",\"htmlText\":\"Multiple data sources\"},{\"level\":4,\"text\":\"Single-Node\",\"anchor\":\"single-node\",\"htmlText\":\"Single-Node\"},{\"level\":4,\"text\":\"Multi-Node\",\"anchor\":\"multi-node\",\"htmlText\":\"Multi-Node\"},{\"level\":4,\"text\":\"SLURM\",\"anchor\":\"slurm\",\"htmlText\":\"SLURM\"},{\"level\":3,\"text\":\"Resuming from a checkpoint:\",\"anchor\":\"resuming-from-a-checkpoint\",\"htmlText\":\"Resuming from a checkpoint:\"},{\"level\":3,\"text\":\"Training CoCa:\",\"anchor\":\"training-coca\",\"htmlText\":\"Training CoCa:\"},{\"level\":3,\"text\":\"Generating text with CoCa\",\"anchor\":\"generating-text-with-coca\",\"htmlText\":\"Generating text with CoCa\"},{\"level\":3,\"text\":\"Fine Tuning CoCa\",\"anchor\":\"fine-tuning-coca\",\"htmlText\":\"Fine Tuning CoCa\"},{\"level\":3,\"text\":\"Training with pre-trained language models as text encoder:\",\"anchor\":\"training-with-pre-trained-language-models-as-text-encoder\",\"htmlText\":\"Training with pre-trained language models as text encoder:\"},{\"level\":3,\"text\":\"Loss Curves\",\"anchor\":\"loss-curves\",\"htmlText\":\"Loss Curves\"},{\"level\":3,\"text\":\"Logging\",\"anchor\":\"logging\",\"htmlText\":\"Logging\"},{\"level\":2,\"text\":\"Evaluation / Zero-Shot\",\"anchor\":\"evaluation--zero-shot\",\"htmlText\":\"Evaluation / Zero-Shot\"},{\"level\":3,\"text\":\"Evaluating local checkpoint:\",\"anchor\":\"evaluating-local-checkpoint\",\"htmlText\":\"Evaluating local checkpoint:\"},{\"level\":3,\"text\":\"Evaluating hosted pretrained checkpoint on ImageNet zero-shot prediction:\",\"anchor\":\"evaluating-hosted-pretrained-checkpoint-on-imagenet-zero-shot-prediction\",\"htmlText\":\"Evaluating hosted pretrained checkpoint on ImageNet zero-shot prediction:\"},{\"level\":3,\"text\":\"Model distillation\",\"anchor\":\"model-distillation\",\"htmlText\":\"Model distillation\"},{\"level\":3,\"text\":\"Gradient accumulation\",\"anchor\":\"gradient-accumulation\",\"htmlText\":\"Gradient accumulation\"},{\"level\":3,\"text\":\"Int8 Support\",\"anchor\":\"int8-support\",\"htmlText\":\"Int8 Support\"},{\"level\":3,\"text\":\"Support for remote loading/training\",\"anchor\":\"support-for-remote-loadingtraining\",\"htmlText\":\"Support for remote loading/training\"},{\"level\":3,\"text\":\"Pushing Models to Hugging Face Hub\",\"anchor\":\"pushing-models-to-hugging-face-hub\",\"htmlText\":\"Pushing Models to Hugging Face Hub\"},{\"level\":2,\"text\":\"Acknowledgments\",\"anchor\":\"acknowledgments\",\"htmlText\":\"Acknowledgments\"},{\"level\":2,\"text\":\"The Team\",\"anchor\":\"the-team\",\"htmlText\":\"The Team\"},{\"level\":2,\"text\":\"Citing\",\"anchor\":\"citing\",\"htmlText\":\"Citing\"}],\"siteNavLoginPath\":\"/login?return_to=https%3A%2F%2Fgithub.com%2Fmlfoundations%2Fopen_clip\"}},{\"displayName\":\"LICENSE\",\"repoName\":\"open_clip\",\"refName\":\"main\",\"path\":\"LICENSE\",\"preferredFileType\":\"license\",\"tabName\":\"License\",\"richText\":null,\"loaded\":false,\"timedOut\":false,\"errorMessage\":null,\"headerInfo\":{\"toc\":null,\"siteNavLoginPath\":\"/login?return_to=https%3A%2F%2Fgithub.com%2Fmlfoundations%2Fopen_clip\"}}],\"overviewFilesProcessingTime\":126.752216}},\"appPayload\":{\"helpUrl\":\"https://docs.github.com\",\"findFileWorkerPath\":\"/assets-cdn/worker/find-file-worker-32bb159cc57c.js\",\"findInFileWorkerPath\":\"/assets-cdn/worker/find-in-file-worker-c6704d501c10.js\",\"githubDevUrl\":null,\"enabled_features\":{\"code_nav_ui_events\":false,\"copilot_conversational_ux\":false,\"copilot_conversational_ux_embedding_update\":false,\"copilot_popover_file_editor_header\":false,\"copilot_smell_icebreaker_ux\":true,\"copilot_workspace\":false,\"codeview_firefox_inert\":true}}}}</script>  <div data-target=\"react-partial.reactRoot\"><style data-styled=\"true\" data-styled-version=\"5.3.6\">.cgQnMS{font-weight:600;font-size:32px;margin:0;}/*!sc*/data-styled.g1[id=\"Heading__StyledHeading-sc-1c1dgg0-0\"]{content:\"cgQnMS,\"}/*!sc*/.izjvBm{margin-top:16px;margin-bottom:16px;}/*!sc*/.rPQgy{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/.eUMEDg{margin-bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;row-gap:16px;}/*!sc*/.eLcVee{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;padding-bottom:16px;padding-top:8px;}/*!sc*/.hsfLlq{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;gap:8px;}/*!sc*/@media screen and (max-width:320px){.hsfLlq{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}}/*!sc*/.gpKoUz{position:relative;}/*!sc*/@media screen and (max-width:380px){.gpKoUz .ref-selector-button-text-container{max-width:80px;}}/*!sc*/@media screen and (max-width:320px){.gpKoUz{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}.gpKoUz .overview-ref-selector{width:100%;}.gpKoUz .overview-ref-selector > span{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;}.gpKoUz .overview-ref-selector > span > span[data-component=\"text\"]{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}}/*!sc*/.kkrdEu{-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}/*!sc*/.bKgizp{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;}/*!sc*/.iPGYsi{margin-right:4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.dKmYfk{font-size:14px;min-width:0;max-width:125px;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}/*!sc*/.trpoQ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;pointer-events:none;}/*!sc*/.laYubZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (max-width:1079px){.laYubZ{display:none;}}/*!sc*/.swnaL{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (min-width:1080px){.swnaL{display:none;}}/*!sc*/@media screen and (max-width:543px){.swnaL{display:none;}}/*!sc*/.bWpuBf{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-left:8px;gap:8px;}/*!sc*/.grHjNb{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:8px;}/*!sc*/@media screen and (max-width:543px){.grHjNb{display:none;}}/*!sc*/.dXTsqj{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (max-width:1011px){.dXTsqj{display:none;}}/*!sc*/.dCOrmu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (min-width:1012px){.dCOrmu{display:none;}}/*!sc*/@media screen and (max-width:544px){.bVvbgP{display:none;}}/*!sc*/.bNDvfp{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (min-width:544px){.bNDvfp{display:none;}}/*!sc*/.yfPnm{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;gap:16px;}/*!sc*/.cAQuiW{width:100%;border-collapse:separate;border-spacing:0;border:1px solid;border-color:var(--borderColor-default,var(--color-border-default,#d0d7de));border-radius:6px;table-layout:fixed;overflow:unset;}/*!sc*/.iiUlLN{height:0px;line-height:0px;}/*!sc*/.iiUlLN tr{height:0px;font-size:0px;}/*!sc*/.jmggSN{padding:16px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));font-size:12px;text-align:left;height:40px;}/*!sc*/.jmggSN th{padding-left:16px;background-color:var(--bgColor-muted,var(--color-canvas-subtle,#f6f8fa));}/*!sc*/.kvYunM{width:100%;border-top-left-radius:6px;}/*!sc*/@media screen and (min-width:544px){.kvYunM{display:none;}}/*!sc*/.hrLuxA{width:40%;border-top-left-radius:6px;}/*!sc*/@media screen and (max-width:543px){.hrLuxA{display:none;}}/*!sc*/@media screen and (max-width:543px){.ePjhhA{display:none;}}/*!sc*/.cuEKae{text-align:right;padding-right:16px;width:136px;border-top-right-radius:6px;}/*!sc*/.jEbBOT{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));font-size:12px;height:40px;}/*!sc*/.bTxCvM{background-color:var(--bgColor-muted,var(--color-canvas-subtle,#f6f8fa));padding:4px;border-top-left-radius:6px;border-top-right-radius:6px;}/*!sc*/.eYedVD{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;gap:8px;min-width:273px;padding-right:8px;padding-left:16px;padding-top:8px;padding-bottom:8px;}/*!sc*/.jGfYmh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:8px;}/*!sc*/.lhFvfi{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/.bqgLjk{display:inherit;}/*!sc*/@media screen and (min-width:544px){.bqgLjk{display:none;}}/*!sc*/@media screen and (min-width:768px){.bqgLjk{display:none;}}/*!sc*/.epsqEd{text-align:center;vertical-align:center;height:40px;border-top:1px solid;border-color:var(--borderColor-default,var(--color-border-default,#d0d7de));}/*!sc*/.ldpruc{border-top:1px solid var(--borderColor-default,var(--color-border-default));cursor:pointer;}/*!sc*/.ehcSsh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;gap:16px;}/*!sc*/.iGmlUb{border:1px solid;border-color:var(--borderColor-default,var(--color-border-default,#d0d7de));border-radius:6px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}/*!sc*/@media screen and (max-width:543px){.iGmlUb{margin-left:-16px;margin-right:-16px;max-width:calc(100% + 32px);}}/*!sc*/@media screen and (min-width:544px){.iGmlUb{max-width:100%;}}/*!sc*/.iRQGXA{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;border-bottom:1px solid;border-bottom-color:var(--borderColor-default,var(--color-border-default,#d0d7de));-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-right:8px;position:-webkit-sticky;position:sticky;top:0;background-color:var(--bgColor-default,var(--color-canvas-default,#ffffff));z-index:1;border-top-left-radius:6px;border-top-right-radius:6px;}/*!sc*/.dvTdPK{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-left:8px;padding-right:8px;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;border-bottom:none;border-bottom-color:var(--borderColor-muted,var(--color-border-muted,hsla(210,18%,87%,1)));align:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-height:48px;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;max-width:100%;}/*!sc*/.gwuIGu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/.kOxwQs{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;margin-right:8px;}/*!sc*/.kOgeFj{font-weight:600;}/*!sc*/.bJMeLZ{padding:32px;overflow:auto;}/*!sc*/data-styled.g2[id=\"Box-sc-g0xbh4-0\"]{content:\"izjvBm,rPQgy,eUMEDg,eLcVee,hsfLlq,gpKoUz,kkrdEu,bKgizp,iPGYsi,dKmYfk,trpoQ,laYubZ,swnaL,bWpuBf,grHjNb,dXTsqj,dCOrmu,bVvbgP,bNDvfp,yfPnm,cAQuiW,iiUlLN,jmggSN,kvYunM,hrLuxA,ePjhhA,cuEKae,jEbBOT,bTxCvM,eYedVD,jGfYmh,lhFvfi,bqgLjk,epsqEd,ldpruc,ehcSsh,iGmlUb,iRQGXA,dvTdPK,gwuIGu,kOxwQs,kOgeFj,bJMeLZ,\"}/*!sc*/.bOMzPg{min-width:0;}/*!sc*/.eUGNHp{font-weight:600;}/*!sc*/.dALsKK{color:var(--fgColor-default,var(--color-fg-default,#1F2328));}/*!sc*/data-styled.g6[id=\"Text-sc-17v1xeu-0\"]{content:\"bOMzPg,eUGNHp,dALsKK,\"}/*!sc*/.dheQRw{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));-webkit-text-decoration:none;text-decoration:none;}/*!sc*/[data-a11y-link-underlines='true'] .Link__StyledLink-sc-14289xe-0[data-inline='true']{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.dheQRw:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.dheQRw:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/.vLMkZ{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));-webkit-text-decoration:none;text-decoration:none;position:relative;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;color:var(--fgColor-default,var(--color-fg-default,#1F2328));text-align:center;-webkit-text-decoration:none;text-decoration:none;line-height:calc(20/14);border-radius:6px;font-size:14px;padding-left:8px;padding-right:8px;padding-top:calc((2rem - 1.25rem) / 2);padding-bottom:calc((2rem - 1.25rem) / 2);}/*!sc*/[data-a11y-link-underlines='true'] .Link__StyledLink-sc-14289xe-0[data-inline='true']{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.vLMkZ:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.vLMkZ:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/.vLMkZ span[data-component=\"icon\"]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/@media (hover:hover){.vLMkZ:hover{background-color:var(--bgColor-neutral-muted,var(--color-neutral-muted,rgba(175,184,193,0.2)));-webkit-transition:background .12s ease-out;transition:background .12s ease-out;-webkit-text-decoration:none;text-decoration:none;}}/*!sc*/.vLMkZ:focus{outline:2px solid transparent;}/*!sc*/.vLMkZ:focus{box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.vLMkZ:focus:not(:focus-visible){box-shadow:none;}/*!sc*/.vLMkZ:focus-visible{outline:2px solid transparent;box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.vLMkZ span[data-content]::before{content:attr(data-content);display:block;height:0;font-weight:600;visibility:hidden;white-space:nowrap;}/*!sc*/.vLMkZ::after{position:absolute;right:50%;bottom:calc(50% - 25px);width:100%;height:2px;content:\"\";background-color:var(--underlineNav-borderColor-active,var(--color-primer-border-active,#fd8c73));border-radius:0;-webkit-transform:translate(50%,-50%);-ms-transform:translate(50%,-50%);transform:translate(50%,-50%);}/*!sc*/@media (forced-colors:active){.vLMkZ::after{background-color:LinkText;}}/*!sc*/.bhqztV{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));-webkit-text-decoration:none;text-decoration:none;position:relative;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;color:var(--fgColor-default,var(--color-fg-default,#1F2328));text-align:center;-webkit-text-decoration:none;text-decoration:none;line-height:calc(20/14);border-radius:6px;font-size:14px;padding-left:8px;padding-right:8px;padding-top:calc((2rem - 1.25rem) / 2);padding-bottom:calc((2rem - 1.25rem) / 2);}/*!sc*/[data-a11y-link-underlines='true'] .Link__StyledLink-sc-14289xe-0[data-inline='true']{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.bhqztV:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.bhqztV:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/.bhqztV span[data-component=\"icon\"]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/@media (hover:hover){.bhqztV:hover{background-color:var(--bgColor-neutral-muted,var(--color-neutral-muted,rgba(175,184,193,0.2)));-webkit-transition:background .12s ease-out;transition:background .12s ease-out;-webkit-text-decoration:none;text-decoration:none;}}/*!sc*/.bhqztV:focus{outline:2px solid transparent;}/*!sc*/.bhqztV:focus{box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.bhqztV:focus:not(:focus-visible){box-shadow:none;}/*!sc*/.bhqztV:focus-visible{outline:2px solid transparent;box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.bhqztV span[data-content]::before{content:attr(data-content);display:block;height:0;font-weight:600;visibility:hidden;white-space:nowrap;}/*!sc*/.bhqztV::after{position:absolute;right:50%;bottom:calc(50% - 25px);width:100%;height:2px;content:\"\";background-color:transparent;border-radius:0;-webkit-transform:translate(50%,-50%);-ms-transform:translate(50%,-50%);transform:translate(50%,-50%);}/*!sc*/@media (forced-colors:active){.bhqztV::after{background-color:transparent;}}/*!sc*/data-styled.g8[id=\"Link__StyledLink-sc-14289xe-0\"]{content:\"dheQRw,vLMkZ,bhqztV,\"}/*!sc*/.izDscS{border-radius:6px;border:1px solid;border-color:var(--button-default-borderColor-rest,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:var(--button-default-bgColor-rest,var(--color-btn-bg,#f6f8fa));box-shadow:var(--button-default-shadow-resting,var(--color-btn-shadow,0 1px 0 rgba(31,35,40,0.04))),var(--button-default-shadow-inset,var(--color-btn-inset-shadow,inset 0 1px 0 rgba(255,255,255,0.25)));}/*!sc*/.izDscS:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.izDscS:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.izDscS:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.izDscS[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.izDscS[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.izDscS:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.izDscS:active{-webkit-transition:none;transition:none;}/*!sc*/.izDscS[data-inactive]{cursor:auto;}/*!sc*/.izDscS:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));border-color:var(--button-default-borderColor-disabled,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));}/*!sc*/.izDscS:disabled [data-component=ButtonCounter]{color:inherit;}/*!sc*/@media (forced-colors:active){.izDscS:focus{outline:solid 1px transparent;}}/*!sc*/.izDscS [data-component=ButtonCounter]{font-size:12px;background-color:var(--buttonCounter-default-bgColor-rest,var(--color-btn-counter-bg,rgba(31,35,40,0.08)));}/*!sc*/.izDscS[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.izDscS[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.izDscS[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.izDscS[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.izDscS[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.izDscS[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.izDscS[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.izDscS[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.izDscS[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.izDscS[data-block=\"block\"]{width:100%;}/*!sc*/.izDscS[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.izDscS[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.izDscS [data-component=\"leadingVisual\"]{grid-area:leadingVisual;}/*!sc*/.izDscS [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.izDscS [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.izDscS [data-component=\"trailingAction\"]{margin-right:-4px;}/*!sc*/.izDscS [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.izDscS [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.izDscS:hover:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-hover,var(--color-btn-hover-bg,#f3f4f6));border-color:var(--button-default-borderColor-hover,var(--button-default-borderColor-hover,var(--color-btn-hover-border,rgba(31,35,40,0.15))));}/*!sc*/.izDscS:active:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/.izDscS[aria-expanded=true]{background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/.izDscS [data-component=\"leadingVisual\"],.izDscS [data-component=\"trailingVisual\"],.izDscS [data-component=\"trailingAction\"]{color:var(--button-color,var(--fgColor-muted,var(--color-fg-muted,#656d76)));}/*!sc*/.izDscS{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/.izDscS svg{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.izDscS > span{width:inherit;}/*!sc*/.cuOWTR{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:transparent;box-shadow:none;}/*!sc*/.cuOWTR:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.cuOWTR:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.cuOWTR:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.cuOWTR[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.cuOWTR[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.cuOWTR:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.cuOWTR:active{-webkit-transition:none;transition:none;}/*!sc*/.cuOWTR[data-inactive]{cursor:auto;}/*!sc*/.cuOWTR:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.cuOWTR:disabled [data-component=ButtonCounter],.cuOWTR:disabled [data-component=\"leadingVisual\"],.cuOWTR:disabled [data-component=\"trailingAction\"]{color:inherit;}/*!sc*/@media (forced-colors:active){.cuOWTR:focus{outline:solid 1px transparent;}}/*!sc*/.cuOWTR [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.cuOWTR[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.cuOWTR[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.cuOWTR[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.cuOWTR[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.cuOWTR[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.cuOWTR[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.cuOWTR[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.cuOWTR[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.cuOWTR[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.cuOWTR[data-block=\"block\"]{width:100%;}/*!sc*/.cuOWTR[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.cuOWTR[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.cuOWTR [data-component=\"leadingVisual\"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.cuOWTR [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.cuOWTR [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.cuOWTR [data-component=\"trailingAction\"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.cuOWTR [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.cuOWTR [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.cuOWTR:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));}/*!sc*/.cuOWTR:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));}/*!sc*/.cuOWTR[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/.cuOWTR[data-component=\"IconButton\"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.cuOWTR[data-no-visuals]{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.cuOWTR:has([data-component=\"ButtonCounter\"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/.cuOWTR:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.cuOWTR:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/.cuOWTR{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));padding-left:4px;padding-right:4px;}/*!sc*/.cuOWTR span[data-component=\"leadingVisual\"]{margin-right:4px !important;}/*!sc*/.tDSzd{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:transparent;box-shadow:none;}/*!sc*/.tDSzd:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.tDSzd:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.tDSzd:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.tDSzd[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.tDSzd[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.tDSzd:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.tDSzd:active{-webkit-transition:none;transition:none;}/*!sc*/.tDSzd[data-inactive]{cursor:auto;}/*!sc*/.tDSzd:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.tDSzd:disabled [data-component=ButtonCounter],.tDSzd:disabled [data-component=\"leadingVisual\"],.tDSzd:disabled [data-component=\"trailingAction\"]{color:inherit;}/*!sc*/@media (forced-colors:active){.tDSzd:focus{outline:solid 1px transparent;}}/*!sc*/.tDSzd [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.tDSzd[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.tDSzd[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.tDSzd[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.tDSzd[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.tDSzd[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.tDSzd[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.tDSzd[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.tDSzd[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.tDSzd[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.tDSzd[data-block=\"block\"]{width:100%;}/*!sc*/.tDSzd[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.tDSzd[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.tDSzd [data-component=\"leadingVisual\"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.tDSzd [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.tDSzd [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.tDSzd [data-component=\"trailingAction\"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.tDSzd [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.tDSzd [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.tDSzd:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));}/*!sc*/.tDSzd:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));}/*!sc*/.tDSzd[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/.tDSzd[data-component=\"IconButton\"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.tDSzd[data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.tDSzd:has([data-component=\"ButtonCounter\"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/.tDSzd:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.tDSzd:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/.ftZGca{border-radius:6px;border:1px solid;border-color:var(--button-default-borderColor-rest,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:var(--button-default-bgColor-rest,var(--color-btn-bg,#f6f8fa));box-shadow:var(--button-default-shadow-resting,var(--color-btn-shadow,0 1px 0 rgba(31,35,40,0.04))),var(--button-default-shadow-inset,var(--color-btn-inset-shadow,inset 0 1px 0 rgba(255,255,255,0.25)));}/*!sc*/.ftZGca:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.ftZGca:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.ftZGca:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.ftZGca[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.ftZGca[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.ftZGca:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.ftZGca:active{-webkit-transition:none;transition:none;}/*!sc*/.ftZGca[data-inactive]{cursor:auto;}/*!sc*/.ftZGca:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));border-color:var(--button-default-borderColor-disabled,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));}/*!sc*/.ftZGca:disabled [data-component=ButtonCounter]{color:inherit;}/*!sc*/@media (forced-colors:active){.ftZGca:focus{outline:solid 1px transparent;}}/*!sc*/.ftZGca [data-component=ButtonCounter]{font-size:12px;background-color:var(--buttonCounter-default-bgColor-rest,var(--color-btn-counter-bg,rgba(31,35,40,0.08)));}/*!sc*/.ftZGca[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.ftZGca[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.ftZGca[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.ftZGca[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.ftZGca[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.ftZGca[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.ftZGca[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.ftZGca[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.ftZGca[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.ftZGca[data-block=\"block\"]{width:100%;}/*!sc*/.ftZGca[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.ftZGca[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.ftZGca [data-component=\"leadingVisual\"]{grid-area:leadingVisual;}/*!sc*/.ftZGca [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.ftZGca [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.ftZGca [data-component=\"trailingAction\"]{margin-right:-4px;}/*!sc*/.ftZGca [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.ftZGca [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.ftZGca:hover:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-hover,var(--color-btn-hover-bg,#f3f4f6));border-color:var(--button-default-borderColor-hover,var(--button-default-borderColor-hover,var(--color-btn-hover-border,rgba(31,35,40,0.15))));}/*!sc*/.ftZGca:active:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/.ftZGca[aria-expanded=true]{background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/.ftZGca [data-component=\"leadingVisual\"],.ftZGca [data-component=\"trailingVisual\"],.ftZGca [data-component=\"trailingAction\"]{color:var(--button-color,var(--fgColor-muted,var(--color-fg-muted,#656d76)));}/*!sc*/.gYvpXq{border-radius:6px;border:1px solid;border-color:var(--button-primary-borderColor-rest,var(--color-btn-primary-border,rgba(31,35,40,0.15)));font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-primary-fgColor-rest,var(--color-btn-primary-text,#ffffff));background-color:var(--button-primary-bgColor-rest,var(--color-btn-primary-bg,#1f883d));box-shadow:var(--shadow-resting-small,var(--color-btn-primary-shadow,0 1px 0 rgba(31,35,40,0.1)));}/*!sc*/.gYvpXq:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.gYvpXq:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.gYvpXq:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.gYvpXq[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.gYvpXq[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.gYvpXq:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.gYvpXq:active{-webkit-transition:none;transition:none;}/*!sc*/.gYvpXq[data-inactive]{cursor:auto;}/*!sc*/.gYvpXq:disabled{cursor:not-allowed;box-shadow:none;color:var(--button-primary-fgColor-disabled,var(--color-btn-primary-disabled-text,rgba(255,255,255,0.8)));background-color:var(--button-primary-bgColor-disabled,var(--color-btn-primary-disabled-bg,#94d3a2));border-color:var(--button-primary-borderColor-disabled,var(--color-btn-primary-disabled-border,rgba(31,35,40,0.15)));}/*!sc*/.gYvpXq:disabled [data-component=ButtonCounter]{color:inherit;}/*!sc*/@media (forced-colors:active){.gYvpXq:focus{outline:solid 1px transparent;}}/*!sc*/.gYvpXq [data-component=ButtonCounter]{font-size:12px;background-color:var(--buttonCounter-primary-bgColor-rest,var(--color-btn-primary-counter-bg,rgba(0,45,17,0.2)));color:var(--button-primary-fgColor-rest,var(--color-btn-primary-text,#ffffff));}/*!sc*/.gYvpXq[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.gYvpXq[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.gYvpXq[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.gYvpXq[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.gYvpXq[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.gYvpXq[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.gYvpXq[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.gYvpXq[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.gYvpXq[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.gYvpXq[data-block=\"block\"]{width:100%;}/*!sc*/.gYvpXq[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.gYvpXq[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.gYvpXq [data-component=\"leadingVisual\"]{grid-area:leadingVisual;}/*!sc*/.gYvpXq [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.gYvpXq [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.gYvpXq [data-component=\"trailingAction\"]{margin-right:-4px;}/*!sc*/.gYvpXq [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.gYvpXq [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.gYvpXq:hover:not([disabled]):not([data-inactive]){color:btn.primary.hoverText;background-color:var(--button-primary-bgColor-hover,var(--color-btn-primary-hover-bg,#1a7f37));}/*!sc*/.gYvpXq:focus:not([disabled]){box-shadow:inset 0 0 0 3px;}/*!sc*/.gYvpXq:focus-visible:not([disabled]){box-shadow:inset 0 0 0 3px;}/*!sc*/.gYvpXq:active:not([disabled]):not([data-inactive]){background-color:var(--button-primary-bgColor-active,var(--color-btn-primary-selected-bg,hsla(137,66%,28%,1)));box-shadow:var(--button-primary-shadow-selected,var(--color-btn-primary-selected-shadow,inset 0 1px 0 rgba(0,45,17,0.2)));}/*!sc*/.gYvpXq[aria-expanded=true]{background-color:var(--button-primary-bgColor-active,var(--color-btn-primary-selected-bg,hsla(137,66%,28%,1)));box-shadow:var(--button-primary-shadow-selected,var(--color-btn-primary-selected-shadow,inset 0 1px 0 rgba(0,45,17,0.2)));}/*!sc*/.gYvpXq svg{color:fg.primary;}/*!sc*/.fAkXQN{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--fgColor-default,var(--color-fg-default,#1F2328));background-color:transparent;box-shadow:none;}/*!sc*/.fAkXQN:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.fAkXQN:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.fAkXQN:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.fAkXQN[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.fAkXQN[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.fAkXQN:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.fAkXQN:active{-webkit-transition:none;transition:none;}/*!sc*/.fAkXQN[data-inactive]{cursor:auto;}/*!sc*/.fAkXQN:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.fAkXQN:disabled [data-component=ButtonCounter],.fAkXQN:disabled [data-component=\"leadingVisual\"],.fAkXQN:disabled [data-component=\"trailingAction\"]{color:inherit;}/*!sc*/@media (forced-colors:active){.fAkXQN:focus{outline:solid 1px transparent;}}/*!sc*/.fAkXQN [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.fAkXQN[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.fAkXQN[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.fAkXQN[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.fAkXQN[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.fAkXQN[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.fAkXQN[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.fAkXQN[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.fAkXQN[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.fAkXQN[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.fAkXQN[data-block=\"block\"]{width:100%;}/*!sc*/.fAkXQN[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.fAkXQN[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.fAkXQN [data-component=\"leadingVisual\"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.fAkXQN [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.fAkXQN [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.fAkXQN [data-component=\"trailingAction\"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.fAkXQN [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.fAkXQN [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.fAkXQN:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.fAkXQN:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.fAkXQN[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/.fAkXQN[data-component=\"IconButton\"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.fAkXQN[data-no-visuals]{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.fAkXQN:has([data-component=\"ButtonCounter\"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/.fAkXQN:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.fAkXQN:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/.fAkXQN:focus:not([disabled]){-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.jPraEl{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:transparent;box-shadow:none;}/*!sc*/.jPraEl:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.jPraEl:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.jPraEl:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.jPraEl[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.jPraEl[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.jPraEl:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.jPraEl:active{-webkit-transition:none;transition:none;}/*!sc*/.jPraEl[data-inactive]{cursor:auto;}/*!sc*/.jPraEl:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.jPraEl:disabled [data-component=ButtonCounter],.jPraEl:disabled [data-component=\"leadingVisual\"],.jPraEl:disabled [data-component=\"trailingAction\"]{color:inherit;}/*!sc*/@media (forced-colors:active){.jPraEl:focus{outline:solid 1px transparent;}}/*!sc*/.jPraEl [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.jPraEl[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.jPraEl[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.jPraEl[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.jPraEl[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.jPraEl[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.jPraEl[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.jPraEl[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.jPraEl[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.jPraEl[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.jPraEl[data-block=\"block\"]{width:100%;}/*!sc*/.jPraEl[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.jPraEl[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.jPraEl [data-component=\"leadingVisual\"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.jPraEl [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.jPraEl [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.jPraEl [data-component=\"trailingAction\"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.jPraEl [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.jPraEl [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.jPraEl:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));}/*!sc*/.jPraEl:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));}/*!sc*/.jPraEl[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/.jPraEl[data-component=\"IconButton\"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.jPraEl[data-no-visuals]{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.jPraEl:has([data-component=\"ButtonCounter\"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/.jPraEl:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.jPraEl:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/.jPraEl{color:var(--fgColor-muted,var(--color-fg-subtle,#6e7781));padding-left:8px;padding-right:8px;}/*!sc*/data-styled.g9[id=\"types__StyledButton-sc-ws60qy-0\"]{content:\"izDscS,cuOWTR,tDSzd,ftZGca,gYvpXq,fAkXQN,jPraEl,\"}/*!sc*/.rTZSs{position:absolute;width:1px;height:1px;padding:0;margin:-1px;overflow:hidden;-webkit-clip:rect(0,0,0,0);clip:rect(0,0,0,0);white-space:nowrap;border-width:0;}/*!sc*/data-styled.g10[id=\"_VisuallyHidden__VisuallyHidden-sc-11jhm7a-0\"]{content:\"rTZSs,\"}/*!sc*/.fUpWeN{display:inline-block;overflow:hidden;text-overflow:ellipsis;vertical-align:top;white-space:nowrap;max-width:125px;max-width:100%;}/*!sc*/data-styled.g15[id=\"Truncate__StyledTruncate-sc-23o1d2-0\"]{content:\"fUpWeN,\"}/*!sc*/.dMjscx{position:relative;display:inline-block;}/*!sc*/.dMjscx::before{position:absolute;z-index:1000001;display:none;width:0px;height:0px;color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));pointer-events:none;content:'';border:6px solid transparent;opacity:0;}/*!sc*/.dMjscx::after{position:absolute;z-index:1000000;display:none;padding:0.5em 0.75em;font:normal normal 11px/1.5 -apple-system,BlinkMacSystemFont,\"Segoe UI\",\"Noto Sans\",Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\";-webkit-font-smoothing:subpixel-antialiased;color:var(--fgColor-onEmphasis,var(--color-fg-on-emphasis,#ffffff));text-align:center;-webkit-text-decoration:none;text-decoration:none;text-shadow:none;text-transform:none;-webkit-letter-spacing:normal;-moz-letter-spacing:normal;-ms-letter-spacing:normal;letter-spacing:normal;word-wrap:break-word;white-space:pre;pointer-events:none;content:attr(aria-label);background:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));border-radius:6px;opacity:0;}/*!sc*/@-webkit-keyframes tooltip-appear{from{opacity:0;}to{opacity:1;}}/*!sc*/@keyframes tooltip-appear{from{opacity:0;}to{opacity:1;}}/*!sc*/.dMjscx:hover::before,.dMjscx:active::before,.dMjscx:focus::before,.dMjscx:focus-within::before,.dMjscx:hover::after,.dMjscx:active::after,.dMjscx:focus::after,.dMjscx:focus-within::after{display:inline-block;-webkit-text-decoration:none;text-decoration:none;-webkit-animation-name:tooltip-appear;animation-name:tooltip-appear;-webkit-animation-duration:0.1s;animation-duration:0.1s;-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-timing-function:ease-in;animation-timing-function:ease-in;-webkit-animation-delay:0.4s;animation-delay:0.4s;}/*!sc*/.dMjscx.tooltipped-no-delay:hover::before,.dMjscx.tooltipped-no-delay:active::before,.dMjscx.tooltipped-no-delay:focus::before,.dMjscx.tooltipped-no-delay:focus-within::before,.dMjscx.tooltipped-no-delay:hover::after,.dMjscx.tooltipped-no-delay:active::after,.dMjscx.tooltipped-no-delay:focus::after,.dMjscx.tooltipped-no-delay:focus-within::after{-webkit-animation-delay:0s;animation-delay:0s;}/*!sc*/.dMjscx.tooltipped-multiline:hover::after,.dMjscx.tooltipped-multiline:active::after,.dMjscx.tooltipped-multiline:focus::after,.dMjscx.tooltipped-multiline:focus-within::after{display:table-cell;}/*!sc*/.dMjscx.tooltipped-s::after,.dMjscx.tooltipped-se::after,.dMjscx.tooltipped-sw::after{top:100%;right:50%;margin-top:6px;}/*!sc*/.dMjscx.tooltipped-s::before,.dMjscx.tooltipped-se::before,.dMjscx.tooltipped-sw::before{top:auto;right:50%;bottom:-7px;margin-right:-6px;border-bottom-color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));}/*!sc*/.dMjscx.tooltipped-se::after{right:auto;left:50%;margin-left:-16px;}/*!sc*/.dMjscx.tooltipped-sw::after{margin-right:-16px;}/*!sc*/.dMjscx.tooltipped-n::after,.dMjscx.tooltipped-ne::after,.dMjscx.tooltipped-nw::after{right:50%;bottom:100%;margin-bottom:6px;}/*!sc*/.dMjscx.tooltipped-n::before,.dMjscx.tooltipped-ne::before,.dMjscx.tooltipped-nw::before{top:-7px;right:50%;bottom:auto;margin-right:-6px;border-top-color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));}/*!sc*/.dMjscx.tooltipped-ne::after{right:auto;left:50%;margin-left:-16px;}/*!sc*/.dMjscx.tooltipped-nw::after{margin-right:-16px;}/*!sc*/.dMjscx.tooltipped-s::after,.dMjscx.tooltipped-n::after{-webkit-transform:translateX(50%);-ms-transform:translateX(50%);transform:translateX(50%);}/*!sc*/.dMjscx.tooltipped-w::after{right:100%;bottom:50%;margin-right:6px;-webkit-transform:translateY(50%);-ms-transform:translateY(50%);transform:translateY(50%);}/*!sc*/.dMjscx.tooltipped-w::before{top:50%;bottom:50%;left:-7px;margin-top:-6px;border-left-color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));}/*!sc*/.dMjscx.tooltipped-e::after{bottom:50%;left:100%;margin-left:6px;-webkit-transform:translateY(50%);-ms-transform:translateY(50%);transform:translateY(50%);}/*!sc*/.dMjscx.tooltipped-e::before{top:50%;right:-7px;bottom:50%;margin-top:-6px;border-right-color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));}/*!sc*/.dMjscx.tooltipped-multiline::after{width:-webkit-max-content;width:-moz-max-content;width:max-content;max-width:250px;word-wrap:break-word;white-space:pre-line;border-collapse:separate;}/*!sc*/.dMjscx.tooltipped-multiline.tooltipped-s::after,.dMjscx.tooltipped-multiline.tooltipped-n::after{right:auto;left:50%;-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%);transform:translateX(-50%);}/*!sc*/.dMjscx.tooltipped-multiline.tooltipped-w::after,.dMjscx.tooltipped-multiline.tooltipped-e::after{right:100%;}/*!sc*/.dMjscx.tooltipped-align-right-2::after{right:0;margin-right:0;}/*!sc*/.dMjscx.tooltipped-align-right-2::before{right:15px;}/*!sc*/.dMjscx.tooltipped-align-left-2::after{left:0;margin-left:0;}/*!sc*/.dMjscx.tooltipped-align-left-2::before{left:10px;}/*!sc*/data-styled.g18[id=\"Tooltip__TooltipBase-sc-17tf59c-0\"]{content:\"dMjscx,\"}/*!sc*/.bPgibo{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;list-style:none;white-space:nowrap;padding-top:0;padding-bottom:0;padding-left:0;padding-right:0;margin:0;margin-bottom:-1px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;gap:8px;position:relative;}/*!sc*/data-styled.g103[id=\"UnderlineNav__NavigationList-sc-1jfr31k-0\"]{content:\"bPgibo,\"}/*!sc*/</style> <!-- --> <!-- --> <div class=\"Box-sc-g0xbh4-0 izjvBm\"><div class=\"Box-sc-g0xbh4-0 rPQgy\"><div class=\"Box-sc-g0xbh4-0 eUMEDg\"></div></div><div class=\"Box-sc-g0xbh4-0 eLcVee\"><div class=\"Box-sc-g0xbh4-0 hsfLlq\"><div class=\"Box-sc-g0xbh4-0 gpKoUz\"><button type=\"button\" id=\"branch-picker-repos-header-ref-selector\" aria-haspopup=\"true\" tabindex=\"0\" aria-label=\"main branch\" data-testid=\"anchor-button\" class=\"types__StyledButton-sc-ws60qy-0 izDscS overview-ref-selector\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"text\"><div class=\"Box-sc-g0xbh4-0 bKgizp\"><div class=\"Box-sc-g0xbh4-0 iPGYsi\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-git-branch\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z\"></path></svg></div><div class=\"Box-sc-g0xbh4-0 dKmYfk ref-selector-button-text-container\"><span class=\"Text-sc-17v1xeu-0 bOMzPg\">\u00a0<!-- -->main</span></div></div></span><span data-component=\"trailingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-triangle-down\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z\"></path></svg></span></span></button><button hidden=\"\" data-hotkey-scope=\"read-only-cursor-text-area\"></button></div><div class=\"Box-sc-g0xbh4-0 laYubZ\"><a style=\"--button-color:fg.muted\" type=\"button\" href=\"/mlfoundations/open_clip/branches\" class=\"types__StyledButton-sc-ws60qy-0 cuOWTR\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-git-branch\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z\"></path></svg></span><span data-component=\"text\">Branches</span></span></a><a style=\"--button-color:fg.muted\" type=\"button\" href=\"/mlfoundations/open_clip/tags\" class=\"types__StyledButton-sc-ws60qy-0 cuOWTR\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-tag\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z\"></path></svg></span><span data-component=\"text\">Tags</span></span></a></div><div class=\"Box-sc-g0xbh4-0 swnaL\"><a style=\"--button-color:fg.muted\" type=\"button\" aria-label=\"Go to Branches page\" href=\"/mlfoundations/open_clip/branches\" data-no-visuals=\"true\" class=\"types__StyledButton-sc-ws60qy-0 tDSzd\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-git-branch\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z\"></path></svg></a><a style=\"--button-color:fg.muted\" type=\"button\" aria-label=\"Go to Tags page\" href=\"/mlfoundations/open_clip/tags\" data-no-visuals=\"true\" class=\"types__StyledButton-sc-ws60qy-0 tDSzd\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-tag\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z\"></path></svg></a></div></div><div class=\"Box-sc-g0xbh4-0 bWpuBf\"><div class=\"Box-sc-g0xbh4-0 grHjNb\"><div class=\"Box-sc-g0xbh4-0 dXTsqj\"><!--$!--><template></template><!--/$--></div><div class=\"Box-sc-g0xbh4-0 dCOrmu\"><button type=\"button\" data-no-visuals=\"true\" class=\"types__StyledButton-sc-ws60qy-0 ftZGca\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"text\">Go to file</span></span></button></div><div class=\"react-directory-add-file-icon\"></div><div class=\"react-directory-remove-file-icon\"></div></div><button type=\"button\" id=\":R2il5:\" aria-haspopup=\"true\" tabindex=\"0\" class=\"types__StyledButton-sc-ws60qy-0 gYvpXq\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><div class=\"Box-sc-g0xbh4-0 bVvbgP\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-code\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z\"></path></svg></div></span><span data-component=\"text\">Code</span></span><span data-component=\"trailingAction\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-triangle-down\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z\"></path></svg></span></button><div class=\"Box-sc-g0xbh4-0 bNDvfp\"><button data-component=\"IconButton\" type=\"button\" aria-label=\"Open more actions menu\" id=\":R3il5:\" aria-haspopup=\"true\" tabindex=\"0\" data-no-visuals=\"true\" class=\"types__StyledButton-sc-ws60qy-0 ftZGca\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-kebab-horizontal\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path></svg></button></div></div></div><div class=\"Box-sc-g0xbh4-0 yfPnm\"><div data-hpc=\"true\" class=\"Box-sc-g0xbh4-0\"><button hidden=\"\" data-testid=\"focus-next-element-button\" data-hotkey=\"j\" data-hotkey-scope=\"read-only-cursor-text-area\"></button><button hidden=\"\" data-hotkey=\"j\"></button><button hidden=\"\" data-testid=\"focus-previous-element-button\" data-hotkey=\"k\" data-hotkey-scope=\"read-only-cursor-text-area\"></button><button hidden=\"\" data-hotkey=\"k\"></button><h2 class=\"Heading__StyledHeading-sc-1c1dgg0-0 cgQnMS sr-only\" data-testid=\"screen-reader-heading\" id=\"folders-and-files\">Folders and files</h2><table aria-labelledby=\"folders-and-files\" class=\"Box-sc-g0xbh4-0 cAQuiW\"><thead class=\"Box-sc-g0xbh4-0 iiUlLN\"><tr class=\"Box-sc-g0xbh4-0 jmggSN\"><th colSpan=\"2\" class=\"Box-sc-g0xbh4-0 kvYunM\"><span class=\"Text-sc-17v1xeu-0 eUGNHp\">Name</span></th><th colSpan=\"1\" class=\"Box-sc-g0xbh4-0 hrLuxA\"><span class=\"Text-sc-17v1xeu-0 eUGNHp\">Name</span></th><th class=\"Box-sc-g0xbh4-0 ePjhhA\"><div title=\"Last commit message\" class=\"Truncate__StyledTruncate-sc-23o1d2-0 fUpWeN\"><span class=\"Text-sc-17v1xeu-0 eUGNHp\">Last commit message</span></div></th><th colSpan=\"1\" class=\"Box-sc-g0xbh4-0 cuEKae\"><div title=\"Last commit date\" class=\"Truncate__StyledTruncate-sc-23o1d2-0 fUpWeN\"><span class=\"Text-sc-17v1xeu-0 eUGNHp\">Last commit date</span></div></th></tr></thead><tbody><tr class=\"Box-sc-g0xbh4-0 jEbBOT\"><td colSpan=\"3\" class=\"Box-sc-g0xbh4-0 bTxCvM\"><div class=\"Box-sc-g0xbh4-0 eYedVD\"><h2 class=\"Heading__StyledHeading-sc-1c1dgg0-0 cgQnMS sr-only\" data-testid=\"screen-reader-heading\">Latest commit</h2><div style=\"width:120px\" class=\"Skeleton Skeleton--text\" data-testid=\"loading\">\u00a0</div><div class=\"Box-sc-g0xbh4-0 jGfYmh\"><div data-testid=\"latest-commit-details\" class=\"Box-sc-g0xbh4-0 lhFvfi\"></div><h2 class=\"Heading__StyledHeading-sc-1c1dgg0-0 cgQnMS sr-only\" data-testid=\"screen-reader-heading\">History</h2><a class=\"types__StyledButton-sc-ws60qy-0 fAkXQN react-last-commit-history-group\" href=\"/mlfoundations/open_clip/commits/main/\" data-size=\"small\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-history\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z\"></path></svg></span><span data-component=\"text\"><span class=\"Text-sc-17v1xeu-0 dALsKK\">512 Commits</span></span></span></a><div class=\"Box-sc-g0xbh4-0 bqgLjk\"></div><span role=\"tooltip\" aria-label=\"Commit history\" class=\"Tooltip__TooltipBase-sc-17tf59c-0 dMjscx tooltipped-n\"><a class=\"types__StyledButton-sc-ws60qy-0 fAkXQN react-last-commit-history-icon\" href=\"/mlfoundations/open_clip/commits/main/\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-history\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z\"></path></svg></span></span></a></span></div></div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-0\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"This path skips through empty directories\" aria-label=\".github/workflows, (Directory)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/tree/main/.github/workflows\"><span class=\"react-directory-default-color\" data-testid=\"path-name-segment\">.github/</span><span class=\"\" data-testid=\"path-name-segment\">workflows</span></a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"This path skips through empty directories\" aria-label=\".github/workflows, (Directory)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/tree/main/.github/workflows\"><span class=\"react-directory-default-color\" data-testid=\"path-name-segment\">.github/</span><span class=\"\" data-testid=\"path-name-segment\">workflows</span></a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-1\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"docs\" aria-label=\"docs, (Directory)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/tree/main/docs\">docs</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"docs\" aria-label=\"docs, (Directory)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/tree/main/docs\">docs</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-2\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"scripts\" aria-label=\"scripts, (Directory)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/tree/main/scripts\">scripts</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"scripts\" aria-label=\"scripts, (Directory)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/tree/main/scripts\">scripts</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-3\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"src\" aria-label=\"src, (Directory)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/tree/main/src\">src</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"src\" aria-label=\"src, (Directory)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/tree/main/src\">src</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-4\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"tests\" aria-label=\"tests, (Directory)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/tree/main/tests\">tests</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"tests\" aria-label=\"tests, (Directory)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/tree/main/tests\">tests</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-5\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"tutorials\" aria-label=\"tutorials, (Directory)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/tree/main/tutorials\">tutorials</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"tutorials\" aria-label=\"tutorials, (Directory)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/tree/main/tutorials\">tutorials</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-6\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\".gitignore\" aria-label=\".gitignore, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/.gitignore\">.gitignore</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\".gitignore\" aria-label=\".gitignore, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/.gitignore\">.gitignore</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-7\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"CITATION.cff\" aria-label=\"CITATION.cff, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/CITATION.cff\">CITATION.cff</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"CITATION.cff\" aria-label=\"CITATION.cff, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/CITATION.cff\">CITATION.cff</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-8\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"HISTORY.md\" aria-label=\"HISTORY.md, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/HISTORY.md\">HISTORY.md</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"HISTORY.md\" aria-label=\"HISTORY.md, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/HISTORY.md\">HISTORY.md</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-9\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"LICENSE\" aria-label=\"LICENSE, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/LICENSE\">LICENSE</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"LICENSE\" aria-label=\"LICENSE, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/LICENSE\">LICENSE</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row truncate-for-mobile\" id=\"folder-row-10\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"MANIFEST.in\" aria-label=\"MANIFEST.in, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/MANIFEST.in\">MANIFEST.in</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"MANIFEST.in\" aria-label=\"MANIFEST.in, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/MANIFEST.in\">MANIFEST.in</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row truncate-for-mobile\" id=\"folder-row-11\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"Makefile\" aria-label=\"Makefile, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/Makefile\">Makefile</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"Makefile\" aria-label=\"Makefile, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/Makefile\">Makefile</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row truncate-for-mobile\" id=\"folder-row-12\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"README.md\" aria-label=\"README.md, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/README.md\">README.md</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"README.md\" aria-label=\"README.md, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/README.md\">README.md</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row truncate-for-mobile\" id=\"folder-row-13\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"pytest.ini\" aria-label=\"pytest.ini, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/pytest.ini\">pytest.ini</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"pytest.ini\" aria-label=\"pytest.ini, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/pytest.ini\">pytest.ini</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row truncate-for-mobile\" id=\"folder-row-14\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"requirements-test.txt\" aria-label=\"requirements-test.txt, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/requirements-test.txt\">requirements-test.txt</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"requirements-test.txt\" aria-label=\"requirements-test.txt, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/requirements-test.txt\">requirements-test.txt</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row truncate-for-mobile\" id=\"folder-row-15\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"requirements-training.txt\" aria-label=\"requirements-training.txt, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/requirements-training.txt\">requirements-training.txt</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"requirements-training.txt\" aria-label=\"requirements-training.txt, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/requirements-training.txt\">requirements-training.txt</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row truncate-for-mobile\" id=\"folder-row-16\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"requirements.txt\" aria-label=\"requirements.txt, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/requirements.txt\">requirements.txt</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"requirements.txt\" aria-label=\"requirements.txt, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/requirements.txt\">requirements.txt</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row truncate-for-mobile\" id=\"folder-row-17\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"setup.py\" aria-label=\"setup.py, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/setup.py\">setup.py</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"setup.py\" aria-label=\"setup.py, (File)\" class=\"Link--primary\" href=\"/mlfoundations/open_clip/blob/main/setup.py\">setup.py</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"Box-sc-g0xbh4-0 epsqEd show-for-mobile\" data-testid=\"view-all-files-row\"><td colSpan=\"3\" class=\"Box-sc-g0xbh4-0 ldpruc\"><div><button class=\"Link__StyledLink-sc-14289xe-0 dheQRw\">View all files</button></div></td></tr></tbody></table></div><div class=\"Box-sc-g0xbh4-0 ehcSsh\"><div class=\"Box-sc-g0xbh4-0 iGmlUb\"><div class=\"Box-sc-g0xbh4-0 iRQGXA\"><h2 class=\"_VisuallyHidden__VisuallyHidden-sc-11jhm7a-0 rTZSs\">Repository files navigation</h2><nav aria-label=\"Repository files\" class=\"Box-sc-g0xbh4-0 dvTdPK\"><ul role=\"list\" class=\"UnderlineNav__NavigationList-sc-1jfr31k-0 bPgibo\"><li class=\"Box-sc-g0xbh4-0 gwuIGu\"><a href=\"#\" aria-current=\"page\" class=\"Link__StyledLink-sc-14289xe-0 vLMkZ\"><span data-component=\"icon\" class=\"Box-sc-g0xbh4-0 kOxwQs\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-book\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z\"></path></svg></span><span data-component=\"text\" data-content=\"README\" class=\"Box-sc-g0xbh4-0 kOgeFj\">README</span></a></li><li class=\"Box-sc-g0xbh4-0 gwuIGu\"><a href=\"#\" class=\"Link__StyledLink-sc-14289xe-0 bhqztV\"><span data-component=\"icon\" class=\"Box-sc-g0xbh4-0 kOxwQs\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-law\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M8.75.75V2h.985c.304 0 .603.08.867.231l1.29.736c.038.022.08.033.124.033h2.234a.75.75 0 0 1 0 1.5h-.427l2.111 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.006.005-.01.01-.045.04c-.21.176-.441.327-.686.45C14.556 10.78 13.88 11 13 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L12.178 4.5h-.162c-.305 0-.604-.079-.868-.231l-1.29-.736a.245.245 0 0 0-.124-.033H8.75V13h2.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1 0-1.5h2.5V3.5h-.984a.245.245 0 0 0-.124.033l-1.289.737c-.265.15-.564.23-.869.23h-.162l2.112 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.016.015-.045.04c-.21.176-.441.327-.686.45C4.556 10.78 3.88 11 3 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L2.178 4.5H1.75a.75.75 0 0 1 0-1.5h2.234a.249.249 0 0 0 .125-.033l1.288-.737c.265-.15.564-.23.869-.23h.984V.75a.75.75 0 0 1 1.5 0Zm2.945 8.477c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L13 6.327Zm-10 0c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L3 6.327Z\"></path></svg></span><span data-component=\"text\" data-content=\"License\" class=\"Box-sc-g0xbh4-0\">License</span></a></li></ul></nav><button style=\"--button-color:fg.subtle\" type=\"button\" aria-label=\"Outline\" id=\":Rdkl5:\" aria-haspopup=\"true\" tabindex=\"0\" class=\"types__StyledButton-sc-ws60qy-0 jPraEl\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-list-unordered\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z\"></path></svg></button></div><div class=\"Box-sc-g0xbh4-0 bJMeLZ js-snippet-clipboard-copy-unpositioned\" data-hpc=\"true\"><article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><div class=\"markdown-heading\" dir=\"auto\"><h1 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">OpenCLIP</h1><a id=\"user-content-openclip\" class=\"anchor-element\" aria-label=\"Permalink: OpenCLIP\" href=\"#openclip\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2212.07143\" rel=\"nofollow\">[Paper]</a> <a href=\"#citing\">[Citations]</a> <a href=\"https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb\" rel=\"nofollow\">[Clip Colab]</a> <a href=\"https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb\" rel=\"nofollow\">[Coca Colab]</a><a href=\"https://pypi.python.org/pypi/open_clip_torch\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8d1068793d4518a7aa8d18b65b7d6f163e22445c3986b2a85834ce1d4c339501/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6f70656e5f636c69705f746f7263682e737667\" alt=\"pypi\" data-canonical-src=\"https://img.shields.io/pypi/v/open_clip_torch.svg\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">Welcome to an open source implementation of OpenAI's <a href=\"https://arxiv.org/abs/2103.00020\" rel=\"nofollow\">CLIP</a> (Contrastive Language-Image Pre-training).</p><p dir=\"auto\">Using this codebase, we have trained several models on a variety of data sources and compute budgets, ranging from <a href=\"/mlfoundations/open_clip/blob/main/docs/LOW_ACC.md\">small-scale experiments</a> to larger runs including models trained on datasets such as <a href=\"https://arxiv.org/abs/2111.02114\" rel=\"nofollow\">LAION-400M</a>, <a href=\"https://arxiv.org/abs/2210.08402\" rel=\"nofollow\">LAION-2B</a> and <a href=\"https://arxiv.org/abs/2304.14108\" rel=\"nofollow\">DataComp-1B</a>.Many of our models and their scaling properties are studied in detail in the paper <a href=\"https://arxiv.org/abs/2212.07143\" rel=\"nofollow\">reproducible scaling laws for contrastive language-image learning</a>.Some of our best models and their zero-shot ImageNet-1k accuracy are shown below, along with the ViT-L model trained by OpenAI.We provide more details about our full collection of pretrained models <a href=\"/mlfoundations/open_clip/blob/main/docs/PRETRAINED.md\">here</a>, and zero-shot results for 38 datasets <a href=\"/mlfoundations/open_clip/blob/main/docs/openclip_results.csv\">here</a>.</p><table><thead><tr><th>Model</th><th>Training data</th><th>Resolution</th><th># of samples seen</th><th>ImageNet zero-shot acc.</th></tr></thead><tbody><tr><td>ConvNext-Base</td><td>LAION-2B</td><td>256px</td><td>13B</td><td>71.5%</td></tr><tr><td>ConvNext-Large</td><td>LAION-2B</td><td>320px</td><td>29B</td><td>76.9%</td></tr><tr><td>ConvNext-XXLarge</td><td>LAION-2B</td><td>256px</td><td>34B</td><td>79.5%</td></tr><tr><td>ViT-B/32</td><td>DataComp-1B</td><td>256px</td><td>34B</td><td>72.8%</td></tr><tr><td>ViT-B/16</td><td>DataComp-1B</td><td>224px</td><td>13B</td><td>73.5%</td></tr><tr><td>ViT-L/14</td><td>LAION-2B</td><td>224px</td><td>32B</td><td>75.3%</td></tr><tr><td>ViT-H/14</td><td>LAION-2B</td><td>224px</td><td>32B</td><td>78.0%</td></tr><tr><td>ViT-L/14</td><td>DataComp-1B</td><td>224px</td><td>13B</td><td>79.2%</td></tr><tr><td>ViT-G/14</td><td>LAION-2B</td><td>224px</td><td>34B</td><td>80.1%</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ViT-L/14</td><td>OpenAI's WIT</td><td>224px</td><td>13B</td><td>75.5%</td></tr></tbody></table><p dir=\"auto\">Model cards with additional model specific details can be found on the Hugging Face Hub under the OpenCLIP library tag: <a href=\"https://huggingface.co/models?library=open_clip\" rel=\"nofollow\">https://huggingface.co/models?library=open_clip</a>.</p><p dir=\"auto\">If you found this repository useful, please consider <a href=\"#citing\">citing</a>.We welcome anyone to submit an issue or send an email if you have any other requests or suggestions.</p><p dir=\"auto\">Note that portions of <code>src/open_clip/</code> modelling and tokenizer code are adaptations of OpenAI's official <a href=\"https://github.com/openai/CLIP\">repository</a>.</p><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Approach</h2><a id=\"user-content-approach\" class=\"anchor-element\" aria-label=\"Permalink: Approach\" href=\"#approach\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><table><thead><tr><th align=\"center\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/CLIP.png\"><img src=\"https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/CLIP.png\" alt=\"CLIP\" style=\"max-width: 100%;\"></a></th></tr></thead><tbody><tr><td align=\"center\">Image Credit: <a href=\"https://github.com/openai/CLIP\">https://github.com/openai/CLIP</a></td></tr></tbody></table><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Usage</h2><a id=\"user-content-usage\" class=\"anchor-element\" aria-label=\"Permalink: Usage\" href=\"#usage\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"pip install open_clip_torch\"><pre class=\"notranslate\"><code>pip install open_clip_torch</code></pre></div><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom PIL import Imageimport open_clipmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')tokenizer = open_clip.get_tokenizer('ViT-B-32')image = preprocess(Image.open(&quot;CLIP.png&quot;)).unsqueeze(0)text = tokenizer([&quot;a diagram&quot;, &quot;a dog&quot;, &quot;a cat&quot;])with torch.no_grad(), torch.cuda.amp.autocast():    image_features = model.encode_image(image)    text_features = model.encode_text(text)    image_features /= image_features.norm(dim=-1, keepdim=True)    text_features /= text_features.norm(dim=-1, keepdim=True)    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)print(&quot;Label probs:&quot;, text_probs)  # prints: [[1., 0., 0.]]\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-v\">PIL</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Image</span><span class=\"pl-k\">import</span> <span class=\"pl-s1\">open_clip</span><span class=\"pl-s1\">model</span>, <span class=\"pl-s1\">_</span>, <span class=\"pl-s1\">preprocess</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">open_clip</span>.<span class=\"pl-en\">create_model_and_transforms</span>(<span class=\"pl-s\">'ViT-B-32'</span>, <span class=\"pl-s1\">pretrained</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'laion2b_s34b_b79k'</span>)<span class=\"pl-s1\">tokenizer</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">open_clip</span>.<span class=\"pl-en\">get_tokenizer</span>(<span class=\"pl-s\">'ViT-B-32'</span>)<span class=\"pl-s1\">image</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">preprocess</span>(<span class=\"pl-v\">Image</span>.<span class=\"pl-en\">open</span>(<span class=\"pl-s\">\"CLIP.png\"</span>)).<span class=\"pl-en\">unsqueeze</span>(<span class=\"pl-c1\">0</span>)<span class=\"pl-s1\">text</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">tokenizer</span>([<span class=\"pl-s\">\"a diagram\"</span>, <span class=\"pl-s\">\"a dog\"</span>, <span class=\"pl-s\">\"a cat\"</span>])<span class=\"pl-k\">with</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">no_grad</span>(), <span class=\"pl-s1\">torch</span>.<span class=\"pl-s1\">cuda</span>.<span class=\"pl-s1\">amp</span>.<span class=\"pl-en\">autocast</span>():    <span class=\"pl-s1\">image_features</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">model</span>.<span class=\"pl-en\">encode_image</span>(<span class=\"pl-s1\">image</span>)    <span class=\"pl-s1\">text_features</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">model</span>.<span class=\"pl-en\">encode_text</span>(<span class=\"pl-s1\">text</span>)    <span class=\"pl-s1\">image_features</span> <span class=\"pl-c1\">/=</span> <span class=\"pl-s1\">image_features</span>.<span class=\"pl-en\">norm</span>(<span class=\"pl-s1\">dim</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-s1\">keepdim</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">True</span>)    <span class=\"pl-s1\">text_features</span> <span class=\"pl-c1\">/=</span> <span class=\"pl-s1\">text_features</span>.<span class=\"pl-en\">norm</span>(<span class=\"pl-s1\">dim</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-s1\">keepdim</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">True</span>)    <span class=\"pl-s1\">text_probs</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">100.0</span> <span class=\"pl-c1\">*</span> <span class=\"pl-s1\">image_features</span> @ <span class=\"pl-s1\">text_features</span>.<span class=\"pl-v\">T</span>).<span class=\"pl-en\">softmax</span>(<span class=\"pl-s1\">dim</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">-</span><span class=\"pl-c1\">1</span>)<span class=\"pl-en\">print</span>(<span class=\"pl-s\">\"Label probs:\"</span>, <span class=\"pl-s1\">text_probs</span>)  <span class=\"pl-c\"># prints: [[1., 0., 0.]]</span></pre></div><p dir=\"auto\">See also this <a href=\"https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb\" rel=\"nofollow\">[Clip Colab]</a>.</p><p dir=\"auto\">To compute billions of embeddings efficiently, you can use <a href=\"https://github.com/rom1504/clip-retrieval\">clip-retrieval</a> which has openclip support.</p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Pretrained models</h3><a id=\"user-content-pretrained-models\" class=\"anchor-element\" aria-label=\"Permalink: Pretrained models\" href=\"#pretrained-models\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">We offer a simple model interface to instantiate both pre-trained and untrained models.To see which pretrained models are available, use the following code snippet.More details about our pretrained models are available <a href=\"/mlfoundations/open_clip/blob/main/docs/PRETRAINED.md\">here</a>.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"&gt;&gt;&gt; import open_clip&gt;&gt;&gt; open_clip.list_pretrained()\"><pre><span class=\"pl-c1\">&gt;&gt;</span><span class=\"pl-c1\">&gt;</span> <span class=\"pl-k\">import</span> <span class=\"pl-s1\">open_clip</span><span class=\"pl-c1\">&gt;&gt;</span><span class=\"pl-c1\">&gt;</span> <span class=\"pl-s1\">open_clip</span>.<span class=\"pl-en\">list_pretrained</span>()</pre></div><p dir=\"auto\">You can find more about the models we support (e.g. number of parameters, FLOPs) in <a href=\"/mlfoundations/open_clip/blob/main/docs/model_profile.csv\">this table</a>.</p><p dir=\"auto\">NOTE: Many existing checkpoints use the QuickGELU activation from the original OpenAI models. This activation is actually less efficient than native torch.nn.GELU in recent versions of PyTorch. The model defaults are now nn.GELU, so one should use model definitions with <code>-quickgelu</code> postfix for the OpenCLIP pretrained weights. All OpenAI pretrained weights will always default to QuickGELU. One can also use the non <code>-quickgelu</code> model definitions with pretrained weights using QuickGELU but there will be an accuracy drop, for fine-tune that will likely vanish for longer runs.Future trained models will use nn.GELU.</p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Loading models</h3><a id=\"user-content-loading-models\" class=\"anchor-element\" aria-label=\"Permalink: Loading models\" href=\"#loading-models\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">Models can be loaded with <code>open_clip.create_model_and_transforms</code>, as shown in the example below. The model name and corresponding <code>pretrained</code> keys are compatible with the outputs of <code>open_clip.list_pretrained()</code>.</p><p dir=\"auto\">The <code>pretrained</code> argument also accepts local paths, for example <code>/path/to/my/b32.pt</code>.You can also load checkpoints from huggingface this way. To do so, download the <code>open_clip_pytorch_model.bin</code> file (for example, <a href=\"https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/blob/main/open_clip_pytorch_model.bin\" rel=\"nofollow\">https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/tree/main</a>), and use <code>pretrained=/path/to/open_clip_pytorch_model.bin</code>.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"# pretrained also accepts local pathsmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k') \"><pre><span class=\"pl-c\"># pretrained also accepts local paths</span><span class=\"pl-s1\">model</span>, <span class=\"pl-s1\">_</span>, <span class=\"pl-s1\">preprocess</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">open_clip</span>.<span class=\"pl-en\">create_model_and_transforms</span>(<span class=\"pl-s\">'ViT-B-32'</span>, <span class=\"pl-s1\">pretrained</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'laion2b_s34b_b79k'</span>) </pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Fine-tuning on classification tasks</h2><a id=\"user-content-fine-tuning-on-classification-tasks\" class=\"anchor-element\" aria-label=\"Permalink: Fine-tuning on classification tasks\" href=\"#fine-tuning-on-classification-tasks\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">This repository is focused on training CLIP models. To fine-tune a <em>trained</em> zero-shot model on a downstream classification task such as ImageNet, please see <a href=\"https://github.com/mlfoundations/wise-ft\">our other repository: WiSE-FT</a>. The <a href=\"https://github.com/mlfoundations/wise-ft\">WiSE-FT repository</a> contains code for our paper on <a href=\"https://arxiv.org/abs/2109.01903\" rel=\"nofollow\">Robust Fine-tuning of Zero-shot Models</a>, in which we introduce a technique for fine-tuning zero-shot models while preserving robustness under distribution shift.</p><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Data</h2><a id=\"user-content-data\" class=\"anchor-element\" aria-label=\"Permalink: Data\" href=\"#data\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">To download datasets as webdataset, we recommend <a href=\"https://github.com/rom1504/img2dataset\">img2dataset</a>.</p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Conceptual Captions</h3><a id=\"user-content-conceptual-captions\" class=\"anchor-element\" aria-label=\"Permalink: Conceptual Captions\" href=\"#conceptual-captions\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">See <a href=\"https://github.com/rom1504/img2dataset/blob/main/dataset_examples/cc3m.md\">cc3m img2dataset example</a>.</p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">YFCC and other datasets</h3><a id=\"user-content-yfcc-and-other-datasets\" class=\"anchor-element\" aria-label=\"Permalink: YFCC and other datasets\" href=\"#yfcc-and-other-datasets\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">In addition to specifying the training data via CSV files as mentioned above, our codebase also supports <a href=\"https://github.com/webdataset/webdataset\">webdataset</a>, which is recommended for larger scale datasets. The expected format is a series of <code>.tar</code> files. Each of these <code>.tar</code> files should contain two files for each training example, one for the image and one for the corresponding text. Both files should have the same name but different extensions. For instance, <code>shard_001.tar</code> could contain files such as <code>abc.jpg</code> and <code>abc.txt</code>. You can learn more about <code>webdataset</code> at <a href=\"https://github.com/webdataset/webdataset\">https://github.com/webdataset/webdataset</a>. We use <code>.tar</code> files with 1,000 data points each, which we create using <a href=\"https://github.com/webdataset/tarp\">tarp</a>.</p><p dir=\"auto\">You can download the YFCC dataset from <a href=\"http://mmcommons.org/\" rel=\"nofollow\">Multimedia Commons</a>.Similar to OpenAI, we used a subset of YFCC to reach the aforementioned accuracy numbers.The indices of images in this subset are in <a href=\"https://github.com/openai/CLIP/blob/main/data/yfcc100m.md\">OpenAI's CLIP repository</a>.</p><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Training CLIP</h2><a id=\"user-content-training-clip\" class=\"anchor-element\" aria-label=\"Permalink: Training CLIP\" href=\"#training-clip\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Install</h3><a id=\"user-content-install\" class=\"anchor-element\" aria-label=\"Permalink: Install\" href=\"#install\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">We advise you first create a virtual environment with:</p><div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"python3 -m venv .envsource .env/bin/activatepip install -U pip\"><pre class=\"notranslate\"><code>python3 -m venv .envsource .env/bin/activatepip install -U pip</code></pre></div><p dir=\"auto\">You can then install openclip for training with <code>pip install 'open_clip_torch[training]'</code>.</p><div class=\"markdown-heading\" dir=\"auto\"><h4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Development</h4><a id=\"user-content-development\" class=\"anchor-element\" aria-label=\"Permalink: Development\" href=\"#development\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">If you want to make changes to contribute code, you can clone openclip then run <code>make install</code> in openclip folder (after creating a virtualenv)</p><p dir=\"auto\">Install pip PyTorch as per <a href=\"https://pytorch.org/get-started/locally/\" rel=\"nofollow\">https://pytorch.org/get-started/locally/</a></p><p dir=\"auto\">You may run <code>make install-training</code> to install training deps</p><div class=\"markdown-heading\" dir=\"auto\"><h4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Testing</h4><a id=\"user-content-testing\" class=\"anchor-element\" aria-label=\"Permalink: Testing\" href=\"#testing\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">Test can be run with <code>make install-test</code> then <code>make test</code></p><p dir=\"auto\"><code>python -m pytest -x -s -v tests -k \"training\"</code> to run a specific test</p><p dir=\"auto\">Running regression tests against a specific git revision or tag:</p><ol dir=\"auto\"><li><p dir=\"auto\">Generate testing data</p><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"python tests/util_test.py --model RN50 RN101 --save_model_list models.txt --git_revision 9d31b2ec4df6d8228f370ff20c8267ec6ba39383\"><pre>python tests/util_test.py --model RN50 RN101 --save_model_list models.txt --git_revision 9d31b2ec4df6d8228f370ff20c8267ec6ba39383</pre></div><p dir=\"auto\"><strong><em>WARNING</em>: This will invoke git and modify your working tree, but will reset it to the current state after data has been generated! <br>Don't modify your working tree while test data is being generated this way.</strong></p></li><li><p dir=\"auto\">Run regression tests</p><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"OPEN_CLIP_TEST_REG_MODELS=models.txt python -m pytest -x -s -v -m regression_test\"><pre>OPEN_CLIP_TEST_REG_MODELS=models.txt python -m pytest -x -s -v -m regression_test</pre></div></li></ol><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Sample single-process running code:</h3><a id=\"user-content-sample-single-process-running-code\" class=\"anchor-element\" aria-label=\"Permalink: Sample single-process running code:\" href=\"#sample-single-process-running-code\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"python -m training.main \\    --save-frequency 1 \\    --zeroshot-frequency 1 \\    --report-to tensorboard \\    --train-data=&quot;/path/to/train_data.csv&quot;  \\    --val-data=&quot;/path/to/validation_data.csv&quot;  \\    --csv-img-key filepath \\    --csv-caption-key title \\    --imagenet-val=/path/to/imagenet/root/val/ \\    --warmup 10000 \\    --batch-size=128 \\    --lr=1e-3 \\    --wd=0.1 \\    --epochs=30 \\    --workers=8 \\    --model RN50\"><pre>python -m training.main \\    --save-frequency 1 \\    --zeroshot-frequency 1 \\    --report-to tensorboard \\    --train-data=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/path/to/train_data.csv<span class=\"pl-pds\">\"</span></span>  \\    --val-data=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/path/to/validation_data.csv<span class=\"pl-pds\">\"</span></span>  \\    --csv-img-key filepath \\    --csv-caption-key title \\    --imagenet-val=/path/to/imagenet/root/val/ \\    --warmup 10000 \\    --batch-size=128 \\    --lr=1e-3 \\    --wd=0.1 \\    --epochs=30 \\    --workers=8 \\    --model RN50</pre></div><p dir=\"auto\">Note: <code>imagenet-val</code> is the path to the <em>validation</em> set of ImageNet for zero-shot evaluation, not the training set!You can remove this argument if you do not want to perform zero-shot evaluation on ImageNet throughout training. Note that the <code>val</code> folder should contain subfolders. If it does not, please use <a href=\"https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh\" rel=\"nofollow\">this script</a>.</p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Multi-GPU and Beyond</h3><a id=\"user-content-multi-gpu-and-beyond\" class=\"anchor-element\" aria-label=\"Permalink: Multi-GPU and Beyond\" href=\"#multi-gpu-and-beyond\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">This code has been battle tested up to 1024 A100s and offers a variety of solutionsfor distributed training. We include native support for SLURM clusters.</p><p dir=\"auto\">As the number of devices used to train increases, so does the space complexity ofthe the logit matrix. Using a na\u00efve all-gather scheme, space complexity will be<code>O(n^2)</code>. Instead, complexity may become effectively linear if the flags<code>--gather-with-grad</code> and <code>--local-loss</code> are used. This alteration results in one-to-onenumerical results as the na\u00efve method.</p><div class=\"markdown-heading\" dir=\"auto\"><h4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Epochs</h4><a id=\"user-content-epochs\" class=\"anchor-element\" aria-label=\"Permalink: Epochs\" href=\"#epochs\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">For larger datasets (eg Laion2B), we recommend setting <code>--train-num-samples</code> to a lower value than the full epoch, for example <code>--train-num-samples 135646078</code> to 1/16 of an epoch in conjunction with <code>--dataset-resampled</code> to do sampling with replacement. This allows having frequent checkpoints to evaluate more often.</p><div class=\"markdown-heading\" dir=\"auto\"><h4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Patch Dropout</h4><a id=\"user-content-patch-dropout\" class=\"anchor-element\" aria-label=\"Permalink: Patch Dropout\" href=\"#patch-dropout\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2212.00794\" rel=\"nofollow\">Recent research</a> has shown that one can dropout half to three-quarters of the visual tokens, leading to up to 2-3x training speeds without loss of accuracy.</p><p dir=\"auto\">You can set this on your visual transformer config with the key <code>patch_dropout</code>.</p><p dir=\"auto\">In the paper, they also finetuned without the patch dropout at the end. You can do this with the command-line argument <code>--force-patch-dropout 0.</code></p><div class=\"markdown-heading\" dir=\"auto\"><h4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Multiple data sources</h4><a id=\"user-content-multiple-data-sources\" class=\"anchor-element\" aria-label=\"Permalink: Multiple data sources\" href=\"#multiple-data-sources\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">OpenCLIP supports using multiple data sources, by separating different data paths with <code>::</code>.For instance, to train on CC12M and on LAION, one might use <code>--train-data \"/data/cc12m/cc12m-train-{0000..2175}.tar::/data/LAION-400M/{00000..41455}.tar\"</code>.Using <code>--dataset-resampled</code> is recommended for these cases.</p><p dir=\"auto\">By default, on expectation the amount of times the model will see a sample from each source is proportional to the size of the source.For instance, when training on one data source with size 400M and one with size 10M, samples from the first source are 40x more likely to be seen in expectation.</p><p dir=\"auto\">We also support different weighting of the data sources, by using the <code>--train-data-upsampling-factors</code> flag.For instance, using <code>--train-data-upsampling-factors=1::1</code> in the above scenario is equivalent to not using the flag, and <code>--train-data-upsampling-factors=1::2</code> is equivalent to upsampling the second data source twice.If you want to sample from data sources with the same frequency, the upsampling factors should be inversely proportional to the sizes of the data sources.For instance, if dataset <code>A</code> has 1000 samples and dataset <code>B</code> has 100 samples, you can use <code>--train-data-upsampling-factors=0.001::0.01</code> (or analogously, <code>--train-data-upsampling-factors=1::10</code>).</p><div class=\"markdown-heading\" dir=\"auto\"><h4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Single-Node</h4><a id=\"user-content-single-node\" class=\"anchor-element\" aria-label=\"Permalink: Single-Node\" href=\"#single-node\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">We make use of <code>torchrun</code> to launch distributed jobs. The following launches aa job on a node of 4 GPUs:</p><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"cd open_clip/srctorchrun --nproc_per_node 4 -m training.main \\    --train-data '/data/cc12m/cc12m-train-{0000..2175}.tar' \\    --train-num-samples 10968539 \\    --dataset-type webdataset \\    --batch-size 320 \\    --precision amp \\    --workers 4 \\    --imagenet-val /data/imagenet/validation/\"><pre><span class=\"pl-c1\">cd</span> open_clip/srctorchrun --nproc_per_node 4 -m training.main \\    --train-data <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/data/cc12m/cc12m-train-{0000..2175}.tar<span class=\"pl-pds\">'</span></span> \\    --train-num-samples 10968539 \\    --dataset-type webdataset \\    --batch-size 320 \\    --precision amp \\    --workers 4 \\    --imagenet-val /data/imagenet/validation/</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Multi-Node</h4><a id=\"user-content-multi-node\" class=\"anchor-element\" aria-label=\"Permalink: Multi-Node\" href=\"#multi-node\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">The same script above works, so long as users include information about the numberof nodes and host node.</p><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"cd open_clip/srctorchrun --nproc_per_node=4 \\    --rdzv_endpoint=$HOSTE_NODE_ADDR \\    -m training.main \\    --train-data '/data/cc12m/cc12m-train-{0000..2175}.tar' \\    --train-num-samples 10968539 \\    --dataset-type webdataset \\    --batch-size 320 \\    --precision amp \\    --workers 4 \\    --imagenet-val /data/imagenet/validation/\"><pre><span class=\"pl-c1\">cd</span> open_clip/srctorchrun --nproc_per_node=4 \\    --rdzv_endpoint=<span class=\"pl-smi\">$HOSTE_NODE_ADDR</span> \\    -m training.main \\    --train-data <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/data/cc12m/cc12m-train-{0000..2175}.tar<span class=\"pl-pds\">'</span></span> \\    --train-num-samples 10968539 \\    --dataset-type webdataset \\    --batch-size 320 \\    --precision amp \\    --workers 4 \\    --imagenet-val /data/imagenet/validation/</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">SLURM</h4><a id=\"user-content-slurm\" class=\"anchor-element\" aria-label=\"Permalink: SLURM\" href=\"#slurm\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">This is likely the easiest solution to utilize. The following script was used totrain our largest models:</p><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"#!/bin/bash -x#SBATCH --nodes=32#SBATCH --gres=gpu:4#SBATCH --ntasks-per-node=4#SBATCH --cpus-per-task=6#SBATCH --wait-all-nodes=1#SBATCH --job-name=open_clip#SBATCH --account=ACCOUNT_NAME#SBATCH --partition PARTITION_NAMEeval &quot;$(/path/to/conda/bin/conda shell.bash hook)&quot; # init condaconda activate open_clipexport CUDA_VISIBLE_DEVICES=0,1,2,3export MASTER_PORT=12802master_addr=$(scontrol show hostnames &quot;$SLURM_JOB_NODELIST&quot; | head -n 1)export MASTER_ADDR=$master_addrcd /shared/open_clipexport PYTHONPATH=&quot;$PYTHONPATH:$PWD/src&quot;srun --cpu_bind=v --accel-bind=gn python -u src/training/main.py \\    --save-frequency 1 \\    --report-to tensorboard \\    --train-data=&quot;/data/LAION-400M/{00000..41455}.tar&quot; \\    --warmup 2000 \\    --batch-size=256 \\    --epochs=32 \\    --workers=8 \\    --model ViT-B-32 \\    --name &quot;ViT-B-32-Vanilla&quot; \\    --seed 0 \\    --local-loss \\    --gather-with-grad\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#!</span>/bin/bash -x</span><span class=\"pl-c\"><span class=\"pl-c\">#</span>SBATCH --nodes=32</span><span class=\"pl-c\"><span class=\"pl-c\">#</span>SBATCH --gres=gpu:4</span><span class=\"pl-c\"><span class=\"pl-c\">#</span>SBATCH --ntasks-per-node=4</span><span class=\"pl-c\"><span class=\"pl-c\">#</span>SBATCH --cpus-per-task=6</span><span class=\"pl-c\"><span class=\"pl-c\">#</span>SBATCH --wait-all-nodes=1</span><span class=\"pl-c\"><span class=\"pl-c\">#</span>SBATCH --job-name=open_clip</span><span class=\"pl-c\"><span class=\"pl-c\">#</span>SBATCH --account=ACCOUNT_NAME</span><span class=\"pl-c\"><span class=\"pl-c\">#</span>SBATCH --partition PARTITION_NAME</span><span class=\"pl-c1\">eval</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-s\"><span class=\"pl-pds\">$(</span>/path/to/conda/bin/conda shell.bash hook<span class=\"pl-pds\">)</span></span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> init conda</span>conda activate open_clip<span class=\"pl-k\">export</span> CUDA_VISIBLE_DEVICES=0,1,2,3<span class=\"pl-k\">export</span> MASTER_PORT=12802master_addr=<span class=\"pl-s\"><span class=\"pl-pds\">$(</span>scontrol show hostnames <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$SLURM_JOB_NODELIST</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">|</span> head -n 1<span class=\"pl-pds\">)</span></span><span class=\"pl-k\">export</span> MASTER_ADDR=<span class=\"pl-smi\">$master_addr</span><span class=\"pl-c1\">cd</span> /shared/open_clip<span class=\"pl-k\">export</span> PYTHONPATH=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$PYTHONPATH</span>:<span class=\"pl-smi\">$PWD</span>/src<span class=\"pl-pds\">\"</span></span>srun --cpu_bind=v --accel-bind=gn python -u src/training/main.py \\    --save-frequency 1 \\    --report-to tensorboard \\    --train-data=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/data/LAION-400M/{00000..41455}.tar<span class=\"pl-pds\">\"</span></span> \\    --warmup 2000 \\    --batch-size=256 \\    --epochs=32 \\    --workers=8 \\    --model ViT-B-32 \\    --name <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ViT-B-32-Vanilla<span class=\"pl-pds\">\"</span></span> \\    --seed 0 \\    --local-loss \\    --gather-with-grad</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Resuming from a checkpoint:</h3><a id=\"user-content-resuming-from-a-checkpoint\" class=\"anchor-element\" aria-label=\"Permalink: Resuming from a checkpoint:\" href=\"#resuming-from-a-checkpoint\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"python -m training.main \\    --train-data=&quot;/path/to/train_data.csv&quot; \\    --val-data=&quot;/path/to/validation_data.csv&quot;  \\    --resume /path/to/checkpoints/epoch_K.pt\"><pre>python -m training.main \\    --train-data=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/path/to/train_data.csv<span class=\"pl-pds\">\"</span></span> \\    --val-data=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/path/to/validation_data.csv<span class=\"pl-pds\">\"</span></span>  \\    --resume /path/to/checkpoints/epoch_K.pt</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Training CoCa:</h3><a id=\"user-content-training-coca\" class=\"anchor-element\" aria-label=\"Permalink: Training CoCa:\" href=\"#training-coca\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">Training <a href=\"https://arxiv.org/abs/2205.01917\" rel=\"nofollow\">CoCa</a> models is enabled through specifying a CoCa config using the <code>--model</code> parameter of the training script. Currently available configs are \"coca_base\", \"coca_ViT-B-32\", and \"coca_roberta-ViT-B-32\" (which uses RoBERTa as the text encoder). CoCa configs are different from CLIP configs because they have an additional \"multimodal_cfg\" component which specifies parameters for the multimodal text decoder. Here's an example from the coca_ViT-B-32 config:</p><div class=\"highlight highlight-source-json notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"&quot;multimodal_cfg&quot;: {&quot;context_length&quot;: 76,&quot;vocab_size&quot;: 49408,&quot;width&quot;: 512,&quot;heads&quot;: 8,&quot;layers&quot;: 12,&quot;latent_dim&quot;: 512,&quot;attn_pooler_heads&quot;: 8}\"><pre><span class=\"pl-ent\">\"multimodal_cfg\"</span>: {<span class=\"pl-ent\">\"context_length\"</span>: <span class=\"pl-c1\">76</span>,<span class=\"pl-ent\">\"vocab_size\"</span>: <span class=\"pl-c1\">49408</span>,<span class=\"pl-ent\">\"width\"</span>: <span class=\"pl-c1\">512</span>,<span class=\"pl-ent\">\"heads\"</span>: <span class=\"pl-c1\">8</span>,<span class=\"pl-ent\">\"layers\"</span>: <span class=\"pl-c1\">12</span>,<span class=\"pl-ent\">\"latent_dim\"</span>: <span class=\"pl-c1\">512</span>,<span class=\"pl-ent\">\"attn_pooler_heads\"</span>: <span class=\"pl-c1\">8</span>}</pre></div><p dir=\"auto\">Credit to <a href=\"https://github.com/lucidrains\">lucidrains</a> for <a href=\"https://github.com/lucidrains/CoCa-pytorch\">initial code</a>, <a href=\"https://github.com/gpucce\">gpucce</a> for adapting the code to open_clip, and <a href=\"https://github.com/iejMac\">iejMac</a> for training the models.</p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Generating text with CoCa</h3><a id=\"user-content-generating-text-with-coca\" class=\"anchor-element\" aria-label=\"Permalink: Generating text with CoCa\" href=\"#generating-text-with-coca\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import open_clipimport torchfrom PIL import Imagemodel, _, transform = open_clip.create_model_and_transforms(  model_name=&quot;coca_ViT-L-14&quot;,  pretrained=&quot;mscoco_finetuned_laion2B-s13B-b90k&quot;)im = Image.open(&quot;cat.jpg&quot;).convert(&quot;RGB&quot;)im = transform(im).unsqueeze(0)with torch.no_grad(), torch.cuda.amp.autocast():  generated = model.generate(im)print(open_clip.decode(generated[0]).split(&quot;&lt;end_of_text&gt;&quot;)[0].replace(&quot;&lt;start_of_text&gt;&quot;, &quot;&quot;))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">open_clip</span><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-v\">PIL</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Image</span><span class=\"pl-s1\">model</span>, <span class=\"pl-s1\">_</span>, <span class=\"pl-s1\">transform</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">open_clip</span>.<span class=\"pl-en\">create_model_and_transforms</span>(  <span class=\"pl-s1\">model_name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"coca_ViT-L-14\"</span>,  <span class=\"pl-s1\">pretrained</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"mscoco_finetuned_laion2B-s13B-b90k\"</span>)<span class=\"pl-s1\">im</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Image</span>.<span class=\"pl-en\">open</span>(<span class=\"pl-s\">\"cat.jpg\"</span>).<span class=\"pl-en\">convert</span>(<span class=\"pl-s\">\"RGB\"</span>)<span class=\"pl-s1\">im</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">transform</span>(<span class=\"pl-s1\">im</span>).<span class=\"pl-en\">unsqueeze</span>(<span class=\"pl-c1\">0</span>)<span class=\"pl-k\">with</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">no_grad</span>(), <span class=\"pl-s1\">torch</span>.<span class=\"pl-s1\">cuda</span>.<span class=\"pl-s1\">amp</span>.<span class=\"pl-en\">autocast</span>():  <span class=\"pl-s1\">generated</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">model</span>.<span class=\"pl-en\">generate</span>(<span class=\"pl-s1\">im</span>)<span class=\"pl-en\">print</span>(<span class=\"pl-s1\">open_clip</span>.<span class=\"pl-en\">decode</span>(<span class=\"pl-s1\">generated</span>[<span class=\"pl-c1\">0</span>]).<span class=\"pl-en\">split</span>(<span class=\"pl-s\">\"&lt;end_of_text&gt;\"</span>)[<span class=\"pl-c1\">0</span>].<span class=\"pl-en\">replace</span>(<span class=\"pl-s\">\"&lt;start_of_text&gt;\"</span>, <span class=\"pl-s\">\"\"</span>))</pre></div><p dir=\"auto\">See also this <a href=\"https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb\" rel=\"nofollow\">[Coca Colab]</a></p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Fine Tuning CoCa</h3><a id=\"user-content-fine-tuning-coca\" class=\"anchor-element\" aria-label=\"Permalink: Fine Tuning CoCa\" href=\"#fine-tuning-coca\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">To fine-tune coca on mscoco, first create the dataset, one way is using a csvdataset and perhaps the simplest way to do it is using <a href=\"https://github.com/LAION-AI/CLIP_benchmark\">CLIP_benchmark</a> which in turn uses <a href=\"https://github.com/cocodataset/cocoapi\">pycocotools</a> (that can be used also by itself).</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"from clip_benchmark.datasets.builder import build_datasetimport pandas as pdimport osroot_path = &quot;path/to/data/dir&quot; # set this to smth meaningfulds = build_dataset(&quot;mscoco_captions&quot;, root=root_path, split=&quot;train&quot;) # this downloads the dataset if it is not there alreadycoco = ds.cocoimgs = coco.loadImgs(coco.getImgIds())future_df = {&quot;filepath&quot;:[], &quot;title&quot;:[]}for img in imgs:    caps = coco.imgToAnns[img[&quot;id&quot;]]    for cap in caps:        future_df[&quot;filepath&quot;].append(img[&quot;file_name&quot;])        future_df[&quot;title&quot;].append(cap[&quot;caption&quot;])pd.DataFrame.from_dict(future_df).to_csv(  os.path.join(root_path, &quot;train2014.csv&quot;), index=False, sep=&quot;\\t&quot;)\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">clip_benchmark</span>.<span class=\"pl-s1\">datasets</span>.<span class=\"pl-s1\">builder</span> <span class=\"pl-k\">import</span> <span class=\"pl-s1\">build_dataset</span><span class=\"pl-k\">import</span> <span class=\"pl-s1\">pandas</span> <span class=\"pl-k\">as</span> <span class=\"pl-s1\">pd</span><span class=\"pl-k\">import</span> <span class=\"pl-s1\">os</span><span class=\"pl-s1\">root_path</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">\"path/to/data/dir\"</span> <span class=\"pl-c\"># set this to smth meaningful</span><span class=\"pl-s1\">ds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">build_dataset</span>(<span class=\"pl-s\">\"mscoco_captions\"</span>, <span class=\"pl-s1\">root</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">root_path</span>, <span class=\"pl-s1\">split</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"train\"</span>) <span class=\"pl-c\"># this downloads the dataset if it is not there already</span><span class=\"pl-s1\">coco</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">ds</span>.<span class=\"pl-s1\">coco</span><span class=\"pl-s1\">imgs</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">coco</span>.<span class=\"pl-en\">loadImgs</span>(<span class=\"pl-s1\">coco</span>.<span class=\"pl-en\">getImgIds</span>())<span class=\"pl-s1\">future_df</span> <span class=\"pl-c1\">=</span> {<span class=\"pl-s\">\"filepath\"</span>:[], <span class=\"pl-s\">\"title\"</span>:[]}<span class=\"pl-k\">for</span> <span class=\"pl-s1\">img</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">imgs</span>:    <span class=\"pl-s1\">caps</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">coco</span>.<span class=\"pl-s1\">imgToAnns</span>[<span class=\"pl-s1\">img</span>[<span class=\"pl-s\">\"id\"</span>]]    <span class=\"pl-k\">for</span> <span class=\"pl-s1\">cap</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">caps</span>:        <span class=\"pl-s1\">future_df</span>[<span class=\"pl-s\">\"filepath\"</span>].<span class=\"pl-en\">append</span>(<span class=\"pl-s1\">img</span>[<span class=\"pl-s\">\"file_name\"</span>])        <span class=\"pl-s1\">future_df</span>[<span class=\"pl-s\">\"title\"</span>].<span class=\"pl-en\">append</span>(<span class=\"pl-s1\">cap</span>[<span class=\"pl-s\">\"caption\"</span>])<span class=\"pl-s1\">pd</span>.<span class=\"pl-v\">DataFrame</span>.<span class=\"pl-en\">from_dict</span>(<span class=\"pl-s1\">future_df</span>).<span class=\"pl-en\">to_csv</span>(  <span class=\"pl-s1\">os</span>.<span class=\"pl-s1\">path</span>.<span class=\"pl-en\">join</span>(<span class=\"pl-s1\">root_path</span>, <span class=\"pl-s\">\"train2014.csv\"</span>), <span class=\"pl-s1\">index</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-s1\">sep</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"<span class=\"pl-cce\">\\t</span>\"</span>)</pre></div><p dir=\"auto\">This should create a csv dataset that one can use to fine-tune coca with open_clip</p><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"python -m training.main \\    --dataset-type &quot;csv&quot; \\    --train-data &quot;path/to/data/dir/train2014.csv&quot; \\    --warmup 1000 \\    --batch-size 128 \\    --lr 1e-5 \\    --wd 0.1 \\    --epochs 1 \\    --workers 3 \\    --model &quot;coca_ViT-L-14&quot; \\    --report-to &quot;wandb&quot; \\    --coca-contrastive-loss-weight 0 \\    --coca-caption-loss-weight 1 \\    --log-every-n-steps 100\"><pre>python -m training.main \\    --dataset-type <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>csv<span class=\"pl-pds\">\"</span></span> \\    --train-data <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>path/to/data/dir/train2014.csv<span class=\"pl-pds\">\"</span></span> \\    --warmup 1000 \\    --batch-size 128 \\    --lr 1e-5 \\    --wd 0.1 \\    --epochs 1 \\    --workers 3 \\    --model <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>coca_ViT-L-14<span class=\"pl-pds\">\"</span></span> \\    --report-to <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>wandb<span class=\"pl-pds\">\"</span></span> \\    --coca-contrastive-loss-weight 0 \\    --coca-caption-loss-weight 1 \\    --log-every-n-steps 100</pre></div><p dir=\"auto\">This is a general setting, open_clip has very parameters that can be set, <code>python -m training.main --help</code> should show them. The only relevant change compared to pre-training are the two arguments</p><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"--coca-contrastive-loss-weight 0--coca-caption-loss-weight 1\"><pre>--coca-contrastive-loss-weight 0--coca-caption-loss-weight 1</pre></div><p dir=\"auto\">which make the model only train the generative side.</p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Training with pre-trained language models as text encoder:</h3><a id=\"user-content-training-with-pre-trained-language-models-as-text-encoder\" class=\"anchor-element\" aria-label=\"Permalink: Training with pre-trained language models as text encoder:\" href=\"#training-with-pre-trained-language-models-as-text-encoder\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">If you wish to use different language models as the text encoder for CLIP you can do so by using one of the Hugging Face model configs in <code>src/open_clip/model_configs</code> and passing in it's tokenizer as the <code>--model</code> and <code>--hf-tokenizer-name</code> parameters respectively. Currently we only support RoBERTa (\"test-roberta\" config), however adding new models should be trivial. You can also determine how many layers, from the end, to leave unfrozen with the <code>--lock-text-unlocked-layers</code> parameter. Here's an example command to train CLIP with the RoBERTa LM that has it's last 10 layers unfrozen:</p><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"python -m training.main \\         --train-data=&quot;pipe:aws s3 cp s3://s-mas/cc3m/{00000..00329}.tar -&quot; \\         --train-num-samples 3000000 \\         --val-data=&quot;pipe:aws s3 cp s3://s-mas/cc3m/{00330..00331}.tar -&quot; \\         --val-num-samples 10000 \\         --dataset-type webdataset \\         --batch-size 256 \\         --warmup 2000 \\         --epochs 10 \\         --lr 5e-4 \\         --precision amp \\         --workers 6 \\         --model &quot;roberta-ViT-B-32&quot; \\         --lock-text \\         --lock-text-unlocked-layers 10 \\         --name &quot;10_unfrozen&quot; \\         --report-to &quot;tensorboard&quot; \\\"><pre>python -m training.main \\         --train-data=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>pipe:aws s3 cp s3://s-mas/cc3m/{00000..00329}.tar -<span class=\"pl-pds\">\"</span></span> \\         --train-num-samples 3000000 \\         --val-data=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>pipe:aws s3 cp s3://s-mas/cc3m/{00330..00331}.tar -<span class=\"pl-pds\">\"</span></span> \\         --val-num-samples 10000 \\         --dataset-type webdataset \\         --batch-size 256 \\         --warmup 2000 \\         --epochs 10 \\         --lr 5e-4 \\         --precision amp \\         --workers 6 \\         --model <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>roberta-ViT-B-32<span class=\"pl-pds\">\"</span></span> \\         --lock-text \\         --lock-text-unlocked-layers 10 \\         --name <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>10_unfrozen<span class=\"pl-pds\">\"</span></span> \\         --report-to <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorboard<span class=\"pl-pds\">\"</span></span> \\</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Loss Curves</h3><a id=\"user-content-loss-curves\" class=\"anchor-element\" aria-label=\"Permalink: Loss Curves\" href=\"#loss-curves\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">When run on a machine with 8 GPUs the command should produce the following training curve for Conceptual Captions:</p><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/clip_zeroshot.png\"><img src=\"https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/clip_zeroshot.png\" alt=\"CLIP zero shot training curve\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">More detailed curves for Conceptual Captions are given at <a href=\"/mlfoundations/open_clip/blob/main/docs/clip_conceptual_captions.md\">/docs/clip_conceptual_captions.md</a>.</p><p dir=\"auto\">When training a RN50 on YFCC the same hyperparameters as above are used, with the exception of <code>lr=5e-4</code> and <code>epochs=32</code>.</p><p dir=\"auto\">Note that to use another model, like <code>ViT-B/32</code> or <code>RN50x4</code> or <code>RN50x16</code> or <code>ViT-B/16</code>, specify with <code>--model RN50x4</code>.</p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Logging</h3><a id=\"user-content-logging\" class=\"anchor-element\" aria-label=\"Permalink: Logging\" href=\"#logging\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">For tensorboard logging, run:</p><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"tensorboard --logdir=logs/tensorboard/ --port=7777\"><pre>tensorboard --logdir=logs/tensorboard/ --port=7777</pre></div><p dir=\"auto\">For wandb logging, we recommend looking at the <code>step</code> variable instead of <code>Step</code>, since the later was not properly set in earlier versions of this codebase.For older runs with models trained before <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load title\" data-id=\"1870186659\" data-permission-text=\"Title is private\" data-url=\"https://github.com/mlfoundations/open_clip/issues/613\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/mlfoundations/open_clip/pull/613/hovercard\" href=\"https://github.com/mlfoundations/open_clip/pull/613\">#613</a>, the <code>Step</code> variable should be ignored.For newer runs, after that PR, the two variables are the same.</p><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Evaluation / Zero-Shot</h2><a id=\"user-content-evaluation--zero-shot\" class=\"anchor-element\" aria-label=\"Permalink: Evaluation / Zero-Shot\" href=\"#evaluation--zero-shot\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">We recommend <a href=\"https://github.com/LAION-AI/CLIP_benchmark#how-to-use\">https://github.com/LAION-AI/CLIP_benchmark#how-to-use</a> for systematic evaluation on 40 datasets.</p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Evaluating local checkpoint:</h3><a id=\"user-content-evaluating-local-checkpoint\" class=\"anchor-element\" aria-label=\"Permalink: Evaluating local checkpoint:\" href=\"#evaluating-local-checkpoint\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"python -m training.main \\    --val-data=&quot;/path/to/validation_data.csv&quot;  \\    --model RN101 \\    --pretrained /path/to/checkpoints/epoch_K.pt\"><pre>python -m training.main \\    --val-data=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/path/to/validation_data.csv<span class=\"pl-pds\">\"</span></span>  \\    --model RN101 \\    --pretrained /path/to/checkpoints/epoch_K.pt</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Evaluating hosted pretrained checkpoint on ImageNet zero-shot prediction:</h3><a id=\"user-content-evaluating-hosted-pretrained-checkpoint-on-imagenet-zero-shot-prediction\" class=\"anchor-element\" aria-label=\"Permalink: Evaluating hosted pretrained checkpoint on ImageNet zero-shot prediction:\" href=\"#evaluating-hosted-pretrained-checkpoint-on-imagenet-zero-shot-prediction\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"python -m training.main \\    --imagenet-val /path/to/imagenet/validation \\    --model ViT-B-32-quickgelu \\    --pretrained laion400m_e32\"><pre>python -m training.main \\    --imagenet-val /path/to/imagenet/validation \\    --model ViT-B-32-quickgelu \\    --pretrained laion400m_e32</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Model distillation</h3><a id=\"user-content-model-distillation\" class=\"anchor-element\" aria-label=\"Permalink: Model distillation\" href=\"#model-distillation\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">You can distill from a pre-trained by using <code>--distill-model</code> and <code>--distill-pretrained</code> to specify the model you'd like to distill from.For instance, to distill from OpenAI ViT-L/14 use <code>--distill-model ViT-L-14 --distill-pretrained openai</code>.</p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Gradient accumulation</h3><a id=\"user-content-gradient-accumulation\" class=\"anchor-element\" aria-label=\"Permalink: Gradient accumulation\" href=\"#gradient-accumulation\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">To simulate larger batches use <code>--accum-freq k</code>. If per gpu batch size, <code>--batch-size</code>, is <code>m</code>, then the effective batch size will be <code>k * m * num_gpus</code>.</p><p dir=\"auto\">When increasing <code>--accum-freq</code> from its default of 1, samples/s will remain approximately constant (batch size will double, as will time-per-batch). It is recommended to use other features to reduce batch size such as <code>--grad-checkpointing --local-loss --gather-with-grad</code> before increasing <code>--accum-freq</code>. <code>--accum-freq</code> can be used in addition to these features.</p><p dir=\"auto\">Instead of 1 forward pass per example, there are now 2 forward passes per-example. However, the first is done with <code>torch.no_grad</code>.</p><p dir=\"auto\">There is some additional GPU memory required --- the features and data from all <code>m</code> batches are stored in memory.</p><p dir=\"auto\">There are also <code>m</code> loss computations instead of the usual 1.</p><p dir=\"auto\">For more information see Cui et al. (<a href=\"https://arxiv.org/abs/2112.09331\" rel=\"nofollow\">https://arxiv.org/abs/2112.09331</a>) or Pham et al. (<a href=\"https://arxiv.org/abs/2111.10050\" rel=\"nofollow\">https://arxiv.org/abs/2111.10050</a>).</p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Int8 Support</h3><a id=\"user-content-int8-support\" class=\"anchor-element\" aria-label=\"Permalink: Int8 Support\" href=\"#int8-support\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">We have beta support for int8 training and inference.You can enable int8 training with <code>--use-bnb-linear SwitchBackLinearGlobal</code> or <code>--use-bnb-linear SwitchBackLinearGlobalMemEfficient</code>.Please see the bitsandbytes library for definitions for these layers.For CLIP VIT-Huge this should currently correspond to a 10% training speedup with no accuracy loss.More speedups comin when the attention layer is refactored so that linear layers man be replaced there, too.</p><p dir=\"auto\">See the tutorial <a href=\"https://github.com/mlfoundations/open_clip/blob/main/tutorials/int8_tutorial.ipynb\">https://github.com/mlfoundations/open_clip/blob/main/tutorials/int8_tutorial.ipynb</a> or <a href=\"https://arxiv.org/abs/2304.13013\" rel=\"nofollow\">paper</a>.</p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Support for remote loading/training</h3><a id=\"user-content-support-for-remote-loadingtraining\" class=\"anchor-element\" aria-label=\"Permalink: Support for remote loading/training\" href=\"#support-for-remote-loadingtraining\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">It is always possible to resume directly from a remote file, e.g., a file in an s3 bucket. Just set <code>--resume s3://&lt;path-to-checkpoint&gt; </code>.This will work with any filesystem supported by <code>fsspec</code>.</p><p dir=\"auto\">It is also possible to train <code>open_clip</code> models while continuously backing up to s3. This can help to avoid slow local file systems.</p><p dir=\"auto\">Say that your node has a local ssd <code>/scratch</code>, an s3 bucket <code>s3://&lt;path-to-bucket&gt;</code>.</p><p dir=\"auto\">In that case, set <code>--logs /scratch</code> and <code>--remote-sync s3://&lt;path-to-bucket&gt;</code>. Then, a background process will sync <code>/scratch/&lt;run-name&gt;</code> to <code>s3://&lt;path-to-bucket&gt;/&lt;run-name&gt;</code>. After syncing, the background process will sleep for <code>--remote-sync-frequency</code> seconds, which defaults to 5 minutes.</p><p dir=\"auto\">There is also experimental support for syncing to other remote file systems, not just s3. To do so, specify <code>--remote-sync-protocol fsspec</code>. However, this is currently very slow and not recommended.</p><p dir=\"auto\">Also, to optionally avoid saving too many checkpoints locally when using these features, you can use <code>--delete-previous-checkpoint</code> which deletes the previous checkpoint after saving a new one.</p><p dir=\"auto\">Note: if you are using this feature with <code>--resume latest</code>, there are a few warnings. First, use with <code>--save-most-recent</code> is not supported. Second, only <code>s3</code> is supported. Finally, since the sync happens in the background, it is possible that the most recent checkpoint may not be finished syncing to the remote.</p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Pushing Models to Hugging Face Hub</h3><a id=\"user-content-pushing-models-to-hugging-face-hub\" class=\"anchor-element\" aria-label=\"Permalink: Pushing Models to Hugging Face Hub\" href=\"#pushing-models-to-hugging-face-hub\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">The module <code>open_clip.push_to_hf_hub</code> includes helpers for pushing models /w weights and config to the HF Hub.</p><p dir=\"auto\">The tool can be run from command line, ex:<code>python -m open_clip.push_to_hf_hub --model convnext_large_d_320 --pretrained /train/checkpoints/epoch_12.pt --repo-id laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft</code></p><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Acknowledgments</h2><a id=\"user-content-acknowledgments\" class=\"anchor-element\" aria-label=\"Permalink: Acknowledgments\" href=\"#acknowledgments\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">We gratefully acknowledge the Gauss Centre for Supercomputing e.V. (<a href=\"http://www.gauss-centre.eu\" rel=\"nofollow\">www.gauss-centre.eu</a>) for funding this part of work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at J\u00fclich Supercomputing Centre (JSC).</p><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">The Team</h2><a id=\"user-content-the-team\" class=\"anchor-element\" aria-label=\"Permalink: The Team\" href=\"#the-team\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">Current development of this repository is led by <a href=\"https://rwightman.com/\" rel=\"nofollow\">Ross Wightman</a>, <a href=\"https://github.com/rom1504\">Romain Beaumont</a>, <a href=\"http://cadegordon.io/\" rel=\"nofollow\">Cade Gordon</a>, and <a href=\"http://vaishaal.com/\" rel=\"nofollow\">Vaishaal Shankar</a>.</p><p dir=\"auto\">The original version of this repository is from a group of researchers at UW, Google, Stanford, Amazon, Columbia, and Berkeley.</p><p dir=\"auto\"><a href=\"http://gabrielilharco.com/\" rel=\"nofollow\">Gabriel Ilharco*</a>, <a href=\"https://mitchellnw.github.io/\" rel=\"nofollow\">Mitchell Wortsman*</a>, <a href=\"https://nicholas.carlini.com/\" rel=\"nofollow\">Nicholas Carlini</a>, <a href=\"https://www.rohantaori.com/\" rel=\"nofollow\">Rohan Taori</a>, <a href=\"http://www.achaldave.com/\" rel=\"nofollow\">Achal Dave</a>, <a href=\"http://vaishaal.com/\" rel=\"nofollow\">Vaishaal Shankar</a>, <a href=\"https://people.eecs.berkeley.edu/~miller_john/\" rel=\"nofollow\">John Miller</a>, <a href=\"https://hsnamkoong.github.io/\" rel=\"nofollow\">Hongseok Namkoong</a>, <a href=\"https://homes.cs.washington.edu/~hannaneh/\" rel=\"nofollow\">Hannaneh Hajishirzi</a>, <a href=\"https://homes.cs.washington.edu/~ali/\" rel=\"nofollow\">Ali Farhadi</a>, <a href=\"https://people.csail.mit.edu/ludwigs/\" rel=\"nofollow\">Ludwig Schmidt</a></p><p dir=\"auto\">Special thanks to <a href=\"https://jongwook.kim/\" rel=\"nofollow\">Jong Wook Kim</a> and <a href=\"https://github.com/Newmu\">Alec Radford</a> for help with reproducing CLIP!</p><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Citing</h2><a id=\"user-content-citing\" class=\"anchor-element\" aria-label=\"Permalink: Citing\" href=\"#citing\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">If you found this repository useful, please consider citing:</p><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@software{ilharco_gabriel_2021_5143773,  author       = {Ilharco, Gabriel and                  Wortsman, Mitchell and                  Wightman, Ross and                  Gordon, Cade and                  Carlini, Nicholas and                  Taori, Rohan and                  Dave, Achal and                  Shankar, Vaishaal and                  Namkoong, Hongseok and                  Miller, John and                  Hajishirzi, Hannaneh and                  Farhadi, Ali and                  Schmidt, Ludwig},  title        = {OpenCLIP},  month        = jul,  year         = 2021,  note         = {If you use this software, please cite it as below.},  publisher    = {Zenodo},  version      = {0.1},  doi          = {10.5281/zenodo.5143773},  url          = {https://doi.org/10.5281/zenodo.5143773}}\"><pre><span class=\"pl-k\">@software</span>{<span class=\"pl-en\">ilharco_gabriel_2021_5143773</span>,  <span class=\"pl-s\">author</span>       = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Ilharco, Gabriel and</span><span class=\"pl-s\">                  Wortsman, Mitchell and</span><span class=\"pl-s\">                  Wightman, Ross and</span><span class=\"pl-s\">                  Gordon, Cade and</span><span class=\"pl-s\">                  Carlini, Nicholas and</span><span class=\"pl-s\">                  Taori, Rohan and</span><span class=\"pl-s\">                  Dave, Achal and</span><span class=\"pl-s\">                  Shankar, Vaishaal and</span><span class=\"pl-s\">                  Namkoong, Hongseok and</span><span class=\"pl-s\">                  Miller, John and</span><span class=\"pl-s\">                  Hajishirzi, Hannaneh and</span><span class=\"pl-s\">                  Farhadi, Ali and</span><span class=\"pl-s\">                  Schmidt, Ludwig<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">title</span>        = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>OpenCLIP<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">month</span>        = jul,  <span class=\"pl-s\">year</span>         = <span class=\"pl-c1\">2021</span>,  <span class=\"pl-s\">note</span>         = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>If you use this software, please cite it as below.<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">publisher</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Zenodo<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">version</span>      = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>0.1<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">doi</span>          = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>10.5281/zenodo.5143773<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">url</span>          = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://doi.org/10.5281/zenodo.5143773<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{cherti2023reproducible,  title={Reproducible scaling laws for contrastive language-image learning},  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},  pages={2818--2829},  year={2023}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">cherti2023reproducible</span>,  <span class=\"pl-s\">title</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>Reproducible scaling laws for contrastive language-image learning<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">author</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">booktitle</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">pages</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>2818--2829<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">year</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Radford2021LearningTV,  title={Learning Transferable Visual Models From Natural Language Supervision},  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},  booktitle={ICML},  year={2021}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Radford2021LearningTV</span>,  <span class=\"pl-s\">title</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>Learning Transferable Visual Models From Natural Language Supervision<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">author</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">booktitle</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>ICML<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">year</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{schuhmann2022laionb,  title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},  author={Christoph Schuhmann and          Romain Beaumont and          Richard Vencu and          Cade W Gordon and          Ross Wightman and          Mehdi Cherti and          Theo Coombes and          Aarush Katta and          Clayton Mullis and          Mitchell Wortsman and          Patrick Schramowski and          Srivatsa R Kundurthy and          Katherine Crowson and          Ludwig Schmidt and          Robert Kaczmarczyk and          Jenia Jitsev},  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},  year={2022},  url={https://openreview.net/forum?id=M3Y74vmsMcY}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">schuhmann2022laionb</span>,  <span class=\"pl-s\">title</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>{LAION}-5B: An open large-scale dataset for training next generation image-text models<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">author</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>Christoph Schuhmann and</span><span class=\"pl-s\">          Romain Beaumont and</span><span class=\"pl-s\">          Richard Vencu and</span><span class=\"pl-s\">          Cade W Gordon and</span><span class=\"pl-s\">          Ross Wightman and</span><span class=\"pl-s\">          Mehdi Cherti and</span><span class=\"pl-s\">          Theo Coombes and</span><span class=\"pl-s\">          Aarush Katta and</span><span class=\"pl-s\">          Clayton Mullis and</span><span class=\"pl-s\">          Mitchell Wortsman and</span><span class=\"pl-s\">          Patrick Schramowski and</span><span class=\"pl-s\">          Srivatsa R Kundurthy and</span><span class=\"pl-s\">          Katherine Crowson and</span><span class=\"pl-s\">          Ludwig Schmidt and</span><span class=\"pl-s\">          Robert Kaczmarczyk and</span><span class=\"pl-s\">          Jenia Jitsev<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">booktitle</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">year</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>,  <span class=\"pl-s\">url</span>=<span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://openreview.net/forum?id=M3Y74vmsMcY<span class=\"pl-pds\">}</span></span>}</pre></div><p dir=\"auto\"><a href=\"https://zenodo.org/badge/latestdoi/390536799\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8b5a675f92c07e9595aa1843dbe843d3d1398f78c403dae68a82ea5bd586ab13/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3339303533363739392e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/390536799.svg\" style=\"max-width: 100%;\"></a></p></article></div></div></div></div></div> <!-- --> <!-- --> <script type=\"application/json\" id=\"__PRIMER_DATA__\">{\"resolvedServerColorMode\":\"day\"}</script></div></react-partial>        <input type=\"hidden\" data-csrf=\"true\" value=\"j7hhUEBjeNatiQrwlxZkWAgN4JDRNL9a1rAfXoD7/eHZysDwYondPveSfy00DVpmtPiuYizMIeHkxvXSyfrYUA==\" /></div>  <div data-view-component=\"true\" class=\"Layout-sidebar\">            <div class=\"BorderGrid about-margin\" data-pjax>        <div class=\"BorderGrid-row\">          <div class=\"BorderGrid-cell\">            <div class=\"hide-sm hide-md\">  <h2 class=\"mb-3 h4\">About</h2>      <p class=\"f4 my-3\">        An open source implementation of CLIP.      </p>    <h3 class=\"sr-only\">Topics</h3>    <div class=\"my-3\">        <div class=\"f6\">      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:computer-vision\" href=\"/topics/computer-vision\" title=\"Topic: computer-vision\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  computer-vision</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:deep-learning\" href=\"/topics/deep-learning\" title=\"Topic: deep-learning\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  deep-learning</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:pytorch\" href=\"/topics/pytorch\" title=\"Topic: pytorch\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  pytorch</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:pretrained-models\" href=\"/topics/pretrained-models\" title=\"Topic: pretrained-models\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  pretrained-models</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:language-model\" href=\"/topics/language-model\" title=\"Topic: language-model\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  language-model</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:contrastive-loss\" href=\"/topics/contrastive-loss\" title=\"Topic: contrastive-loss\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  contrastive-loss</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:multi-modal-learning\" href=\"/topics/multi-modal-learning\" title=\"Topic: multi-modal-learning\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  multi-modal-learning</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:zero-shot-classification\" href=\"/topics/zero-shot-classification\" title=\"Topic: zero-shot-classification\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  zero-shot-classification</a>  </div>    </div>    <h3 class=\"sr-only\">Resources</h3>    <div class=\"mt-2\">      <a class=\"Link--muted\" data-analytics-event=\"{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:readme&quot;}\" href=\"#readme-ov-file\">        <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-book mr-2\">    <path d=\"M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z\"></path></svg>        Readme</a>    </div>      <h3 class=\"sr-only\">License</h3>  <div class=\"mt-2\">    <a href=\"#License-1-ov-file\"      class=\"Link--muted\"            data-analytics-event=\"{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:license&quot;}\"    >      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-law mr-2\">    <path d=\"M8.75.75V2h.985c.304 0 .603.08.867.231l1.29.736c.038.022.08.033.124.033h2.234a.75.75 0 0 1 0 1.5h-.427l2.111 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.006.005-.01.01-.045.04c-.21.176-.441.327-.686.45C14.556 10.78 13.88 11 13 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L12.178 4.5h-.162c-.305 0-.604-.079-.868-.231l-1.29-.736a.245.245 0 0 0-.124-.033H8.75V13h2.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1 0-1.5h2.5V3.5h-.984a.245.245 0 0 0-.124.033l-1.289.737c-.265.15-.564.23-.869.23h-.162l2.112 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.016.015-.045.04c-.21.176-.441.327-.686.45C4.556 10.78 3.88 11 3 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L2.178 4.5H1.75a.75.75 0 0 1 0-1.5h2.234a.249.249 0 0 0 .125-.033l1.288-.737c.265-.15.564-.23.869-.23h.984V.75a.75.75 0 0 1 1.5 0Zm2.945 8.477c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L13 6.327Zm-10 0c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L3 6.327Z\"></path></svg>     View license    </a>  </div>  <include-fragment  src=\"/mlfoundations/open_clip/hovercards/citation/sidebar_partial?tree_name=main\">  </include-fragment>  <div class=\"mt-2\">    <a href=\"/mlfoundations/open_clip/activity\" data-view-component=\"true\" class=\"Link Link--muted\">      <svg text=\"gray\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-pulse mr-2\">    <path d=\"M6 2c.306 0 .582.187.696.471L10 10.731l1.304-3.26A.751.751 0 0 1 12 7h3.25a.75.75 0 0 1 0 1.5h-2.742l-1.812 4.528a.751.751 0 0 1-1.392 0L6 4.77 4.696 8.03A.75.75 0 0 1 4 8.5H.75a.75.75 0 0 1 0-1.5h2.742l1.812-4.529A.751.751 0 0 1 6 2Z\"></path></svg>      <span class=\"color-fg-muted\">Activity</span></a>  </div>    <div class=\"mt-2\">      <a href=\"/mlfoundations/open_clip/custom-properties\" data-view-component=\"true\" class=\"Link Link--muted\">        <svg text=\"gray\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-note mr-2\">    <path d=\"M0 3.75C0 2.784.784 2 1.75 2h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 14H1.75A1.75 1.75 0 0 1 0 12.25Zm1.75-.25a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-8.5a.25.25 0 0 0-.25-.25ZM3.5 6.25a.75.75 0 0 1 .75-.75h7a.75.75 0 0 1 0 1.5h-7a.75.75 0 0 1-.75-.75Zm.75 2.25h4a.75.75 0 0 1 0 1.5h-4a.75.75 0 0 1 0-1.5Z\"></path></svg>        <span class=\"color-fg-muted\">Custom properties</span></a>    </div>  <h3 class=\"sr-only\">Stars</h3>  <div class=\"mt-2\">    <a href=\"/mlfoundations/open_clip/stargazers\" data-view-component=\"true\" class=\"Link Link--muted\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-star mr-2\">    <path d=\"M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z\"></path></svg>      <strong>7.9k</strong>      stars</a>  </div>  <h3 class=\"sr-only\">Watchers</h3>  <div class=\"mt-2\">    <a href=\"/mlfoundations/open_clip/watchers\" data-view-component=\"true\" class=\"Link Link--muted\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-eye mr-2\">    <path d=\"M8 2c1.981 0 3.671.992 4.933 2.078 1.27 1.091 2.187 2.345 2.637 3.023a1.62 1.62 0 0 1 0 1.798c-.45.678-1.367 1.932-2.637 3.023C11.67 13.008 9.981 14 8 14c-1.981 0-3.671-.992-4.933-2.078C1.797 10.83.88 9.576.43 8.898a1.62 1.62 0 0 1 0-1.798c.45-.677 1.367-1.931 2.637-3.022C4.33 2.992 6.019 2 8 2ZM1.679 7.932a.12.12 0 0 0 0 .136c.411.622 1.241 1.75 2.366 2.717C5.176 11.758 6.527 12.5 8 12.5c1.473 0 2.825-.742 3.955-1.715 1.124-.967 1.954-2.096 2.366-2.717a.12.12 0 0 0 0-.136c-.412-.621-1.242-1.75-2.366-2.717C10.824 4.242 9.473 3.5 8 3.5c-1.473 0-2.825.742-3.955 1.715-1.124.967-1.954 2.096-2.366 2.717ZM8 10a2 2 0 1 1-.001-3.999A2 2 0 0 1 8 10Z\"></path></svg>      <strong>75</strong>      watching</a>  </div>  <h3 class=\"sr-only\">Forks</h3>  <div class=\"mt-2\">    <a href=\"/mlfoundations/open_clip/forks\" data-view-component=\"true\" class=\"Link Link--muted\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo-forked mr-2\">    <path d=\"M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z\"></path></svg>      <strong>786</strong>      forks</a>  </div>    <div class=\"mt-2\">      <a class=\"Link--muted\" href=\"/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fmlfoundations%2Fopen_clip&amp;report=mlfoundations+%28user%29\">          Report repository</a>    </div></div>          </div>        </div>                    <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\">  <a href=\"/mlfoundations/open_clip/releases\" data-view-component=\"true\" class=\"Link--primary no-underline Link\">    Releases      <span title=\"45\" data-view-component=\"true\" class=\"Counter\">45</span></a></h2>  <a class=\"Link--primary d-flex no-underline\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" href=\"/mlfoundations/open_clip/releases/tag/v2.24.0\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-tag flex-shrink-0 mt-1 color-fg-success\">    <path d=\"M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z\"></path></svg>    <div class=\"ml-2 min-width-0\">      <div class=\"d-flex\">        <span class=\"css-truncate css-truncate-target text-bold mr-2\" style=\"max-width: none;\">v2.24.0</span>        <span title=\"Label: Latest\" data-view-component=\"true\" class=\"Label Label--success flex-shrink-0\">          Latest</span>      </div>      <div class=\"text-small color-fg-muted\"><relative-time datetime=\"2024-01-08T10:35:50Z\" class=\"no-wrap\">Jan 8, 2024</relative-time></div>    </div></a>    <div data-view-component=\"true\" class=\"mt-3\">      <a text=\"small\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" href=\"/mlfoundations/open_clip/releases\" data-view-component=\"true\" class=\"Link\">        + 44 releases</a></div>              </div>            </div>                            <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\">  <a href=\"/orgs/mlfoundations/packages?repo_name=open_clip\" data-view-component=\"true\" class=\"Link--primary no-underline Link d-flex flex-items-center\">    Packages      <span title=\"0\" hidden=\"hidden\" data-view-component=\"true\" class=\"Counter ml-1\">0</span></a></h2>      <div class=\"text-small color-fg-muted\">        No packages published <br>      </div>              </div>            </div>                    <div class=\"BorderGrid-row\" hidden>              <div class=\"BorderGrid-cell\">                <include-fragment src=\"/mlfoundations/open_clip/used_by_list\" accept=\"text/fragment+html\"></include-fragment>              </div>            </div>                    <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\">  <a href=\"/mlfoundations/open_clip/graphs/contributors\" data-view-component=\"true\" class=\"Link--primary no-underline Link d-flex flex-items-center\">    Contributors      <span title=\"48\" data-view-component=\"true\" class=\"Counter ml-1\">48</span></a></h2>      <ul class=\"list-style-none d-flex flex-wrap mb-n2\">    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/rwightman\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/rwightman/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/5702664?s=64&amp;v=4\" alt=\"@rwightman\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/rom1504\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/rom1504/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/2346494?s=64&amp;v=4\" alt=\"@rom1504\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/gabrielilharco\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/gabrielilharco/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/3949547?s=64&amp;v=4\" alt=\"@gabrielilharco\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/mitchellnw\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/mitchellnw/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/41302489?s=64&amp;v=4\" alt=\"@mitchellnw\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/Zasder3\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/Zasder3/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/5066934?s=64&amp;v=4\" alt=\"@Zasder3\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/iejMac\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/iejMac/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/61431446?s=64&amp;v=4\" alt=\"@iejMac\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/bryant1410\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/bryant1410/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/3905501?s=64&amp;v=4\" alt=\"@bryant1410\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/gpucce\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/gpucce/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/32967787?s=64&amp;v=4\" alt=\"@gpucce\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/EIFY\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/EIFY/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/2584418?s=64&amp;v=4\" alt=\"@EIFY\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/lopho\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/lopho/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/1366969?s=64&amp;v=4\" alt=\"@lopho\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/zw615\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/zw615/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/26880977?s=64&amp;v=4\" alt=\"@zw615\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/carlini\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/carlini/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/1269300?s=64&amp;v=4\" alt=\"@carlini\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/JeniaJitsev\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/JeniaJitsev/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/2384186?s=64&amp;v=4\" alt=\"@JeniaJitsev\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/visheratin\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/visheratin/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/3251552?s=64&amp;v=4\" alt=\"@visheratin\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li></ul>  <div data-view-component=\"true\" class=\"mt-3\">    <a text=\"small\" href=\"/mlfoundations/open_clip/graphs/contributors\" data-view-component=\"true\" class=\"Link\">      + 34 contributors</a></div>              </div>            </div>                            <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\">Languages</h2><div class=\"mb-2\">  <span data-view-component=\"true\" class=\"Progress\">    <span style=\"background-color:#DA5B0B !important;;width: 54.4%;\" itemprop=\"keywords\" aria-label=\"Jupyter Notebook 54.4\" data-view-component=\"true\" class=\"Progress-item color-bg-success-emphasis\"></span>    <span style=\"background-color:#3572A5 !important;;width: 45.3%;\" itemprop=\"keywords\" aria-label=\"Python 45.3\" data-view-component=\"true\" class=\"Progress-item color-bg-success-emphasis\"></span>    <span style=\"background-color:#ededed !important;;width: 0.3%;\" itemprop=\"keywords\" aria-label=\"Other 0.3\" data-view-component=\"true\" class=\"Progress-item color-bg-success-emphasis\"></span></span></div><ul class=\"list-style-none\">    <li class=\"d-inline\">        <a class=\"d-inline-flex flex-items-center flex-nowrap Link--secondary no-underline text-small mr-3\" href=\"/mlfoundations/open_clip/search?l=jupyter-notebook\"  data-ga-click=\"Repository, language stats search click, location:repo overview\">          <svg style=\"color:#DA5B0B;\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-dot-fill mr-2\">    <path d=\"M8 4a4 4 0 1 1 0 8 4 4 0 0 1 0-8Z\"></path></svg>          <span class=\"color-fg-default text-bold mr-1\">Jupyter Notebook</span>          <span>54.4%</span>        </a>    </li>    <li class=\"d-inline\">        <a class=\"d-inline-flex flex-items-center flex-nowrap Link--secondary no-underline text-small mr-3\" href=\"/mlfoundations/open_clip/search?l=python\"  data-ga-click=\"Repository, language stats search click, location:repo overview\">          <svg style=\"color:#3572A5;\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-dot-fill mr-2\">    <path d=\"M8 4a4 4 0 1 1 0 8 4 4 0 0 1 0-8Z\"></path></svg>          <span class=\"color-fg-default text-bold mr-1\">Python</span>          <span>45.3%</span>        </a>    </li>    <li class=\"d-inline\">      <span class=\"d-inline-flex flex-items-center flex-nowrap text-small mr-3\">        <svg style=\"color:#ededed;\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-dot-fill mr-2\">    <path d=\"M8 4a4 4 0 1 1 0 8 4 4 0 0 1 0-8Z\"></path></svg>        <span class=\"color-fg-default text-bold mr-1\">Other</span>        <span>0.3%</span>      </span>    </li></ul>              </div>            </div>              </div></div>  </div></div>  </div>  </div></turbo-frame>    </main>  </div>  </div>          <footer class=\"footer pt-8 pb-6 f6 color-fg-muted p-responsive\" role=\"contentinfo\" >  <h2 class='sr-only'>Footer</h2>    <div class=\"d-flex flex-justify-center flex-items-center flex-column-reverse flex-lg-row flex-wrap flex-lg-nowrap\">    <div class=\"d-flex flex-items-center flex-shrink-0 mx-2\">      <a aria-label=\"Homepage\" title=\"GitHub\" class=\"footer-octicon mr-2\" href=\"https://github.com\">        <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-mark-github\">    <path d=\"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z\"></path></svg></a>      <span>        &copy; 2024 GitHub,&nbsp;Inc.      </span>    </div>    <nav aria-label=\"Footer\">      <h3 class=\"sr-only\" id=\"sr-footer-heading\">Footer navigation</h3>      <ul class=\"list-style-none d-flex flex-justify-center flex-wrap mb-2 mb-lg-0\" aria-labelledby=\"sr-footer-heading\">          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to Terms&quot;,&quot;label&quot;:&quot;text:terms&quot;}\" href=\"https://docs.github.com/site-policy/github-terms/github-terms-of-service\" data-view-component=\"true\" class=\"Link--secondary Link\">Terms</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to privacy&quot;,&quot;label&quot;:&quot;text:privacy&quot;}\" href=\"https://docs.github.com/site-policy/privacy-policies/github-privacy-statement\" data-view-component=\"true\" class=\"Link--secondary Link\">Privacy</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to security&quot;,&quot;label&quot;:&quot;text:security&quot;}\" href=\"/security\" data-view-component=\"true\" class=\"Link--secondary Link\">Security</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to status&quot;,&quot;label&quot;:&quot;text:status&quot;}\" href=\"https://www.githubstatus.com/\" data-view-component=\"true\" class=\"Link--secondary Link\">Status</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to docs&quot;,&quot;label&quot;:&quot;text:docs&quot;}\" href=\"https://docs.github.com\" data-view-component=\"true\" class=\"Link--secondary Link\">Docs</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to contact&quot;,&quot;label&quot;:&quot;text:contact&quot;}\" href=\"https://support.github.com?tags=dotcom-footer\" data-view-component=\"true\" class=\"Link--secondary Link\">Contact</a>          </li>          <li class=\"mx-2\" >  <cookie-consent-link>    <button type=\"button\" class=\"Link--secondary underline-on-hover border-0 p-0 color-bg-transparent\" data-action=\"click:cookie-consent-link#showConsentManagement\">      Manage cookies    </button>  </cookie-consent-link></li><li class=\"mx-2\">  <cookie-consent-link>    <button type=\"button\" class=\"Link--secondary underline-on-hover border-0 p-0 color-bg-transparent\" data-action=\"click:cookie-consent-link#showConsentManagement\">      Do not share my personal information    </button>  </cookie-consent-link></li>      </ul>    </nav>  </div></footer>    <cookie-consent id=\"cookie-consent-banner\" class=\"position-fixed bottom-0 left-0\" style=\"z-index: 999999\" data-initial-cookie-consent-allowed=\"\" data-cookie-consent-required=\"false\"></cookie-consent>  <div id=\"ajax-error-message\" class=\"ajax-error-message flash flash-error\" hidden>    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-alert\">    <path d=\"M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>    <button type=\"button\" class=\"flash-close js-ajax-error-dismiss\" aria-label=\"Dismiss error\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg>    </button>    You can\u2019t perform that action at this time.  </div>    <template id=\"site-details-dialog\">  <details class=\"details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm\" open>    <summary role=\"button\" aria-label=\"Close dialog\"></summary>    <details-dialog class=\"Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal\">      <button class=\"Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0\" type=\"button\" aria-label=\"Close dialog\" data-close-dialog>        <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg>      </button>      <div class=\"octocat-spinner my-6 js-details-dialog-spinner\"></div>    </details-dialog>  </details></template>    <div class=\"Popover js-hovercard-content position-absolute\" style=\"display: none; outline: none;\" tabindex=\"0\">  <div class=\"Popover-message Popover-message--bottom-left Popover-message--large Box color-shadow-large\" style=\"width:360px;\">  </div></div>    <template id=\"snippet-clipboard-copy-button\">  <div class=\"zeroclipboard-container position-absolute right-0 top-0\">    <clipboard-copy aria-label=\"Copy\" class=\"ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay\" data-copy-feedback=\"Copied!\" data-tooltip-direction=\"w\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copy js-clipboard-copy-icon m-2\">    <path d=\"M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z\"></path><path d=\"M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path></svg>      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2\">    <path d=\"M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z\"></path></svg>    </clipboard-copy>  </div></template><template id=\"snippet-clipboard-copy-button-unpositioned\">  <div class=\"zeroclipboard-container\">    <clipboard-copy aria-label=\"Copy\" class=\"ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center\" data-copy-feedback=\"Copied!\" data-tooltip-direction=\"w\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copy js-clipboard-copy-icon\">    <path d=\"M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z\"></path><path d=\"M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path></svg>      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-check js-clipboard-check-icon color-fg-success d-none\">    <path d=\"M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z\"></path></svg>    </clipboard-copy>  </div></template>    </div>    <div id=\"js-global-screen-reader-notice\" class=\"sr-only\" aria-live=\"polite\" aria-atomic=\"true\" ></div>    <div id=\"js-global-screen-reader-notice-assertive\" class=\"sr-only\" aria-live=\"assertive\" aria-atomic=\"true\"></div>  </body></html>",
  "embeddings": []
}