{
  "libtype": "pypi",
  "libname": "deepspeed",
  "url": "http://deepspeed.ai",
  "html": "<!doctype html><!--  Minimal Mistakes Jekyll Theme 4.19.0 by Michael Rose  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes  Free for personal and commercial use under the MIT license  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE--><html lang=\"en\" class=\"no-js\">  <head>    <meta charset=\"utf-8\"><!-- begin _includes/seo.html --><title>Latest News - DeepSpeed</title><meta name=\"description\" content=\"DeepSpeed is a deep learning optimization library that makes distributed training easy, efficient, and effective.\"><meta property=\"og:type\" content=\"website\"><meta property=\"og:locale\" content=\"en_US\"><meta property=\"og:site_name\" content=\"DeepSpeed\"><meta property=\"og:title\" content=\"Latest News\"><meta property=\"og:url\" content=\"https://www.deepspeed.ai/\">  <meta property=\"og:description\" content=\"DeepSpeed is a deep learning optimization library that makes distributed training easy, efficient, and effective.\"><link rel=\"canonical\" href=\"https://www.deepspeed.ai/\"><script type=\"application/ld+json\">  {    \"@context\": \"https://schema.org\",          \"@type\": \"Person\",      \"name\": null,      \"url\": \"https://www.deepspeed.ai/\"      }</script><!-- end _includes/seo.html --><link href=\"/feed.xml\" type=\"application/atom+xml\" rel=\"alternate\" title=\"DeepSpeed Feed\"><!-- https://t.co/dKP3o1e --><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"><script>  document.documentElement.className = document.documentElement.className.replace(/\\bno-js\\b/g, '') + ' js ';</script><!-- For all browsers --><link rel=\"stylesheet\" href=\"/assets/css/main.css\"><link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css\"><!--[if IE]>  <style>    /* old IE unsupported flexbox fixes */    .greedy-nav .site-title {      padding-right: 3em;    }    .greedy-nav button {      position: absolute;      top: 0;      right: 0;      height: 100%;    }  </style><![endif]-->    <!-- start custom head snippets --><!-- insert favicons. use https://realfavicongenerator.net/ --><!-- end custom head snippets -->  </head>  <body class=\"layout--single\">    <nav class=\"skip-links\">  <h2 class=\"screen-reader-text\">Skip links</h2>  <ul>    <li><a href=\"#site-nav\" class=\"screen-reader-shortcut\">Skip to primary navigation</a></li>    <li><a href=\"#main\" class=\"screen-reader-shortcut\">Skip to content</a></li>    <li><a href=\"#footer\" class=\"screen-reader-shortcut\">Skip to footer</a></li>  </ul></nav>    <!--[if lt IE 9]><div class=\"notice--danger align-center\" style=\"margin: 0;\">You are using an <strong>outdated</strong> browser. Please <a href=\"https://browsehappy.com/\">upgrade your browser</a> to improve your experience.</div><![endif]-->    <div class=\"masthead\">  <div class=\"masthead__inner-wrap\">    <div class=\"masthead__menu\">      <nav id=\"site-nav\" class=\"greedy-nav\">                  <a class=\"site-logo\" href=\"/\"><img src=\"/assets/images/deepspeed-logo-uppercase-bold-white-1.15.svg\" alt=\"\"></a>                <!-- <a class=\"site-title\" href=\"/\">          DeepSpeed                  </a> -->        <ul class=\"visible-links\"><li class=\"masthead__menu-item\">              <a href=\"/getting-started/\">Getting Started</a>            </li><li class=\"masthead__menu-item\">              <a href=\"/posts/\">Blog</a>            </li><li class=\"masthead__menu-item\">              <a href=\"/tutorials/\">Tutorials</a>            </li><li class=\"masthead__menu-item\">              <a href=\"https://deepspeed.readthedocs.io/\">Documentation</a>            </li><li class=\"masthead__menu-item\">              <a href=\"https://github.com/microsoft/DeepSpeed\">GitHub</a>            </li></ul>                <button class=\"search__toggle\" type=\"button\">          <span class=\"visually-hidden\">Toggle search</span>          <svg class=\"icon\" width=\"16\" height=\"16\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 15.99 16\">            <path d=\"M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z\" transform=\"translate(-.01)\"></path>          </svg>        </button>                <button class=\"greedy-nav__toggle hidden\" type=\"button\">          <span class=\"visually-hidden\">Toggle menu</span>          <div class=\"navicon\"></div>        </button>        <ul class=\"hidden-links hidden\"></ul>      </nav>    </div>  </div></div>    <div class=\"initial-content\">      <div id=\"main\" role=\"main\">    <div class=\"sidebar sticky\">                                              <nav class=\"nav__list\">    <input id=\"ac-toc\" name=\"accordion-toc\" type=\"checkbox\" />  <label for=\"ac-toc\">Toggle Menu</label>  <ul class=\"nav__items\">          <li>                  <a href=\"/training/\"><span class=\"nav__sub-title\">Training</span></a>                      </li>          <li>                  <a href=\"/inference/\"><span class=\"nav__sub-title\">Inference</span></a>                      </li>          <li>                  <a href=\"/compression/\"><span class=\"nav__sub-title\">Compression</span></a>                      </li>          <li>                  <a href=\"/deepspeed4science/\"><span class=\"nav__sub-title\">Science</span></a>                      </li>          <li>                  <a href=\"/getting-started/\"><span class=\"nav__sub-title\">Getting Started</span></a>                      </li>          <li>                  <a href=\"/docs/config-json/\"><span class=\"nav__sub-title\">ds_config</span></a>                        <ul>                      <li><a href=\"/docs/config-json/#autotuning\">Autotuning</a></li>                      <li><a href=\"/docs/config-json/#batch-size-related-parameters\">Batch size</a></li>                      <li><a href=\"/docs/config-json/#optimizer-parameters\">Optimizer</a></li>                      <li><a href=\"/docs/config-json/#fp16-training-options\">FP16</a></li>                      <li><a href=\"/docs/config-json/#bfloat16-training-options\">BFLOAT16</a></li>                      <li><a href=\"/docs/config-json/#zero-optimizations-for-fp16-training\">ZeRO optimizations</a></li>                      <li><a href=\"/docs/config-json/#logging\">Logging</a></li>                      <li><a href=\"/docs/config-json/#flops-profiler\">Flops Profiler</a></li>                      <li><a href=\"/docs/config-json/#monitoring-module-tensorboard-wandb-csv\">Monitoring</a></li>                      <li><a href=\"/docs/config-json/#communication-logging\">Communication Logging</a></li>                      <li><a href=\"/docs/config-json/#compression\">Model Compression</a></li>                      <li><a href=\"/docs/config-json/#data-efficiency\">Data Efficiency</a></li>                  </ul>              </li>          <li>                  <a href=\"/tutorials/\"><span class=\"nav__sub-title\">Tutorials</span></a>                        <ul>                      <li><a href=\"/getting-started/\">Getting started</a></li>                      <li><a href=\"/tutorials/azure/\">Getting started on Azure</a></li>                      <li><a href=\"/tutorials/automatic-tensor-parallelism/\">Automatic Tensor Parallelism</a></li>                      <li><a href=\"/tutorials/autotuning/\">Autotuning</a></li>                      <li><a href=\"/tutorials/bert-finetuning/\">BingBertSQuAD Fine-tuning</a></li>                      <li><a href=\"/tutorials/bert-pretraining/\">BERT Pre-training</a></li>                      <li><a href=\"/tutorials/cifar-10/\">CIFAR-10</a></li>                      <li><a href=\"/tutorials/curriculum-learning/\">Curriculum Learning</a></li>                      <li><a href=\"/tutorials/data-efficiency/\">Data Efficiency</a></li>                      <li><a href=\"/tutorials/ds4sci_evoformerattention/\">DS4Sci_EvoformerAttention</a></li>                      <li><a href=\"/tutorials/flops-profiler/\">Flops Profiler</a></li>                      <li><a href=\"/tutorials/pytorch-profiler/\">PyTorch Profiler</a></li>                      <li><a href=\"/tutorials/gan/\">GAN</a></li>                      <li><a href=\"/tutorials/inference-tutorial/\">Inference</a></li>                      <li><a href=\"/tutorials/lrrt/\">Learning Rate Range Test</a></li>                      <li><a href=\"/tutorials/megatron/\">Megatron-LM GPT2</a></li>                      <li><a href=\"/tutorials/mixture-of-experts/\">Mixture-of-Experts (MoE)</a></li>                      <li><a href=\"/tutorials/mixture-of-experts-nlg/\">MoE for NLG</a></li>                      <li><a href=\"/tutorials/mixture-of-experts-inference/\">MoE Inference</a></li>                      <li><a href=\"/tutorials/model-compression/\">Model Compression</a></li>                      <li><a href=\"/tutorials/MoQ-tutorial/\">Mixture-of-Quantization</a></li>                      <li><a href=\"/tutorials/monitor\">Monitoring</a></li>                      <li><a href=\"/tutorials/comms-logging\">Communication Logging</a></li>                      <li><a href=\"/tutorials/one-cycle/\">One-Cycle Schedule</a></li>                      <li><a href=\"/tutorials/onebit-adam/\">One-Bit Adam</a></li>                      <li><a href=\"/tutorials/zero-one-adam/\">Zero-One Adam</a></li>                      <li><a href=\"/tutorials/onebit-lamb/\">One-Bit LAMB</a></li>                      <li><a href=\"/tutorials/pipeline/\">Pipeline Parallelism</a></li>                      <li><a href=\"/tutorials/progressive_layer_dropping/\">Progressive Layer Dropping</a></li>                      <li><a href=\"/tutorials/sparse-attention/\">Sparse Attention</a></li>                      <li><a href=\"/tutorials/transformer_kernel/\">Transformer Kernel</a></li>                      <li><a href=\"/tutorials/zero-offload/\">ZeRO-Offload</a></li>                      <li><a href=\"/tutorials/zero/\">ZeRO</a></li>                      <li><a href=\"/tutorials/zeropp/\">ZeRO++</a></li>                  </ul>              </li>          <li>                  <a href=\"/contributing/\"><span class=\"nav__sub-title\">Contributing</span></a>                      </li>      </ul></nav>        </div>  <article class=\"page\" itemscope itemtype=\"https://schema.org/CreativeWork\">    <meta itemprop=\"headline\" content=\"Latest News\">                <div class=\"page__inner-wrap\">              <header>          <h1 id=\"page-title\" class=\"page__title\" itemprop=\"headline\">Latest News</h1>                  </header>            <section class=\"page__content\" itemprop=\"text\">                  <aside class=\"sidebar__right \">            <nav class=\"toc\">              <header><h4 class=\"nav__title\"><i class=\"fas fa-file-alt\"></i> Contents</h4></header>              <ul class=\"toc__menu\">  <li><a href=\"#extreme-speed-and-scale-for-dl-training-and-inference\">Extreme Speed and Scale for DL Training and Inference</a></li>  <li><a href=\"#deepspeed-has-four-innovation-pillars\">DeepSpeed has four innovation pillars:</a>    <ul>      <li><a href=\"#deepspeed-training\">DeepSpeed-Training</a></li>      <li><a href=\"#deepspeed-inference\">DeepSpeed-Inference</a></li>      <li><a href=\"#deepspeed-compression\">DeepSpeed-Compression</a></li>      <li><a href=\"#deepspeed4science\">DeepSpeed4Science</a></li>    </ul>  </li>  <li><a href=\"#deepspeed-software-suite\">DeepSpeed Software Suite</a>    <ul>      <li><a href=\"#deepspeed-library\">DeepSpeed Library</a></li>      <li><a href=\"#model-implementations-for-inference-mii\">Model Implementations for Inference (MII)</a></li>      <li><a href=\"#deepspeed-on-azure\">DeepSpeed on Azure</a></li>    </ul>  </li>  <li><a href=\"#deepspeed-adoption\">DeepSpeed Adoption</a></li>  <li><a href=\"#contributing\">Contributing</a>    <ul>      <li><a href=\"#contributor-license-agreement\">Contributor License Agreement</a></li>      <li><a href=\"#code-of-conduct\">Code of Conduct</a></li>    </ul>  </li>  <li><a href=\"#publications\">Publications</a></li>  <li><a href=\"#videos\">Videos</a></li></ul>            </nav>          </aside>                <p><b> <span style=\"color:orange\"> DeepSpeed empowers ChatGPT-like model training with a single click, offering 15x speedup over SOTA RLHF systems with unprecedented cost reduction at all scales; <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat\">learn how</a></span>.</b></p><ul>  <li>[2024/01] <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen/2024-01-19\">DeepSpeed-FastGen: Introducting Mixtral, Phi-2, and Falcon support with major performance and feature enhancements.</a></li>  <li>[2023/11] <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/intel-inference\">Llama 2 Inference on 4th Gen Intel\u00ae Xeon\u00ae Scalable Processor with DeepSpeed</a> <a href=\"https://www.intel.com/content/www/us/en/developer/articles/technical/xllama-2-on-xeon-scalable-processor-with-deepspeed.html\">[Intel version]</a></li>  <li>[2023/11] <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-offloadpp\">DeepSpeed ZeRO-Offload++: 6x Higher Training Throughput via Collaborative CPU/GPU Twin-Flow</a></li>  <li>[2023/11] <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen\">DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference</a> [<a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen\">English</a>] [<a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen/chinese/README.md\">\u4e2d\u6587</a>] [<a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen/japanese/README.md\">\u65e5\u672c\u8a9e</a>]</li>  <li>[2023/10] <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-visualchat/10-03-2023/README.md\">DeepSpeed-VisualChat: Improve Your Chat Experience with Multi-Round Multi-Image Inputs</a> [<a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-visualchat/10-03-2023/README.md\">English</a>] [<a href=\"https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-visualchat/10-03-2023/README-Chinese.md\">\u4e2d\u6587</a>] [<a href=\"https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-visualchat/10-03-2023/README-Japanese.md\">\u65e5\u672c\u8a9e</a>]</li>  <li>[2023/09] Announcing the DeepSpeed4Science Initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies [<a href=\"https://deepspeed4science.ai/\">DeepSpeed4Science website</a>] [<a href=\"https://www.deepspeed.ai/deepspeed4science/\">Tutorials</a>] [<a href=\"https://arxiv.org/abs/2310.04610\">White paper</a>] [<a href=\"https://www.microsoft.com/en-us/research/blog/announcing-the-deepspeed4science-initiative-enabling-large-scale-scientific-discovery-through-sophisticated-ai-system-technologies/\">Blog</a>] [<a href=\"https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed4science/chinese/README.md\">\u4e2d\u6587</a>] [<a href=\"https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed4science/japanese/README.md\">\u65e5\u672c\u8a9e</a>]</li>  <li>[2023/08] <a href=\"https://github.com/microsoft/DeepSpeedExamples/blob/master/inference/huggingface/zero_inference/README.md\">DeepSpeed ZeRO-Inference: 20x faster inference through weight quantization and KV cache offloading</a></li></ul><!-- NOTE: we must use html for news items otherwise links will be broken in the 'more news' section --><details> <summary>More news</summary> <ul>  <li>[2023/08] <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat/ds-chat-release-8-31/README.md\">DeepSpeed-Chat: Llama/Llama-2 system support, efficiency boost, and training stability improvements</a></li>  <li>[2023/08] <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ulysses\">DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models</a> [<a href=\"https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-ulysses/chinese/README.md\">\u4e2d\u6587</a>] [<a href=\"https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-ulysses/japanese/README.md\">\u65e5\u672c\u8a9e</a>]</li>  <li>[2023/06] <a href=\"https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/\">ZeRO++: A leap in speed for LLM and chat model training with 4X less communication</a> [<a href=\"https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/\">English</a>] [<a href=\"https://github.com/microsoft/DeepSpeed/blob/master/blogs/zeropp/chinese/README.md\">\u4e2d\u6587</a>] [<a href=\"https://github.com/microsoft/DeepSpeed/blob/master/blogs/zeropp/japanese/README.md\">\u65e5\u672c\u8a9e</a>]</li> </ul></details><h1 id=\"extreme-speed-and-scale-for-dl-training-and-inference\">Extreme Speed and Scale for DL Training and Inference</h1><p><strong><em><a href=\"https://www.deepspeed.ai/\">DeepSpeed</a> enables world\u2019s most powerful language models like <a href=\"https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/\">MT-530B</a> and <a href=\"https://huggingface.co/blog/bloom-megatron-deepspeed\">BLOOM</a></em></strong>. It is an easy-to-use deep learning optimization software suite that powers unprecedented scale and speed for both training and inference. With DeepSpeed you can:</p><ul>  <li>Train/Inference dense or sparse models with billions or trillions of parameters</li>  <li>Achieve excellent system throughput and efficiently scale to thousands of GPUs</li>  <li>Train/Inference on resource-constrained GPU systems</li>  <li>Achieve unprecedented low latency and high throughput for inference</li>  <li>Achieve extreme compression for an unparalleled inference latency and model size reduction with low costs</li></ul><h1 id=\"deepspeed-has-four-innovation-pillars\">DeepSpeed has four innovation pillars:</h1><p><a href=\"https://deepspeed4science.ai/\"><img src=\"/assets/images/DeepSpeed-pillars.png\" alt=\"Four innovation pillars\" class=\"align-center\" /></a></p><h2 id=\"deepspeed-training\">DeepSpeed-Training</h2><p>DeepSpeed offers a confluence of system innovations, that has made large-scale DL training effective, and efficient, greatly improved ease of use, and redefined the DL training landscape in terms of scale that is possible. These innovations such as ZeRO, 3D-Parallelism, DeepSpeed-MoE, ZeRO-Infinity, etc fall under the DeepSpeed-Training pillar. Learn more: <a href=\"https://www.deepspeed.ai/training\">DeepSpeed-Training</a></p><h2 id=\"deepspeed-inference\">DeepSpeed-Inference</h2><p>DeepSpeed brings together innovations in parallelism technology such as tensor, pipeline, expert and ZeRO-parallelism, and combines them with high-performance custom inference kernels, communication optimizations and heterogeneous memory technologies to enable inference at an unprecedented scale, while achieving unparalleled latency, throughput and cost reduction. This systematic composition of system technologies for inference falls under the DeepSpeed-Inference. Learn more: <a href=\"https://www.deepspeed.ai/inference\">DeepSpeed-Inference</a></p><h2 id=\"deepspeed-compression\">DeepSpeed-Compression</h2><p>To further increase the inference efficiency, DeepSpeed offers easy-to-use and flexible-to-compose compression techniques for researchers and practitioners to compress their models while delivering faster speed, smaller model size, and significantly reduced compression cost. Moreover, SoTA innovations on compression like ZeroQuant and XTC are included under the DeepSpeed-Compression pillar. Learn more: <a href=\"https://www.deepspeed.ai/compression\">DeepSpeed-Compression</a></p><h2 id=\"deepspeed4science\">DeepSpeed4Science</h2><p>In line with Microsoft\u2019s mission to solve humanity\u2019s most pressing challenges, the DeepSpeed team at Microsoft is responding to this opportunity by launching a new initiative called <em>DeepSpeed4Science</em>, aiming to build unique capabilities through AI system technology innovations to help domain experts to unlock today\u2019s biggest science mysteries. Learn more: <a href=\"https://deepspeed4science.ai/\">DeepSpeed4Science website</a> and <a href=\"/deepspeed4science/\">tutorials</a></p><h1 id=\"deepspeed-software-suite\">DeepSpeed Software Suite</h1><h2 id=\"deepspeed-library\">DeepSpeed Library</h2><p>The <a href=\"https://github.com/microsoft/deepspeed\">DeepSpeed</a> library implements and packages the innovations and technologies in DeepSpeed Training, Inference and Compression Pillars into a single easy-to-use, open-sourced repository. It allows for an easy composition of a multitude of features within a single training, inference or compression pipeline. The DeepSpeed Library is heavily adopted by the DL community, and has been used to enable some of the most powerful models (see <a href=\"#deepspeed-adoption\">DeepSpeed Adoption</a>).</p><h2 id=\"model-implementations-for-inference-mii\">Model Implementations for Inference (MII)</h2><p><a href=\"https://github.com/microsoft/deepspeed-mii\">Model Implementations for Inference (MII)</a> is an open-sourced repository for making low-latency and high-throughput inference accessible to all data scientists by alleviating the need to apply complex system optimization techniques themselves. Out-of-box, MII offers support for thousands of widely used DL models, optimized using DeepSpeed-Inference, that can be deployed with a few lines of code, while achieving significant latency reduction compared to their vanilla open-sourced versions.</p><h2 id=\"deepspeed-on-azure\">DeepSpeed on Azure</h2><p>DeepSpeed users are diverse and have access to different environments. We recommend trying DeepSpeed on Azure as it is the simplest and easiest method. The recommended method to try DeepSpeed on Azure is through AzureML <a href=\"https://github.com/Azure/azureml-examples/tree/main/python-sdk/workflows/train/deepspeed\">recipes</a>. The job submission and data preparation scripts have been made available <a href=\"https://github.com/microsoft/Megatron-DeepSpeed/tree/main/examples_deepspeed/azureml\">here</a>. For more details on how to use DeepSpeed on Azure, please follow the <a href=\"https://www.deepspeed.ai/tutorials/azure/\">Azure tutorial</a>.</p><h1 id=\"deepspeed-adoption\">DeepSpeed Adoption</h1><p>DeepSpeed has been used to train many different large-scale models. Below is a list of several examples that we are aware of (if you\u2019d like to include your model please submit a PR):</p><ul>  <li><a href=\"https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/\">Megatron-Turing NLG (530B)</a></li>  <li><a href=\"https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf\">Jurassic-1 (178B)</a></li>  <li><a href=\"https://huggingface.co/blog/bloom-megatron-deepspeed\">BLOOM (176B)</a></li>  <li><a href=\"https://github.com/THUDM/GLM-130B\">GLM (130B)</a></li>  <li><a href=\"https://github.com/yandex/YaLM-100B\">YaLM (100B)</a></li>  <li><a href=\"https://github.com/EleutherAI/gpt-neox\">GPT-NeoX (20B)</a></li>  <li><a href=\"https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning\">AlexaTM (20B)</a></li>  <li><a href=\"https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/\">Turing NLG (17B</a></li>  <li><a href=\"https://arxiv.org/pdf/2204.06644.pdf\">METRO-LM (5.4B)</a></li></ul><p>DeepSpeed has been integrated with several different popular open-source DL frameworks such as:</p><table>  <thead>    <tr>      <th>\u00a0</th>      <th>Documentation</th>    </tr>  </thead>  <tbody>    <tr>      <td><img src=\"assets/images/transformers-light.png\" width=\"300px\" /></td>      <td><a href=\"https://huggingface.co/docs/transformers/main/main_classes/deepspeed\">Transformers with DeepSpeed</a></td>    </tr>    <tr>      <td><img src=\"assets/images/accelerate-light.png\" width=\"300px\" /></td>      <td><a href=\"https://huggingface.co/docs/accelerate/main/en/deepspeed\">Accelerate with DeepSpeed</a></td>    </tr>    <tr>      <td><img src=\"assets/images/lightning-light.svg\" width=\"250px\" /></td>      <td><a href=\"https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.strategies.DeepSpeedStrategy.html\">Lightning with DeepSpeed</a></td>    </tr>    <tr>      <td><img src=\"assets/images/mosaicml.svg\" width=\"250px\" /></td>      <td><a href=\"https://docs.mosaicml.com/en/latest/trainer/using_the_trainer.html?highlight=deepspeed#deepspeed-integration\">MosaicML with DeepSpeed</a></td>    </tr>  </tbody></table><p>DeepSpeed is an integral part of <a href=\"https://www.microsoft.com/en-us/research/project/ai-at-scale/\">Microsoft\u2019s AI at Scale initiative</a> to enable next-generation AI capabilities at scale.</p><h1 id=\"contributing\">Contributing</h1><p>DeepSpeed welcomes your contributions! Please see our<a href=\"/contributing/\">contributing</a> guide for more details on formatting, testing,etc.</p><h2 id=\"contributor-license-agreement\">Contributor License Agreement</h2><p>This project welcomes contributions and suggestions. Most contributions require you toagree to a Contributor License Agreement (CLA) declaring that you have the right to, andactually do, grant us the rights to use your contribution. For details, visithttps://cla.opensource.microsoft.com.</p><p>When you submit a pull request, a CLA bot will automatically determine whether you needto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simplyfollow the instructions provided by the bot. You will only need to do this once acrossall repos using our CLA.</p><h2 id=\"code-of-conduct\">Code of Conduct</h2><p>This project has adopted the <a href=\"https://opensource.microsoft.com/codeofconduct/\">Microsoft Open Source Code ofConduct</a>. For more information see the<a href=\"https://opensource.microsoft.com/codeofconduct/faq/\">Code of Conduct FAQ</a> or contact<a href=\"mailto:opencode@microsoft.com\">opencode@microsoft.com</a> with any additional questions orcomments.</p><h1 id=\"publications\">Publications</h1><ol>  <li>Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He. (2019) ZeRO: memory optimizations toward training trillion parameter models. <a href=\"https://arxiv.org/abs/1910.02054\">arXiv:1910.02054</a> and <a href=\"https://dl.acm.org/doi/10.5555/3433701.3433727\">In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC \u201820)</a>.</li>  <li>Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. <a href=\"https://dl.acm.org/doi/10.1145/3394486.3406703\">In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD \u201820, Tutorial)</a>.</li>  <li>Minjia Zhang, Yuxiong He. (2020) Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. <a href=\"https://arxiv.org/abs/2010.13369\">arXiv:2010.13369</a> and <a href=\"https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html\">NeurIPS 2020</a>.</li>  <li>Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He. (2021) ZeRO-Offload: Democratizing Billion-Scale Model Training. <a href=\"https://arxiv.org/abs/2101.06840\">arXiv:2101.06840</a> and <a href=\"https://www.usenix.org/conference/atc21/presentation/ren-jie\">USENIX ATC 2021</a>. <a href=\"https://arxiv.org/abs/2101.06840\">[paper]</a> <a href=\"https://www.usenix.org/system/files/atc21_slides_ren-jie.pdf\">[slides]</a> <a href=\"https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/\">[blog]</a></li>  <li>Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam\u2019s Convergence Speed. <a href=\"https://arxiv.org/abs/2102.02888\">arXiv:2102.02888</a> and <a href=\"http://proceedings.mlr.press/v139/tang21a.html\">ICML 2021</a>.</li>  <li>Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He. (2021) ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. <a href=\"https://arxiv.org/abs/2104.07857\">arXiv:2104.07857</a> and <a href=\"https://dl.acm.org/doi/abs/10.1145/3458817.3476205\">SC 2021</a>. <a href=\"https://arxiv.org/abs/2104.07857\">[paper]</a> <a href=\"https://github.com/microsoft/DeepSpeed/blob/master/docs/assets/files/SC21-ZeRO-Infinity.pdf\">[slides]</a> <a href=\"https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/\">[blog]</a></li>  <li>Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, Yuxiong He. (2021) 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB\u2019s Convergence Speed. <a href=\"https://arxiv.org/abs/2104.06069\">arXiv:2104.06069</a> and <a href=\"https://hipc.org/advance-program/\">HiPC 2022</a>.</li>  <li>Conglong Li, Minjia Zhang, Yuxiong He. (2021) The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models. <a href=\"https://arxiv.org/abs/2108.06084\">arXiv:2108.06084</a> and <a href=\"https://openreview.net/forum?id=JpZ5du_Kdh\">NeurIPS 2022</a>.</li>  <li>Yucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, Yuxiong He. (2022) Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam. <a href=\"https://arxiv.org/abs/2202.06009\">arXiv:2202.06009</a>.</li>  <li>Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He. (2022) DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale <a href=\"https://arxiv.org/abs/2201.05596\">arXiv:2201.05596</a> and <a href=\"https://proceedings.mlr.press/v162/rajbhandari22a.html\">ICML 2022</a>. <a href=\"https://arxiv.org/abs/2201.05596\">[pdf]</a> <a href=\"https://github.com/microsoft/DeepSpeed/blob/master/docs/assets/files/ICML-5mins.pdf\">[slides]</a> <a href=\"https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/\">[blog]</a></li>  <li>Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro. (2022) Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model <a href=\"https://arxiv.org/abs/2201.11990\">arXiv:2201.11990</a>.</li>  <li>Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, Yuxiong He. (2022) Extreme Compression for Pre-trained Transformers Made Simple and Efficient. <a href=\"https://arxiv.org/abs/2206.01859\">arXiv:2206.01859</a> and <a href=\"https://openreview.net/forum?id=xNeAhc2CNAl\">NeurIPS 2022</a>.</li>  <li>Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He. (2022) ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. <a href=\"https://arxiv.org/abs/2206.01861\">arXiv:2206.01861</a> and <a href=\"https://openreview.net/forum?id=f-fVCElZ-G1\">NeurIPS 2022</a> <a href=\"https://github.com/microsoft/DeepSpeed/blob/master/docs/assets/files/zeroquant_series.pdf\">[slides]</a> <a href=\"https://www.microsoft.com/en-us/research/blog/deepspeed-compression-a-composable-library-for-extreme-compression-and-zero-cost-quantization/\">[blog]</a></li>  <li>Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, Yuxiong He. (2022) DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. <a href=\"https://arxiv.org/abs/2207.00032\">arXiv:2207.00032</a> and <a href=\"https://dl.acm.org/doi/abs/10.5555/3571885.3571946\">SC 2022</a>. <a href=\"https://arxiv.org/abs/2207.00032\">[paper]</a> <a href=\"https://github.com/microsoft/DeepSpeed/blob/master/docs/assets/files/sc22-ds-inference.pdf\">[slides]</a> <a href=\"https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/\">[blog]</a></li>  <li>Zhewei Yao, Xiaoxia Wu, Conglong Li, Connor Holmes, Minjia Zhang, Cheng Li, Yuxiong He. (2022) Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers. <a href=\"https://arxiv.org/abs/2211.11586\">arXiv:2211.11586</a>.</li>  <li>Conglong Li, Zhewei Yao, Xiaoxia Wu, Minjia Zhang, Yuxiong He. (2022) DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. <a href=\"https://arxiv.org/abs/2212.03597\">arXiv:2212.03597</a> <a href=\"https://neurips2023-enlsp.github.io/\">ENLSP2023 Workshop at NeurIPS2023</a></li>  <li>Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, Yuxiong He. (2023) Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. <a href=\"https://arxiv.org/abs/2301.12017\">arXiv:2301.12017</a> and <a href=\"https://icml.cc/Conferences/2023\">ICML2023</a>.</li>  <li>Syed Zawad, Cheng Li, Zhewei Yao, Elton Zheng, Yuxiong He, Feng Yan. (2023) DySR: Adaptive Super-Resolution via Algorithm and System Co-design. <a href=\"https://openreview.net/forum?id=Pgtn4l6eKjv\">ICLR:2023</a>.</li>  <li>Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He. (2023) Scaling Vision-Language Models with Sparse Mixture of Experts. <a href=\"https://arxiv.org/abs/2303.07226\">arXiv:2303.07226</a> and <a href=\"https://2023.emnlp.org/\">Finding at EMNLP2023</a>.</li>  <li>Quentin Anthony, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He, Aamir Shafi, Mustafa Abduljabbar, Hari Subramoni, Dhabaleswar Panda. (2023) MCR-DL: Mix-and-Match Communication Runtime for Deep Learning <a href=\"https://arxiv.org/abs/2303.08374\">arXiv:2303.08374</a> and will appear at IPDPS 2023.</li>  <li>Siddharth Singh, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He, Abhinav Bhatele. (2023) A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training <a href=\"https://arxiv.org/abs/2303.06318\">arXiv:2303.06318</a> and will appear at ICS 2023.</li>  <li>Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Xiaoxia Wu, Connor Holmes, Zhewei Yao, Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei Yang, Yuxiong He. (2023) ZeRO++: Extremely Efficient Collective Communication for Giant Model Training <a href=\"https://arxiv.org/abs/2306.10209\">arXiv:2306.10209</a> and <a href=\"http://mlforsystems.org/\">ML for Sys Workshop at NeurIPS2023</a> <a href=\"https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/\">[blog]</a></li>  <li>Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, Yuxiong He. (2023) ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation <a href=\"https://arxiv.org/abs/2303.08302\">arXiv:2303.08302</a> and <a href=\"https://neurips2023-enlsp.github.io/\">ENLSP2023 Workshop at NeurIPS2023</a> <a href=\"https://github.com/microsoft/DeepSpeed/blob/master/docs/assets/files/zeroquant_series.pdf\">[slides]</a></li>  <li>Pareesa Ameneh Golnari, Zhewei Yao, Yuxiong He. (2023) Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important? <a href=\"https://arxiv.org/abs/2305.09847\">arXiv:2305.09847</a></li>  <li>Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, Yuxiong He. (2023) DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales <a href=\"https://arxiv.org/abs/2308.01320\">arXiv:2308.01320</a>.</li>  <li>Xiaoxia Wu, Zhewei Yao, Yuxiong He. (2023) ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats <a href=\"https://arxiv.org/abs/2307.09782\">arXiv:2307.09782</a> and <a href=\"https://neurips2023-enlsp.github.io/\">ENLSP2023 Workshop at NeurIPS2023</a> <a href=\"https://github.com/microsoft/DeepSpeed/blob/master/docs/assets/files/zeroquant_series.pdf\">[slides]</a></li>  <li>Zhewei Yao, Xiaoxia Wu, Conglong Li, Minjia Zhang, Heyang Qin, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He. (2023) DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention <a href=\"https://arxiv.org/pdf/2309.14327.pdf\">arXiv:2309.14327</a></li>  <li>Shuaiwen Leon Song, Bonnie Kruft, Minjia Zhang, Conglong Li, Shiyang Chen, Chengming Zhang, Masahiro Tanaka, Xiaoxia Wu, Jeff Rasley, Ammar Ahmad Awan, Connor Holmes, Martin Cai, Adam Ghanem, Zhongzhu Zhou, Yuxiong He, et al. (2023) DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies <a href=\"https://arxiv.org/abs/2310.04610\">arXiv:2310.04610</a> <a href=\"https://www.microsoft.com/en-us/research/blog/announcing-the-deepspeed4science-initiative-enabling-large-scale-scientific-discovery-through-sophisticated-ai-system-technologies/\">[blog]</a></li>  <li>Zhewei Yao, Reza Yazdani Aminabadi, Stephen Youn, Xiaoxia Wu, Elton Zheng, Yuxiong He. (2023) ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers <a href=\"https://arxiv.org/abs/2310.17723\">arXiv:2310.17723</a></li></ol><h1 id=\"videos\">Videos</h1><ol>  <li>DeepSpeed KDD 2020 Tutorial    <ol>      <li><a href=\"https://www.youtube.com/watch?v=CaseqC45DNc&amp;list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&amp;index=29\">Overview</a></li>      <li><a href=\"https://www.youtube.com/watch?v=y4_bCiAsIAk&amp;list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&amp;index=28\">ZeRO + large model training</a></li>      <li><a href=\"https://www.youtube.com/watch?v=9V-ZbP92drg&amp;list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&amp;index=27\">17B T-NLG demo</a></li>      <li><a href=\"https://www.youtube.com/watch?v=o1K-ZG9F6u0&amp;list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&amp;index=26\">Fastest BERT training + RScan tuning</a></li>      <li>DeepSpeed hands on deep dive: <a href=\"https://www.youtube.com/watch?v=_NOk-mBwDYg&amp;list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&amp;index=92\">part 1</a>, <a href=\"https://www.youtube.com/watch?v=sG6_c4VXLww&amp;list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&amp;index=94\">part 2</a>, <a href=\"https://www.youtube.com/watch?v=k9yPkBTayos&amp;list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&amp;index=93\">part 3</a></li>      <li><a href=\"https://www.youtube.com/watch?v=nsHu6vEgPew&amp;list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&amp;index=24\">FAQ</a></li>    </ol>  </li>  <li>Microsoft Research Webinar    <ul>      <li>Registration is free and all videos are available on-demand.</li>      <li><a href=\"https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html\">ZeRO &amp; Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed</a>.</li>    </ul>  </li>  <li><a href=\"https://youtu.be/yBVXR8G8Bg8\">DeepSpeed on AzureML</a></li>  <li><a href=\"https://www.youtube.com/watch?v=cntxC3g22oU\">Large Model Training and Inference with DeepSpeed // Samyam Rajbhandari // LLMs in Prod Conference</a> <a href=\"docs/assets/files/presentation-mlops.pdf\">[slides]</a></li>  <li>Community Tutorials    <ul>      <li><a href=\"https://www.youtube.com/watch?v=pDGI668pNg0\">DeepSpeed: All the tricks to scale to gigantic models (Mark Saroufim)</a></li>      <li><a href=\"https://www.youtube.com/watch?v=tC01FRB0M7w\">Turing-NLG, DeepSpeed and the ZeRO optimizer (Yannic Kilcher)</a></li>      <li><a href=\"https://www.youtube.com/watch?v=hc0u4avAkuM\">Ultimate Guide To Scaling ML Models (The AI Epiphany)</a></li>    </ul>  </li></ol>              </section>      <footer class=\"page__meta\">                              </footer>                </div>      </article>    </div>    </div>          <div class=\"search-content\">        <div class=\"search-content__inner-wrap\"><form class=\"search-content__form\" onkeydown=\"return event.key != 'Enter';\">    <label class=\"sr-only\" for=\"search\">      Enter your search term...    </label>    <input type=\"search\" id=\"search\" class=\"search-input\" tabindex=\"-1\" placeholder=\"Enter your search term...\" />  </form>  <div id=\"results\" class=\"results\"></div></div>      </div>        <div id=\"footer\" class=\"page__footer\">      <footer>        <!-- start custom footer snippets --><!-- end custom footer snippets -->        <div class=\"page__footer-follow\">  <ul class=\"social-icons\">            <li><a href=\"/feed.xml\"><i class=\"fas fa-fw fa-rss-square\" aria-hidden=\"true\"></i> Feed</a></li>  </ul></div><div class=\"page__footer-copyright\">&copy; 2024 DeepSpeed. Powered by <a href=\"https://jekyllrb.com\" rel=\"nofollow\">Jekyll</a> &amp; <a href=\"https://mademistakes.com/work/minimal-mistakes-jekyll-theme/\" rel=\"nofollow\">Minimal Mistakes</a>.</div>      </footer>    </div>      <script src=\"/assets/js/main.min.js\"></script><script src=\"/assets/js/lunr/lunr.min.js\"></script><script src=\"/assets/js/lunr/lunr-store.js\"></script><script src=\"/assets/js/lunr/lunr-en.js\"></script>  <!-- Global site tag (gtag.js) - Google Analytics --><script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-169781858-1\"></script><script>  window.dataLayer = window.dataLayer || [];  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());  gtag('config', 'UA-169781858-1', { 'anonymize_ip': false});</script>  </body></html>",
  "embeddings": []
}