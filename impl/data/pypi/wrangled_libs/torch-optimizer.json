{
  "classifiers": [
    "development status :: 3 - alpha",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "operating system :: os independent",
    "programming language :: python :: 3",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "pytorch-optimizer\ntorch-optimizer\n===============\n.. image:: https://github.com/jettify/pytorch-optimizer/workflows/ci/badge.svg\n   :target: https://github.com/jettify/pytorch-optimizer/actions?query=workflow%3aci\n   :alt: github actions status for master branch\n.. image:: https://codecov.io/gh/jettify/pytorch-optimizer/branch/master/graph/badge.svg\n    :target: https://codecov.io/gh/jettify/pytorch-optimizer\n.. image:: https://img.shields.io/pypi/pyversions/torch-optimizer.svg\n    :target: https://pypi.org/project/torch-optimizer\n.. image:: https://readthedocs.org/projects/pytorch-optimizer/badge/?version=latest\n    :target: https://pytorch-optimizer.readthedocs.io/en/latest/?badge=latest\n    :alt: documentation status\n.. image:: https://img.shields.io/pypi/v/torch-optimizer.svg\n    :target: https://pypi.python.org/pypi/torch-optimizer\n.. image:: https://static.deepsource.io/deepsource-badge-light-mini.svg\n    :target: https://deepsource.io/gh/jettify/pytorch-optimizer/?ref=repository-badge\n\n\n**torch-optimizer** -- collection of optimizers for pytorch_ compatible with optim_\nmodule.\n\n\nsimple example\n--------------\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.diffgrad(model.parameters(), lr=0.001)\n    optimizer.step()\n\n\ninstallation\n------------\ninstallation process is simple, just::\n\n    $ pip install torch_optimizer\n\n\ndocumentation\n-------------\nhttps://pytorch-optimizer.rtfd.io\n\n\ncitation\n--------\nplease cite original authors of optimization algorithms. if you like this\npackage::\n\n    @software{novik_torchoptimizers,\n    \ttitle        = {{torch-optimizer -- collection of optimization algorithms for pytorch.}},\n    \tauthor       = {novik, mykola},\n    \tyear         = 2020,\n    \tmonth        = 1,\n    \tversion      = {1.0.1}\n    }\n\nor use github feature: \"cite this repository\" button.\n\n\nsupported optimizers\n====================\n\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `a2gradexp`_  | https://arxiv.org/abs/1810.00553                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `a2gradinc`_  | https://arxiv.org/abs/1810.00553                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `a2graduni`_  | https://arxiv.org/abs/1810.00553                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `accsgd`_     | https://arxiv.org/abs/1803.05591                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `adabelief`_  | https://arxiv.org/abs/2010.07468                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `adabound`_   | https://arxiv.org/abs/1902.09843                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `adamod`_     | https://arxiv.org/abs/1910.12249                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `adafactor`_  | https://arxiv.org/abs/1804.04235                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `adahessian`_ | https://arxiv.org/abs/2006.00719                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `adamp`_      | https://arxiv.org/abs/2006.08217                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `aggmo`_      | https://arxiv.org/abs/1804.00325                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `apollo`_     | https://arxiv.org/abs/2009.13586                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `diffgrad`_   | https://arxiv.org/abs/1909.11015                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `lamb`_       | https://arxiv.org/abs/1904.00962                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `lookahead`_  | https://arxiv.org/abs/1907.08610                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `madgrad`_    | https://arxiv.org/abs/2101.11075                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `novograd`_   | https://arxiv.org/abs/1905.11286                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `pid`_        | https://www4.comp.polyu.edu.hk/~cslzhang/paper/cvpr18_pid.pdf                                                                        |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `qhadam`_     | https://arxiv.org/abs/1810.06801                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `qhm`_        | https://arxiv.org/abs/1810.06801                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `radam`_      | https://arxiv.org/abs/1908.03265                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `ranger`_     | https://medium.com/@lessw/new-deep-learning-optimizer-ranger-synergistic-combination-of-radam-lookahead-for-the-best-of-2dc83f79a48d |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `rangerqh`_   | https://arxiv.org/abs/1810.06801                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `rangerva`_   | https://arxiv.org/abs/1908.00700v2                                                                                                   |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `sgdp`_       | https://arxiv.org/abs/2006.08217                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `sgdw`_       | https://arxiv.org/abs/1608.03983                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `swats`_      | https://arxiv.org/abs/1712.07628                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `shampoo`_    | https://arxiv.org/abs/1802.09568                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `yogi`_       | https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization                                                        |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n\n\nvisualizations\n--------------\nvisualizations help us to see how different algorithms deals with simple\nsituations like: saddle points, local minima, valleys etc, and may provide\ninteresting insights into inner workings of algorithm. rosenbrock_ and rastrigin_\nbenchmark_ functions was selected, because:\n\n* rosenbrock_ (also known as banana function), is non-convex function that has\n  one global minima  `(1.0. 1.0)`. the global minimum is inside a long,\n  narrow, parabolic shaped flat valley. to find the valley is trivial. to\n  converge to the global minima, however, is difficult. optimization\n  algorithms might pay a lot of attention to one coordinate, and have\n  problems to follow valley which is relatively flat.\n\n .. image::  https://upload.wikimedia.org/wikipedia/commons/3/32/rosenbrock_function.svg\n\n* rastrigin_ function is a non-convex and has one global minima in `(0.0, 0.0)`.\n  finding the minimum of this function is a fairly difficult problem due to\n  its large search space and its large number of local minima.\n\n  .. image::  https://upload.wikimedia.org/wikipedia/commons/8/8b/rastrigin_function.png\n\neach optimizer performs `501` optimization steps. learning rate is best one found\nby hyper parameter search algorithm, rest of tuning parameters are default. it\nis very easy to extend script and tune other optimizer parameters.\n\n\n.. code::\n\n    python examples/viz_optimizers.py\n\n\nwarning\n-------\ndo not pick optimizer based on visualizations, optimization approaches\nhave unique properties and may be tailored for different purposes or may\nrequire explicit learning rate schedule etc. best way to find out, is to try one\non your particular problem and see if it improves scores.\n\nif you do not know which optimizer to use start with built in sgd/adam, once\ntraining logic is ready and baseline scores are established, swap optimizer and\nsee if there is any improvement.\n\n\na2gradexp\n---------\n\n+--------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_a2gradexp.png   |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_a2gradexp.png  |\n+--------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.a2gradexp(\n        model.parameters(),\n        kappa=1000.0,\n        beta=10.0,\n        lips=10.0,\n        rho=0.5,\n    )\n    optimizer.step()\n\n\n**paper**: *optimal adaptive and accelerated stochastic gradient descent* (2018) [https://arxiv.org/abs/1810.00553]\n\n**reference code**: https://github.com/severilov/a2grad_optimizer\n\n\na2gradinc\n---------\n\n+--------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_a2gradinc.png   |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_a2gradinc.png  |\n+--------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.a2gradinc(\n        model.parameters(),\n        kappa=1000.0,\n        beta=10.0,\n        lips=10.0,\n    )\n    optimizer.step()\n\n\n**paper**: *optimal adaptive and accelerated stochastic gradient descent* (2018) [https://arxiv.org/abs/1810.00553]\n\n**reference code**: https://github.com/severilov/a2grad_optimizer\n\n\na2graduni\n---------\n\n+--------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_a2graduni.png   |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_a2graduni.png  |\n+--------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.a2graduni(\n        model.parameters(),\n        kappa=1000.0,\n        beta=10.0,\n        lips=10.0,\n    )\n    optimizer.step()\n\n\n**paper**: *optimal adaptive and accelerated stochastic gradient descent* (2018) [https://arxiv.org/abs/1810.00553]\n\n**reference code**: https://github.com/severilov/a2grad_optimizer\n\n\naccsgd\n------\n\n+-----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_accsgd.png   |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_accsgd.png  |\n+-----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.accsgd(\n        model.parameters(),\n        lr=1e-3,\n        kappa=1000.0,\n        xi=10.0,\n        small_const=0.7,\n        weight_decay=0\n    )\n    optimizer.step()\n\n\n**paper**: *on the insufficiency of existing momentum schemes for stochastic optimization* (2019) [https://arxiv.org/abs/1803.05591]\n\n**reference code**: https://github.com/rahulkidambi/accsgd\n\n\nadabelief\n---------\n\n+-------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_adabelief.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_adabelief.png |\n+-------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.adabelief(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-3,\n        weight_decay=0,\n        amsgrad=false,\n        weight_decouple=false,\n        fixed_decay=false,\n        rectify=false,\n    )\n    optimizer.step()\n\n\n**paper**: *adabelief optimizer, adapting stepsizes by the belief in observed gradients* (2020) [https://arxiv.org/abs/2010.07468]\n\n**reference code**: https://github.com/juntang-zhuang/adabelief-optimizer\n\n\nadabound\n--------\n\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_adabound.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_adabound.png |\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.adabound(\n        m.parameters(),\n        lr= 1e-3,\n        betas= (0.9, 0.999),\n        final_lr = 0.1,\n        gamma=1e-3,\n        eps= 1e-8,\n        weight_decay=0,\n        amsbound=false,\n    )\n    optimizer.step()\n\n\n**paper**: *adaptive gradient methods with dynamic bound of learning rate* (2019) [https://arxiv.org/abs/1902.09843]\n\n**reference code**: https://github.com/luolc/adabound\n\nadamod\n------\nadamod method restricts the adaptive learning rates with adaptive and momental\nupper bounds. the dynamic learning rate bounds are based on the exponential\nmoving averages of the adaptive learning rates themselves, which smooth out\nunexpected large learning rates and stabilize the training of deep neural networks.\n\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_adamod.png    |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_adamod.png   |\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.adamod(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        beta3=0.999,\n        eps=1e-8,\n        weight_decay=0,\n    )\n    optimizer.step()\n\n**paper**: *an adaptive and momental bound method for stochastic learning.* (2019) [https://arxiv.org/abs/1910.12249]\n\n**reference code**: https://github.com/lancopku/adamod\n\n\nadafactor\n---------\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_adafactor.png |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_adafactor.png |\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.adafactor(\n        m.parameters(),\n        lr= 1e-3,\n        eps2= (1e-30, 1e-3),\n        clip_threshold=1.0,\n        decay_rate=-0.8,\n        beta1=none,\n        weight_decay=0.0,\n        scale_parameter=true,\n        relative_step=true,\n        warmup_init=false,\n    )\n    optimizer.step()\n\n**paper**: *adafactor: adaptive learning rates with sublinear memory cost.* (2018) [https://arxiv.org/abs/1804.04235]\n\n**reference code**: https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py\n\n\nadahessian\n----------\n+-------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_adahessian.png |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_adahessian.png  |\n+-------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.adahessian(\n        m.parameters(),\n        lr= 1.0,\n        betas= (0.9, 0.999),\n        eps= 1e-4,\n        weight_decay=0.0,\n        hessian_power=1.0,\n    )\n\t  loss_fn(m(input), target).backward(create_graph = true) # create_graph=true is necessary for hessian calculation\n    optimizer.step()\n\n\n**paper**: *adahessian: an adaptive second order optimizer for machine learning* (2020) [https://arxiv.org/abs/2006.00719]\n\n**reference code**: https://github.com/amirgholami/adahessian\n\n\nadamp\n------\nadamp propose a simple and effective solution: at each iteration of adam optimizer\napplied on scale-invariant weights (e.g., conv weights preceding a bn layer), adamp\nremove the radial component (i.e., parallel to the weight vector) from the update vector.\nintuitively, this operation prevents the unnecessary update along the radial direction\nthat only increases the weight norm without contributing to the loss minimization.\n\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_adamp.png     |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_adamp.png    |\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.adamp(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n        delta = 0.1,\n        wd_ratio = 0.1\n    )\n    optimizer.step()\n\n**paper**: *slowing down the weight norm increase in momentum-based optimizers.* (2020) [https://arxiv.org/abs/2006.08217]\n\n**reference code**: https://github.com/clovaai/adamp\n\n\naggmo\n-----\n\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_aggmo.png     |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_aggmo.png    |\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.aggmo(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.0, 0.9, 0.99),\n        weight_decay=0,\n    )\n    optimizer.step()\n\n**paper**: *aggregated momentum: stability through passive damping.* (2019) [https://arxiv.org/abs/1804.00325]\n\n**reference code**: https://github.com/athemathmo/aggmo\n\n\napollo\n------\n\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_apollo.png    |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_apollo.png   |\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.apollo(\n        m.parameters(),\n        lr= 1e-2,\n        beta=0.9,\n        eps=1e-4,\n        warmup=0,\n        init_lr=0.01,\n        weight_decay=0,\n    )\n    optimizer.step()\n\n**paper**: *apollo: an adaptive parameter-wise diagonal quasi-newton method for nonconvex stochastic optimization.* (2020) [https://arxiv.org/abs/2009.13586]\n\n**reference code**: https://github.com/xuezhemax/apollo\n\n\ndiffgrad\n--------\noptimizer based on the difference between the present and the immediate past\ngradient, the step size is adjusted for each parameter in such\na way that it should have a larger step size for faster gradient changing\nparameters and a lower step size for lower gradient changing parameters.\n\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_diffgrad.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_diffgrad.png  |\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.diffgrad(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n    )\n    optimizer.step()\n\n\n**paper**: *diffgrad: an optimization method for convolutional neural networks.* (2019) [https://arxiv.org/abs/1909.11015]\n\n**reference code**: https://github.com/shivram1987/diffgrad\n\nlamb\n----\n\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_lamb.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_lamb.png  |\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.lamb(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n    )\n    optimizer.step()\n\n\n**paper**: *large batch optimization for deep learning: training bert in 76 minutes* (2019) [https://arxiv.org/abs/1904.00962]\n\n**reference code**: https://github.com/cybertronai/pytorch-lamb\n\nlookahead\n---------\n\n+-----------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_lookaheadyogi.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_lookaheadyogi.png  |\n+-----------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    # base optimizer, any other optimizer can be used like adam or diffgrad\n    yogi = optim.yogi(\n        m.parameters(),\n        lr= 1e-2,\n        betas=(0.9, 0.999),\n        eps=1e-3,\n        initial_accumulator=1e-6,\n        weight_decay=0,\n    )\n\n    optimizer = optim.lookahead(yogi, k=5, alpha=0.5)\n    optimizer.step()\n\n\n**paper**: *lookahead optimizer: k steps forward, 1 step back* (2019) [https://arxiv.org/abs/1907.08610]\n\n**reference code**: https://github.com/alphadl/lookahead.pytorch\n\n\nmadgrad\n---------\n\n+-----------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_madgrad.png        |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_madgrad.png        |\n+-----------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.madgrad(\n        m.parameters(),\n        lr=1e-2,\n        momentum=0.9,\n        weight_decay=0,\n        eps=1e-6,\n    )\n    optimizer.step()\n\n\n**paper**: *adaptivity without compromise: a momentumized, adaptive, dual averaged gradient method for stochastic optimization* (2021) [https://arxiv.org/abs/2101.11075]\n\n**reference code**: https://github.com/facebookresearch/madgrad\n\n\nnovograd\n--------\n\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_novograd.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_novograd.png  |\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.novograd(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n        grad_averaging=false,\n        amsgrad=false,\n    )\n    optimizer.step()\n\n\n**paper**: *stochastic gradient methods with layer-wise adaptive moments for training of deep networks* (2019) [https://arxiv.org/abs/1905.11286]\n\n**reference code**: https://github.com/nvidia/deeplearningexamples/\n\n\npid\n---\n\n+-------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_pid.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_pid.png  |\n+-------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.pid(\n        m.parameters(),\n        lr=1e-3,\n        momentum=0,\n        dampening=0,\n        weight_decay=1e-2,\n        integral=5.0,\n        derivative=10.0,\n    )\n    optimizer.step()\n\n\n**paper**: *a pid controller approach for stochastic optimization of deep networks* (2018) [http://www4.comp.polyu.edu.hk/~cslzhang/paper/cvpr18_pid.pdf]\n\n**reference code**: https://github.com/tensorboy/pidoptimizer\n\n\nqhadam\n------\n\n+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_qhadam.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_qhadam.png  |\n+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.qhadam(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        nus=(1.0, 1.0),\n        weight_decay=0,\n        decouple_weight_decay=false,\n        eps=1e-8,\n    )\n    optimizer.step()\n\n\n**paper**: *quasi-hyperbolic momentum and adam for deep learning* (2019) [https://arxiv.org/abs/1810.06801]\n\n**reference code**: https://github.com/facebookresearch/qhoptim\n\n\nqhm\n---\n\n+-------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_qhm.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_qhm.png  |\n+-------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.qhm(\n        m.parameters(),\n        lr=1e-3,\n        momentum=0,\n        nu=0.7,\n        weight_decay=1e-2,\n        weight_decay_type='grad',\n    )\n    optimizer.step()\n\n\n**paper**: *quasi-hyperbolic momentum and adam for deep learning* (2019) [https://arxiv.org/abs/1810.06801]\n\n**reference code**: https://github.com/facebookresearch/qhoptim\n\n\nradam\n-----\n\n+---------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_radam.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_radam.png  |\n+---------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.radam(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n    )\n    optimizer.step()\n\n\n**paper**: *on the variance of the adaptive learning rate and beyond* (2019) [https://arxiv.org/abs/1908.03265]\n\n**reference code**: https://github.com/liyuanlucasliu/radam\n\n\nranger\n------\n\n+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_ranger.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_ranger.png  |\n+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.ranger(\n        m.parameters(),\n        lr=1e-3,\n        alpha=0.5,\n        k=6,\n        n_sma_threshhold=5,\n        betas=(.95, 0.999),\n        eps=1e-5,\n        weight_decay=0\n    )\n    optimizer.step()\n\n\n**paper**: *new deep learning optimizer, ranger: synergistic combination of radam + lookahead for the best of both* (2019) [https://medium.com/@lessw/new-deep-learning-optimizer-ranger-synergistic-combination-of-radam-lookahead-for-the-best-of-2dc83f79a48d]\n\n**reference code**: https://github.com/lessw2020/ranger-deep-learning-optimizer\n\n\nrangerqh\n--------\n\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_rangerqh.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_rangerqh.png  |\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.rangerqh(\n        m.parameters(),\n        lr=1e-3,\n        betas=(0.9, 0.999),\n        nus=(.7, 1.0),\n        weight_decay=0.0,\n        k=6,\n        alpha=.5,\n        decouple_weight_decay=false,\n        eps=1e-8,\n    )\n    optimizer.step()\n\n\n**paper**: *quasi-hyperbolic momentum and adam for deep learning* (2018) [https://arxiv.org/abs/1810.06801]\n\n**reference code**: https://github.com/lessw2020/ranger-deep-learning-optimizer\n\n\nrangerva\n--------\n\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_rangerva.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_rangerva.png  |\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.rangerva(\n        m.parameters(),\n        lr=1e-3,\n        alpha=0.5,\n        k=6,\n        n_sma_threshhold=5,\n        betas=(.95, 0.999),\n        eps=1e-5,\n        weight_decay=0,\n        amsgrad=true,\n        transformer='softplus',\n        smooth=50,\n        grad_transformer='square'\n    )\n    optimizer.step()\n\n\n**paper**: *calibrating the adaptive learning rate to improve convergence of adam* (2019) [https://arxiv.org/abs/1908.00700v2]\n\n**reference code**: https://github.com/lessw2020/ranger-deep-learning-optimizer\n\n\nsgdp\n----\n\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_sgdp.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_sgdp.png  |\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.sgdp(\n        m.parameters(),\n        lr= 1e-3,\n        momentum=0,\n        dampening=0,\n        weight_decay=1e-2,\n        nesterov=false,\n        delta = 0.1,\n        wd_ratio = 0.1\n    )\n    optimizer.step()\n\n\n**paper**: *slowing down the weight norm increase in momentum-based optimizers.* (2020) [https://arxiv.org/abs/2006.08217]\n\n**reference code**: https://github.com/clovaai/adamp\n\n\nsgdw\n----\n\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_sgdw.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_sgdw.png  |\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.sgdw(\n        m.parameters(),\n        lr= 1e-3,\n        momentum=0,\n        dampening=0,\n        weight_decay=1e-2,\n        nesterov=false,\n    )\n    optimizer.step()\n\n\n**paper**: *sgdr: stochastic gradient descent with warm restarts* (2017) [https://arxiv.org/abs/1608.03983]\n\n**reference code**: https://github.com/pytorch/pytorch/pull/22466\n\n\nswats\n-----\n\n+---------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_swats.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_swats.png  |\n+---------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.swats(\n        model.parameters(),\n        lr=1e-1,\n        betas=(0.9, 0.999),\n        eps=1e-3,\n        weight_decay= 0.0,\n        amsgrad=false,\n        nesterov=false,\n    )\n    optimizer.step()\n\n\n**paper**: *improving generalization performance by switching from adam to sgd* (2017) [https://arxiv.org/abs/1712.07628]\n\n**reference code**: https://github.com/mrpatekful/swats\n\n\nshampoo\n-------\n\n+-----------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_shampoo.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_shampoo.png  |\n+-----------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.shampoo(\n        m.parameters(),\n        lr=1e-1,\n        momentum=0.0,\n        weight_decay=0.0,\n        epsilon=1e-4,\n        update_freq=1,\n    )\n    optimizer.step()\n\n\n**paper**: *shampoo: preconditioned stochastic tensor optimization* (2018) [https://arxiv.org/abs/1802.09568]\n\n**reference code**: https://github.com/moskomule/shampoo.pytorch\n\n\nyogi\n----\n\nyogi is optimization algorithm based on adam with more fine grained effective\nlearning rate control, and has similar theoretical guarantees on convergence as adam.\n\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_yogi.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_yogi.png  |\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.yogi(\n        m.parameters(),\n        lr= 1e-2,\n        betas=(0.9, 0.999),\n        eps=1e-3,\n        initial_accumulator=1e-6,\n        weight_decay=0,\n    )\n    optimizer.step()\n\n\n**paper**: *adaptive methods for nonconvex optimization* (2018) [https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization]\n\n**reference code**: https://github.com/4rtemi5/yogi-optimizer_keras\n\n\nadam (pytorch built-in)\n-----------------------\n\n+---------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_adam.png   |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_adam.png  |\n+---------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n\nsgd (pytorch built-in)\n----------------------\n\n+--------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_sgd.png   |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_sgd.png  |\n+--------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n\n.. _python: https://www.python.org\n.. _pytorch: https://github.com/pytorch/pytorch\n.. _rastrigin: https://en.wikipedia.org/wiki/rastrigin_function\n.. _rosenbrock: https://en.wikipedia.org/wiki/rosenbrock_function\n.. _benchmark: https://en.wikipedia.org/wiki/test_functions_for_optimization\n.. _optim: https://pytorch.org/docs/stable/optim.html\n\nchanges\n-------\n\n0.3.0 (2021-10-30)\n------------------\n* revert for drop radam.\n\n0.2.0 (2021-10-25)\n------------------\n* drop radam optimizer since it is included in pytorch.\n* do not include tests as installable package.\n* preserver memory layout where possible.\n* add madgrad optimizer.\n\n0.1.0 (2021-01-01)\n------------------\n* initial release.\n* added support for a2gradexp, a2gradinc, a2graduni, accsgd, adabelief,\n  adabound, adamod, adafactor, adahessian, adamp, aggmo, apollo,\n  diffgrad, lamb, lookahead, novograd, pid, qhadam, qhm, radam, ranger,\n  rangerqh, rangerva, sgdp, sgdw, swats, shampoo, yogi.\n\n",
  "docs_url": null,
  "keywords": "torch-optimizer,pytorch,accsgd,adabound,adamod,diffgrad,lamb,lookahead,madgrad,novograd,pid,qhadam,qhm,radam,sgdw,yogi,ranger",
  "license": "apache 2",
  "name": "torch-optimizer",
  "package_url": "https://pypi.org/project/torch-optimizer/",
  "project_url": "https://pypi.org/project/torch-optimizer/",
  "project_urls": {
    "Documentation": "https://pytorch-optimizer.readthedocs.io",
    "Download": "https://pypi.org/project/torch-optimizer/",
    "Homepage": "https://github.com/jettify/pytorch-optimizer",
    "Issues": "https://github.com/jettify/pytorch-optimizer/issues",
    "Website": "https://github.com/jettify/pytorch-optimizer"
  },
  "release_url": "https://pypi.org/project/torch-optimizer/0.3.0/",
  "requires_dist": [
    "torch (>=1.5.0)",
    "pytorch-ranger (>=0.1.1)"
  ],
  "requires_python": ">=3.6.0",
  "summary": "pytorch-optimizer",
  "version": "0.3.0",
  "releases": [],
  "developers": [
    "nickolainovik@gmail.com",
    "nikolay_novik"
  ],
  "kwds": "torch_optimizer viz_optimizers optimizers novik_torchoptimizers pytorch_",
  "license_kwds": "apache 2",
  "libtype": "pypi",
  "id": "pypi_torch_optimizer",
  "homepage": "https://github.com/jettify/pytorch-optimizer",
  "release_count": 21,
  "dependency_ids": [
    "pypi_pytorch_ranger",
    "pypi_torch"
  ],
  "documentation_summary": "The GitHub repository \"jettify/pytorch-optimizer\" is a collection of optimizers for PyTorch, compatible with the optim module. It includes a wide range of optimization algorithms for deep learning, such as A2GradExp, AdaBelief, AdaBound, Adafactor, and many others, each with references to their respective papers and implementation details. The repository provides a simple example of how to use these optimizers with PyTorch models, installation instructions, and a link to the documentation. It also features visualizations to help understand how different algorithms perform under various conditions. The project is licensed under the Apache-2.0 license and has garnered 2.9k stars and 289 forks, indicating its popularity and utility within the PyTorch community.",
  "embedding": [
    -0.025387678295373917,
    -0.0204168614000082,
    -0.012061955407261848,
    -0.02771862782537937,
    0.01949009858071804,
    0.025612348690629005,
    -0.0029874052852392197,
    -0.005560575518757105,
    -0.018310582265257835,
    -0.014772034250199795,
    0.029291315004229546,
    0.01673789508640766,
    -0.017973577603697777,
    0.012342792935669422,
    -0.003447276074439287,
    -0.015052871778607368,
    0.017496155574917793,
    -0.034177880734205246,
    -0.006059061735868454,
    0.005827371031045914,
    0.008544470183551311,
    0.012490232475101948,
    -0.02460133470594883,
    -0.03142567723989487,
    -0.0034121714998036623,
    -0.00699635548517108,
    0.016822146251797676,
    -0.03603140637278557,
    0.010559476912021637,
    -0.01517924852669239,
    0.031201006844639778,
    0.010341827757656574,
    -0.02976873889565468,
    -0.024334538727998734,
    -0.017102982848882675,
    -0.010390974581241608,
    0.004812846891582012,
    0.004202025942504406,
    0.029291315004229546,
    0.015670713037252426,
    0.032099686563014984,
    0.010854355990886688,
    -0.0015990165993571281,
    0.011029879562556744,
    -0.009815258905291557,
    0.00790556613355875,
    0.0017245156923308969,
    -0.024011576548218727,
    -0.022719725966453552,
    0.026370609179139137,
    -0.015839215368032455,
    0.0356382355093956,
    -0.012862341478466988,
    0.003026020247489214,
    0.00529026985168457,
    0.016190262511372566,
    0.0027592249680310488,
    0.04097414016723633,
    -0.019616475328803062,
    -0.00157356564886868,
    0.021020660176873207,
    -0.00226249429397285,
    -0.03707050532102585,
    0.003949272446334362,
    -0.007322828751057386,
    -0.0003449031210038811,
    -0.004514457192271948,
    0.01821228861808777,
    0.007603665813803673,
    -0.02052919566631317,
    0.05066302418708801,
    0.027156952768564224,
    -0.017566364258527756,
    -0.035610150545835495,
    0.00400895019993186,
    -0.03749176114797592,
    -0.02696036733686924,
    0.016681727021932602,
    -0.017145108431577682,
    -0.011949621140956879,
    0.014996703714132309,
    0.0012900956207886338,
    0.003910657484084368,
    0.02460133470594883,
    0.003959803842008114,
    -0.003826406318694353,
    -0.017861243337392807,
    -0.011373904533684254,
    -0.03723900765180588,
    -0.0025012060068547726,
    0.03075166791677475,
    0.012097060680389404,
    0.03749176114797592,
    0.014406945556402206,
    -0.0053464374504983425,
    0.02288822829723358,
    -0.03942953795194626,
    0.013094032183289528,
    -7.657200330868363e-05,
    -0.02040281891822815,
    0.00824959110468626,
    0.016457056626677513,
    -0.024980464950203896,
    -0.010180346667766571,
    -0.03471147269010544,
    -0.02432049810886383,
    -0.016246428713202477,
    0.023211190477013588,
    -0.0056623793207108974,
    0.04046863317489624,
    -2.31608373724157e-05,
    0.03990695998072624,
    -0.017692741006612778,
    -0.03150992840528488,
    -0.016358764842152596,
    -0.006554036866873503,
    0.02092236839234829,
    0.008460219018161297,
    -0.0009513358818367124,
    -0.019405847415328026,
    0.023365650326013565,
    0.059986814856529236,
    0.0218350887298584,
    -0.011921537108719349,
    -0.003931720275431871,
    0.0180999543517828,
    -0.021540209650993347,
    0.01633067987859249,
    0.006592652294784784,
    -0.009702923707664013,
    0.010650749318301678,
    0.01084031444042921,
    0.008565532974898815,
    0.0037386445328593254,
    -0.020388776436448097,
    0.02632848359644413,
    -0.008123214356601238,
    -0.01735573634505272,
    -0.026637403294444084,
    0.006006404757499695,
    0.0042371307499706745,
    0.02712886966764927,
    -0.01748211309313774,
    -0.010706916451454163,
    -0.03510464355349541,
    0.02419412136077881,
    -0.006448722910135984,
    -0.009155291132628918,
    0.02909472957253456,
    0.0008973624790087342,
    0.012047913856804371,
    -0.0059607685543596745,
    -0.002367808250710368,
    0.032633278518915176,
    0.014533322304487228,
    -0.008993810042738914,
    0.008748077787458897,
    0.002071174094453454,
    -0.03886786103248596,
    0.011598573997616768,
    -0.007954712025821209,
    0.0090499771758914,
    -0.019925396889448166,
    -0.02196146547794342,
    0.014020795002579689,
    0.013122116215527058,
    0.01988327130675316,
    -0.002880336018279195,
    -0.005718546453863382,
    -0.012799153104424477,
    0.02339373528957367,
    0.011289653368294239,
    -0.02057132124900818,
    -0.008888496086001396,
    0.009969718754291534,
    0.010770104825496674,
    0.006462764926254749,
    0.019630517810583115,
    -0.012363855727016926,
    0.026581237092614174,
    0.016358764842152596,
    -0.01564262993633747,
    -0.0037526865489780903,
    0.021540209650993347,
    0.002569659845903516,
    0.004609239753335714,
    -0.0020975025836378336,
    0.017397861927747726,
    0.011970683932304382,
    -0.012995739467442036,
    0.014589490368962288,
    0.023253316059708595,
    -0.005578128155320883,
    0.00019910915580112487,
    -0.6146963834762573,
    -0.014842243865132332,
    -0.023084813728928566,
    -0.03473955765366554,
    -0.003836937714368105,
    -0.005651847925037146,
    -0.0049006082117557526,
    0.014420988038182259,
    -0.023941367864608765,
    0.03252094238996506,
    -0.03167843073606491,
    0.014491196721792221,
    -0.004584666341543198,
    -0.029291315004229546,
    -0.008502344600856304,
    -0.02783096209168434,
    0.007975774817168713,
    -0.02430645562708378,
    0.003320899326354265,
    0.007975774817168713,
    -0.023407775908708572,
    0.005030495580285788,
    -0.008607658557593822,
    -0.0016894110012799501,
    0.016878312453627586,
    0.029122812673449516,
    0.01770678162574768,
    -0.015881340950727463,
    0.019279470667243004,
    -0.0019290002528578043,
    -0.010215451009571552,
    0.03092017024755478,
    0.019181177020072937,
    -0.02017815038561821,
    0.053780313581228256,
    0.001515642972663045,
    0.002957566175609827,
    0.021287456154823303,
    0.0006077491561882198,
    0.04639429599046707,
    -0.020599404349923134,
    -0.006076613906770945,
    0.023295441642403603,
    -0.006171396467834711,
    -0.006283731199800968,
    0.010812230408191681,
    0.025556180626153946,
    0.016695769503712654,
    -0.004395101685076952,
    -0.0030751668382436037,
    0.00784237775951624,
    -0.011591553688049316,
    -0.0012514805421233177,
    -0.002910174895077944,
    0.005307822022587061,
    -0.006848915945738554,
    0.033194951713085175,
    -0.03041466325521469,
    0.001205844571813941,
    -0.004395101685076952,
    -0.0008995565003715456,
    0.020136024802923203,
    -0.010222472250461578,
    -0.023871157318353653,
    0.018998634070158005,
    -0.020220275968313217,
    -0.03268944472074509,
    0.008067047223448753,
    0.007266661152243614,
    -0.023295441642403603,
    0.009401023387908936,
    -0.0044688209891319275,
    0.004960286431014538,
    0.015769006684422493,
    0.032801780849695206,
    0.024868130683898926,
    0.03226818889379501,
    0.011731971986591816,
    0.0014068186283111572,
    0.02960023656487465,
    0.01112115103751421,
    0.011710909195244312,
    -0.0001670761703280732,
    -0.015797089785337448,
    0.029459817335009575,
    0.022775894030928612,
    0.004760189913213253,
    -0.009794196113944054,
    0.018071871250867844,
    -0.03041466325521469,
    0.003371800994500518,
    0.009190395474433899,
    -0.01995347999036312,
    -0.03867127746343613,
    0.014119087718427181,
    0.022902270779013634,
    -0.0004822500632144511,
    0.002436262322589755,
    0.005234102252870798,
    -0.023028647527098656,
    -0.025654474273324013,
    -0.021301498636603355,
    0.017117025330662727,
    -0.007870460860431194,
    0.0008618190186098218,
    0.026890156790614128,
    -0.024924296885728836,
    -0.020136024802923203,
    0.018788006156682968,
    -0.02368861436843872,
    -0.028196049854159355,
    -0.02506471611559391,
    -0.01100881677120924,
    0.011156256310641766,
    0.034121714532375336,
    -0.03268944472074509,
    0.006522442679852247,
    0.012567462399601936,
    0.00914124958217144,
    -0.01804378628730774,
    0.008003858849406242,
    -0.018886297941207886,
    0.0076317498460412025,
    -0.0003633330634329468,
    0.018956508487462997,
    0.01054543536156416,
    0.013508266769349575,
    -0.032324355095624924,
    -0.030442748218774796,
    -0.013817188329994678,
    0.03063933365046978,
    0.02833646908402443,
    0.0169625636190176,
    -0.006638288032263517,
    0.018128037452697754,
    0.0017174946842715144,
    0.023548195138573647,
    0.006659350823611021,
    0.015123080462217331,
    -0.020444944500923157,
    -0.013136157765984535,
    0.02034665085375309,
    0.018479084596037865,
    -0.010713937692344189,
    -0.007108690682798624,
    -0.0032717527355998755,
    0.008530428633093834,
    0.009316772222518921,
    0.004897098056972027,
    0.013346785679459572,
    -0.011500281281769276,
    0.020079856738448143,
    -0.03591907024383545,
    0.01880204677581787,
    0.006080124527215958,
    -0.013718894682824612,
    -0.04046863317489624,
    -0.03788493201136589,
    -0.012497253715991974,
    -0.028308385983109474,
    0.008432135917246342,
    0.034458719193935394,
    -0.008565532974898815,
    -0.0037386445328593254,
    -0.036817751824855804,
    -0.00888147484511137,
    -0.05571809038519859,
    0.026707613840699196,
    -0.02552809752523899,
    -0.03195926919579506,
    0.005371010396629572,
    -0.022607391700148582,
    -0.004402122460305691,
    0.022101884707808495,
    0.007617707829922438,
    0.005985341966152191,
    -0.0163728054612875,
    -0.023365650326013565,
    0.009197416715323925,
    -0.005683442112058401,
    -0.012398960068821907,
    0.0247136689722538,
    -0.04454077407717705,
    -0.01805782876908779,
    0.012160249054431915,
    0.01060862373560667,
    0.007652812637388706,
    0.0024257309269160032,
    -0.024517083540558815,
    -0.012455128133296967,
    -0.022846102714538574,
    0.052572716027498245,
    -0.0063925557769834995,
    0.021624460816383362,
    -0.017229359596967697,
    -0.017748907208442688,
    0.011668783612549305,
    -0.009787174873054028,
    0.004886566661298275,
    0.039064448326826096,
    0.03499231114983559,
    -0.0015472371596843004,
    0.009239542298018932,
    -0.02168062888085842,
    -0.005065600387752056,
    -0.005511429160833359,
    0.014505239203572273,
    -0.01793145202100277,
    0.0007806395296938717,
    0.007428142707794905,
    0.016527267172932625,
    0.003510464448481798,
    -0.03881169483065605,
    -0.017903368920087814,
    -0.0029400140047073364,
    0.007470268290489912,
    0.019574349746108055,
    0.026468900963664055,
    0.0063890451565384865,
    -0.020992577075958252,
    0.0029171959031373262,
    0.018310582265257835,
    0.014463113620877266,
    -0.0264408178627491,
    -0.008200445212423801,
    0.00977313332259655,
    0.012090039439499378,
    -0.009295709431171417,
    0.015993675217032433,
    -0.013072969391942024,
    -0.01465969905257225,
    0.03142567723989487,
    0.0011584532912820578,
    -0.0029347483068704605,
    0.010397995822131634,
    0.0070068868808448315,
    0.017847200855612755,
    -0.03580673784017563,
    0.019756894558668137,
    -0.01338891126215458,
    0.016583433374762535,
    -0.005858965218067169,
    0.008656805381178856,
    -0.0015656671021133661,
    -0.005258675664663315,
    0.016007717698812485,
    0.03591907024383545,
    0.016077926382422447,
    -0.0032647319603711367,
    -0.006557547487318516,
    -0.013150199316442013,
    0.0009688881691545248,
    0.01700468920171261,
    0.023140981793403625,
    0.03701433539390564,
    -0.008544470183551311,
    0.027507999911904335,
    0.011914515867829323,
    0.019167136400938034,
    0.021301498636603355,
    0.004644344560801983,
    -0.0032190957572311163,
    0.006862957961857319,
    -0.005311332643032074,
    0.03628415986895561,
    -0.009120186790823936,
    0.0104892672970891,
    -0.019588392227888107,
    0.012771070003509521,
    -0.027044618502259254,
    -0.01627451367676258,
    -0.007533456664532423,
    -0.004749658517539501,
    -0.01926542818546295,
    0.01683618687093258,
    -0.004331913311034441,
    -0.009316772222518921,
    0.02380094863474369,
    0.008684889413416386,
    0.01531966682523489,
    -0.01224450021982193,
    -0.032324355095624924,
    0.028238175436854362,
    0.011205402202904224,
    0.006789238192141056,
    0.025893185287714005,
    -0.007863440550863743,
    0.0074351634830236435,
    -0.006761154625564814,
    0.02599147893488407,
    -0.004082669969648123,
    0.006122250109910965,
    -0.025331512093544006,
    -0.004342444706708193,
    -0.017889326438307762,
    -0.018436959013342857,
    0.02776075340807438,
    -0.03782876580953598,
    -0.013494225218892097,
    -0.009892488829791546,
    0.03052699938416481,
    0.010875418782234192,
    -0.01713106594979763,
    -0.01054543536156416,
    0.05018559843301773,
    0.022270387038588524,
    0.004293297883123159,
    -0.008944663219153881,
    0.0014893144834786654,
    -0.0016771244117990136,
    0.0014419232029467821,
    -0.018928423523902893,
    0.003759707324206829,
    -0.044063348323106766,
    -0.004795294255018234,
    -0.014196318574249744,
    -0.003043572651222348,
    0.014786075800657272,
    0.018605461344122887,
    0.01270788162946701,
    0.0003045327903237194,
    0.0038825736846774817,
    0.0007973142201080918,
    0.002067663474008441,
    0.03487997502088547,
    0.028870059177279472,
    -0.002406423445791006,
    -0.004921671003103256,
    -0.019110968336462975,
    0.011338800191879272,
    -0.03249285742640495,
    0.004117774777114391,
    0.0014270037645474076,
    0.007161347195506096,
    -0.0005787878180854023,
    0.009471233002841473,
    0.0025714151561260223,
    -0.006876999977976084,
    0.00497081782668829,
    -0.00632234662771225,
    -0.008081088773906231,
    0.007554519455879927,
    -0.005571106914430857,
    -0.005184955894947052,
    -0.0019869229290634394,
    0.01000482402741909,
    -0.005700994282960892,
    -0.034514885395765305,
    -0.0013418750604614615,
    0.0026872605085372925,
    0.023140981793403625,
    0.02374478057026863,
    0.00550440838560462,
    -0.02350606955587864,
    -0.0075615402311086655,
    0.013873355463147163,
    -0.0036754561588168144,
    0.011865369975566864,
    -0.016990648582577705,
    0.029628319665789604,
    0.011345821432769299,
    0.02352011203765869,
    -0.006824342999607325,
    0.018310582265257835,
    0.01787528395652771,
    0.0018377280794084072,
    0.006097676698118448,
    0.01100881677120924,
    0.015249457210302353,
    -0.021540209650993347,
    -0.016653643921017647,
    0.02523321844637394,
    -0.007393037900328636,
    0.0005656235734932125,
    0.00042893487261608243,
    0.00018605461809784174,
    -0.04740530997514725,
    -0.014161213301122189,
    -0.0018903850577771664,
    0.020964493975043297,
    0.022663557901978493,
    -0.0007898545009084046,
    0.03465530648827553,
    -0.051196612417697906,
    -0.014091004617512226,
    -0.040440551936626434,
    -0.01517924852669239,
    -0.015965592116117477,
    0.010412037372589111,
    0.0009372939821332693,
    0.002411689143627882,
    -0.01307999063283205,
    -0.004865503869950771,
    -0.011809201911091805,
    -0.027620334178209305,
    -0.021175121888518333,
    -0.010706916451454163,
    0.0023941367398947477,
    0.011626658029854298,
    0.007737063802778721,
    0.006922635715454817,
    0.005444730166345835,
    0.02915089577436447,
    -0.001211110269650817,
    -0.014814159832894802,
    -0.00512878829613328,
    -0.012258541770279408,
    -0.03810960054397583,
    0.021568292751908302,
    -0.016316639259457588,
    -0.0020237828139215708,
    -0.01058053970336914,
    0.010440121404826641,
    0.034346383064985275,
    0.016176220029592514,
    -0.012195353396236897,
    -0.011549428105354309,
    -0.006097676698118448,
    0.012623630464076996,
    0.0039001258555799723,
    0.004061607178300619,
    0.012469169683754444,
    0.01261660922318697,
    -0.008397030644118786,
    0.016751935705542564,
    -0.011001795530319214,
    0.016653643921017647,
    -0.023309484124183655,
    0.004612750373780727,
    0.017889326438307762,
    0.010896481573581696,
    0.027957338839769363,
    -0.013129136525094509,
    -0.00018956507847178727,
    0.0109175443649292,
    -0.004145858343690634,
    0.004349465481936932,
    0.008607658557593822,
    0.02512088418006897,
    0.008579575456678867,
    0.01175303477793932,
    0.014406945556402206,
    -0.019981563091278076,
    -0.014076962135732174,
    -0.005936195142567158,
    -0.00767387542873621,
    0.02839263714849949,
    0.02172275446355343,
    0.008916579186916351,
    0.020051773637533188,
    -0.006957740522921085,
    -0.015909424051642418,
    -0.03156609460711479,
    0.00022247567540034652,
    0.019293513149023056,
    0.012967655435204506,
    -0.009611651301383972,
    -0.006006404757499695,
    -0.01621834561228752,
    -0.01891438290476799,
    -0.020360693335533142,
    0.0014296366134658456,
    -0.01546008512377739,
    -0.02378690615296364,
    -0.013964627869427204,
    -0.001093509723432362,
    -0.028898142278194427,
    0.0047882734797894955,
    0.013416995294392109,
    -0.022677600383758545,
    -0.023014605045318604,
    -0.008642763830721378,
    0.004567114170640707,
    0.031706515699625015,
    -0.016414931043982506,
    0.011394967325031757,
    -0.010798188857734203,
    -0.026244232431054115,
    -0.024854088202118874,
    0.0022203687112778425,
    0.0016718587139621377,
    -0.003773749340325594,
    0.010805209167301655,
    0.05088769271969795,
    0.031060589477419853,
    -0.0063293674029409885,
    0.01181622315198183,
    0.022607391700148582,
    0.003413926577195525,
    -0.015769006684422493,
    0.009850363247096539,
    0.011703887954354286,
    -0.011163276620209217,
    -0.017678698524832726,
    0.006680413614958525,
    -0.00454254075884819,
    0.01568475551903248,
    -0.014231422916054726,
    -0.008263633586466312,
    0.014266527257859707,
    -0.002485408913344145,
    0.0013892663409933448,
    -0.001708718598820269,
    -0.0494554229080677,
    -0.013901439495384693,
    0.005385052412748337,
    0.0009434373350813985,
    0.024348581209778786,
    -0.03754792734980583,
    -0.025654474273324013,
    0.02833646908402443,
    0.006255647633224726,
    0.020473027601838112,
    0.005813329014927149,
    -0.0015472371596843004,
    -0.004672428127378225,
    0.006055551115423441,
    0.022719725966453552,
    0.018605461344122887,
    -0.01028566062450409,
    -0.002325682668015361,
    -0.01032076496630907,
    -0.033953212201595306,
    0.0195462666451931,
    0.01770678162574768,
    0.004700511693954468,
    -0.000532712962012738,
    0.002625827444717288,
    -0.0021203204523772,
    0.029235146939754486,
    0.011760056018829346,
    -0.0015279296785593033,
    0.01845100149512291,
    -0.02667952887713909,
    -0.004890076816082001,
    -0.014329715631902218,
    -0.042799580842256546,
    -0.010201409459114075,
    0.0004179646784905344,
    0.003238403471186757,
    0.00017640083387959749,
    0.004149368964135647,
    -0.016134094446897507,
    -0.007421121932566166,
    0.014603531919419765,
    6.357231904985383e-05,
    0.035834819078445435,
    0.019518181681632996,
    0.02517705038189888,
    0.02965640276670456,
    0.02592126838862896,
    0.01359251793473959,
    -0.010306723415851593,
    0.004286277107894421,
    0.024222204461693764,
    0.013171262107789516,
    0.002208082005381584,
    -0.019153093919157982,
    -0.03617182374000549,
    0.0074983518570661545,
    -0.02535959519445896,
    -0.01468778308480978,
    -0.01765061542391777,
    0.019419889897108078,
    0.012005788274109364,
    0.010629686526954174,
    -0.014147171750664711,
    -0.012160249054431915,
    -0.014954578131437302,
    0.002927727298811078,
    -0.0031488866079598665,
    0.015656670555472374,
    0.013164241798222065,
    0.008930621668696404,
    -0.017973577603697777,
    0.022846102714538574,
    0.01821228861808777,
    0.027774794027209282,
    0.022958436980843544,
    -0.018380790948867798,
    0.00606257189065218,
    -0.03018999472260475,
    -0.017917409539222717,
    0.01345209963619709,
    -0.0011610861402004957,
    0.03895211219787598,
    -0.017257442697882652,
    0.006462764926254749,
    0.04897800087928772,
    0.02172275446355343,
    -0.018703754991292953,
    -0.0020290485117584467,
    -0.0014814159367233515,
    0.035722486674785614,
    -0.024474957957863808,
    -0.004907629452645779,
    -0.0039141676388680935,
    -0.023379692807793617,
    0.014371841214597225,
    0.010615644045174122,
    -0.01468778308480978,
    -0.013564434833824635,
    0.01336082722991705,
    0.007171878591179848,
    0.009738028049468994,
    -0.007933649234473705,
    -0.04468119144439697,
    0.002592477947473526,
    0.008628721348941326,
    -0.0018517699791118503,
    -0.01425950601696968,
    -0.018479084596037865,
    0.005606211721897125,
    -0.040215879678726196,
    -0.026075730100274086,
    0.012672776356339455,
    -0.0107771260663867,
    -0.013704853132367134,
    0.028954310342669487,
    -0.006613715086132288,
    0.001862301374785602,
    0.036705415695905685,
    -0.013234450481832027,
    0.0024274862371385098,
    0.01977093517780304,
    -5.836147101945244e-05,
    -0.01564262993633747,
    0.012490232475101948,
    -0.01689235493540764,
    0.0014743950450792909,
    0.021568292751908302,
    -0.024334538727998734,
    -0.044231850653886795,
    -0.013732937164604664,
    -0.004546051379293203,
    0.02753608301281929,
    0.02350606955587864,
    -0.026454860344529152,
    -0.010060991160571575,
    -0.004331913311034441,
    0.021470000967383385,
    0.006002894137054682,
    -0.015951549634337425,
    0.018928423523902893,
    -0.016513224691152573,
    0.008081088773906231,
    0.021175121888518333,
    -0.005858965218067169,
    0.02547192946076393,
    -0.006006404757499695,
    -0.0025433313567191362,
    -0.0027346517890691757,
    -0.029937241226434708,
    -0.03485189005732536,
    -0.009702923707664013,
    -0.00853744987398386,
    -0.006167885847389698,
    0.03246477618813515,
    -0.007603665813803673,
    -0.031004421412944794,
    -0.010426078923046589,
    0.03431830182671547,
    0.02086620032787323,
    0.021638503298163414,
    -0.015769006684422493,
    0.016176220029592514,
    -0.010770104825496674,
    0.012862341478466988,
    0.01118433941155672,
    -0.043894845992326736,
    -0.013171262107789516,
    0.0087691405788064,
    -0.025219175964593887,
    -0.016162177547812462,
    -0.004742637276649475,
    0.00128219707403332,
    0.0367896668612957,
    -0.035216979682445526,
    -0.02482600510120392,
    -0.004507436417043209,
    -0.008719993755221367,
    0.012026851065456867,
    -0.02236867882311344,
    0.018872257322072983,
    -0.006318836007267237,
    0.01242002286016941,
    0.021132996305823326,
    -0.004216067958623171,
    -0.009000830352306366,
    0.016681727021932602,
    0.009035935625433922,
    0.004100222606211901,
    0.000719645235221833,
    0.016934480518102646,
    -0.020374735817313194,
    -0.025148967280983925,
    -0.04720872640609741,
    -0.030049575492739677,
    0.0020237828139215708,
    0.018703754991292953,
    0.007828335277736187,
    0.017861243337392807,
    -0.012230457738041878,
    0.005648337304592133,
    -0.0019904333166778088,
    -0.0014796607429161668,
    -0.006080124527215958,
    -0.0022431868128478527,
    0.04105839133262634,
    -0.018310582265257835,
    -0.004904118832200766,
    0.005335906054824591,
    -0.0024538147263228893,
    0.013599539175629616,
    -0.016021760180592537,
    0.006016936153173447,
    -0.006838384550064802,
    0.0006525075878016651,
    -0.021245330572128296,
    0.0019763915333896875,
    -0.021076828241348267,
    0.014800118282437325,
    -0.0017806830583140254,
    -0.011963662691414356,
    0.0005774713936261833,
    0.0014076962834224105,
    -0.0031208028085529804,
    0.00810917280614376,
    -0.02201763354241848,
    0.025387678295373917,
    -0.021132996305823326,
    -0.0044091432355344296,
    0.02058536373078823,
    0.007666854187846184,
    -0.011219444684684277,
    -0.007884503342211246,
    0.02662336267530918,
    -0.010678832419216633,
    0.008797223679721355,
    -6.505329656647518e-05,
    -0.024559209123253822,
    -0.010222472250461578,
    -0.0008978012483566999,
    0.0014989683404564857,
    -0.004451268818229437,
    0.009983761236071587,
    0.004082669969648123,
    0.007150815799832344,
    -0.013543372042477131,
    -0.014076962135732174,
    0.018956508487462997,
    -0.004149368964135647,
    0.005648337304592133,
    0.00681030098348856,
    -0.01943393051624298,
    -0.023829031735658646,
    -0.004289787728339434,
    -0.021343624219298363,
    -0.024587292224168777,
    -0.00191144784912467,
    0.18950891494750977,
    0.0057747140526771545,
    0.0075615402311086655,
    0.046871721744537354,
    0.009492295794188976,
    0.023239273577928543,
    -0.0028557628393173218,
    -0.014589490368962288,
    -0.020669614896178246,
    0.00744218472391367,
    0.024460915476083755,
    0.012771070003509521,
    0.00830575916916132,
    -0.002509982092306018,
    0.0024239756166934967,
    -0.013571455143392086,
    -0.022565266117453575,
    -0.025500014424324036,
    -0.03035849705338478,
    -0.004465310834348202,
    -0.0006691822782158852,
    0.012771070003509521,
    -0.0022835570853203535,
    -0.029852990061044693,
    0.006227564066648483,
    0.004352976102381945,
    -0.020136024802923203,
    0.0034086608793586493,
    0.033897045999765396,
    -0.0005441220127977431,
    -0.01405589934438467,
    0.009920572862029076,
    -0.010770104825496674,
    -0.02367457188665867,
    -0.006845405790954828,
    -0.02116107940673828,
    0.012876383028924465,
    0.004672428127378225,
    0.012293646112084389,
    0.03240860626101494,
    -0.007737063802778721,
    -0.011254549026489258,
    -0.003594715613871813,
    -0.013662727549672127,
    -0.008221508003771305,
    -0.021189162507653236,
    -0.017566364258527756,
    0.017959535121917725,
    -0.00830575916916132,
    0.014561406336724758,
    -0.03518889471888542,
    -0.017102982848882675,
    0.03271752968430519,
    0.010433100163936615,
    -0.0103488489985466,
    -0.015839215368032455,
    0.00885339081287384,
    0.030498914420604706,
    0.04701213911175728,
    0.006606693845242262,
    -0.00824959110468626,
    0.014343757182359695,
    -0.0044723316095769405,
    0.00914124958217144,
    -0.018324624747037888,
    -0.001378734945319593,
    -0.014757992699742317,
    -0.004577645566314459,
    -0.009738028049468994,
    0.010369911789894104,
    -0.006424149964004755,
    -0.035160813480615616,
    -0.0204168614000082,
    0.004609239753335714,
    -0.031762681901454926,
    -0.024671543389558792,
    0.040496718138456345,
    0.008579575456678867,
    0.01454736478626728,
    0.03712667152285576,
    0.00029773125424981117,
    -0.013697831891477108,
    -0.016007717698812485,
    0.0037561969365924597,
    -0.014631615951657295,
    -0.032633278518915176,
    -0.00015928731590975076,
    -0.012567462399601936,
    -0.01971476897597313,
    0.0015472371596843004,
    -0.01673789508640766,
    -0.040159713476896286,
    -0.010994774289429188,
    -0.02172275446355343,
    0.010299702174961567,
    -0.00914124958217144,
    -0.00032844781526364386,
    0.014898410998284817,
    -0.018015703186392784,
    0.008460219018161297,
    -0.03861510753631592,
    0.059705980122089386,
    0.00790556613355875,
    0.0120759978890419,
    -0.013803145848214626,
    0.001916713546961546,
    -0.015263499692082405,
    0.02235463820397854,
    0.006346919573843479,
    -0.005655358079820871,
    0.012742985971271992,
    -0.03190309926867485,
    0.007491331081837416,
    -0.0017447008285671473,
    -0.008130235597491264,
    0.011851327493786812,
    -0.014533322304487228,
    -0.00902189314365387,
    0.017201276496052742,
    -0.0011470442404970527,
    0.023309484124183655,
    -0.0019079374615103006,
    -0.003020754549652338,
    -0.010264597833156586,
    -0.03128525987267494,
    -0.010601602494716644,
    -0.013697831891477108,
    -0.02166658639907837,
    -0.016471099108457565,
    -0.0018693222664296627,
    0.01815612241625786,
    -0.01135284174233675,
    0.044568855315446854,
    -0.012462148442864418,
    -0.0031681940890848637,
    -0.03370045870542526,
    -0.01534775085747242,
    -0.02535959519445896,
    -0.011717930436134338,
    0.015165206044912338,
    -0.026525069028139114,
    0.0077581265941262245,
    0.02781691960990429,
    0.0003525822830852121,
    0.019967522472143173,
    -0.02316906489431858,
    0.018408875912427902,
    -0.007020928896963596,
    0.008467240259051323,
    -0.004742637276649475,
    0.0011330023407936096,
    -0.02161041833460331,
    -0.009794196113944054,
    -0.01548816915601492,
    0.016316639259457588,
    0.0027100786101073027,
    -0.003994908649474382,
    -0.02300056256353855,
    0.010910523124039173,
    -0.012061955407261848,
    -0.04226599261164665,
    0.01885821484029293,
    -0.01112115103751421,
    0.004805825650691986,
    -0.039935044944286346,
    -0.02592126838862896,
    -0.18040978908538818,
    0.017327653244137764,
    0.026454860344529152,
    -0.04030013084411621,
    0.01822633109986782,
    -0.0089095588773489,
    0.023239273577928543,
    -0.002309885574504733,
    -0.01683618687093258,
    -0.020051773637533188,
    0.03662116453051567,
    0.00671200780197978,
    -0.013087010942399502,
    -0.03990695998072624,
    0.011247527785599232,
    0.0043459548614919186,
    -0.005578128155320883,
    0.0021273414604365826,
    0.0339251272380352,
    0.0175382811576128,
    0.028069673106074333,
    -0.02224230207502842,
    0.014182276092469692,
    0.010075032711029053,
    -0.009534421376883984,
    -0.009843342006206512,
    -0.002985649975016713,
    0.02432049810886383,
    0.011465176939964294,
    -0.009415065869688988,
    -0.009042955935001373,
    -0.0013488959521055222,
    0.036761581897735596,
    -0.004026502836495638,
    0.0018465042812749743,
    0.0015551357064396143,
    -0.0028662942349910736,
    -0.0033156336285173893,
    0.006234584841877222,
    0.018254414200782776,
    0.0402439646422863,
    0.011675804853439331,
    -0.002129096770659089,
    0.003018999472260475,
    -0.005097194109112024,
    0.026426775380969048,
    0.020037731155753136,
    -0.0284488033503294,
    0.017678698524832726,
    -0.02982490509748459,
    0.0350484773516655,
    -0.009365919046103954,
    0.00015643506776541471,
    0.0026626873295754194,
    0.015502210706472397,
    -0.02488217130303383,
    0.01908288523554802,
    0.009829300455749035,
    0.010566498152911663,
    -0.017805075272917747,
    -0.02287418581545353,
    -0.02114703692495823,
    -0.0006055551348254085,
    -0.009766112081706524,
    -0.005532491952180862,
    -0.017636572942137718,
    -0.023407775908708572,
    -0.00487252464517951,
    -0.03718283772468567,
    -0.007884503342211246,
    0.004440737422555685,
    0.02495238184928894,
    0.009415065869688988,
    0.033138785511255264,
    0.016878312453627586,
    0.025851059705018997,
    -0.02264951728284359,
    0.023098856210708618,
    0.02437666431069374,
    -0.016176220029592514,
    -0.006599673070013523,
    0.020051773637533188,
    0.005423667374998331,
    0.02367457188665867,
    0.0158111322671175,
    0.008832328021526337,
    0.005722057074308395,
    -0.004925181623548269,
    -0.027957338839769363,
    -0.012665756046772003,
    0.017510196194052696,
    0.003371800994500518,
    0.00997673999518156,
    -0.001587607548572123,
    0.01793145202100277,
    0.004507436417043209,
    -0.0020852158777415752,
    0.0057747140526771545,
    0.011998767033219337,
    -0.01411206740885973,
    0.044568855315446854,
    0.007294745184481144,
    -0.011015837080776691,
    0.00043200651998631656,
    0.03524506092071533,
    0.014631615951657295,
    -0.0402439646422863,
    0.028813891112804413,
    0.02081003226339817,
    -0.0004052392323501408,
    -0.019040759652853012,
    0.017271485179662704,
    -0.004675938747823238,
    0.004837419837713242,
    -0.005304311867803335,
    0.03513272851705551,
    0.010622665286064148,
    -0.02534555271267891,
    0.03375662490725517,
    -0.0036157784052193165,
    0.03844660520553589,
    0.026763780042529106,
    -0.01270788162946701,
    0.013887397013604641,
    -0.02454516664147377,
    -0.04535520076751709,
    -0.06661457568407059,
    -0.021540209650993347,
    0.03816577047109604,
    0.0061082080937922,
    -0.03145376220345497,
    -0.031144840642809868,
    -0.009808237664401531,
    0.022452929988503456,
    -0.018001660704612732,
    0.004977838601917028,
    -0.01301680225878954,
    -0.0158111322671175,
    -0.003808853914961219,
    0.018268456682562828,
    -0.024334538727998734,
    -0.019630517810583115,
    -0.010868397541344166,
    -0.003763217944651842,
    -0.014406945556402206,
    0.011394967325031757,
    -0.011731971986591816,
    0.015446043573319912,
    0.016007717698812485,
    0.01058053970336914,
    0.0011005306150764227,
    -0.005076131783425808,
    -0.015221374109387398,
    0.024292413145303726,
    0.005651847925037146,
    -0.009829300455749035,
    0.027339497581124306,
    -0.01649918220937252,
    -0.01707489974796772,
    -0.026202106848359108,
    -0.0052165500819683075,
    0.004075649194419384,
    -0.02541576325893402,
    -0.008340863510966301,
    0.02535959519445896,
    -0.02982490509748459,
    0.007315807975828648,
    -0.01548816915601492,
    0.009358897805213928,
    0.0076317498460412025,
    -0.04499011114239693,
    0.0015533805126324296,
    -0.010426078923046589,
    0.03361620754003525,
    0.014589490368962288,
    -0.03485189005732536,
    -0.008067047223448753,
    -0.0045249885879457,
    -0.021891256794333458,
    -0.01805782876908779,
    0.036761581897735596,
    -0.009386981837451458,
    0.029796821996569633,
    0.022747809067368507,
    -0.00864978414028883,
    -0.001191802672110498,
    -0.00033590756356716156,
    -0.0037667283322662115,
    0.005255165044218302,
    -0.000996971852146089,
    0.0022361658047884703,
    0.014364819973707199,
    -0.0010742021258920431,
    -0.015895383432507515,
    0.0038299167063087225,
    -0.03518889471888542,
    -0.014757992699742317,
    0.023548195138573647,
    -0.006045019719749689,
    0.02954406850039959,
    -0.03167843073606491,
    -0.006332878023386002,
    -0.0037035399582237005,
    -0.018001660704612732,
    0.0010592826874926686,
    -0.014772034250199795,
    -0.015052871778607368,
    -0.0024976953864097595,
    -0.009695902466773987,
    -0.003419192275032401,
    0.0014120843261480331,
    0.0001766202476574108,
    0.006982313934713602,
    -0.011809201911091805,
    0.025570223107933998,
    -0.043838679790496826,
    0.015249457210302353,
    0.0035245062317699194,
    0.02378690615296364,
    -0.005725567694753408,
    -0.009632714092731476,
    -0.018408875912427902,
    0.0005414891638793051,
    0.0023713186383247375,
    -0.012455128133296967,
    0.01689235493540764,
    -0.04645046591758728,
    -0.013262534514069557,
    -0.08065643161535263,
    0.034683387726545334,
    0.003180480794981122,
    -0.028841976076364517,
    0.005985341966152191,
    -0.007723021786659956,
    0.006357450969517231,
    -0.006034488324075937,
    0.008354905061423779,
    0.00191144784912467,
    -0.01638684794306755,
    -0.011795160360634327,
    -0.0015507476637139916,
    -0.0001222080463776365,
    -0.0072455983608961105,
    -0.01993943750858307,
    0.028954310342669487,
    -0.014982662163674831,
    0.02976873889565468,
    -0.003083942923694849,
    -0.011317737400531769,
    -0.007301765959709883,
    0.03608757257461548,
    0.012525336816906929,
    -0.045439451932907104,
    -0.025148967280983925,
    -0.032857947051525116,
    0.01822633109986782,
    -0.002406423445791006,
    -0.01301680225878954,
    -0.0075615402311086655,
    -0.006722539197653532,
    -0.0030453279614448547,
    0.0186897125095129,
    0.006680413614958525,
    0.019784977659583092,
    0.02759225107729435,
    -0.0071332636289298534,
    0.009688882157206535,
    0.01919521950185299,
    -0.012574483640491962,
    -0.028069673106074333,
    0.0322120226919651,
    -0.010875418782234192,
    -0.006273199804127216,
    -0.019391804933547974,
    0.010110137984156609,
    -0.017173191532492638,
    0.01482820138335228,
    0.024966422468423843,
    0.030667416751384735,
    0.023843074217438698,
    -0.015207331627607346,
    -0.05358373001217842,
    0.015769006684422493,
    -0.00914124958217144,
    0.029572151601314545,
    0.002976873889565468,
    -0.0218350887298584,
    -0.011043921113014221,
    0.0327736958861351,
    -0.011977704241871834,
    0.013557413592934608,
    0.00734389154240489,
    0.025837017223238945,
    -0.004939223639667034,
    -0.01135284174233675,
    0.01735573634505272,
    0.009457191452383995,
    -0.020374735817313194,
    -0.026412734761834145,
    -0.020501112565398216,
    0.018380790948867798,
    0.025907227769494057,
    0.03271752968430519,
    0.0005283249192871153,
    -0.0021045233588665724,
    -0.00589406955987215,
    -0.011001795530319214,
    0.01258852519094944,
    0.03243669122457504,
    -0.012672776356339455,
    -0.0028575181495398283,
    0.008944663219153881,
    0.0036930085625499487,
    -0.014154192991554737,
    -0.027578208595514297,
    0.012040892615914345,
    0.00629426259547472,
    0.009843342006206512,
    -0.005300801247358322,
    0.014048879034817219,
    -0.01655535027384758,
    0.0016244674334302545,
    -0.020894283428788185,
    -0.0012558687012642622,
    -0.01845100149512291,
    -0.021076828241348267,
    0.010110137984156609,
    0.012742985971271992,
    0.012061955407261848,
    -0.0005164770991541445,
    0.01000482402741909,
    -0.0009820524137467146,
    -0.023295441642403603,
    -0.0025714151561260223,
    -0.006346919573843479,
    -0.009646756574511528,
    0.001439290470443666,
    0.026202106848359108,
    0.021989548578858376,
    -0.014182276092469692,
    -0.011507302522659302,
    0.0033788220025599003,
    -0.019644558429718018,
    0.016246428713202477,
    -0.004195005167275667,
    -0.029178980737924576,
    -0.00022576673654839396,
    0.012370876967906952,
    0.010194388218224049,
    -0.011535385623574257,
    0.033307287842035294,
    -0.015123080462217331,
    0.017383819445967674,
    0.0030330412555485964,
    0.008755098097026348,
    -0.030077658593654633,
    0.01147219818085432,
    0.0066909450106322765,
    -0.017805075272917747,
    0.003370045917108655,
    -0.0028522524517029524,
    6.07749170740135e-05,
    -0.011823244392871857,
    -0.03502039238810539,
    -0.0060274675488471985,
    0.015951549634337425,
    0.009014872834086418,
    0.08374563604593277,
    0.020444944500923157,
    -0.02575276605784893,
    -0.013669748790562153,
    -0.010552455671131611,
    0.0029452797025442123,
    -0.008158319629728794,
    0.02564043179154396,
    -0.01794549450278282,
    -0.020711740478873253,
    -0.03398129716515541,
    -0.01242002286016941,
    -0.02982490509748459,
    0.0011742503847926855,
    -0.016021760180592537,
    -0.0015911180526018143,
    -0.013690811581909657,
    0.01534775085747242,
    0.018717795610427856,
    0.005950237158685923,
    0.025907227769494057,
    0.019518181681632996,
    -0.01614813692867756,
    0.0029013988096266985,
    -0.01488436944782734,
    -0.025598306208848953,
    0.032633278518915176,
    0.008916579186916351,
    -0.01098073273897171,
    -0.03350387141108513,
    0.021947422996163368,
    0.011703887954354286,
    -0.03810960054397583,
    -0.028813891112804413,
    0.02127341367304325,
    -0.015207331627607346,
    0.0002768878766801208,
    -0.012111102230846882,
    0.01620430313050747,
    0.018380790948867798,
    0.021694669499993324,
    -0.03063933365046978,
    -0.014870326966047287,
    -0.023871157318353653,
    0.011654742062091827,
    -0.008916579186916351,
    -0.0032945708371698856,
    -0.016428973525762558,
    -0.05451049283146858
  ]
}