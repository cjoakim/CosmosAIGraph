{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved",
    "programming language :: python",
    "programming language :: python :: 3",
    "programming language :: python :: 3.5",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8"
  ],
  "description": "[![build status](https://travis-ci.org/dgasmith/opt_einsum.svg?branch=master)](https://travis-ci.org/dgasmith/opt_einsum)\n[![codecov](https://codecov.io/gh/dgasmith/opt_einsum/branch/master/graph/badge.svg)](https://codecov.io/gh/dgasmith/opt_einsum)\n[![anaconda-server badge](https://anaconda.org/conda-forge/opt_einsum/badges/version.svg)](https://anaconda.org/conda-forge/opt_einsum)\n[![pypi](https://img.shields.io/pypi/v/opt_einsum.svg)](https://pypi.org/project/opt-einsum/#description)\n[![pypistats](https://img.shields.io/pypi/dm/opt_einsum)](https://pypistats.org/packages/opt-einsum)\n[![documentation status](https://readthedocs.org/projects/optimized-einsum/badge/?version=latest)](http://optimized-einsum.readthedocs.io/en/latest/?badge=latest)\n[![doi](http://joss.theoj.org/papers/10.21105/joss.00753/status.svg)](https://doi.org/10.21105/joss.00753)\n\n\noptimized einsum: a tensor contraction order optimizer\n======================================================\n\noptimized einsum can significantly reduce the overall execution time of einsum-like expressions (e.g.,\n[`np.einsum`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html),\n[`dask.array.einsum`](https://docs.dask.org/en/latest/array-api.html#dask.array.einsum),\n[`pytorch.einsum`](https://pytorch.org/docs/stable/torch.html#torch.einsum),\n[`tensorflow.einsum`](https://www.tensorflow.org/api_docs/python/tf/einsum),\n)\nby optimizing the expression's contraction order and dispatching many\noperations to canonical blas, cublas, or other specialized routines. optimized\neinsum is agnostic to the backend and can handle numpy, dask, pytorch,\ntensorflow, cupy, sparse, theano, jax, and autograd arrays as well as potentially\nany library which conforms to a standard api. see the\n[**documentation**](http://optimized-einsum.readthedocs.io) for more\ninformation.\n\n## example usage\n\nthe [`opt_einsum.contract`](https://optimized-einsum.readthedocs.io/en/latest/autosummary/opt_einsum.contract.html#opt-einsum-contract)\nfunction can often act as a drop-in replacement for `einsum`\nfunctions without futher changes to the code while providing superior performance.\nhere, a tensor contraction is preformed with and without optimization:\n\n```python\nimport numpy as np\nfrom opt_einsum import contract\n\nn = 10\nc = np.random.rand(n, n)\ni = np.random.rand(n, n, n, n)\n\n%timeit np.einsum('pi,qj,ijkl,rk,sl->pqrs', c, c, i, c, c)\n1 loops, best of 3: 934 ms per loop\n\n%timeit contract('pi,qj,ijkl,rk,sl->pqrs', c, c, i, c, c)\n1000 loops, best of 3: 324 us per loop\n```\n\nin this particular example, we see a ~3000x performance improvement which is\nnot uncommon when compared against unoptimized contractions. see the [backend\nexamples](https://optimized-einsum.readthedocs.io/en/latest/backends.html)\nfor more information on using other backends.\n\n## features\n\nthe algorithms found in this repository often power the `einsum` optimizations\nin many of the above projects. for example, the optimization of `np.einsum`\nhas been passed upstream and most of the same features that can be found in\nthis repository can be enabled with `np.einsum(..., optimize=true)`. however,\nthis repository often has more up to date algorithms for complex contractions.\n\nthe following capabilities are enabled by `opt_einsum`:\n\n* inspect [detailed information](http://optimized-einsum.readthedocs.io/en/latest/path_finding.html) about the path chosen.\n* perform contractions with [numerous backends](http://optimized-einsum.readthedocs.io/en/latest/backends.html), including on the gpu and with libraries such as [tensorflow](https://www.tensorflow.org) and [pytorch](https://pytorch.org).\n* generate [reusable expressions](http://optimized-einsum.readthedocs.io/en/latest/reusing_paths.html), potentially with [constant tensors](http://optimized-einsum.readthedocs.io/en/latest/reusing_paths.html#specifying-constants), that can be compiled for greater performance.\n* use an arbitrary number of indices to find contractions for [hundreds or even thousands of tensors](http://optimized-einsum.readthedocs.io/en/latest/ex_large_expr_with_greedy.html).\n* share [intermediate computations](http://optimized-einsum.readthedocs.io/en/latest/sharing_intermediates.html) among multiple contractions.\n* compute gradients of tensor contractions using [autograd](https://github.com/hips/autograd) or [jax](https://github.com/google/jax)\n\nplease see the [documentation](http://optimized-einsum.readthedocs.io/en/latest/?badge=latest) for more features!\n\n\n## installation\n\n`opt_einsum` can either be installed via `pip install opt_einsum` or from conda `conda install opt_einsum -c conda-forge`. see the installation [documenation](http://optimized-einsum.readthedocs.io/en/latest/install.html) for further methods.\n\n## citation\n\nif this code has benefited your research, please support us by citing:\n\ndaniel g. a. smith and johnnie gray, opt_einsum - a python package for optimizing contraction order for einsum-like expressions. *journal of open source software*, **2018**, 3(26), 753\n\ndoi: https://doi.org/10.21105/joss.00753\n\n## contributing\n\nall contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.\n\na detailed overview on how to contribute can be found in the [contributing guide](https://github.com/dgasmith/opt_einsum/blob/master/.github/contributing.md).\n\n\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "opt-einsum",
  "package_url": "https://pypi.org/project/opt-einsum/",
  "project_url": "https://pypi.org/project/opt-einsum/",
  "project_urls": {
    "Homepage": "https://github.com/dgasmith/opt_einsum"
  },
  "release_url": "https://pypi.org/project/opt-einsum/3.3.0/",
  "requires_dist": [
    "numpy (>=1.7)",
    "sphinx (==1.2.3) ; extra == 'docs'",
    "sphinxcontrib-napoleon ; extra == 'docs'",
    "sphinx-rtd-theme ; extra == 'docs'",
    "numpydoc ; extra == 'docs'",
    "pytest ; extra == 'tests'",
    "pytest-cov ; extra == 'tests'",
    "pytest-pep8 ; extra == 'tests'"
  ],
  "requires_python": ">=3.5",
  "summary": "optimizing numpys einsum function",
  "version": "3.3.0",
  "releases": [],
  "developers": [
    "daniel_smith",
    "dgasmith@icloud.com"
  ],
  "kwds": "opt_einsum badge badges einsum svg",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_opt_einsum",
  "homepage": "https://github.com/dgasmith/opt_einsum",
  "release_count": 14,
  "dependency_ids": [
    "pypi_numpy",
    "pypi_numpydoc",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_pytest_pep8",
    "pypi_sphinx",
    "pypi_sphinx_rtd_theme",
    "pypi_sphinxcontrib_napoleon"
  ]
}