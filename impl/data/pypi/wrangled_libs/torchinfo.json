{
  "classifiers": [
    "license :: osi approved :: mit license",
    "operating system :: os independent",
    "programming language :: python :: 3",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "model summary in pytorch, based off of the original torchsummary.\n# torchinfo\n\n[![python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/release/python-370/)\n[![pypi version](https://badge.fury.io/py/torchinfo.svg)](https://badge.fury.io/py/torchinfo)\n[![conda version](https://img.shields.io/conda/vn/conda-forge/torchinfo)](https://anaconda.org/conda-forge/torchinfo)\n[![build status](https://github.com/tyleryep/torchinfo/actions/workflows/test.yml/badge.svg)](https://github.com/tyleryep/torchinfo/actions/workflows/test.yml)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/tyleryep/torchinfo/main.svg)](https://results.pre-commit.ci/latest/github/tyleryep/torchinfo/main)\n[![github license](https://img.shields.io/github/license/tyleryep/torchinfo)](https://github.com/tyleryep/torchinfo/blob/main/license)\n[![codecov](https://codecov.io/gh/tyleryep/torchinfo/branch/main/graph/badge.svg)](https://codecov.io/gh/tyleryep/torchinfo)\n[![downloads](https://pepy.tech/badge/torchinfo)](https://pepy.tech/project/torchinfo)\n\n(formerly torch-summary)\n\ntorchinfo provides information complementary to what is provided by `print(your_model)` in pytorch, similar to tensorflow's `model.summary()` api to view the visualization of the model, which is helpful while debugging your network. in this project, we implement a similar functionality in pytorch and create a clean, simple interface to use in your projects.\n\nthis is a completely rewritten version of the original torchsummary and torchsummaryx projects by @sksq96 and @nmhkahn. this project addresses all of the issues and pull requests left on the original projects by introducing a completely new api.\n\nsupports pytorch versions 1.4.0+.\n\n# usage\n\n```\npip install torchinfo\n```\n\nalternatively, via conda:\n\n```\nconda install -c conda-forge torchinfo\n```\n\n# how to use\n\n```python\nfrom torchinfo import summary\n\nmodel = convnet()\nbatch_size = 16\nsummary(model, input_size=(batch_size, 1, 28, 28))\n```\n\n```\n================================================================================================================\nlayer (type:depth-idx)          input shape          output shape         param #            mult-adds\n================================================================================================================\nsingleinputnet                  [7, 1, 28, 28]       [7, 10]              --                 --\n\u251c\u2500conv2d: 1-1                   [7, 1, 28, 28]       [7, 10, 24, 24]      260                1,048,320\n\u251c\u2500conv2d: 1-2                   [7, 10, 12, 12]      [7, 20, 8, 8]        5,020              2,248,960\n\u251c\u2500dropout2d: 1-3                [7, 20, 8, 8]        [7, 20, 8, 8]        --                 --\n\u251c\u2500linear: 1-4                   [7, 320]             [7, 50]              16,050             112,350\n\u251c\u2500linear: 1-5                   [7, 50]              [7, 10]              510                3,570\n================================================================================================================\ntotal params: 21,840\ntrainable params: 21,840\nnon-trainable params: 0\ntotal mult-adds (m): 3.41\n================================================================================================================\ninput size (mb): 0.02\nforward/backward pass size (mb): 0.40\nparams size (mb): 0.09\nestimated total size (mb): 0.51\n================================================================================================================\n```\n\n<!-- single_input_all_cols.out -->\n\nnote: if you are using a jupyter notebook or google colab, `summary(model, ...)` must be the returned value of the cell.\nif it is not, you should wrap the summary in a print(), e.g. `print(summary(model, ...))`.\nsee `tests/jupyter_test.ipynb` for examples.\n\n**this version now supports:**\n\n- rnns, lstms, and other recursive layers\n- branching output used to explore model layers using specified depths\n- returns modelstatistics object containing all summary data fields\n- configurable rows/columns\n- jupyter notebook / google colab\n\n**other new features:**\n\n- verbose mode to show weights and bias layers\n- accepts either input data or simply the input shape!\n- customizable line widths and batch dimension\n- comprehensive unit/output testing, linting, and code coverage testing\n\n**community contributions:**\n\n- sequentials & modulelists (thanks to @roym899)\n- improved mult-add calculations (thanks to @te-stefanuhlich, @zmzhang2000)\n- dict/misc input data (thanks to @e-dorigatti)\n- pruned layer support (thanks to @majorcarrot)\n\n# documentation\n\n```python\ndef summary(\n    model: nn.module,\n    input_size: optional[input_size_type] = none,\n    input_data: optional[input_data_type] = none,\n    batch_dim: optional[int] = none,\n    cache_forward_pass: optional[bool] = none,\n    col_names: optional[iterable[str]] = none,\n    col_width: int = 25,\n    depth: int = 3,\n    device: optional[torch.device] = none,\n    dtypes: optional[list[torch.dtype]] = none,\n    mode: str | none = none,\n    row_settings: optional[iterable[str]] = none,\n    verbose: int = 1,\n    **kwargs: any,\n) -> modelstatistics:\n\"\"\"\nsummarize the given pytorch model. summarized information includes:\n    1) layer names,\n    2) input/output shapes,\n    3) kernel shape,\n    4) # of parameters,\n    5) # of operations (mult-adds),\n    6) whether layer is trainable\n\nnote: if neither input_data or input_size are provided, no forward pass through the\nnetwork is performed, and the provided model information is limited to layer names.\n\nargs:\n    model (nn.module):\n            pytorch model to summarize. the model should be fully in either train()\n            or eval() mode. if layers are not all in the same mode, running summary\n            may have side effects on batchnorm or dropout statistics. if you\n            encounter an issue with this, please open a github issue.\n\n    input_size (sequence of sizes):\n            shape of input data as a list/tuple/torch.size\n            (dtypes must match model input, default is floattensors).\n            you should include batch size in the tuple.\n            default: none\n\n    input_data (sequence of tensors):\n            arguments for the model's forward pass (dtypes inferred).\n            if the forward() function takes several parameters, pass in a list of\n            args or a dict of kwargs (if your forward() function takes in a dict\n            as its only argument, wrap it in a list).\n            default: none\n\n    batch_dim (int):\n            batch_dimension of input data. if batch_dim is none, assume\n            input_data / input_size contains the batch dimension, which is used\n            in all calculations. else, expand all tensors to contain the batch_dim.\n            specifying batch_dim can be an runtime optimization, since if batch_dim\n            is specified, torchinfo uses a batch size of 1 for the forward pass.\n            default: none\n\n    cache_forward_pass (bool):\n            if true, cache the run of the forward() function using the model\n            class name as the key. if the forward pass is an expensive operation,\n            this can make it easier to modify the formatting of your model\n            summary, e.g. changing the depth or enabled column types, especially\n            in jupyter notebooks.\n            warning: modifying the model architecture or input data/input size when\n            this feature is enabled does not invalidate the cache or re-run the\n            forward pass, and can cause incorrect summaries as a result.\n            default: false\n\n    col_names (iterable[str]):\n            specify which columns to show in the output. currently supported: (\n                \"input_size\",\n                \"output_size\",\n                \"num_params\",\n                \"params_percent\",\n                \"kernel_size\",\n                \"mult_adds\",\n                \"trainable\",\n            )\n            default: (\"output_size\", \"num_params\")\n            if input_data / input_size are not provided, only \"num_params\" is used.\n\n    col_width (int):\n            width of each column.\n            default: 25\n\n    depth (int):\n            depth of nested layers to display (e.g. sequentials).\n            nested layers below this depth will not be displayed in the summary.\n            default: 3\n\n    device (torch.device):\n            uses this torch device for model and input_data.\n            if not specified, uses the dtype of input_data if given, or the\n            parameters of the model. otherwise, uses the result of\n            torch.cuda.is_available().\n            default: none\n\n    dtypes (list[torch.dtype]):\n            if you use input_size, torchinfo assumes your input uses floattensors.\n            if your model use a different data type, specify that dtype.\n            for multiple inputs, specify the size of both inputs, and\n            also specify the types of each parameter here.\n            default: none\n\n    mode (str)\n            either \"train\" or \"eval\", which determines whether we call\n            model.train() or model.eval() before calling summary().\n            default: \"eval\".\n\n    row_settings (iterable[str]):\n            specify which features to show in a row. currently supported: (\n                \"ascii_only\",\n                \"depth\",\n                \"var_names\",\n            )\n            default: (\"depth\",)\n\n    verbose (int):\n            0 (quiet): no output\n            1 (default): print model summary\n            2 (verbose): show weight and bias layers in full detail\n            default: 1\n            if using a juypter notebook or google colab, the default is 0.\n\n    **kwargs:\n            other arguments used in `model.forward` function. passing *args is no\n            longer supported.\n\nreturn:\n    modelstatistics object\n            see torchinfo/model_statistics.py for more information.\n\"\"\"\n```\n\n# examples\n\n## get model summary as string\n\n```python\nfrom torchinfo import summary\n\nmodel_stats = summary(your_model, (1, 3, 28, 28), verbose=0)\nsummary_str = str(model_stats)\n# summary_str contains the string representation of the summary!\n```\n\n## explore different configurations\n\n```python\nclass lstmnet(nn.module):\n    def __init__(self, vocab_size=20, embed_dim=300, hidden_dim=512, num_layers=2):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.embedding = nn.embedding(vocab_size, embed_dim)\n        self.encoder = nn.lstm(embed_dim, hidden_dim, num_layers=num_layers, batch_first=true)\n        self.decoder = nn.linear(hidden_dim, vocab_size)\n\n    def forward(self, x):\n        embed = self.embedding(x)\n        out, hidden = self.encoder(embed)\n        out = self.decoder(out)\n        out = out.view(-1, out.size(2))\n        return out, hidden\n\nsummary(\n    lstmnet(),\n    (1, 100),\n    dtypes=[torch.long],\n    verbose=2,\n    col_width=16,\n    col_names=[\"kernel_size\", \"output_size\", \"num_params\", \"mult_adds\"],\n    row_settings=[\"var_names\"],\n)\n```\n\n```\n========================================================================================================================\nlayer (type (var_name))                  kernel shape         output shape         param #              mult-adds\n========================================================================================================================\nlstmnet (lstmnet)                        --                   [100, 20]            --                   --\n\u251c\u2500embedding (embedding)                  --                   [1, 100, 300]        6,000                6,000\n\u2502    \u2514\u2500weight                            [300, 20]                                 \u2514\u25006,000\n\u251c\u2500lstm (encoder)                         --                   [1, 100, 512]        3,768,320            376,832,000\n\u2502    \u2514\u2500weight_ih_l0                      [2048, 300]                               \u251c\u2500614,400\n\u2502    \u2514\u2500weight_hh_l0                      [2048, 512]                               \u251c\u25001,048,576\n\u2502    \u2514\u2500bias_ih_l0                        [2048]                                    \u251c\u25002,048\n\u2502    \u2514\u2500bias_hh_l0                        [2048]                                    \u251c\u25002,048\n\u2502    \u2514\u2500weight_ih_l1                      [2048, 512]                               \u251c\u25001,048,576\n\u2502    \u2514\u2500weight_hh_l1                      [2048, 512]                               \u251c\u25001,048,576\n\u2502    \u2514\u2500bias_ih_l1                        [2048]                                    \u251c\u25002,048\n\u2502    \u2514\u2500bias_hh_l1                        [2048]                                    \u2514\u25002,048\n\u251c\u2500linear (decoder)                       --                   [1, 100, 20]         10,260               10,260\n\u2502    \u2514\u2500weight                            [512, 20]                                 \u251c\u250010,240\n\u2502    \u2514\u2500bias                              [20]                                      \u2514\u250020\n========================================================================================================================\ntotal params: 3,784,580\ntrainable params: 3,784,580\nnon-trainable params: 0\ntotal mult-adds (m): 376.85\n========================================================================================================================\ninput size (mb): 0.00\nforward/backward pass size (mb): 0.67\nparams size (mb): 15.14\nestimated total size (mb): 15.80\n========================================================================================================================\n\n```\n\n<!-- lstm.out -->\n\n## resnet\n\n```python\nimport torchvision\n\nmodel = torchvision.models.resnet152()\nsummary(model, (1, 3, 224, 224), depth=3)\n```\n\n```\n==========================================================================================\nlayer (type:depth-idx)                   output shape              param #\n==========================================================================================\nresnet                                   [1, 1000]                 --\n\u251c\u2500conv2d: 1-1                            [1, 64, 112, 112]         9,408\n\u251c\u2500batchnorm2d: 1-2                       [1, 64, 112, 112]         128\n\u251c\u2500relu: 1-3                              [1, 64, 112, 112]         --\n\u251c\u2500maxpool2d: 1-4                         [1, 64, 56, 56]           --\n\u251c\u2500sequential: 1-5                        [1, 256, 56, 56]          --\n\u2502    \u2514\u2500bottleneck: 2-1                   [1, 256, 56, 56]          --\n\u2502    \u2502    \u2514\u2500conv2d: 3-1                  [1, 64, 56, 56]           4,096\n\u2502    \u2502    \u2514\u2500batchnorm2d: 3-2             [1, 64, 56, 56]           128\n\u2502    \u2502    \u2514\u2500relu: 3-3                    [1, 64, 56, 56]           --\n\u2502    \u2502    \u2514\u2500conv2d: 3-4                  [1, 64, 56, 56]           36,864\n\u2502    \u2502    \u2514\u2500batchnorm2d: 3-5             [1, 64, 56, 56]           128\n\u2502    \u2502    \u2514\u2500relu: 3-6                    [1, 64, 56, 56]           --\n\u2502    \u2502    \u2514\u2500conv2d: 3-7                  [1, 256, 56, 56]          16,384\n\u2502    \u2502    \u2514\u2500batchnorm2d: 3-8             [1, 256, 56, 56]          512\n\u2502    \u2502    \u2514\u2500sequential: 3-9              [1, 256, 56, 56]          16,896\n\u2502    \u2502    \u2514\u2500relu: 3-10                   [1, 256, 56, 56]          --\n\u2502    \u2514\u2500bottleneck: 2-2                   [1, 256, 56, 56]          --\n\n  ...\n  ...\n  ...\n\n\u251c\u2500adaptiveavgpool2d: 1-9                 [1, 2048, 1, 1]           --\n\u251c\u2500linear: 1-10                           [1, 1000]                 2,049,000\n==========================================================================================\ntotal params: 60,192,808\ntrainable params: 60,192,808\nnon-trainable params: 0\ntotal mult-adds (g): 11.51\n==========================================================================================\ninput size (mb): 0.60\nforward/backward pass size (mb): 360.87\nparams size (mb): 240.77\nestimated total size (mb): 602.25\n==========================================================================================\n```\n\n<!-- resnet152.out -->\n\n## multiple inputs w/ different data types\n\n```python\nclass multipleinputnetdifferentdtypes(nn.module):\n    def __init__(self):\n        super().__init__()\n        self.fc1a = nn.linear(300, 50)\n        self.fc1b = nn.linear(50, 10)\n\n        self.fc2a = nn.linear(300, 50)\n        self.fc2b = nn.linear(50, 10)\n\n    def forward(self, x1, x2):\n        x1 = f.relu(self.fc1a(x1))\n        x1 = self.fc1b(x1)\n        x2 = x2.type(torch.float)\n        x2 = f.relu(self.fc2a(x2))\n        x2 = self.fc2b(x2)\n        x = torch.cat((x1, x2), 0)\n        return f.log_softmax(x, dim=1)\n\nsummary(model, [(1, 300), (1, 300)], dtypes=[torch.float, torch.long])\n```\n\nalternatively, you can also pass in the input_data itself, and\ntorchinfo will automatically infer the data types.\n\n```python\ninput_data = torch.randn(1, 300)\nother_input_data = torch.randn(1, 300).long()\nmodel = multipleinputnetdifferentdtypes()\n\nsummary(model, input_data=[input_data, other_input_data, ...])\n```\n\n## sequentials & modulelists\n\n```python\nclass containermodule(nn.module):\n\n    def __init__(self):\n        super().__init__()\n        self._layers = nn.modulelist()\n        self._layers.append(nn.linear(5, 5))\n        self._layers.append(containerchildmodule())\n        self._layers.append(nn.linear(5, 5))\n\n    def forward(self, x):\n        for layer in self._layers:\n            x = layer(x)\n        return x\n\n\nclass containerchildmodule(nn.module):\n\n    def __init__(self):\n        super().__init__()\n        self._sequential = nn.sequential(nn.linear(5, 5), nn.linear(5, 5))\n        self._between = nn.linear(5, 5)\n\n    def forward(self, x):\n        out = self._sequential(x)\n        out = self._between(out)\n        for l in self._sequential:\n            out = l(out)\n\n        out = self._sequential(x)\n        for l in self._sequential:\n            out = l(out)\n        return out\n\nsummary(containermodule(), (1, 5))\n```\n\n```\n==========================================================================================\nlayer (type:depth-idx)                   output shape              param #\n==========================================================================================\ncontainermodule                          [1, 5]                    --\n\u251c\u2500modulelist: 1-1                        --                        --\n\u2502    \u2514\u2500linear: 2-1                       [1, 5]                    30\n\u2502    \u2514\u2500containerchildmodule: 2-2         [1, 5]                    --\n\u2502    \u2502    \u2514\u2500sequential: 3-1              [1, 5]                    --\n\u2502    \u2502    \u2502    \u2514\u2500linear: 4-1             [1, 5]                    30\n\u2502    \u2502    \u2502    \u2514\u2500linear: 4-2             [1, 5]                    30\n\u2502    \u2502    \u2514\u2500linear: 3-2                  [1, 5]                    30\n\u2502    \u2502    \u2514\u2500sequential: 3-3              --                        (recursive)\n\u2502    \u2502    \u2502    \u2514\u2500linear: 4-3             [1, 5]                    (recursive)\n\u2502    \u2502    \u2502    \u2514\u2500linear: 4-4             [1, 5]                    (recursive)\n\u2502    \u2502    \u2514\u2500sequential: 3-4              [1, 5]                    (recursive)\n\u2502    \u2502    \u2502    \u2514\u2500linear: 4-5             [1, 5]                    (recursive)\n\u2502    \u2502    \u2502    \u2514\u2500linear: 4-6             [1, 5]                    (recursive)\n\u2502    \u2502    \u2502    \u2514\u2500linear: 4-7             [1, 5]                    (recursive)\n\u2502    \u2502    \u2502    \u2514\u2500linear: 4-8             [1, 5]                    (recursive)\n\u2502    \u2514\u2500linear: 2-3                       [1, 5]                    30\n==========================================================================================\ntotal params: 150\ntrainable params: 150\nnon-trainable params: 0\ntotal mult-adds (m): 0.00\n==========================================================================================\ninput size (mb): 0.00\nforward/backward pass size (mb): 0.00\nparams size (mb): 0.00\nestimated total size (mb): 0.00\n==========================================================================================\n```\n\n<!-- container.out -->\n\n# contributing\n\nall issues and pull requests are much appreciated! if you are wondering how to build the project:\n\n- torchinfo is actively developed using the lastest version of python.\n  - changes should be backward compatible to python 3.7, and will follow python's end-of-life guidance for old versions.\n  - run `pip install -r requirements-dev.txt`. we use the latest versions of all dev packages.\n  - run `pre-commit install`.\n  - to use auto-formatting tools, use `pre-commit run -a`.\n  - to run unit tests, run `pytest`.\n  - to update the expected output files, run `pytest --overwrite`.\n  - to skip output file tests, use `pytest --no-output`\n\n# references\n\n- thanks to @sksq96, @nmhkahn, and @sangyx for providing the inspiration for this project.\n- for model size estimation @jacobkimmel ([details here](https://github.com/sksq96/pytorch-summary/pull/21))\n",
  "docs_url": null,
  "keywords": "torch pytorch torchsummary torch-summary summary keras deep-learning ml torchinfo torch-info visualize model statistics layer stats",
  "license": "mit",
  "name": "torchinfo",
  "package_url": "https://pypi.org/project/torchinfo/",
  "project_url": "https://pypi.org/project/torchinfo/",
  "project_urls": {
    "Homepage": "https://github.com/tyleryep/torchinfo"
  },
  "release_url": "https://pypi.org/project/torchinfo/1.8.0/",
  "requires_dist": [],
  "requires_python": ">=3.7",
  "summary": "model summary in pytorch, based off of the original torchsummary.",
  "version": "1.8.0",
  "releases": [],
  "developers": [
    "tyep@cs.stanford.edu",
    "tyler_yep"
  ],
  "kwds": "torchinfo torchvision torch pytorch torchsummary",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_torchinfo",
  "homepage": "https://github.com/tyleryep/torchinfo",
  "release_count": 32,
  "dependency_ids": [],
  "documentation_summary": "Torchinfo, available on PyPI, is a Python package that provides detailed summaries for PyTorch models, similar to TensorFlow's model.summary() API. It offers a comprehensive view of models, including layer names, input/output shapes, kernel shapes, parameter counts, and operations, aiding in debugging and optimizing neural networks. The package, compatible with PyTorch versions 1.4.0 and above, supports various features like RNNs, LSTMs, branching outputs, and customizable summary displays. It requires Python 3.7 or newer and is licensed under the MIT License. The latest version, 1.8.0, was released on May 14, 2023. Torchinfo is a rewritten version of the original torchsummary and torchsummaryX projects, addressing previous issues and introducing a new API.",
  "embedding": [
    -0.024348493665456772,
    0.0067976959981024265,
    0.008596880361437798,
    -0.023830439895391464,
    -0.002026707399636507,
    -0.0059506092220544815,
    0.0015550341922789812,
    -0.022696323692798615,
    -0.02213626727461815,
    -0.027078771963715553,
    0.03598368540406227,
    0.033491428941488266,
    -0.03475155681371689,
    0.01986803486943245,
    0.007322749588638544,
    -0.015233559533953667,
    0.008575878106057644,
    0.0004449830739758909,
    0.02373242937028408,
    -0.0007871430716477334,
    0.004091918934136629,
    0.008561876602470875,
    -0.027400804683566093,
    -0.0029718042351305485,
    -0.013518383726477623,
    0.0338834710419178,
    0.0075887772254645824,
    -0.03438752144575119,
    0.017851827666163445,
    0.0004532964085228741,
    0.024978557601571083,
    0.013427374884486198,
    -0.023046359419822693,
    -0.03279135748744011,
    -0.013595391996204853,
    -0.0011367413681000471,
    0.006356650963425636,
    -0.01413444709032774,
    0.03909200429916382,
    -0.006080122664570808,
    0.025090569630265236,
    0.011733201332390308,
    -0.013434375636279583,
    0.009906014427542686,
    -0.0026952759362757206,
    0.010060030035674572,
    0.01186621468514204,
    -0.027036767452955246,
    0.0050370157696306705,
    0.01904194988310337,
    0.008582878857851028,
    0.011271154507994652,
    -0.00220172549597919,
    0.0032990877516567707,
    -0.010361060500144958,
    -0.012293258681893349,
    0.02262631617486477,
    0.024334492161870003,
    -0.023130368441343307,
    -0.013126344420015812,
    0.03102717734873295,
    -0.0018569400999695063,
    -0.028982967138290405,
    -4.6379747800529e-05,
    0.006090623792260885,
    -0.03354743495583534,
    -0.0048129926435649395,
    0.018789924681186676,
    0.00018540960445534438,
    0.009787001647055149,
    0.02663072757422924,
    0.04600870981812477,
    -0.006570172496140003,
    -0.013945427723228931,
    0.0306631401181221,
    -0.0332954078912735,
    -0.026420705020427704,
    0.03371545299887657,
    -0.01972801983356476,
    -0.015121548436582088,
    0.007385756354779005,
    -0.010627088136970997,
    -0.004137423820793629,
    0.027736840769648552,
    0.0190279483795166,
    0.03542362526059151,
    -0.01565360277891159,
    0.02940301038324833,
    -0.012167246080935001,
    -0.018691914156079292,
    0.026266690343618393,
    -0.003519610268995166,
    0.002238479210063815,
    0.013854418881237507,
    0.00978000182658434,
    0.014995535835623741,
    -0.025580618530511856,
    0.04231233149766922,
    -0.013553387485444546,
    0.01218124758452177,
    -0.017655808478593826,
    0.0011568684130907059,
    -0.027022765949368477,
    -0.019826030358672142,
    -0.038671959191560745,
    -0.025972658768296242,
    0.004140924196690321,
    -0.006178132724016905,
    0.021072156727313995,
    0.03262333944439888,
    -0.01579361781477928,
    0.03363144397735596,
    -0.017361776903271675,
    -0.028002867475152016,
    0.0014473982155323029,
    0.026574721559882164,
    0.01745978742837906,
    0.003811890259385109,
    -0.0007442636997438967,
    -0.030103081837296486,
    0.010228047147393227,
    -0.0037488839589059353,
    0.008008820004761219,
    -0.017067747190594673,
    0.010865112766623497,
    -0.001576911425217986,
    -0.015443581156432629,
    0.004760487470775843,
    -0.026406703516840935,
    -0.006755691487342119,
    0.012776307761669159,
    -0.0063881538808345795,
    0.0031240698881447315,
    0.01631166972219944,
    -0.0005674955900758505,
    -0.02115616574883461,
    -0.011572184972465038,
    0.019952042028307915,
    -0.009976021014153957,
    -0.03181125596165657,
    0.02892696112394333,
    0.029935065656900406,
    -0.0034601043444126844,
    -0.013630395755171776,
    -0.032343313097953796,
    0.03757984936237335,
    0.0024502507876604795,
    0.0035441129002720118,
    -0.004613472148776054,
    -0.005936607718467712,
    0.023004354909062386,
    -0.032063283026218414,
    0.00034959829645231366,
    0.009969021193683147,
    0.021408192813396454,
    0.011201146990060806,
    0.002247230149805546,
    0.01918196491897106,
    0.009618984535336494,
    0.014099443331360817,
    0.013854418881237507,
    0.0365157388150692,
    -0.021086158230900764,
    0.002576263854280114,
    -0.003397097811102867,
    0.018313875421881676,
    0.01709575019776821,
    -0.026378700509667397,
    0.0008212715620175004,
    0.011817210353910923,
    0.030803153291344643,
    0.025118572637438774,
    -0.004487459547817707,
    -0.004322942811995745,
    0.010445069521665573,
    0.01316134724766016,
    0.021380189806222916,
    -0.010298054665327072,
    -0.013133345171809196,
    0.0027985365595668554,
    0.019798027351498604,
    0.011425170116126537,
    -0.013063337653875351,
    0.035759661346673965,
    0.007287746295332909,
    -0.018943939357995987,
    0.012895320542156696,
    0.01138316560536623,
    -0.019475994631648064,
    -0.02164621651172638,
    0.01904194988310337,
    0.020078055560588837,
    0.013329364359378815,
    -0.014162450097501278,
    -0.5721545815467834,
    -0.01026305090636015,
    0.004602971486747265,
    -0.02973904460668564,
    0.021562207490205765,
    -0.02019006758928299,
    -0.041724272072315216,
    -0.00743476115167141,
    -0.02635069750249386,
    0.026238687336444855,
    -0.01583562232553959,
    0.02661672607064247,
    0.0050615184009075165,
    -0.01477151270955801,
    -0.03332341089844704,
    -0.02663072757422924,
    -0.03637572377920151,
    -0.010669092647731304,
    0.0006541294860653579,
    0.0076307812705636024,
    -0.015065542422235012,
    0.037523843348026276,
    -0.02002204954624176,
    0.02212226577103138,
    -0.018047848716378212,
    0.00012152807175880298,
    -0.010228047147393227,
    -0.009219944477081299,
    0.027176782488822937,
    -0.007385756354779005,
    -0.02472653239965439,
    0.00236274185590446,
    0.016703709959983826,
    -0.00021877240214962512,
    0.045756686478853226,
    -0.03584367036819458,
    -0.01663370244204998,
    0.05684582144021988,
    0.008652886375784874,
    0.04732484742999077,
    -0.02920699119567871,
    -0.015065542422235012,
    0.008939915336668491,
    0.014302464202046394,
    -0.0055550686083734035,
    0.018159858882427216,
    -0.004655476659536362,
    0.023928450420498848,
    0.020610110834240913,
    -0.018621906638145447,
    -0.017627805471420288,
    -0.006748691201210022,
    -0.012727303430438042,
    -0.0026847748085856438,
    0.015205556526780128,
    0.0013126344420015812,
    -0.0003760697436518967,
    -2.3627419068361633e-05,
    0.010578082874417305,
    -0.027008766308426857,
    -0.00986400991678238,
    0.029094979166984558,
    0.0045049614273011684,
    -0.025272587314248085,
    -0.01429546345025301,
    0.02661672607064247,
    0.0008960917475633323,
    -0.008337853476405144,
    -0.011117137968540192,
    -0.031139187514781952,
    0.011614189483225346,
    0.010361060500144958,
    0.01379841286689043,
    0.007945813238620758,
    0.0031608236022293568,
    0.03875596821308136,
    0.032875366508960724,
    0.03547963127493858,
    0.001366014825180173,
    0.04732484742999077,
    0.016843724995851517,
    0.014092442579567432,
    0.0054325563833117485,
    -0.020456094294786453,
    0.013497382402420044,
    -0.005303042940795422,
    0.01403643749654293,
    0.004749986343085766,
    -0.00022555435134563595,
    0.008211840875446796,
    0.007609779015183449,
    -0.0074207596480846405,
    -0.012300259433686733,
    -0.0212961807847023,
    0.006913207937031984,
    0.024656524881720543,
    -0.038531944155693054,
    0.015359573066234589,
    -0.02373242937028408,
    -0.04186428710818291,
    -0.02454451285302639,
    0.00879290048032999,
    -0.013924425467848778,
    0.01827187091112137,
    0.007182735484093428,
    0.016339672729372978,
    -0.01692773401737213,
    -0.016437683254480362,
    -0.004431453533470631,
    -0.030411113053560257,
    -0.024600518867373466,
    -0.006104624830186367,
    -0.01645168475806713,
    0.011187145486474037,
    0.027680834755301476,
    -0.025888651609420776,
    0.04598070681095123,
    0.026756739243865013,
    0.0020547104068100452,
    0.012853316031396389,
    0.007518769707530737,
    0.004431453533470631,
    0.0137214045971632,
    0.003755884477868676,
    -0.0005876226932741702,
    0.02419447712600231,
    0.04542065039277077,
    0.00653866957873106,
    -0.04684879630804062,
    0.006577173247933388,
    0.0008033322519622743,
    -0.007350752595812082,
    0.009142936207354069,
    -0.023298384621739388,
    0.007210738491266966,
    0.009892012923955917,
    0.011033129878342152,
    0.013448377139866352,
    -0.01234226394444704,
    -0.01477151270955801,
    -0.00573008693754673,
    0.017165757715702057,
    -0.023648422211408615,
    -0.02779284492135048,
    -0.011152141727507114,
    -0.02166021801531315,
    0.01008803304284811,
    0.004431453533470631,
    0.0140504390001297,
    -0.006976214237511158,
    -0.025412602350115776,
    0.0027547820936888456,
    -0.026756739243865013,
    -0.008379857987165451,
    0.0012776307994499803,
    -0.0013730155769735575,
    -0.0046659777872264385,
    -0.0062166363932192326,
    -0.0070882258005440235,
    -0.01709575019776821,
    0.011096135713160038,
    0.014029436744749546,
    -0.001620665891095996,
    -0.0005285541410557926,
    -0.03265134245157242,
    -0.018649909645318985,
    -0.013168347999453545,
    0.029907062649726868,
    0.011257153004407883,
    -0.037691857665777206,
    -0.006503665819764137,
    -0.03069114312529564,
    0.0013467628741636872,
    0.018957940861582756,
    -0.0047079818323254585,
    0.008190838620066643,
    0.005131525453180075,
    0.0034461028408259153,
    -0.013518383726477623,
    -0.02051210030913353,
    -0.008540874347090721,
    0.017487790435552597,
    -0.041752275079488754,
    -0.02213626727461815,
    0.025622623041272163,
    0.035927679389715195,
    0.013616394251585007,
    -0.011565184220671654,
    -0.011306157335639,
    0.014645499177277088,
    -0.006314646452665329,
    -0.0006589424447156489,
    -0.02000804804265499,
    0.012783308513462543,
    -0.013063337653875351,
    0.003993908874690533,
    0.03477955982089043,
    -0.0006891330704092979,
    0.008386858738958836,
    0.009520974941551685,
    0.03469555079936981,
    0.05054517462849617,
    0.005362548865377903,
    -0.030607134103775024,
    -0.02164621651172638,
    -0.024740533903241158,
    0.04556066542863846,
    -0.006542169954627752,
    0.02796086296439171,
    -0.005992613732814789,
    0.03242731839418411,
    -0.02875894494354725,
    -0.04220031946897507,
    -0.018789924681186676,
    -0.004819993395358324,
    0.019980045035481453,
    0.002963053295388818,
    0.022080261260271072,
    -0.007686787284910679,
    -0.012923323549330235,
    0.0033218401949852705,
    -0.005919105838984251,
    0.01827187091112137,
    0.0033883468713611364,
    0.019475994631648064,
    -0.0009818505495786667,
    0.02230428345501423,
    0.015625599771738052,
    0.012090237811207771,
    -0.03038311004638672,
    0.009794002398848534,
    0.02114216424524784,
    0.005310043692588806,
    -0.01647968776524067,
    0.02537059783935547,
    0.009457968175411224,
    0.03595568239688873,
    -0.02406846359372139,
    0.020442092791199684,
    -0.0066681825555861,
    -0.009296951815485954,
    0.015233559533953667,
    0.02097414806485176,
    -0.029655035585165024,
    0.03340741991996765,
    0.007147731725126505,
    0.014351469464600086,
    -0.009198942221701145,
    -0.008603881113231182,
    -0.026420705020427704,
    0.012062234804034233,
    0.02937500737607479,
    0.012531283311545849,
    -0.005173529498279095,
    0.019433990120887756,
    -0.010858112014830112,
    -0.017011741176247597,
    -0.005677581299096346,
    0.021870238706469536,
    0.007469764910638332,
    0.0024169974494725466,
    -0.0011507428716868162,
    0.033659446984529495,
    0.00017479914822615683,
    0.02601466327905655,
    -0.019167963415384293,
    -0.004060415551066399,
    -0.03049512207508087,
    -0.022052258253097534,
    0.01097012311220169,
    -0.029655035585165024,
    -0.006594675127416849,
    -0.0020127061288803816,
    -0.02472653239965439,
    0.02098814956843853,
    0.0187339186668396,
    0.03570365533232689,
    0.020764125511050224,
    0.010718096978962421,
    0.008736894465982914,
    -0.01984003186225891,
    -0.015877624973654747,
    0.027358802035450935,
    0.0026270190719515085,
    0.015317568555474281,
    0.005964610725641251,
    -0.027204785495996475,
    0.011040130630135536,
    0.013882421888411045,
    -0.011600187979638577,
    -0.001611915067769587,
    -0.0076307812705636024,
    -0.002317237202078104,
    -0.00887690857052803,
    -0.013609393499791622,
    -0.0005180530715733767,
    0.029010970145463943,
    -0.039960090070962906,
    -0.03245532140135765,
    0.0018481892766430974,
    0.00044520184746943414,
    -0.016857726499438286,
    -0.007679786533117294,
    0.010228047147393227,
    0.02842291072010994,
    0.0004909252747893333,
    -0.008883909322321415,
    -0.01519155502319336,
    -0.01300033088773489,
    -0.008190838620066643,
    0.005786092486232519,
    -0.019896037876605988,
    0.0008667762740515172,
    -0.007693787571042776,
    0.012258254922926426,
    0.005810595117509365,
    0.017851827666163445,
    0.006006615236401558,
    0.028338901698589325,
    0.018117854371666908,
    0.007861805148422718,
    -0.03136321157217026,
    -0.022682322189211845,
    -0.0020319579634815454,
    0.03049512207508087,
    0.042704373598098755,
    -0.03629171475768089,
    0.005632076878100634,
    -0.026238687336444855,
    -0.03573165833950043,
    -0.009415963664650917,
    0.029963068664073944,
    0.022892344743013382,
    -0.0030628135427832603,
    -0.00017086123989429325,
    0.004039413761347532,
    0.0025342595763504505,
    0.016549695283174515,
    0.0017064247513189912,
    0.0010921118082478642,
    -0.0012898821150884032,
    0.006951711606234312,
    0.01584962196648121,
    0.019672013819217682,
    -0.027316797524690628,
    0.00863188412040472,
    0.0031013174448162317,
    0.011516178958117962,
    0.03729981929063797,
    0.0023592414800077677,
    0.009317954070866108,
    0.034947577863931656,
    -0.009226944297552109,
    -0.01888793334364891,
    -0.009723995812237263,
    0.008540874347090721,
    0.006909707561135292,
    0.01146717369556427,
    -0.06171831861138344,
    0.036907777190208435,
    0.018439888954162598,
    0.011691196821630001,
    -0.006713687442243099,
    0.011817210353910923,
    -0.00197420222684741,
    0.008778898976743221,
    0.009955019690096378,
    0.009303952567279339,
    0.00791081041097641,
    -0.027246790006756783,
    -0.013917424716055393,
    0.006006615236401558,
    -0.015723610296845436,
    0.004921503830701113,
    0.024488506838679314,
    0.013329364359378815,
    -0.043488454073667526,
    -0.03441552445292473,
    0.017627805471420288,
    -0.00493200495839119,
    -0.021744227036833763,
    -0.012692299671471119,
    0.016171656548976898,
    -0.01695573702454567,
    -0.003860895289108157,
    -0.023844441398978233,
    0.018355879932641983,
    -0.009373960085213184,
    0.005775591358542442,
    -0.006500165443867445,
    -0.006335648708045483,
    0.033211398869752884,
    -0.02472653239965439,
    0.0043754479847848415,
    -0.0027862852439284325,
    -0.025300590321421623,
    -0.008435864001512527,
    -0.03794388473033905,
    0.04444054886698723,
    0.006412656512111425,
    -0.024656524881720543,
    -0.0031940769404172897,
    0.009002922102808952,
    -0.009801003150641918,
    0.005058018025010824,
    -0.015247561037540436,
    -0.03158723562955856,
    -0.03620770573616028,
    0.0068221986293792725,
    -0.0009310953319072723,
    -0.004378948360681534,
    -0.020750124007463455,
    0.00613962858915329,
    0.041444242000579834,
    0.014302464202046394,
    0.025566617026925087,
    0.0037383828312158585,
    -0.004644975531846285,
    0.004911002703011036,
    0.009079929441213608,
    -0.00872989371418953,
    0.012790309265255928,
    0.02275232970714569,
    -0.01793583668768406,
    0.027890855446457863,
    -0.02080613002181053,
    -0.007476765662431717,
    -0.008239843882620335,
    0.013364368118345737,
    -0.0021702221129089594,
    0.00231898739002645,
    0.005506063811480999,
    -0.0044419546611607075,
    0.004897001199424267,
    0.02665872871875763,
    -0.015065542422235012,
    -0.0026620225980877876,
    0.01745978742837906,
    -0.02069411799311638,
    -0.022724326699972153,
    -0.011698197573423386,
    0.01227925717830658,
    -0.0036018688697367907,
    -0.01519155502319336,
    0.006755691487342119,
    -0.03441552445292473,
    0.0024029959458857775,
    0.019574003294110298,
    0.00839385949075222,
    0.01856590062379837,
    0.0005954984808340669,
    -0.02049809880554676,
    -0.01923796907067299,
    0.010200044140219688,
    0.027526818215847015,
    0.0031013174448162317,
    -0.010361060500144958,
    -0.023802436888217926,
    -0.0027372802142053843,
    0.021730225533246994,
    -0.03685177490115166,
    0.009156937710940838,
    -0.0037488839589059353,
    -0.021870238706469536,
    -0.0035266110207885504,
    -0.0039659058675169945,
    0.0014150198549032211,
    0.013140344992280006,
    0.0016022890340536833,
    -0.025062566623091698,
    -0.025580618530511856,
    0.002191224368289113,
    0.02856292389333248,
    0.0253565963357687,
    0.019125958904623985,
    0.011411168612539768,
    -0.031895264983177185,
    -0.024740533903241158,
    -0.0135743897408247,
    -0.026406703516840935,
    -0.020442092791199684,
    0.008540874347090721,
    0.0025360097642987967,
    0.025748636573553085,
    0.027246790006756783,
    -0.004939005710184574,
    0.01050107553601265,
    -0.0020249574445188046,
    -0.016507690772414207,
    0.008603881113231182,
    0.012083237059414387,
    -0.00617113197222352,
    -0.026700733229517937,
    -0.0406881645321846,
    0.009324954822659492,
    0.002942051272839308,
    -0.013035334646701813,
    -0.0190279483795166,
    -0.010074031539261341,
    -0.00585959991440177,
    0.004473458044230938,
    -0.024040462449193,
    -0.016185658052563667,
    -0.005901604425162077,
    0.003199327504262328,
    -0.009219944477081299,
    -0.009247946552932262,
    0.03169924393296242,
    -0.033015381544828415,
    0.0007565149571746588,
    0.027708837762475014,
    -0.007679786533117294,
    0.0297110415995121,
    0.036739762872457504,
    -0.023508407175540924,
    0.005177029874175787,
    0.004763987846672535,
    0.014981534332036972,
    0.021716224029660225,
    -0.0033235903829336166,
    -0.005096521694213152,
    -0.028142880648374557,
    -0.022808335721492767,
    0.005807094741612673,
    -0.007322749588638544,
    0.011586186476051807,
    0.011243151500821114,
    0.021870238706469536,
    0.005100022070109844,
    0.014449479058384895,
    0.005583071615546942,
    -0.0016749214846640825,
    0.019125958904623985,
    -0.00887690857052803,
    -0.01888793334364891,
    -0.01453348807990551,
    -0.02180023305118084,
    -0.023270383477211,
    0.017207762226462364,
    -0.01661970093846321,
    0.011355162598192692,
    0.0093669593334198,
    -0.0009450967772863805,
    -0.016675706952810287,
    -0.00010697970719775185,
    0.017319774255156517,
    0.036095697432756424,
    0.04404851049184799,
    0.021576208993792534,
    0.015317568555474281,
    0.0020722122862935066,
    -0.019910039380192757,
    -0.011684196069836617,
    -0.007798798382282257,
    -0.006605176255106926,
    0.023942451924085617,
    0.012391269207000732,
    -0.01088611502200365,
    -0.01413444709032774,
    0.032203298062086105,
    -0.00593310734257102,
    -0.035451628267765045,
    -0.03886798024177551,
    0.022038256749510765,
    0.02776484377682209,
    0.01533157005906105,
    -0.05558568984270096,
    0.001009853440336883,
    0.0022209773305803537,
    0.001100862747989595,
    -0.02146419696509838,
    -0.011187145486474037,
    0.0075257704593241215,
    -0.008722892962396145,
    0.0026917755603790283,
    0.04113621264696121,
    0.005656579043716192,
    -0.0035563642159104347,
    0.007280745543539524,
    -0.021520202979445457,
    -0.00961198378354311,
    -0.03021509386599064,
    -0.026378700509667397,
    -0.003094316925853491,
    0.014981534332036972,
    0.0355636402964592,
    -0.0015602847561240196,
    0.0028825451154261827,
    0.02906697615981102,
    -0.015485585667192936,
    -0.030467119067907333,
    -0.0016766716726124287,
    -0.004434953909367323,
    0.04620473086833954,
    -0.0371037982404232,
    -0.017053745687007904,
    -0.04052015021443367,
    -0.02842291072010994,
    0.010683094151318073,
    -0.011593187227845192,
    -0.00215797103010118,
    -0.018761921674013138,
    0.005646077916026115,
    0.009499972686171532,
    -0.005695083178579807,
    -0.01309134066104889,
    0.0014850270235911012,
    -0.003755884477868676,
    0.016213659197092056,
    0.008323851972818375,
    -0.01542957965284586,
    0.00301030813716352,
    -0.008169836364686489,
    -0.029459016397595406,
    -0.002224477706477046,
    -0.02068011835217476,
    -0.016507690772414207,
    0.017837826162576675,
    0.01282531302422285,
    0.006293644197285175,
    -0.0054745604284107685,
    0.05947808921337128,
    -0.03049512207508087,
    -0.00994101818650961,
    -0.006269142031669617,
    0.026574721559882164,
    -0.037355825304985046,
    -0.016563696786761284,
    0.007448762655258179,
    -0.003734882455319166,
    0.016829723492264748,
    -0.02341039665043354,
    -0.03741183131933212,
    0.020400088280439377,
    0.011922220699489117,
    0.027904856950044632,
    0.009962020441889763,
    -0.026742737740278244,
    -0.0031573232263326645,
    0.02259831316769123,
    -0.013945427723228931,
    -0.01235626544803381,
    -0.008757896721363068,
    0.021100159734487534,
    -0.02037208527326584,
    -0.03167124465107918,
    -0.00549206230789423,
    0.023970454931259155,
    0.028520919382572174,
    -0.023018356412649155,
    -0.00217897305265069,
    0.023130368441343307,
    -0.05415754392743111,
    -0.015499587170779705,
    -0.022164270281791687,
    -0.016437683254480362,
    -0.02212226577103138,
    0.04038013517856598,
    0.00028877955628558993,
    -0.02131018228828907,
    -0.020316079258918762,
    0.037691857665777206,
    -0.014379472471773624,
    -0.02259831316769123,
    -0.00855487585067749,
    0.030607134103775024,
    0.012769307009875774,
    0.01379841286689043,
    -0.017235765233635902,
    -0.048024918884038925,
    -0.013833416625857353,
    0.012258254922926426,
    -0.04631674289703369,
    -0.014589494094252586,
    -0.010991125367581844,
    0.05947808921337128,
    0.015695607289671898,
    -0.0003994783910457045,
    -0.017627805471420288,
    -0.012790309265255928,
    -0.032035280019044876,
    -0.01600363850593567,
    -0.03570365533232689,
    0.03500358387827873,
    -0.015709608793258667,
    0.0169417355209589,
    0.0091289347037673,
    0.018047848716378212,
    0.005044016521424055,
    0.030579131096601486,
    0.0040429141372442245,
    0.0069412109442055225,
    0.006752191577106714,
    0.01325235702097416,
    0.013392371125519276,
    -0.02437649667263031,
    -0.026868751272559166,
    -0.013084339909255505,
    -0.005922606214880943,
    0.0011201146990060806,
    -0.012146243825554848,
    0.015261562541127205,
    -0.0017633055103942752,
    -0.03239931911230087,
    -0.0015401577111333609,
    0.002529009012505412,
    -0.016675706952810287,
    -0.0023837441112846136,
    0.012405269779264927,
    -0.0139594292268157,
    -0.023480404168367386,
    0.015037539415061474,
    0.016171656548976898,
    -0.007224739529192448,
    -0.01632567122578621,
    0.012790309265255928,
    0.007004217244684696,
    -0.01097012311220169,
    -0.013917424716055393,
    -0.019419988617300987,
    -0.01968601532280445,
    -0.004735984839498997,
    0.009058927185833454,
    0.021562207490205765,
    -0.00994101818650961,
    0.03662775084376335,
    -0.01904194988310337,
    -0.010242048650979996,
    -0.028044871985912323,
    0.024320490658283234,
    -0.017361776903271675,
    0.004137423820793629,
    0.041080206632614136,
    -0.008904911577701569,
    0.0035546140279620886,
    0.011194146238267422,
    0.027820847928524017,
    0.0009906013729050756,
    -0.004343944601714611,
    0.007147731725126505,
    -0.020862136036157608,
    -0.04211631417274475,
    0.014393473975360394,
    0.005541067570447922,
    -0.003558114403858781,
    0.004060415551066399,
    0.0028597929049283266,
    -0.01010203454643488,
    0.008673887699842453,
    -0.01808985136449337,
    -0.00992701668292284,
    -0.047884903848171234,
    0.002537759952247143,
    0.0065071661956608295,
    0.0004062603402417153,
    -0.029795050621032715,
    0.00018179987091571093,
    -0.00903092510998249,
    -0.0274428091943264,
    -0.014645499177277088,
    0.1849309355020523,
    -0.003951904829591513,
    0.016437683254480362,
    0.013686401769518852,
    -0.007532771211117506,
    0.003197577316313982,
    0.009149936959147453,
    0.005992613732814789,
    -0.0043754479847848415,
    0.022696323692798615,
    0.006181633099913597,
    -0.022024255245923996,
    0.005044016521424055,
    0.005856099538505077,
    0.00199520424939692,
    0.005334546323865652,
    -0.032203298062086105,
    -0.03791588172316551,
    -0.03631971776485443,
    0.01985403336584568,
    -0.04936905577778816,
    -0.0032605838496237993,
    -0.01696973666548729,
    -0.02759682573378086,
    0.010375062003731728,
    -0.01970001682639122,
    -0.02406846359372139,
    -0.0025885149370878935,
    0.037859875708818436,
    0.004648475907742977,
    -0.023802436888217926,
    0.016437683254480362,
    0.0077497935853898525,
    -0.0019426989601925015,
    -0.04407651349902153,
    -0.02068011835217476,
    0.002016206504777074,
    -0.011348161846399307,
    0.0200500525534153,
    0.00537304999306798,
    0.012286257930099964,
    -0.00905192643404007,
    0.019644010812044144,
    -0.05454958602786064,
    0.004641475155949593,
    -0.013343365862965584,
    -0.022892344743013382,
    0.018971942365169525,
    0.00702521950006485,
    0.010403065010905266,
    -0.030103081837296486,
    -0.015709608793258667,
    0.03469555079936981,
    0.005576070863753557,
    0.004200430121272802,
    -0.003234331263229251,
    0.0012119991006329656,
    -0.00549206230789423,
    0.014981534332036972,
    -0.004760487470775843,
    -0.02402646094560623,
    0.024292487651109695,
    -0.02210826426744461,
    0.023536410182714462,
    -0.0393720306456089,
    0.025972658768296242,
    0.004102420061826706,
    0.00896791834384203,
    -0.000949472188949585,
    0.007532771211117506,
    -0.023536410182714462,
    -0.0075257704593241215,
    -0.025762638077139854,
    -0.02566462755203247,
    -0.036431729793548584,
    -0.014561491087079048,
    0.037523843348026276,
    0.014316465705633163,
    0.007567774970084429,
    0.007847803644835949,
    -0.0017204261384904385,
    -0.0004961757804267108,
    -0.001180495833978057,
    0.03119519352912903,
    -0.0009170938865281641,
    -0.012048233300447464,
    0.005919105838984251,
    -0.0401001051068306,
    0.004133923444896936,
    -0.0021352185867726803,
    -0.021268177777528763,
    -0.0135743897408247,
    0.002408246509730816,
    0.01179620809853077,
    0.0033323410898447037,
    0.0203440822660923,
    -0.004280938301235437,
    0.027568822726607323,
    -0.01104713138192892,
    0.011831211857497692,
    -0.017949838191270828,
    0.03391147404909134,
    0.023172372952103615,
    -0.017221763730049133,
    -0.01041006576269865,
    0.001009853440336883,
    -0.004067416302859783,
    0.016045643016695976,
    0.0016749214846640825,
    -0.022556308656930923,
    -0.01759980246424675,
    -0.025636624544858932,
    -0.012412270531058311,
    0.02355041168630123,
    -0.0071267299354076385,
    -0.0024467504117637873,
    -0.010228047147393227,
    0.014477482065558434,
    0.012566287070512772,
    0.011656193062663078,
    0.021198170259594917,
    -0.023634420707821846,
    0.0016084146918728948,
    -0.017473788931965828,
    -0.04001609608530998,
    -0.024642523378133774,
    -0.0013275109231472015,
    0.04413251951336861,
    -3.6753765016328543e-05,
    0.00839385949075222,
    0.03421950340270996,
    -0.029178988188505173,
    0.0387839712202549,
    0.004805991891771555,
    -0.0031223197001963854,
    -0.010942120105028152,
    -0.035759661346673965,
    0.0074207596480846405,
    -0.0027530319057404995,
    0.01477151270955801,
    -0.015373574569821358,
    -0.023970454931259155,
    -0.003519610268995166,
    -0.014981534332036972,
    -0.012328262440860271,
    -0.016045643016695976,
    0.012832313776016235,
    0.0004021036729682237,
    0.047072820365428925,
    -0.01905595138669014,
    -0.0069447108544409275,
    -0.013553387485444546,
    -0.006062620785087347,
    -0.0015550341922789812,
    0.034807562828063965,
    -0.0009415964013896883,
    -0.0015865374589338899,
    -0.006342649459838867,
    -0.0047044819220900536,
    0.003997409250587225,
    -0.021380189806222916,
    -0.010921117849647999,
    -0.016857726499438286,
    0.013056336902081966,
    -0.03587167337536812,
    -0.024292487651109695,
    -0.1756339818239212,
    0.03346342593431473,
    0.03491957485675812,
    -0.03968006372451782,
    0.035283613950014114,
    0.023634420707821846,
    0.014463480561971664,
    -0.0020949644967913628,
    0.013847418129444122,
    0.008099829778075218,
    0.025076568126678467,
    -0.00831685122102499,
    -0.011838211677968502,
    -0.025888651609420776,
    -0.01678771898150444,
    -0.0048514967784285545,
    0.019139960408210754,
    0.033211398869752884,
    0.0397360697388649,
    0.02182823419570923,
    0.03298737853765488,
    -0.02276633121073246,
    0.020288078114390373,
    0.03603969141840935,
    -0.0008838404901325703,
    -0.020274076610803604,
    0.00798081699758768,
    0.018285872414708138,
    0.016101649031043053,
    -0.007847803644835949,
    -0.013924425467848778,
    -0.04166826605796814,
    -0.014575492590665817,
    -0.001806184882298112,
    0.017795821651816368,
    0.00039488417678512633,
    0.028982967138290405,
    -0.029290998354554176,
    -0.011152141727507114,
    0.0348355658352375,
    0.02243029698729515,
    0.007301747798919678,
    0.010767102241516113,
    -0.01106113288551569,
    -0.005173529498279095,
    -0.010018025524914265,
    0.0014080192195251584,
    -0.02341039665043354,
    0.00479899113997817,
    -0.026098672300577164,
    0.04183628410100937,
    -0.033799462020397186,
    0.00887690857052803,
    -0.013441376388072968,
    -0.004711482208222151,
    0.017991842702031136,
    0.009871010668575764,
    -0.002339989645406604,
    0.008148834109306335,
    -0.003143321955576539,
    -0.024460503831505775,
    -0.027708837762475014,
    0.002387244487181306,
    0.009058927185833454,
    -0.00165654462762177,
    -0.004098919685930014,
    -0.022640317678451538,
    -0.0012715051416307688,
    3.161261338391341e-05,
    -0.013189350254833698,
    0.01179620809853077,
    -0.0006077497382648289,
    0.004364946857094765,
    0.008036823011934757,
    0.02940301038324833,
    0.015401576645672321,
    -0.017529794946312904,
    -0.018369881436228752,
    -0.011551182717084885,
    -0.007046221289783716,
    -0.0116351917386055,
    0.03385546803474426,
    -0.009667989797890186,
    -0.003531861584633589,
    0.006990215741097927,
    0.021282179281115532,
    0.0133363651111722,
    0.005131525453180075,
    -0.03455553948879242,
    -0.0008392109302803874,
    0.006654181517660618,
    -0.004186428617686033,
    -0.002896546619012952,
    -0.001390517340041697,
    0.02937500737607479,
    0.019798027351498604,
    0.009983021765947342,
    -0.0011551182251423597,
    0.021184168756008148,
    -0.0200500525534153,
    -0.024180475622415543,
    0.028002867475152016,
    -0.019574003294110298,
    0.020428091287612915,
    0.01761380396783352,
    -0.005863100290298462,
    -0.03990408405661583,
    0.009142936207354069,
    0.029431013390421867,
    0.0011069882893934846,
    -0.012622292153537273,
    -0.002607767004519701,
    0.010788104496896267,
    0.028478916734457016,
    -0.0332954078912735,
    0.041108209639787674,
    -0.023158371448516846,
    -0.008288849145174026,
    0.01583562232553959,
    -0.0038328925147652626,
    0.036403726786375046,
    -0.007413758896291256,
    0.0040429141372442245,
    0.023466402664780617,
    -0.014183452352881432,
    -0.031111186370253563,
    -0.08647285401821136,
    -0.05541767552495003,
    0.021100159734487534,
    0.010718096978962421,
    0.005362548865377903,
    -0.006846701260656118,
    -0.0040289126336574554,
    0.012965327128767967,
    -0.013581390492618084,
    0.006244639400392771,
    -0.027358802035450935,
    0.011278154328465462,
    -0.01082310825586319,
    0.027372803539037704,
    0.006605176255106926,
    -0.004245934542268515,
    -0.039792075753211975,
    0.023620419204235077,
    0.0033410920295864344,
    0.009464968927204609,
    -0.01757179945707321,
    -0.014071440324187279,
    0.007329750340431929,
    0.01244027353823185,
    -0.03144722059369087,
    0.012748305685818195,
    -0.02712077647447586,
    0.03004707582294941,
    0.003871396416798234,
    0.014603495597839355,
    0.020134061574935913,
    -0.0005053642671555281,
    -0.0004333881370257586,
    0.0030295602045953274,
    0.004805991891771555,
    0.015513588674366474,
    -0.0190279483795166,
    -0.03945603966712952,
    0.030551128089427948,
    -0.0064511606469750404,
    0.0070497216656804085,
    0.004385949112474918,
    -0.0159896370023489,
    0.005985612981021404,
    -0.01696973666548729,
    -0.0020809629932045937,
    -0.023774433881044388,
    0.018831927329301834,
    0.027680834755301476,
    -0.023956453427672386,
    0.016255663707852364,
    8.080905899987556e-06,
    0.01058508362621069,
    -0.0014307715464383364,
    0.025748636573553085,
    0.019097955897450447,
    0.012937325052917004,
    -0.006598175503313541,
    0.010865112766623497,
    0.034807562828063965,
    -0.007798798382282257,
    0.022864341735839844,
    -0.0035528638400137424,
    0.01951799914240837,
    0.004298440180718899,
    -0.011936222203075886,
    0.011117137968540192,
    0.0037278817035257816,
    0.027484813705086708,
    -0.027876853942871094,
    -0.014204454608261585,
    0.0022279780823737383,
    -0.016507690772414207,
    0.018789924681186676,
    -0.011068133637309074,
    -0.027246790006756783,
    -0.011299156583845615,
    -0.022388292476534843,
    -0.015779616311192513,
    -0.00759577751159668,
    -0.02230428345501423,
    0.016101649031043053,
    -0.0116351917386055,
    -0.0009555978467687964,
    0.0020914641208946705,
    0.007532771211117506,
    -0.00034150370629504323,
    -0.016087647527456284,
    0.02355041168630123,
    -0.04505661502480507,
    -0.0034461028408259153,
    0.004417452495545149,
    0.04217231646180153,
    0.00879290048032999,
    -0.008183837868273258,
    -0.030075078830122948,
    0.008134832605719566,
    -0.004140924196690321,
    -0.015177554450929165,
    -0.00013399809540715069,
    -0.0397360697388649,
    -0.01282531302422285,
    -0.07723190635442734,
    0.04701681435108185,
    0.003993908874690533,
    -0.03875596821308136,
    0.015751613304018974,
    -0.004182928241789341,
    -0.0014920277753844857,
    -0.025244584307074547,
    0.022542309015989304,
    0.01453348807990551,
    -0.0301870908588171,
    0.0011743702925741673,
    0.008974919095635414,
    0.0035406125243753195,
    -0.010438068769872189,
    -0.01743178442120552,
    0.02322837896645069,
    -0.03214729204773903,
    0.05068518966436386,
    0.0012111240066587925,
    0.016507690772414207,
    0.005303042940795422,
    0.0384199321269989,
    -0.0012321261456236243,
    -0.0164656862616539,
    -0.032847363501787186,
    -0.0008440239471383393,
    0.016437683254480362,
    0.00815583486109972,
    -0.032231301069259644,
    0.004074417054653168,
    0.009632986038923264,
    0.006073121912777424,
    0.024348493665456772,
    0.008533873595297337,
    0.017823824658989906,
    0.012076236307621002,
    -0.00585959991440177,
    0.018299873918294907,
    -0.001946199219673872,
    -0.017683811485767365,
    -0.03477955982089043,
    0.030243096873164177,
    0.007973816245794296,
    -0.010613086633384228,
    -0.006437159143388271,
    -0.01008803304284811,
    0.024740533903241158,
    0.012447274290025234,
    0.015261562541127205,
    0.011579185724258423,
    0.016521692276000977,
    -0.01428146194666624,
    -0.0076447827741503716,
    0.009794002398848534,
    0.0023347390815615654,
    0.030439116060733795,
    0.007546772714704275,
    -0.009058927185833454,
    -0.006654181517660618,
    0.04099619761109352,
    0.015387575142085552,
    0.0048514967784285545,
    -0.014365470968186855,
    0.007455763407051563,
    -0.029094979166984558,
    -0.06188633665442467,
    -0.0006458161515183747,
    -0.01074609998613596,
    -0.002003955189138651,
    0.015247561037540436,
    -0.016339672729372978,
    0.006188633851706982,
    0.012951326556503773,
    0.025118572637438774,
    0.018313875421881676,
    0.01745978742837906,
    -0.010795105248689651,
    -0.0032395815942436457,
    0.04743685573339462,
    0.02002204954624176,
    -0.02903897315263748,
    -0.02275232970714569,
    0.007700788322836161,
    0.04346045106649399,
    0.003755884477868676,
    -0.03455553948879242,
    0.020764125511050224,
    0.004161925986409187,
    -0.004753486718982458,
    -0.010529078543186188,
    0.016563696786761284,
    0.003300837939605117,
    0.014302464202046394,
    0.015485585667192936,
    0.0019654512871056795,
    -0.0025990160647779703,
    -0.019602006301283836,
    0.021842235699295998,
    0.01421145536005497,
    -0.019279973581433296,
    0.011166143231093884,
    0.00013935801689513028,
    0.005793093238025904,
    -0.012146243825554848,
    -0.003351593157276511,
    0.017403781414031982,
    -0.013245356269180775,
    0.023508407175540924,
    0.014183452352881432,
    0.003605369245633483,
    -0.032539330422878265,
    -0.0007578275981359184,
    0.01790783368051052,
    -0.018117854371666908,
    0.028352903202176094,
    0.007399757858365774,
    0.0030908165499567986,
    -0.015765614807605743,
    -0.023046359419822693,
    0.01291632279753685,
    -0.02051210030913353,
    0.0332954078912735,
    -0.0025797642301768064,
    0.037523843348026276,
    0.022836338728666306,
    0.029010970145463943,
    -0.02244429849088192,
    0.01709575019776821,
    0.0038083898834884167,
    -0.01741778291761875,
    -0.0024730032309889793,
    -0.022206272929906845,
    -0.024768536910414696,
    -0.025314591825008392,
    -0.012153244577348232,
    0.004459456540644169,
    -0.0067171878181397915,
    -0.010536078363656998,
    0.07829601317644119,
    -0.0006165006197988987,
    0.0028475415892899036,
    -0.007441761903464794,
    -0.012048233300447464,
    0.006167631596326828,
    0.00031897015287540853,
    -0.01970001682639122,
    -0.022668320685625076,
    -0.011124138720333576,
    -0.018803926184773445,
    -0.017165757715702057,
    -0.02695276029407978,
    -0.005856099538505077,
    -0.029347004368901253,
    -0.01179620809853077,
    -0.016381677240133286,
    0.001449148403480649,
    0.0019199466332793236,
    -0.009415963664650917,
    0.017193760722875595,
    0.009639986790716648,
    0.0005806219414807856,
    -0.003717380575835705,
    -0.023060360923409462,
    -0.002189474180340767,
    0.022514306008815765,
    -0.00039466540329158306,
    -0.008379857987165451,
    -0.05412954092025757,
    0.012720302678644657,
    0.016871728003025055,
    -0.012293258681893349,
    -0.002851041965186596,
    0.0025675129145383835,
    0.007518769707530737,
    -0.021534204483032227,
    0.0044804587960243225,
    0.012615291401743889,
    0.02097414806485176,
    0.003927402198314667,
    -0.011810209602117538,
    -0.04122022166848183,
    -0.021408192813396454,
    -0.0059506092220544815,
    -0.0033655946608632803,
    0.011915219947695732,
    -0.018495893105864525,
    -0.040436141192913055
  ]
}