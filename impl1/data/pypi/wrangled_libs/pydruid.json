{
  "classifiers": [
    "license :: osi approved :: apache software license",
    "programming language :: python",
    "programming language :: python :: 3",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8"
  ],
  "description": "# pydruid\n\npydruid exposes a simple api to create, execute, and analyze [druid](http://druid.io/) queries. pydruid can parse query results into [pandas](http://pandas.pydata.org/) dataframe objects for subsequent data analysis -- this offers a tight integration between [druid](http://druid.io/), the [scipy](http://www.scipy.org/stackspec.html) stack (for scientific computing) and [scikit-learn](http://scikit-learn.org/stable/) (for machine learning). pydruid can export query results into tsv or json for further processing with your favorite tool, e.g., r, julia, matlab, excel. it provides both synchronous and asynchronous clients.\n\nadditionally, pydruid implements the [python db api 2.0](https://www.python.org/dev/peps/pep-0249/), a [sqlalchemy dialect](http://docs.sqlalchemy.org/en/latest/dialects/), and a provides a command line interface to interact with druid.\n\nto install:\n```python\npip install pydruid\n# or, if you intend to use asynchronous client\npip install pydruid[async]\n# or, if you intend to export query results into pandas\npip install pydruid[pandas]\n# or, if you intend to do both\npip install pydruid[async, pandas]\n# or, if you want to use the sqlalchemy engine\npip install pydruid[sqlalchemy]\n# or, if you want to use the cli\npip install pydruid[cli]\n```\ndocumentation: https://pythonhosted.org/pydruid/.\n\n# examples\n\nthe following exampes show how to execute and analyze the results of three types of queries: timeseries, topn, and groupby. we will use these queries to ask simple questions about twitter's public data set.\n\n## timeseries\n\nwhat was the average tweet length, per day, surrounding the 2014 sochi olympics?\n\n```python\nfrom pydruid.client import *\nfrom pylab import plt\n\nquery = pydruid(druid_url_goes_here, 'druid/v2')\n\nts = query.timeseries(\n    datasource='twitterstream',\n    granularity='day',\n    intervals='2014-02-02/p4w',\n    aggregations={'length': doublesum('tweet_length'), 'count': doublesum('count')},\n    post_aggregations={'avg_tweet_length': (field('length') / field('count'))},\n    filter=dimension('first_hashtag') == 'sochi2014'\n)\ndf = query.export_pandas()\ndf['timestamp'] = df['timestamp'].map(lambda x: x.split('t')[0])\ndf.plot(x='timestamp', y='avg_tweet_length', ylim=(80, 140), rot=20,\n        title='sochi 2014')\nplt.ylabel('avg tweet length (chars)')\nplt.show()\n```\n\n![alt text](https://github.com/metamx/pydruid/raw/master/docs/figures/avg_tweet_length.png \"avg. tweet length\")\n\n## topn\n\nwho were the top ten mentions (@user_name) during the 2014 oscars?\n\n```python\ntop = query.topn(\n    datasource='twitterstream',\n    granularity='all',\n    intervals='2014-03-03/p1d',  # utc time of 2014 oscars\n    aggregations={'count': doublesum('count')},\n    dimension='user_mention_name',\n    filter=(dimension('user_lang') == 'en') & (dimension('first_hashtag') == 'oscars') &\n           (dimension('user_time_zone') == 'pacific time (us & canada)') &\n           ~(dimension('user_mention_name') == 'no mention'),\n    metric='count',\n    threshold=10\n)\n\ndf = query.export_pandas()\nprint df\n\n   count                 timestamp user_mention_name\n0   1303  2014-03-03t00:00:00.000z      theellenshow\n1     44  2014-03-03t00:00:00.000z        theacademy\n2     21  2014-03-03t00:00:00.000z               mtv\n3     21  2014-03-03t00:00:00.000z         peoplemag\n4     17  2014-03-03t00:00:00.000z               thr\n5     16  2014-03-03t00:00:00.000z      itsqueenelsa\n6     16  2014-03-03t00:00:00.000z           eonline\n7     15  2014-03-03t00:00:00.000z       perezhilton\n8     14  2014-03-03t00:00:00.000z     realjohngreen\n9     12  2014-03-03t00:00:00.000z       kevinspacey\n\n```\n\n## groupby\n\nwhat does the social network of users replying to other users look like?\n\n```python\nfrom igraph import *\nfrom cairo import *\nfrom pandas import concat\n\ngroup = query.groupby(\n    datasource='twitterstream',\n    granularity='hour',\n    intervals='2013-10-04/pt12h',\n    dimensions=[\"user_name\", \"reply_to_name\"],\n    filter=(~(dimension(\"reply_to_name\") == \"not a reply\")) &\n           (dimension(\"user_location\") == \"california\"),\n    aggregations={\"count\": doublesum(\"count\")}\n)\n\ndf = query.export_pandas()\n\n# map names to categorical variables with a lookup table\nnames = concat([df['user_name'], df['reply_to_name']]).unique()\nnamelookup = dict([pair[::-1] for pair in enumerate(names)])\ndf['user_name_lookup'] = df['user_name'].map(namelookup.get)\ndf['reply_to_name_lookup'] = df['reply_to_name'].map(namelookup.get)\n\n# create the graph with igraph\ng = graph(len(names), directed=false)\nvertices = zip(df['user_name_lookup'], df['reply_to_name_lookup'])\ng.vs[\"name\"] = names\ng.add_edges(vertices)\nlayout = g.layout_fruchterman_reingold()\nplot(g, \"tweets.png\", layout=layout, vertex_size=2, bbox=(400, 400), margin=25, edge_width=1, vertex_color=\"blue\")\n```\n\n![alt text](https://github.com/metamx/pydruid/raw/master/docs/figures/twitter_graph.png \"social network\")\n\n# asynchronous client\n```pydruid.async_client.asyncpydruid``` implements an asynchronous client. to achieve that, it utilizes an asynchronous\nhttp client from ```tornado``` framework. the asynchronous client is suitable for use with async frameworks such as tornado\nand provides much better performance at scale. it lets you serve multiple requests at the same time, without blocking on\ndruid executing your queries.\n\n## example\n```python\nfrom tornado import gen\nfrom pydruid.async_client import asyncpydruid\nfrom pydruid.utils.aggregators import longsum\nfrom pydruid.utils.filters import dimension\n\nclient = asyncpydruid(url_to_druid_broker, 'druid/v2')\n\n@gen.coroutine\ndef your_asynchronous_method_serving_top10_mentions_for_day(day\n    top_mentions = yield client.topn(\n        datasource='twitterstream',\n        granularity='all',\n        intervals=\"%s/p1d\" % (day, ),\n        aggregations={'count': doublesum('count')},\n        dimension='user_mention_name',\n        filter=(dimension('user_lang') == 'en') & (dimension('first_hashtag') == 'oscars') &\n               (dimension('user_time_zone') == 'pacific time (us & canada)') &\n               ~(dimension('user_mention_name') == 'no mention'),\n        metric='count',\n        threshold=10)\n\n    # asynchronously return results\n    # can be simply ```return top_mentions``` in python 3.x\n    raise gen.return(top_mentions)\n```\n\n\n# thetasketches\ntheta sketch post aggregators are built slightly differently to normal post aggregators, as they have different operators.\nnote: you must have the ```druid-datasketches``` extension loaded into your druid cluster in order to use these.\nsee the [druid datasketches](http://druid.io/docs/latest/development/extensions-core/datasketches-aggregators.html) documentation for details.\n\n```python\nfrom pydruid.client import *\nfrom pydruid.utils import aggregators\nfrom pydruid.utils import filters\nfrom pydruid.utils import postaggregator\n\nquery = pydruid(url_to_druid_broker, 'druid/v2')\nts = query.groupby(\n    datasource='test_datasource',\n    granularity='all',\n    intervals='2016-09-01/p1m',\n    filter = ( filters.dimension('product').in_(['product_a', 'product_b'])),\n    aggregations={\n        'product_a_users': aggregators.filtered(\n            filters.dimension('product') == 'product_a',\n            aggregators.thetasketch('user_id')\n            ),\n        'product_b_users': aggregators.filtered(\n            filters.dimension('product') == 'product_b',\n            aggregators.thetasketch('user_id')\n            )\n    },\n    post_aggregations={\n        'both_a_and_b': postaggregator.thetasketchestimate(\n            postaggregator.thetasketch('product_a_users') & postaggregator.thetasketch('product_b_users')\n            )\n    }\n)\n```\n\n# db api\n\n```python\nfrom pydruid.db import connect\n\nconn = connect(host='localhost', port=8082, path='/druid/v2/sql/', scheme='http')\ncurs = conn.cursor()\ncurs.execute(\"\"\"\n    select place,\n           cast(regexp_extract(place, '(.*),', 1) as float) as lat,\n           cast(regexp_extract(place, ',(.*)', 1) as float) as lon\n      from places\n     limit 10\n\"\"\")\nfor row in curs:\n    print(row)\n```\n\n# sqlalchemy\n\n```python\nfrom sqlalchemy import *\nfrom sqlalchemy.engine import create_engine\nfrom sqlalchemy.schema import *\n\nengine = create_engine('druid://localhost:8082/druid/v2/sql/')  # uses http by default :(\n# engine = create_engine('druid+http://localhost:8082/druid/v2/sql/')\n# engine = create_engine('druid+https://localhost:8082/druid/v2/sql/')\n\nplaces = table('places', metadata(bind=engine), autoload=true)\nprint(select([func.count('*')], from_obj=places).scalar())\n```\n\n\n## column headers\n\nin version 0.13.0 druid sql added support for including the column names in the\nresponse which can be requested via the \"header\" field in the request. this\nhelps to ensure that the cursor description is defined (which is a requirement\nfor sqlalchemy query statements) regardless on whether the result set contains\nany rows. historically this was problematic for result sets which contained no\nrows at one could not infer the expected column names.\n\nenabling the header can be configured via the sqlalchemy uri by using the query\nparameter, i.e.,\n\n```python\nengine = create_engine('druid://localhost:8082/druid/v2/sql?header=true')\n```\n\nnote the current default is `false` to ensure backwards compatibility but should\nbe set to `true` for druid versions >= 0.13.0.\n\n\n# command line\n\n```bash\n$ pydruid http://localhost:8082/druid/v2/sql/\n> select count(*) as cnt from places\n  cnt\n-----\n12345\n> select table_name from information_schema.tables;\ntable_name\n----------\ntest_table\ncolumns\nschemata\ntables\n> bye;\ngoodbye!\n```\n\n# contributing\n\ncontributions are welcomed of course. we like to use `black` and `flake8`.\n\n```bash\npip install -r requirements-dev.txt  # installs useful dev deps\npre-commit install  # installs useful commit hooks\n```\n",
  "docs_url": "https://pythonhosted.org/pydruid/",
  "keywords": "",
  "license": "apache license, version 2.0",
  "name": "pydruid",
  "package_url": "https://pypi.org/project/pydruid/",
  "project_url": "https://pypi.org/project/pydruid/",
  "project_urls": {
    "Bug Tracker": "https://github.com/druid-io/pydruid/issues",
    "Documentation": "https://pythonhosted.org/pydruid/",
    "Homepage": "https://druid.apache.org",
    "Source Code": "https://github.com/druid-io/pydruid"
  },
  "release_url": "https://pypi.org/project/pydruid/0.6.6/",
  "requires_dist": [
    "requests",
    "pandas; extra == \"pandas\"",
    "tornado; extra == \"async\"",
    "sqlalchemy; extra == \"sqlalchemy\"",
    "pygments; extra == \"cli\"",
    "prompt_toolkit>=2.0.0; extra == \"cli\"",
    "tabulate; extra == \"cli\""
  ],
  "requires_python": "",
  "summary": "a python connector for druid.",
  "version": "0.6.6",
  "releases": [],
  "developers": [
    "druid-development@googlegroups.com",
    "druid_developers"
  ],
  "kwds": "pydruid pydata python sqlalchemy asyncpydruid",
  "license_kwds": "apache license, version 2.0",
  "libtype": "pypi",
  "id": "pypi_pydruid",
  "homepage": "https://druid.apache.org",
  "release_count": 30,
  "dependency_ids": [
    "pypi_pandas",
    "pypi_prompt_toolkit",
    "pypi_pygments",
    "pypi_requests",
    "pypi_sqlalchemy",
    "pypi_tabulate",
    "pypi_tornado"
  ]
}