{
  "classifiers": [
    "intended audience :: developers",
    "natural language :: chinese (simplified)",
    "operating system :: os independent",
    "programming language :: python :: 3",
    "programming language :: python :: 3.2",
    "programming language :: python :: 3.3",
    "programming language :: python :: 3.4",
    "programming language :: python :: 3.5",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "topic :: utilities"
  ],
  "description": "english | [\u7b80\u4f53\u4e2d\u6587](readme_ch.md) | [\u0939\u093f\u0928\u094d\u0926\u0940](./doc/doc_i18n/readme_\u0939\u093f\u0928\u094d\u0926.md) | [\u65e5\u672c\u8a9e](./doc/doc_i18n/readme_\u65e5\u672c\u8a9e.md) | [\ud55c\uad6d\uc778](./doc/doc_i18n/readme_\ud55c\uad6d\uc5b4.md) | [p\u0443\u0301\u0441\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u0301\u043a](./doc/doc_i18n/readme_\u0440\u0443\u0301\u0441\u0441\u043a\u0438\u0439_\u044f\u0437\u044b\u0301\u043a.md)\n\n<p align=\"center\">\n <img src=\"./doc/paddleocr_log.png\" align=\"middle\" width = \"600\"/>\n<p align=\"center\">\n<p align=\"left\">\n    <a href=\"./license\"><img src=\"https://img.shields.io/badge/license-apache%202-dfd.svg\"></a>\n    <a href=\"https://github.com/paddlepaddle/paddleocr/releases\"><img src=\"https://img.shields.io/github/v/release/paddlepaddle/paddleocr?color=ffa\"></a>\n    <a href=\"\"><img src=\"https://img.shields.io/badge/python-3.7+-aff.svg\"></a>\n    <a href=\"\"><img src=\"https://img.shields.io/badge/os-linux%2c%20win%2c%20mac-pink.svg\"></a>\n    <a href=\"\"><img src=\"https://img.shields.io/pypi/format/paddleocr?color=c77\"></a>\n    <a href=\"https://pypi.org/project/paddleocr/\"><img src=\"https://img.shields.io/pypi/dm/paddleocr?color=9cf\"></a>\n    <a href=\"https://github.com/paddlepaddle/paddleocr/stargazers\"><img src=\"https://img.shields.io/github/stars/paddlepaddle/paddleocr?color=ccf\"></a>\n</p>\n\n# note from maintainer:\n\nthis is a fork of the paddleocr repository, created with the purpose of complying with an apache license. the original repo contains at least one dependency that is not apache compliant so this fork was created to remove any non-compliant dependencies.\n\nthe original documentation at the time of the fork is found below.\n\n## introduction\n\npaddleocr aims to create multilingual, awesome, leading, and practical ocr tools that help users train better models and apply them into practice.\n\n<div align=\"center\">\n    <img src=\"./doc/imgs_results/pp-ocrv3/en/en_4.png\" width=\"800\">\n</div>\n\n<div align=\"center\">\n    <img src=\"./doc/imgs_results/ch_ppocr_mobile_v2.0/00006737.jpg\" width=\"800\">\n</div>\n\n## \ud83d\udce3 recent updates\n- \ud83d\udd28**2022.11 add implementation of [4 cutting-edge algorithms](doc/doc_en/algorithm_overview_en.md)**\uff1atext detection [drrg](doc/doc_en/algorithm_det_drrg_en.md),  text recognition [rfl](./doc/doc_en/algorithm_rec_rfl_en.md), image super-resolution [text telescope](doc/doc_en/algorithm_sr_telescope_en.md)\uff0chandwritten mathematical expression recognition [can](doc/doc_en/algorithm_rec_can_en.md)\n- **2022.10 release [optimized js version pp-ocrv3 model](./deploy/paddlejs/readme.md)** with 4.3m model size, 8x faster inference time, and a ready-to-use web demo\n\n- \ud83d\udca5 **live playback: introduction to pp-structurev2 optimization strategy**. scan [the qr code below](#community) using wechat, follow the paddlepaddle official account and fill out the questionnaire to join the wechat group, get the live link and 20g ocr learning materials (including pdf2word application, 10 models in vertical scenarios, etc.)\n\n\n- **\ud83d\udd252022.8.24 release paddleocr [release/2.6](https://github.com/paddlepaddle/paddleocr/tree/release/2.6)**\n  - release [pp-structurev2](./ppstructure/)\uff0cwith functions and performance fully upgraded, adapted to chinese scenes, and new support for [layout recovery](./ppstructure/recovery) and **one line command to convert pdf to word**;\n  - [layout analysis](./ppstructure/layout) optimization: model storage reduced by 95%, while speed increased by 11 times, and the average cpu time-cost is only 41ms;\n  - [table recognition](./ppstructure/table) optimization: 3 optimization strategies are designed, and the model accuracy is improved by 6% under comparable time consumption;\n  - [key information extraction](./ppstructure/kie) optimization\uff1aa visual-independent model structure is designed, the accuracy of semantic entity recognition is increased by 2.8%, and the accuracy of relation extraction is increased by 9.1%.\n- **\ud83d\udd252022.8 release [ocr scene application collection](./applications/readme_en.md)**\n    - release **9 vertical models** such as digital tube, lcd screen, license plate, handwriting recognition model, high-precision svtr model, etc, covering the main ocr vertical applications in general, manufacturing, finance, and transportation industries.\n- **2022.8 add implementation of [8 cutting-edge algorithms](doc/doc_en/algorithm_overview_en.md)**\n  - text detection: [fcenet](doc/doc_en/algorithm_det_fcenet_en.md), [db++](doc/doc_en/algorithm_det_db_en.md)\n  - text recognition: [vitstr](doc/doc_en/algorithm_rec_vitstr_en.md), [abinet](doc/doc_en/algorithm_rec_abinet_en.md), [visionlan](doc/doc_en/algorithm_rec_visionlan_en.md), [spin](doc/doc_en/algorithm_rec_spin_en.md), [robustscanner](doc/doc_en/algorithm_rec_robustscanner_en.md)\n  - table recognition: [tablemaster](doc/doc_en/algorithm_table_master_en.md)\n- **2022.5.9 release paddleocr [release/2.5](https://github.com/paddlepaddle/paddleocr/tree/release/2.5)**\n    - release [pp-ocrv3](./doc/doc_en/ppocr_introduction_en.md#pp-ocrv3): with comparable speed, the effect of chinese scene is further improved by 5% compared with pp-ocrv2, the effect of english scene is improved by 11%, and the average recognition accuracy of 80 language multilingual models is improved by more than 5%.\n    - release [ppocrlabelv2](./ppocrlabel): add the annotation function for table recognition task, key information extraction task and irregular text image.\n    - release interactive e-book [*\"dive into ocr\"*](./doc/doc_en/ocr_book_en.md), covers the cutting-edge theory and code practice of ocr full stack technology.\n- [more](./doc/doc_en/update_en.md)\n\n\n## \ud83c\udf1f features\n\npaddleocr support a variety of cutting-edge algorithms related to ocr, and developed industrial featured models/solution [pp-ocr](./doc/doc_en/ppocr_introduction_en.md) and [pp-structure](./ppstructure/readme.md) on this basis, and get through the whole process of data production, model training, compression, inference and deployment.\n\n<div align=\"center\">\n    <img src=\"https://user-images.githubusercontent.com/25809855/186171245-40abc4d7-904f-4949-ade1-250f86ed3a90.png\">\n</div>\n\n> it is recommended to start with the \u201cquick experience\u201d in the document tutorial\n\n\n## \u26a1 quick experience\n\n- web online experience for the ultra-lightweight ocr: [online experience](https://www.paddlepaddle.org.cn/hub/scene/ocr)\n- mobile demo experience (based on easyedge and paddle-lite, supports ios and android systems): [sign in to the website to obtain the qr code for  installing the app](https://ai.baidu.com/easyedge/app/opensource?from=paddlelite)\n- one line of code quick use: [quick start](./doc/doc_en/quickstart_en.md)\n\n\n<a name=\"book\"></a>\n## \ud83d\udcda e-book: *dive into ocr*\n- [dive into ocr ](./doc/doc_en/ocr_book_en.md)\n\n<a name=\"community\"></a>\n\n## \ud83d\udc6b community\n\n- for international developers, we regard [paddleocr discussions](https://github.com/paddlepaddle/paddleocr/discussions) as our international community platform. all ideas and questions can be discussed here in english.\n\n- for chinese develops, scan the qr code below with your wechat, you can join the official technical discussion group. for richer community content, please refer to [\u4e2d\u6587readme](readme_ch.md), looking forward to your participation.\n\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/paddlepaddle/paddleocr/dygraph/doc/joinus.png\"  width = \"150\" height = \"150\" />\n</div>\n\n<a name=\"supported-chinese-model-list\"></a>\n\n## \ud83d\udee0\ufe0f pp-ocr series model list\uff08update on september 8th\uff09\n\n| model introduction                                           | model name                   | recommended scene | detection model                                              | direction classifier                                         | recognition model                                            |\n| ------------------------------------------------------------ | ---------------------------- | ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| chinese and english ultra-lightweight pp-ocrv3 model\uff0816.2m\uff09     | ch_pp-ocrv3_xx          | mobile & server | [inference model](https://paddleocr.bj.bcebos.com/pp-ocrv3/chinese/ch_pp-ocrv3_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/pp-ocrv3/chinese/ch_pp-ocrv3_det_distill_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/pp-ocrv3/chinese/ch_pp-ocrv3_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/pp-ocrv3/chinese/ch_pp-ocrv3_rec_train.tar) |\n| english ultra-lightweight pp-ocrv3 model\uff0813.4m\uff09     | en_pp-ocrv3_xx          | mobile & server | [inference model](https://paddleocr.bj.bcebos.com/pp-ocrv3/english/en_pp-ocrv3_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/pp-ocrv3/english/en_pp-ocrv3_det_distill_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) | [inference model](https://paddleocr.bj.bcebos.com/pp-ocrv3/english/en_pp-ocrv3_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/pp-ocrv3/english/en_pp-ocrv3_rec_train.tar) |\n| chinese and english ultra-lightweight pp-ocrv2 model\uff0811.6m\uff09 |  ch_pp-ocrv2_xx |mobile & server|[inference model](https://paddleocr.bj.bcebos.com/pp-ocrv2/chinese/ch_pp-ocrv2_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/pp-ocrv2/chinese/ch_pp-ocrv2_det_distill_train.tar)| [inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) |[inference model](https://paddleocr.bj.bcebos.com/pp-ocrv2/chinese/ch_pp-ocrv2_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/pp-ocrv2/chinese/ch_pp-ocrv2_rec_train.tar)|\n| chinese and english ultra-lightweight pp-ocr model (9.4m)       | ch_ppocr_mobile_v2.0_xx      | mobile & server   |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_det_train.tar)|[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar) |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_rec_train.tar)      |\n| chinese and english general pp-ocr model (143.4m)               | ch_ppocr_server_v2.0_xx      | server            |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_det_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_det_train.tar)    |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar)    |[inference model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_rec_infer.tar) / [trained model](https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_rec_train.tar)  |\n\n\n- for more model downloads (including multiple languages), please refer to [pp-ocr series model downloads](./doc/doc_en/models_list_en.md).\n- for a new language request, please refer to [guideline for new language_requests](#language_requests).\n- for structural document analysis models, please refer to [pp-structure models](./ppstructure/docs/models_list_en.md).\n\n## \ud83d\udcd6 tutorials\n- [environment preparation](./doc/doc_en/environment_en.md)\n- [pp-ocr \ud83d\udd25](./doc/doc_en/ppocr_introduction_en.md)\n    - [quick start](./doc/doc_en/quickstart_en.md)\n    - [model zoo](./doc/doc_en/models_en.md)\n    - [model training](./doc/doc_en/training_en.md)\n        - [text detection](./doc/doc_en/detection_en.md)\n        - [text recognition](./doc/doc_en/recognition_en.md)\n        - [text direction classification](./doc/doc_en/angle_class_en.md)\n    - model compression\n        - [model quantization](./deploy/slim/quantization/readme_en.md)\n        - [model pruning](./deploy/slim/prune/readme_en.md)\n        - [knowledge distillation](./doc/doc_en/knowledge_distillation_en.md)\n    - [inference and deployment](./deploy/readme.md)\n        - [python inference](./doc/doc_en/inference_ppocr_en.md)\n        - [c++ inference](./deploy/cpp_infer/readme.md)\n        - [serving](./deploy/pdserving/readme.md)\n        - [mobile](./deploy/lite/readme.md)\n        - [paddle2onnx](./deploy/paddle2onnx/readme.md)\n        - [paddlecloud](./deploy/paddlecloud/readme.md)\n        - [benchmark](./doc/doc_en/benchmark_en.md)  \n- [pp-structure \ud83d\udd25](./ppstructure/readme.md)\n    - [quick start](./ppstructure/docs/quickstart_en.md)\n    - [model zoo](./ppstructure/docs/models_list_en.md)\n    - [model training](./doc/doc_en/training_en.md)  \n        - [layout analysis](./ppstructure/layout/readme.md)\n        - [table recognition](./ppstructure/table/readme.md)\n        - [key information extraction](./ppstructure/kie/readme.md)\n    - [inference and deployment](./deploy/readme.md)\n        - [python inference](./ppstructure/docs/inference_en.md)\n        - [c++ inference](./deploy/cpp_infer/readme.md)\n        - [serving](./deploy/hubserving/readme_en.md)\n- [academic algorithms](./doc/doc_en/algorithm_overview_en.md)\n    - [text detection](./doc/doc_en/algorithm_overview_en.md)\n    - [text recognition](./doc/doc_en/algorithm_overview_en.md)\n    - [end-to-end ocr](./doc/doc_en/algorithm_overview_en.md)\n    - [table recognition](./doc/doc_en/algorithm_overview_en.md)\n    - [key information extraction](./doc/doc_en/algorithm_overview_en.md)  \n    - [add new algorithms to paddleocr](./doc/doc_en/add_new_algorithm_en.md)\n- data annotation and synthesis\n    - [semi-automatic annotation tool: ppocrlabel](./ppocrlabel/readme.md)\n    - [data synthesis tool: style-text](./styletext/readme.md)\n    - [other data annotation tools](./doc/doc_en/data_annotation_en.md)\n    - [other data synthesis tools](./doc/doc_en/data_synthesis_en.md)\n- datasets\n    - [general ocr datasets(chinese/english)](doc/doc_en/dataset/datasets_en.md)\n    - [handwritten_ocr_datasets(chinese)](doc/doc_en/dataset/handwritten_datasets_en.md)\n    - [various ocr datasets(multilingual)](doc/doc_en/dataset/vertical_and_multilingual_datasets_en.md)\n    - [layout analysis](doc/doc_en/dataset/layout_datasets_en.md)\n    - [table recognition](doc/doc_en/dataset/table_datasets_en.md)\n    - [key information extraction](doc/doc_en/dataset/kie_datasets_en.md)\n- [code structure](./doc/doc_en/tree_en.md)\n- [visualization](#visualization)\n- [community](#community)\n- [new language requests](#language_requests)\n- [faq](./doc/doc_en/faq_en.md)\n- [references](./doc/doc_en/reference_en.md)\n- [license](#license)\n\n\n<a name=\"visualization\"></a>\n## \ud83d\udc40 visualization [more](./doc/doc_en/visualization_en.md)\n\n<details open>\n<summary>pp-ocrv3 chinese model</summary>\n<div align=\"center\">\n    <img src=\"doc/imgs_results/pp-ocrv3/ch/pp-ocrv3-pic001.jpg\" width=\"800\">\n    <img src=\"doc/imgs_results/pp-ocrv3/ch/pp-ocrv3-pic002.jpg\" width=\"800\">\n    <img src=\"doc/imgs_results/pp-ocrv3/ch/pp-ocrv3-pic003.jpg\" width=\"800\">\n</div>\n</details>\n\n<details open>\n<summary>pp-ocrv3 english model</summary>\n<div align=\"center\">\n    <img src=\"doc/imgs_results/pp-ocrv3/en/en_1.png\" width=\"800\">\n    <img src=\"doc/imgs_results/pp-ocrv3/en/en_2.png\" width=\"800\">\n</div>\n</details>\n\n<details open>\n<summary>pp-ocrv3 multilingual model</summary>\n<div align=\"center\">\n    <img src=\"doc/imgs_results/pp-ocrv3/multi_lang/japan_2.jpg\" width=\"800\">\n    <img src=\"doc/imgs_results/pp-ocrv3/multi_lang/korean_1.jpg\" width=\"800\">\n</div>\n</details>\n\n<details open>\n<summary>pp-structurev2</summary>\n\n- layout analysis + table recognition  \n<div align=\"center\">\n    <img src=\"./ppstructure/docs/table/ppstructure.gif\" width=\"800\">\n</div>\n\n- ser (semantic entity recognition)\n<div align=\"center\">\n    <img src=\"https://user-images.githubusercontent.com/14270174/197464552-69de557f-edff-4c7f-acbf-069df1ba097f.png\" width=\"600\">\n</div>\n\n<div align=\"center\">\n    <img src=\"https://user-images.githubusercontent.com/14270174/185310636-6ce02f7c-790d-479f-b163-ea97a5a04808.jpg\" width=\"600\">\n</div>\n\n<div align=\"center\">\n    <img src=\"https://user-images.githubusercontent.com/14270174/185539517-ccf2372a-f026-4a7c-ad28-c741c770f60a.png\" width=\"600\">\n</div>\n\n- re (relation extraction)\n<div align=\"center\">\n    <img src=\"https://user-images.githubusercontent.com/25809855/186094813-3a8e16cc-42e5-4982-b9f4-0134dfb5688d.png\" width=\"600\">\n</div>  \n\n<div align=\"center\">\n    <img src=\"https://user-images.githubusercontent.com/14270174/185393805-c67ff571-cf7e-4217-a4b0-8b396c4f22bb.jpg\" width=\"600\">\n</div>\n\n<div align=\"center\">\n    <img src=\"https://user-images.githubusercontent.com/14270174/185540080-0431e006-9235-4b6d-b63d-0b3c6e1de48f.jpg\" width=\"600\">\n</div>\n\n</details>\n\n<a name=\"language_requests\"></a>\n## \ud83c\uddfa\ud83c\uddf3 guideline for new language requests\n\nif you want to request a new language support, a pr with 1 following files are needed\uff1a\n\n1. in folder [ppocr/utils/dict](./ppocr/utils/dict),\nit is necessary to submit the dict text to this path and name it with `{language}_dict.txt` that contains a list of all characters. please see the format example from other files in that folder.\n\nif your language has unique elements, please tell me in advance within any way, such as useful links, wikipedia and so on.\n\nmore details, please refer to [multilingual ocr development plan](https://github.com/paddlepaddle/paddleocr/issues/1048).\n\n\n<a name=\"license\"></a>\n## \ud83d\udcc4 license\nthis project is released under <a href=\"https://github.com/paddlepaddle/paddleocr/blob/master/license\">apache 2.0 license</a>\n\n\n",
  "docs_url": null,
  "keywords": "ocr textdetection textrecognition paddleocr crnn east star-net rosetta ocrlite db chineseocr chinesetextdetection chinesetextrecognition",
  "license": "apache license 2.0",
  "name": "unstructured.paddleocr",
  "package_url": "https://pypi.org/project/unstructured.paddleocr/",
  "project_url": "https://pypi.org/project/unstructured.paddleocr/",
  "project_urls": {
    "Download": "https://github.com/UnstructureIO/unstructured.PaddleOCR.git",
    "Homepage": "https://github.com/UnstructureIO/unstructured.PaddleOCR"
  },
  "release_url": "https://pypi.org/project/unstructured.paddleocr/2.6.1.3/",
  "requires_dist": [
    "shapely",
    "scikit-image",
    "imgaug",
    "pyclipper",
    "lmdb",
    "tqdm",
    "numpy",
    "visualdl",
    "rapidfuzz",
    "opencv-python ==4.8.0.76",
    "opencv-contrib-python ==4.8.0.76",
    "cython",
    "lxml",
    "premailer",
    "openpyxl",
    "attrdict",
    "Polygon3",
    "lanms-neo ==1.0.2",
    "pdf2image"
  ],
  "requires_python": "",
  "summary": "awesome ocr toolkits based on paddlepaddle \uff088.6m ultra-lightweight pre-trained model, support training and deployment among server, mobile, embeded and iot devices",
  "version": "2.6.1.3",
  "releases": [],
  "developers": [],
  "kwds": "chinesetextdetection chinesetextrecognition chineseocr \u4e2d\u6587readme readme_\ud55c\uad6d\uc5b4",
  "license_kwds": "apache license 2.0",
  "libtype": "pypi",
  "id": "pypi_unstructured.paddleocr",
  "homepage": "https://github.com/unstructureio/unstructured.paddleocr",
  "release_count": 7,
  "dependency_ids": [
    "pypi_attrdict",
    "pypi_cython",
    "pypi_imgaug",
    "pypi_lanms_neo",
    "pypi_lmdb",
    "pypi_lxml",
    "pypi_numpy",
    "pypi_opencv_contrib_python",
    "pypi_opencv_python",
    "pypi_openpyxl",
    "pypi_pdf2image",
    "pypi_polygon3",
    "pypi_premailer",
    "pypi_pyclipper",
    "pypi_rapidfuzz",
    "pypi_scikit_image",
    "pypi_shapely",
    "pypi_tqdm",
    "pypi_visualdl"
  ]
}