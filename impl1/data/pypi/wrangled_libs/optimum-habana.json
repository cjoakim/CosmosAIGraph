{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: developers",
    "intended audience :: education",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "operating system :: os independent",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "<!---\ncopyright 2022 the huggingface team. all rights reserved.\n\nlicensed under the apache license, version 2.0 (the \"license\");\nyou may not use this file except in compliance with the license.\nyou may obtain a copy of the license at\n\n    http://www.apache.org/licenses/license-2.0\n\nunless required by applicable law or agreed to in writing, software\ndistributed under the license is distributed on an \"as is\" basis,\nwithout warranties or conditions of any kind, either express or implied.\nsee the license for the specific language governing permissions and\nlimitations under the license.\n-->\n\n![](https://github.com/huggingface/optimum-habana/blob/main/readme_logo.png)\n\n\n# optimum habana\n\n\ud83e\udd17 optimum habana is the interface between the \ud83e\udd17 transformers and diffusers libraries and [habana's gaudi processor (hpu)](https://docs.habana.ai/en/latest/index.html).\nit provides a set of tools enabling easy model loading, training and inference on single- and multi-hpu settings for different downstream tasks.\nthe list of officially validated models and tasks is available [here](https://github.com/huggingface/optimum-habana#validated-models). users can try other models and tasks with only few changes.\n\n\n## what is a habana processing unit (hpu)?\n\nhpus offer fast model training and inference as well as a great price-performance ratio.\ncheck out [this blog post about bert pre-training](https://huggingface.co/blog/pretraining-bert) and [this article benchmarking habana gaudi2 versus nvidia a100 gpus](https://huggingface.co/blog/habana-gaudi-2-benchmark) for concrete examples.\nif you are not familiar with hpus and would like to know more about them, we recommend you take a look at [our conceptual guide](https://huggingface.co/docs/optimum/habana/concept_guides/hpu).\n\n\n## install\nto install the latest stable release of this package:\n\n```bash\npip install --upgrade-strategy eager optimum[habana]\n```\n\nthe `--upgrade-strategy eager` option is needed to ensure `optimum-habana` is upgraded to the latest stable release.\n\n> to use deepspeed on hpus, you also need to run the following command:\n>```bash\n>pip install git+https://github.com/habanaai/deepspeed.git@1.13.0\n>```\n\noptimum habana is a fast-moving project, and you may want to install it from source:\n\n```bash\npip install git+https://github.com/huggingface/optimum-habana.git\n```\n\nlast but not least, don't forget to install the requirements for every example:\n\n```bash\ncd <example-folder>\npip install -r requirements.txt\n```\n\n\n## how to use it?\n\n### quick start\n\n\ud83e\udd17 optimum habana was designed with one goal in mind: **to make training and inference straightforward for any \ud83e\udd17 transformers and \ud83e\udd17 diffusers user while leveraging the complete power of gaudi processors**.\n\n#### transformers interface\n\nthere are two main classes one needs to know:\n- [gauditrainer](https://huggingface.co/docs/optimum/habana/package_reference/trainer): the trainer class that takes care of compiling and distributing the model to run on hpus, and performing training and evaluation.\n- [gaudiconfig](https://huggingface.co/docs/optimum/habana/package_reference/gaudi_config): the class that enables to configure habana mixed precision and to decide whether optimized operators and optimizers should be used or not.\n\nthe [gauditrainer](https://huggingface.co/docs/optimum/habana/package_reference/trainer) is very similar to the [\ud83e\udd17 transformers trainer](https://huggingface.co/docs/transformers/main_classes/trainer), and adapting a script using the trainer to make it work with gaudi will mostly consist in simply swapping the `trainer` class for the `gauditrainer` one.\nthat's how most of the [example scripts](https://github.com/huggingface/optimum-habana/tree/main/examples) were adapted from their [original counterparts](https://github.com/huggingface/transformers/tree/main/examples/pytorch).\n\nhere is an example:\n```diff\n- from transformers import trainer, trainingarguments\n+ from optimum.habana import gaudiconfig, gauditrainer, gauditrainingarguments\n\n- training_args = trainingarguments(\n+ training_args = gauditrainingarguments(\n  # training arguments...\n+ use_habana=true,\n+ use_lazy_mode=true,  # whether to use lazy or eager mode\n+ gaudi_config_name=path_to_gaudi_config,\n)\n\n# a lot of code here\n\n# initialize our trainer\n- trainer = trainer(\n+ trainer = gauditrainer(\n    model=model,\n    args=training_args,  # original training arguments.\n    train_dataset=train_dataset if training_args.do_train else none,\n    eval_dataset=eval_dataset if training_args.do_eval else none,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n```\n\nwhere `gaudi_config_name` is the name of a model from the [hub](https://huggingface.co/habana) (gaudi configurations are stored in model repositories) or a path to a local gaudi configuration file (you can see [here](https://huggingface.co/docs/optimum/habana/package_reference/gaudi_config) how to write your own).\n\n\n#### diffusers interface\n\nyou can generate images from prompts using stable diffusion on gaudi using the [`gaudistablediffusionpipeline`](https://huggingface.co/docs/optimum/habana/package_reference/stable_diffusion_pipeline) class and the [`gaudiddimscheduler`] which have been both optimized for hpus. here is how to use them and the differences with the \ud83e\udd17 diffusers library:\n\n```diff\n- from diffusers import ddimscheduler, stablediffusionpipeline\n+ from optimum.habana.diffusers import gaudiddimscheduler, gaudistablediffusionpipeline\n\n\nmodel_name = \"runwayml/stable-diffusion-v1-5\"\n\n- scheduler = ddimscheduler.from_pretrained(model_name, subfolder=\"scheduler\")\n+ scheduler = gaudiddimscheduler.from_pretrained(model_name, subfolder=\"scheduler\")\n\n- pipeline = stablediffusionpipeline.from_pretrained(\n+ pipeline = gaudistablediffusionpipeline.from_pretrained(\n    model_name,\n    scheduler=scheduler,\n+   use_habana=true,\n+   use_hpu_graphs=true,\n+   gaudi_config=\"habana/stable-diffusion\",\n)\n\noutputs = generator(\n    [\"an image of a squirrel in picasso style\"],\n    num_images_per_prompt=16,\n+   batch_size=4,\n)\n```\n\n\n### documentation\n\ncheck out [the documentation of optimum habana](https://huggingface.co/docs/optimum/habana/index) for more advanced usage.\n\n\n## validated models\n\nthe following model architectures, tasks and device distributions have been validated for \ud83e\udd17 optimum habana:\n\n> in the tables below, :heavy_check_mark: means single-card, multi-card and deepspeed have all been validated.\n\n- transformers:\n<div align=\"center\">\n\n| architecture | training | inference | <center>tasks</center> |\n|--------------|:--------:|:---------:|:-----------------------|\n| bert         | :heavy_check_mark: | :heavy_check_mark: | <li>[text classification](https://github.com/huggingface/optimum-habana/tree/main/examples/text-classification)</li><li>[question answering](https://github.com/huggingface/optimum-habana/tree/main/examples/question-answering)</li><li>[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)</li> |\n| roberta | :heavy_check_mark: | :heavy_check_mark: | <li>[question answering](https://github.com/huggingface/optimum-habana/tree/main/examples/question-answering)</li><li>[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)</li> |\n| albert | :heavy_check_mark: | :heavy_check_mark: | <li>[question answering](https://github.com/huggingface/optimum-habana/tree/main/examples/question-answering)</li><li>[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)</li> |\n| distilbert |:heavy_check_mark: | :heavy_check_mark: | <li>[question answering](https://github.com/huggingface/optimum-habana/tree/main/examples/question-answering)</li><li>[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)</li> |\n| gpt2             | :heavy_check_mark: | :heavy_check_mark: | <li>[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)</li><li>[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)</li> |\n| bloom(z) | :x: | <div style=\"text-align:left\"><li>deepspeed</li></div> | <li>[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)</li> |\n| starcoder | :x: | <div style=\"text-align:left\"><li>single card</li></div> | <li>[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)</li> |\n| gpt-j | <div style=\"text-align:left\"><li>deepspeed</li></div> | <div style=\"text-align:left\"><li>single card</li><li>deepspeed</li></div> | <li>[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)</li><li>[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)</li> |\n| gpt-neox | <div style=\"text-align:left\"><li>deepspeed</li></div> | <div style=\"text-align:left\"><li>deepspeed</li></div> | <li>[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)</li><li>[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)</li> |\n| opt | :x: | <div style=\"text-align:left\"><li>deepspeed</li></div> | <li>[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)</li> |\n| llama 2 / codellama | <div style=\"text-align:left\"><li>deepspeed</li><li>lora</li></div> | :heavy_check_mark: | <li>[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)</li><li>[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)</li> |\n| stablelm | :x: | <div style=\"text-align:left\"><li>single card</li></div> | <li>[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)</li> |\n| falcon | <div style=\"text-align:left\"><li>lora</li></div> | :heavy_check_mark: | <li>[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)</li> |\n| codegen | :x: | <div style=\"text-align:left\"><li>single card</li></div> | <li>[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)</li> |\n| mpt | :x: | <div style=\"text-align:left\"><li>single card</li></div> | <li>[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)</li> |\n| mistral | :x: | <div style=\"text-align:left\"><li>single card</li></div> | <li>[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)</li> |\n| t5 | :heavy_check_mark: | :heavy_check_mark: | <li>[summarization](https://github.com/huggingface/optimum-habana/tree/main/examples/summarization)</li><li>[translation](https://github.com/huggingface/optimum-habana/tree/main/examples/translation)</li><li>[question answering](https://github.com/huggingface/optimum-habana/tree/main/examples/question-answering#fine-tuning-t5-on-squad20)</li> |\n| bart | :x: | <div style=\"text-align:left\"><li>single card</li></div> | <li>[summarization](https://github.com/huggingface/optimum-habana/tree/main/examples/summarization)</li><li>[translation](https://github.com/huggingface/optimum-habana/tree/main/examples/translation)</li><li>[question answering](https://github.com/huggingface/optimum-habana/tree/main/examples/question-answering#fine-tuning-t5-on-squad20)</li> |\n| vit | :heavy_check_mark: | :heavy_check_mark: | <li>[image classification](https://github.com/huggingface/optimum-habana/tree/main/examples/image-classification)</li> |\n| swin | :heavy_check_mark: | :heavy_check_mark: | <li>[image classification](https://github.com/huggingface/optimum-habana/tree/main/examples/image-classification)</li> |\n| wav2vec2 | :heavy_check_mark: | :heavy_check_mark: | <li>[audio classification](https://github.com/huggingface/optimum-habana/tree/main/examples/audio-classification)</li><li>[speech recognition](https://github.com/huggingface/optimum-habana/tree/main/examples/speech-recognition)</li> |\n| clip | :heavy_check_mark: | :heavy_check_mark: | <li>[contrastive image-text training](https://github.com/huggingface/optimum-habana/tree/main/examples/contrastive-image-text)</li> |\n| bridgetower | :heavy_check_mark: | :heavy_check_mark: | <li>[contrastive image-text training](https://github.com/huggingface/optimum-habana/tree/main/examples/contrastive-image-text)</li> |\n| esmfold | :x: | <div style=\"text-align:left\"><li>single card</li></div> | <li>[protein folding](https://github.com/huggingface/optimum-habana/tree/main/examples/protein-folding)</li> |\n\n</div>\n\n- diffusers:\n\n<div align=\"center\">\n\n| architecture     | training | inference            | tasks |\n|------------------|:--------:|:--------------------:|:-----:|\n| stable diffusion | :x:      | <li>single card</li> | <li>[text-to-image generation](https://github.com/huggingface/optimum-habana/tree/main/examples/stable-diffusion)</li> |\n| ldm3d            | :x:      | <li>single card</li> | <li>[text-to-image generation](https://github.com/huggingface/optimum-habana/tree/main/examples/stable-diffusion)</li> |\n\n</div>\n\nother models and tasks supported by the \ud83e\udd17 transformers and \ud83e\udd17 diffusers library may also work. you can refer to this [section](https://github.com/huggingface/optimum-habana#how-to-use-it) for using them with \ud83e\udd17 optimum habana. besides, [this page](https://github.com/huggingface/optimum-habana/tree/main/examples) explains how to modify any [example](https://github.com/huggingface/transformers/tree/main/examples/pytorch) from the \ud83e\udd17 transformers library to make it work with \ud83e\udd17 optimum habana.\n\nif you find any issues while using those, please open an issue or a pull request.\n\n\n## gaudi setup\n\nplease refer to habana gaudi's official [installation guide](https://docs.habana.ai/en/latest/installation_guide/index.html).\n\n> tests should be run in a docker container based on habana docker images.\n>\n> the current version has been validated for synapseai 1.11.\n\n\n## development\n\ncheck the [contributor guide](https://github.com/huggingface/optimum/blob/main/contributing.md) for instructions.\n",
  "docs_url": null,
  "keywords": "transformers,diffusers,mixed-precision training,fine-tuning,gaudi,hpu",
  "license": "apache",
  "name": "optimum-habana",
  "package_url": "https://pypi.org/project/optimum-habana/",
  "project_url": "https://pypi.org/project/optimum-habana/",
  "project_urls": {
    "Homepage": "https://huggingface.co/hardware/habana"
  },
  "release_url": "https://pypi.org/project/optimum-habana/1.9.0/",
  "requires_dist": [
    "transformers (<4.35.0,>=4.34.0)",
    "optimum",
    "torch",
    "accelerate (>=0.23.0)",
    "diffusers (<0.24.0,>=0.18.0)",
    "ruff ; extra == 'quality'",
    "hf-doc-builder ; extra == 'quality'",
    "pytest ; extra == 'tests'",
    "psutil ; extra == 'tests'",
    "parameterized ; extra == 'tests'",
    "GitPython ; extra == 'tests'",
    "optuna ; extra == 'tests'",
    "sentencepiece ; extra == 'tests'",
    "datasets ; extra == 'tests'",
    "safetensors ; extra == 'tests'"
  ],
  "requires_python": "",
  "summary": "optimum habana is the interface between the hugging face transformers and diffusers libraries and habana's gaudi processor (hpu). it provides a set of tools enabling easy model loading, training and inference on single- and multi-hpu settings for different downstream tasks.",
  "version": "1.9.0",
  "releases": [],
  "developers": [
    "hardware@huggingface.co",
    "huggingface_inc"
  ],
  "kwds": "use_habana apache software licensed documentation",
  "license_kwds": "apache",
  "libtype": "pypi",
  "id": "pypi_optimum_habana",
  "homepage": "https://huggingface.co/hardware/habana",
  "release_count": 29,
  "dependency_ids": [
    "pypi_accelerate",
    "pypi_datasets",
    "pypi_diffusers",
    "pypi_gitpython",
    "pypi_hf_doc_builder",
    "pypi_optimum",
    "pypi_optuna",
    "pypi_parameterized",
    "pypi_psutil",
    "pypi_pytest",
    "pypi_ruff",
    "pypi_safetensors",
    "pypi_sentencepiece",
    "pypi_torch",
    "pypi_transformers"
  ]
}