{
  "classifiers": [],
  "description": "![nattenlogo](https://www.shi-labs.com/natten/assets/img/natten_light.png) \n\n<a href=\"https://www.shi-labs.com/natten/\"><img src=\"https://img.shields.io/badge/pip%20install%20natten-read%20more-%23c209c1\" /></a>\n\n*neighborhood attention extension*\n\nbringing attention to a neighborhood near you!\n\nnatten is an extension to pytorch, which provides the first fast sliding window attention with efficient cuda kernels. \nit provides <a href=\"https://arxiv.org/abs/2204.07143\">neighborhood attention</a> (local attention)\nand <a href=\"https://arxiv.org/abs/2209.15001\">dilated neighborhood attention</a> \n(sparse global attention, a.k.a. dilated local attention) as pytorch modules for both 1d and 2d data. \n\n## about natten\nsliding window self attention mechanisms have been relatively overlooked, in part due to implementation difficulties.\nfor example, in a paper proposing one of the earliest examples of such methods, \n[sasa](https://proceedings.neurips.cc/paper/2019/file/3416a75f4cea9109507cacd8e2f2aefc-paper.pdf), \nit was noted that\nalthough such methods are theoretically efficient, they're relatively slow in practice, compared to convolutions, \nwhich have been implemented in most well-known deep learning libraries.\n\nthat is why we started developing natten, an extension to existing libraries with efficient implementations of sliding window\nattention mechanisms, which will enable research in this direction including building powerful hierarchical vision\ntransformers.\n\nfor more information, we highly recommend reading our preprints [nat](https://arxiv.org/abs/2204.07143) and\n[dinat](https://arxiv.org/abs/2209.15001), and check out their [repository](https://github.com/shi-labs/neighborhood-attention-transformer).\n\n### how fast is natten?\nthe latest version of natten runs pretty fast on ampere with the latest torch and cuda versions.\n\n![timeplot](https://www.shi-labs.com/natten/assets/img/cudatime_light.png)\n![memplot](https://www.shi-labs.com/natten/assets/img/cudamemory_light.png)\n\n\n## requirements\nnatten supports pytorch version 1.8 and later, and python versions 3.7, 3.8, 3.9, 3.10(only torch >= 1.11), and 3.11 (only torch >= 1.13).\n\n**note:** the current version of natten comes with linux-only wheels, and supports pascal and above (`sm >= 60`, i.e. tesla p100).\nmake sure your gpu is supported by referring to \n[this webpage](https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/).\nfuture versions will extend support to older gpus.\n\nyou may try and build from source on windows, but do so at your own risk.\nwe also welcome contributions in all forms.\n\n## getting started\n\n### linux\njust refer to our website, [shi-labs.com/natten](https://www.shi-labs.com/natten/), select your pytorch version and the cuda\nversion it was compiled with, copy-paste the command and install in seconds!\n\nfor example, if you're on `torch==2.0.0+cu118`, you should install natten using the following wheel:\n```bash\npip3 install natten -f https://shi-labs.com/natten/wheels/cu118/torch2.0.0/index.html\n```\n\nmore generally:\n```bash\npip3 install natten -f https://shi-labs.com/natten/wheels/{cu_version}/torch{torch_version}/index.html\n```\n\n**note:** if you do not specify a wheel url, pip will collect natten and try to compile on locally, which depending\non your system might take up to 30 minutes.\nwe strongly recommend using our website if you're a linux user.\n\n### mac\nunfortunately we are not yet able to build mac wheels, but you can compile on install, so just run:\n\n```bash\npip3 install natten\n```\n\n### windows\nnatten should support windows devices with cuda, but does not yet have windows wheels.\nyou can try and build natten from source (see below).\n\n### build from source\nonce you've set up your python environment and installed pytorch with cuda, simply clone and build:\n\n```bash\npip install ninja # recommended, not required\ngit clone https://github.com/shi-labs/natten\ncd natten\nmake\n```\n\n#### optional: run unit tests\nyou can optionally run unit tests to verify building from source finished successfully:\n```bash\nmake test\n```\n\n\n## catalog\n- [x] neighborhood attention 1d (cuda)\n- [x] neighborhood attention 2d (cuda)\n- [ ] neighborhood attention 3d (cuda)\n- [x] neighborhood attention 1d (cpu)\n- [x] neighborhood attention 2d (cpu)\n- [ ] neighborhood attention 3d (cpu)\n- [x] dilation support\n- [x] float16 support and utilization\n- [ ] bfloat16 support\n- [ ] kepler and maxwell (30<=sm<60) support\n- [ ] windows builds\n\n## usage\nsimply import `neighborhoodattention1d` or `neighborhoodattention2d` from `natten`:\n```python\nfrom natten import neighborhoodattention1d\nfrom natten import neighborhoodattention2d\n\nna1d = neighborhoodattention1d(dim=128, kernel_size=7, dilation=2, num_heads=4)\nna2d = neighborhoodattention2d(dim=128, kernel_size=7, dilation=2, num_heads=4)\n```\n\n### flops\nwe recommend counting flops through [fvcore](https://github.com/facebookresearch/fvcore).\n\n```shell\npip install fvcore\n```\n\nonce you have fvcore installed, you can directly use our dedicated flop counter:\n```python\nfrom natten.flops import get_flops\n\nflops = get_flops(model, input)\n```\n\nalternatively, if you are using fvcore's `flopcountanalysis` directly, be sure to add our op handles:\n```python\nfrom fvcore.nn import flopcountanalysis\nfrom natten.flops import add_natten_handle\n\n# ...\n\nflop_ctr = flopcountanalysis(model, input)\nflop_ctr = add_natten_handle(flop_ctr)\n\n# ...\n```\n\n## license\nnatten is released under the [mit license](https://github.com/shi-labs/natten/blob/main/license).\n\n## citation\n```bibtex\n@inproceedings{hassani2023neighborhood,\n\ttitle        = {neighborhood attention transformer},\n\tauthor       = {ali hassani and steven walton and jiachen li and shen li and humphrey shi},\n\tyear         = 2023,\n        booktitle    = {ieee/cvf conference on computer vision and pattern recognition (cvpr)}\n}\n@article{hassani2022dilated,\n\ttitle        = {dilated neighborhood attention transformer},\n\tauthor       = {ali hassani and humphrey shi},\n\tyear         = 2022,\n\turl          = {https://arxiv.org/abs/2209.15001},\n\teprint       = {2209.15001},\n\tarchiveprefix = {arxiv},\n\tprimaryclass = {cs.cv}\n}\n```",
  "docs_url": null,
  "keywords": "machine learning,science,ml,artificial intelligence,ai",
  "license": "",
  "name": "natten",
  "package_url": "https://pypi.org/project/natten/",
  "project_url": "https://pypi.org/project/natten/",
  "project_urls": {
    "Homepage": "https://github.com/SHI-Labs/NATTEN"
  },
  "release_url": "https://pypi.org/project/natten/0.14.6/",
  "requires_dist": [],
  "requires_python": ">=3.7",
  "summary": "neighborhood attention extension.",
  "version": "0.14.6",
  "releases": [],
  "developers": [
    "ali_hassani"
  ],
  "kwds": "attention pytorch cuda cudatime_light window",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_natten",
  "homepage": "https://github.com/shi-labs/natten",
  "release_count": 5,
  "dependency_ids": []
}