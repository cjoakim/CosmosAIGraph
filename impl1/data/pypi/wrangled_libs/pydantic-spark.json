{
  "classifiers": [
    "license :: osi approved :: mit license",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.12",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "[![python package](https://github.com/godatadriven/pydantic-spark/actions/workflows/python-package.yml/badge.svg)](https://github.com/godatadriven/pydantic-spark/actions/workflows/python-package.yml)\n[![codecov](https://codecov.io/gh/godatadriven/pydantic-spark/branch/main/graph/badge.svg?token=5l08goeraw)](https://codecov.io/gh/godatadriven/pydantic-spark)\n[![pypi version](https://badge.fury.io/py/pydantic-spark.svg)](https://badge.fury.io/py/pydantic-spark)\n[![codeql](https://github.com/godatadriven/pydantic-spark/actions/workflows/codeql-analysis.yml/badge.svg)](https://github.com/godatadriven/pydantic-spark/actions/workflows/codeql-analysis.yml)\n\n# pydantic-spark\n\nthis library can convert a pydantic class to a spark schema or generate python code from a spark schema.\n\n### install\n\n```bash\npip install pydantic-spark\n```\n\n### pydantic class to spark schema\n\n```python\nimport json\nfrom typing import optional\n\nfrom pydantic_spark.base import sparkbase\n\nclass testmodel(sparkbase):\n    key1: str\n    key2: int\n    key2: optional[str]\n\nschema_dict: dict = testmodel.spark_schema()\nprint(json.dumps(schema_dict))\n\n```\n#### coerce type\npydantic-spark provides a `coerce_type` option that allows type coercion. \nwhen applied to a field, pydantic-spark converts the column's data type to the specified coercion type. \n\n```python\nimport json\nfrom pydantic import field\nfrom pydantic_spark.base import sparkbase, coercetype\n\nclass testmodel(sparkbase):\n    key1: str = field(extra_json_schema={\"coerce_type\": coercetype.integer})\n\nschema_dict: dict = testmodel.spark_schema()\nprint(json.dumps(schema_dict))\n\n```\n\n\n### install for developers\n\n###### install package\n\n- requirement: poetry 1.*\n\n```shell\npoetry install\n```\n\n###### run unit tests\n```shell\npytest\ncoverage run -m pytest  # with coverage\n# or (depends on your local env) \npoetry run pytest\npoetry run coverage run -m pytest  # with coverage\n```\n\n##### run linting\n\nthe linting is checked in the github workflow. to fix and review issues run this:\n```shell\nblack .   # auto fix all issues\nisort .   # auto fix all issues\npflake .  # only display issues, fixing is manual\n```\n",
  "docs_url": null,
  "keywords": "pydantic,spark",
  "license": "mit",
  "name": "pydantic-spark",
  "package_url": "https://pypi.org/project/pydantic-spark/",
  "project_url": "https://pypi.org/project/pydantic-spark/",
  "project_urls": {
    "Homepage": "https://github.com/godatadriven/pydantic-spark",
    "Repository": "https://github.com/godatadriven/pydantic-spark"
  },
  "release_url": "https://pypi.org/project/pydantic-spark/1.0.1/",
  "requires_dist": [
    "pydantic (>=2.5.2,<3.0.0)",
    "pyspark (>=3.1.2,<3.3.0) ; extra == \"spark\""
  ],
  "requires_python": ">=3.8,<4.0",
  "summary": "converting pydantic classes to spark schemas",
  "version": "1.0.1",
  "releases": [],
  "developers": [
    "peter.vanthof@godatadriven.com",
    "peter_van"
  ],
  "kwds": "pydantic_spark sparkbase spark_schema spark pytest",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_pydantic_spark",
  "homepage": "https://github.com/godatadriven/pydantic-spark",
  "release_count": 6,
  "dependency_ids": [
    "pypi_pydantic",
    "pypi_pyspark"
  ]
}