{
  "classifiers": [
    "intended audience :: developers",
    "license :: osi approved :: apache software license",
    "operating system :: macos",
    "operating system :: microsoft :: windows",
    "operating system :: os independent",
    "operating system :: posix",
    "operating system :: posix :: linux",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.12",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: software development :: libraries :: python modules",
    "typing :: typed"
  ],
  "description": "# openai python api library\n\n[![pypi version](https://img.shields.io/pypi/v/openai.svg)](https://pypi.org/project/openai/)\n\nthe openai python library provides convenient access to the openai rest api from any python 3.7+\napplication. the library includes type definitions for all request params and response fields,\nand offers both synchronous and asynchronous clients powered by [httpx](https://github.com/encode/httpx).\n\nit is generated from our [openapi specification](https://github.com/openai/openai-openapi) with [stainless](https://stainlessapi.com/).\n\n## documentation\n\nthe api documentation can be found [here](https://platform.openai.com/docs).\n\n## installation\n\n> [!important]\n> the sdk was rewritten in v1, which was released november 6th 2023. see the [v1 migration guide](https://github.com/openai/openai-python/discussions/742), which includes scripts to automatically update your code.\n\n```sh\npip install openai\n```\n\n## usage\n\nthe full api of this library can be found in [api.md](https://www.github.com/openai/openai-python/blob/main/api.md).\n\n```python\nimport os\nfrom openai import openai\n\nclient = openai(\n    # this is the default and can be omitted\n    api_key=os.environ.get(\"openai_api_key\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"say this is a test\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n```\n\nwhile you can provide an `api_key` keyword argument,\nwe recommend using [python-dotenv](https://pypi.org/project/python-dotenv/)\nto add `openai_api_key=\"my api key\"` to your `.env` file\nso that your api key is not stored in source control.\n\n## async usage\n\nsimply import `asyncopenai` instead of `openai` and use `await` with each api call:\n\n```python\nimport os\nimport asyncio\nfrom openai import asyncopenai\n\nclient = asyncopenai(\n    # this is the default and can be omitted\n    api_key=os.environ.get(\"openai_api_key\"),\n)\n\n\nasync def main() -> none:\n    chat_completion = await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"say this is a test\",\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n\n\nasyncio.run(main())\n```\n\nfunctionality between the synchronous and asynchronous clients is otherwise identical.\n\n## streaming responses\n\nwe provide support for streaming responses using server side events (sse).\n\n```python\nfrom openai import openai\n\nclient = openai()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"say this is a test\"}],\n    stream=true,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n```\n\nthe async client uses the exact same interface.\n\n```python\nfrom openai import asyncopenai\n\nclient = asyncopenai()\n\n\nasync def main():\n    stream = await client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"say this is a test\"}],\n        stream=true,\n    )\n    async for chunk in stream:\n        print(chunk.choices[0].delta.content or \"\", end=\"\")\n\n\nasyncio.run(main())\n```\n\n## module-level client\n\n> [!important]\n> we highly recommend instantiating client instances instead of relying on the global client.\n\nwe also expose a global client instance that is accessible in a similar fashion to versions prior to v1.\n\n```py\nimport openai\n\n# optional; defaults to `os.environ['openai_api_key']`\nopenai.api_key = '...'\n\n# all client options can be configured just like the `openai` instantiation counterpart\nopenai.base_url = \"https://...\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"how do i output all files in a directory using python?\",\n        },\n    ],\n)\nprint(completion.choices[0].message.content)\n```\n\nthe api is the exact same as the standard client instance based api.\n\nthis is intended to be used within repls or notebooks for faster iteration, **not** in application code.\n\nwe recommend that you always instantiate a client (e.g., with `client = openai()`) in application code because:\n\n- it can be difficult to reason about where client options are configured\n- it's not possible to change certain client options without potentially causing race conditions\n- it's harder to mock for testing purposes\n- it's not possible to control cleanup of network connections\n\n## using types\n\nnested request parameters are [typeddicts](https://docs.python.org/3/library/typing.html#typing.typeddict). responses are [pydantic models](https://docs.pydantic.dev), which provide helper methods for things like:\n\n- serializing back into json, `model.model_dump_json(indent=2, exclude_unset=true)`\n- converting to a dictionary, `model.model_dump(exclude_unset=true)`\n\ntyped requests and responses provide autocomplete and documentation within your editor. if you would like to see type errors in vs code to help catch bugs earlier, set `python.analysis.typecheckingmode` to `basic`.\n\n## pagination\n\nlist methods in the openai api are paginated.\n\nthis library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:\n\n```python\nimport openai\n\nclient = openai()\n\nall_jobs = []\n# automatically fetches more pages as needed.\nfor job in client.fine_tuning.jobs.list(\n    limit=20,\n):\n    # do something with job here\n    all_jobs.append(job)\nprint(all_jobs)\n```\n\nor, asynchronously:\n\n```python\nimport asyncio\nimport openai\n\nclient = asyncopenai()\n\n\nasync def main() -> none:\n    all_jobs = []\n    # iterate through items across all pages, issuing requests as needed.\n    async for job in client.fine_tuning.jobs.list(\n        limit=20,\n    ):\n        all_jobs.append(job)\n    print(all_jobs)\n\n\nasyncio.run(main())\n```\n\nalternatively, you can use the `.has_next_page()`, `.next_page_info()`, or `.get_next_page()` methods for more granular control working with pages:\n\n```python\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\nif first_page.has_next_page():\n    print(f\"will fetch next page using these details: {first_page.next_page_info()}\")\n    next_page = await first_page.get_next_page()\n    print(f\"number of items we just fetched: {len(next_page.data)}\")\n\n# remove `await` for non-async usage.\n```\n\nor just work directly with the returned data:\n\n```python\nfirst_page = await client.fine_tuning.jobs.list(\n    limit=20,\n)\n\nprint(f\"next page cursor: {first_page.after}\")  # => \"next page cursor: ...\"\nfor job in first_page.data:\n    print(job.id)\n\n# remove `await` for non-async usage.\n```\n\n## nested params\n\nnested parameters are dictionaries, typed using `typeddict`, for example:\n\n```python\nfrom openai import openai\n\nclient = openai()\n\ncompletion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"can you generate an example json object describing a fruit?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo-1106\",\n    response_format={\"type\": \"json_object\"},\n)\n```\n\n## file uploads\n\nrequest parameters that correspond to file uploads can be passed as `bytes`, a [`pathlike`](https://docs.python.org/3/library/os.html#os.pathlike) instance or a tuple of `(filename, contents, media type)`.\n\n```python\nfrom pathlib import path\nfrom openai import openai\n\nclient = openai()\n\nclient.files.create(\n    file=path(\"input.jsonl\"),\n    purpose=\"fine-tune\",\n)\n```\n\nthe async client uses the exact same interface. if you pass a [`pathlike`](https://docs.python.org/3/library/os.html#os.pathlike) instance, the file contents will be read asynchronously automatically.\n\n## handling errors\n\nwhen the library is unable to connect to the api (for example, due to network connection problems or a timeout), a subclass of `openai.apiconnectionerror` is raised.\n\nwhen the api returns a non-success status code (that is, 4xx or 5xx\nresponse), a subclass of `openai.apistatuserror` is raised, containing `status_code` and `response` properties.\n\nall errors inherit from `openai.apierror`.\n\n```python\nimport openai\nfrom openai import openai\n\nclient = openai()\n\ntry:\n    client.fine_tunes.create(\n        training_file=\"file-xginujblhpwglsztz8cps8xy\",\n    )\nexcept openai.apiconnectionerror as e:\n    print(\"the server could not be reached\")\n    print(e.__cause__)  # an underlying exception, likely raised within httpx.\nexcept openai.ratelimiterror as e:\n    print(\"a 429 status code was received; we should back off a bit.\")\nexcept openai.apistatuserror as e:\n    print(\"another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n```\n\nerror codes are as followed:\n\n| status code | error type                 |\n| ----------- | -------------------------- |\n| 400         | `badrequesterror`          |\n| 401         | `authenticationerror`      |\n| 403         | `permissiondeniederror`    |\n| 404         | `notfounderror`            |\n| 422         | `unprocessableentityerror` |\n| 429         | `ratelimiterror`           |\n| >=500       | `internalservererror`      |\n| n/a         | `apiconnectionerror`       |\n\n### retries\n\ncertain errors are automatically retried 2 times by default, with a short exponential backoff.\nconnection errors (for example, due to a network connectivity problem), 408 request timeout, 409 conflict,\n429 rate limit, and >=500 internal errors are all retried by default.\n\nyou can use the `max_retries` option to configure or disable retry settings:\n\n```python\nfrom openai import openai\n\n# configure the default for all requests:\nclient = openai(\n    # default is 2\n    max_retries=0,\n)\n\n# or, configure per-request:\nclient.with_options(max_retries=5).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"how can i get the name of the current day in node.js?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n```\n\n### timeouts\n\nby default requests time out after 10 minutes. you can configure this with a `timeout` option,\nwhich accepts a float or an [`httpx.timeout`](https://www.python-httpx.org/advanced/#fine-tuning-the-configuration) object:\n\n```python\nfrom openai import openai\n\n# configure the default for all requests:\nclient = openai(\n    # 20 seconds (default is 10 minutes)\n    timeout=20.0,\n)\n\n# more granular control:\nclient = openai(\n    timeout=httpx.timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# override per-request:\nclient.with_options(timeout=5 * 1000).chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"how can i list all files in a directory using python?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\",\n)\n```\n\non timeout, an `apitimeouterror` is thrown.\n\nnote that requests that time out are [retried twice by default](#retries).\n\n## advanced\n\n### logging\n\nwe use the standard library [`logging`](https://docs.python.org/3/library/logging.html) module.\n\nyou can enable logging by setting the environment variable `openai_log` to `debug`.\n\n```shell\n$ export openai_log=debug\n```\n\n### how to tell whether `none` means `null` or missing\n\nin an api response, a field may be explicitly `null`, or missing entirely; in either case, its value is `none` in this library. you can differentiate the two cases with `.model_fields_set`:\n\n```py\nif response.my_field is none:\n  if 'my_field' not in response.model_fields_set:\n    print('got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('got json like {\"my_field\": null}.')\n```\n\n### accessing raw response data (e.g. headers)\n\nthe \"raw\" response object can be accessed by prefixing `.with_raw_response.` to any http method call.\n\n```py\nfrom openai import openai\n\nclient = openai()\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"say this is a test\",\n    }],\n    model=\"gpt-3.5-turbo\",\n)\nprint(response.headers.get('x-my-header'))\n\ncompletion = response.parse()  # get the object that `chat.completions.create()` would have returned\nprint(completion)\n```\n\nthese methods return an [`apiresponse`](https://github.com/openai/openai-python/tree/main/src/openai/_response.py) object.\n\n### configuring the http client\n\nyou can directly override the [httpx client](https://www.python-httpx.org/api/#client) to customize it for your use case, including:\n\n- support for proxies\n- custom transports\n- additional [advanced](https://www.python-httpx.org/advanced/#client-instances) functionality\n\n```python\nimport httpx\nfrom openai import openai\n\nclient = openai(\n    # or use the `openai_base_url` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.httptransport(local_address=\"0.0.0.0\"),\n    ),\n)\n```\n\n### managing http resources\n\nby default the library closes underlying http connections whenever the client is [garbage collected](https://docs.python.org/3/reference/datamodel.html#object.__del__). you can manually close the client using the `.close()` method if desired, or with a context manager that closes when exiting.\n\n## microsoft azure openai\n\nto use this library with [azure openai](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview), use the `azureopenai`\nclass instead of the `openai` class.\n\n> [!important]\n> the azure api shape differs from the core api shape which means that the static types for responses / params\n> won't always be correct.\n\n```py\nfrom openai import azureopenai\n\n# gets the api key from environment variable azure_openai_api_key\nclient = azureopenai(\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n    api_version=\"2023-07-01-preview\",\n    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deployment-name\",  # e.g. gpt-35-instant\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"how do i output all files in a directory using python?\",\n        },\n    ],\n)\nprint(completion.model_dump_json(indent=2))\n```\n\nin addition to the options provided in the base `openai` client, the following options are provided:\n\n- `azure_endpoint` (or the `azure_openai_endpoint` environment variable)\n- `azure_deployment`\n- `api_version` (or the `openai_api_version` environment variable)\n- `azure_ad_token` (or the `azure_openai_ad_token` environment variable)\n- `azure_ad_token_provider`\n\nan example of using the client with azure active directory can be found [here](https://github.com/openai/openai-python/blob/main/examples/azure_ad.py).\n\n## versioning\n\nthis package generally follows [semver](https://semver.org/spec/v2.0.0.html) conventions, though certain backwards-incompatible changes may be released as minor versions:\n\n1. changes that only affect static types, without breaking runtime behavior.\n2. changes to library internals which are technically public but not intended or documented for external use. _(please open a github issue to let us know if you are relying on such internals)_.\n3. changes that we do not expect to impact the vast majority of users in practice.\n\nwe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nwe are keen for your feedback; please open an [issue](https://www.github.com/openai/openai-python/issues) with questions, bugs, or suggestions.\n\n## requirements\n\npython 3.7 or higher.\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "openai",
  "package_url": "https://pypi.org/project/openai/",
  "project_url": "https://pypi.org/project/openai/",
  "project_urls": {
    "Homepage": "https://github.com/openai/openai-python",
    "Repository": "https://github.com/openai/openai-python"
  },
  "release_url": "https://pypi.org/project/openai/1.6.1/",
  "requires_dist": [
    "anyio<5,>=3.5.0",
    "distro<2,>=1.7.0",
    "httpx<1,>=0.23.0",
    "pydantic<3,>=1.9.0",
    "sniffio",
    "tqdm>4",
    "typing-extensions<5,>=4.7",
    "numpy>=1; extra == 'datalib'",
    "pandas-stubs>=1.1.0.11; extra == 'datalib'",
    "pandas>=1.2.3; extra == 'datalib'"
  ],
  "requires_python": ">=3.7.1",
  "summary": "the official python library for the openai api",
  "version": "1.6.1",
  "releases": [],
  "developers": [
    "support@openai.com"
  ],
  "kwds": "openai_api_version openapi openai_api_key openai azure_openai_endpoint",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_openai",
  "homepage": "",
  "release_count": 102,
  "dependency_ids": [
    "pypi_anyio",
    "pypi_distro",
    "pypi_httpx",
    "pypi_numpy",
    "pypi_pandas",
    "pypi_pandas_stubs",
    "pypi_pydantic",
    "pypi_sniffio",
    "pypi_tqdm",
    "pypi_typing_extensions"
  ]
}