{
  "classifiers": [
    "license :: osi approved :: mit license",
    "operating system :: os independent",
    "programming language :: python :: 3"
  ],
  "description": "===================================================\nstrelok\n===================================================\n\n.. image:: https://badge.fury.io/py/strelok.svg\n    :target: https://badge.fury.io/py/strelok\n\n\n.. image:: https://static.pepy.tech/badge/strelok\n   :target: https://pepy.tech/project/strelok\n\n.. image:: https://static.pepy.tech/badge/strelok/week\n   :target: https://pepy.tech/project/strelok\n\n.. image:: https://static.pepy.tech/badge/strelok/month\n   :target: https://pepy.tech/project/strelok\n\noverview\n========\n\nthe strelok library is a tool that offers support in the feature engineering process of machine learning projects. it aids in generating new features, dealing with missing values, creating interaction features, and executing feature selection. with strelok, feature engineering can become a more streamlined process, contributing to shortened development times and potentially improved model performance.\n\ninstallation\n============\n\nto install strelok, you can use `pip`:\n\n.. code-block:: bash\n\n   pip install strelok\n\nfeatures\n========\n\nthe strelok library offers the following key features:\n\n1. mathematical transformations: generate new features by applying various mathematical transformations, such as logarithmic, exponential, square root, and more.\n\n2. missing value imputation: fill missing values in your dataset using strategies like mean, median, mode, constant, forward fill, backward fill, interpolation, or knn imputation.\n\n3. interaction feature generation: create new features by combining existing features through operations like multiplication, addition, subtraction, and division.\n\n4. feature selection: select the most relevant features from your dataset using methods like univariate selection, recursive feature elimination (rfe), l1 regularization (lasso), random forest importance, or correlation-based selection.\n\nmethod details\n==============\n\nbefore diving into examples of how to use the strelok library, let's understand some of the core classes and their input parameters.\n\npipeline\n~~~~~~~~\nthis is the main class you'll interact with when using strelok. it orchestrates the entire feature engineering process.\n\n- `target_col` (string): the name of the column in your dataset which you want to target with mathematical transformation or missingvalueimputation.\n\nmethods:\n\n- `add_feature(feature)`: adds a feature object (e.g., an instance of `mathematicaltransformationfeature`, `missingvalueimputation`, or `interactionfeature`) to the pipeline. the pipeline will process these features in the order they were added.\n\n- `set_feature_selector(selector, not_x_col, y_col)`: sets a feature selection method for the pipeline. only one feature selection method can be active at a time. the `selector` should be an instance of `featureselection`.\n   - `feature_selector` (featureselection): an instance of the featureselection class representing the feature selection method.\n   - `not_x_col` (list of strings, optional): a list of column names that should not be included as input features for feature selection. by default, it is an empty list. if y_col is provided, it is automatically appended to the not_x_col list to prevent the target column from being selected as the most relevant feature during feature selection. this ensures that the target column is excluded from the set of input features considered for selection.\n   - `y_col` (list of strings): a list of column names representing the target variable. default is an empty list.\n\n- `generate_features(data)`: applies all added features and the feature selector (if any) to the provided dataframe `data`, and returns a new dataframe with the engineered features. \n\n\n\nmathematicaltransformation\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nthis class performs mathematical transformations on a feature. \n\n- `name` (string): the name of the new feature. this name will be used to represent the transformed feature in the output dataframe.\n- `transformation_type` (string): the type of mathematical transformation to apply. supported values are:\n    - 'logarithm': applies the natural logarithm transformation. the input column should only contain positive numbers.\n    - 'square_root': applies the square root transformation. the input column should only contain non-negative numbers.\n    - 'exponential': applies the exponential transformation. optional parameter 'power' (int or float) can be provided to specify the power of the transformation. default is 1.\n    - 'box_cox': applies the box-cox transformation. the box-cox transformation requires the input column to contain positive values.\n    - 'reciprocal': applies the reciprocal transformation. the input column should contain non-zero values.\n    - 'power': applies the power transformation. optional parameter 'power' (int or float) can be provided to specify the power of the transformation. default is 2.\n    - 'binning': applies binning to the input column. optional parameter 'num_bins' (int) can be provided to specify the number of bins. default is 10.\n    - 'standardization': applies standardization to the input column. optional parameters 'mean' (float) and 'std' (float) can be provided to specify the mean and standard deviation for standardization. by default, the mean and standard deviation are calculated from the input column.\n    - 'rank': computes the rank of the values in the input column.\n    - 'difference': computes the difference between the values in the input column and another feature specified by the 'other_feature' parameter.\n    - 'relative_difference': computes the relative difference between the values in the input column and a specified 'other_value'.\n    - 'sin': applies the sine transformation to the values in the input column.\n    - 'cos': applies the cosine transformation to the values in the input column.\n    - 'mod_tan': applies the modified tangent transformation to the values in the input column. it computes the tangent of the values as the ratio of the sine to the cosine of the values, with a small constant added to the denominator to prevent division by zero.\n\n- `diff_col` (string, optional): the name of the existing column to be transformed, if not defined the column default to target_col in `pipeline`\n- `kwargs` (dictionary, optional): additional parameters for specific transformation types.\n\nin addition to the common inputs mentioned earlier, some mathematical transformations in the `mathematicaltransformation` class require additional parameters:\n\n- 'exponential' transformation:\n\n  - `power` (int or float, optional): the power of the exponential transformation. default is 1.\n\n- 'power' transformation:\n\n  - `power` (int or float, optional): the power of the power transformation. default is 2.\n\n- 'binning' transformation:\n\n  - `num_bins` (int, optional): the number of bins for binning. default is 10.\n\n- 'standardization' transformation:\n\n  - `mean` (float, optional): the mean value for standardization. if not provided, the mean is calculated from the input column.\n  - `std` (float, optional): the standard deviation for standardization. if not provided, the standard deviation is calculated from the input column.\n\n- 'difference' transformation:\n\n  - `other_feature` (string): the name of the other feature to compute the difference with.\n\n- 'relative_difference' transformation:\n\n  - `other_value` (float): the value to compute the relative difference with.\n\nmissingvalueimputation\n~~~~~~~~~~~~~~~~~~~~~~\n\nthis class imputes missing values in a feature.\n\n- `name` (string): the name of the new feature. this name will be used to represent the imputed feature in the output dataframe.\n- `imputation_strategy` (string): the imputation strategy. supported values are:\n    - 'mean': replaces missing values with the mean value of the non-missing values in the column. suitable for numeric columns.\n    - 'median': replaces missing values with the median value of the non-missing values in the column. suitable for numeric columns.\n    - 'mode': replaces missing values with the most frequent value in the column. suitable for both numeric and categorical columns.\n    - 'constant': replaces missing values with a constant value (0).\n    - 'forward_fill': fills missing values with the previous non-missing value in the column (forward fill).\n    - 'backward_fill': fills missing values with the next non-missing value in the column (backward fill).\n    - 'interpolation': performs linear interpolation to fill missing values.\n    - 'knn': performs k-nearest neighbors imputation using the specified number of neighbors.\n    - 'multiple': performs multiple imputation using an iterative imputer.\n    - 'missing_indicator': creates a binary indicator column that flags missing values.\n\n- `diff_col` (string, optional): the name of the existing column to be transformed. if not defined, the column defaults to the `target_col` in the `pipeline`.\n\nin addition to the common inputs mentioned earlier, some imputation strategies in the `missingvalueimputation` class require additional parameters:\n\n- `knn` strategy:\n    - `n_neighbors` (int): the number of nearest neighbors to consider when performing k-nearest neighbors imputation.\n\n- `multiple` strategy:\n    - no additional inputs are required. the `max_iter` and `random_state` parameters are set to default values.\n\ninteractionfeature\n~~~~~~~~~~~~~~~~~~\nthis class creates a new feature that is the interaction of two or more features.\n\n- `name` (string): the name of the new feature. this name will be used to represent the interaction feature in the output dataframe.\n- `interaction_type` (string): the type of interaction. supported values are:\n    - 'addition': adds the values in the specified columns.\n    - 'subtraction': subtracts the values in the second column from the first. only two columns are allowed in this case.\n    - 'multiplication': multiplies the values in the specified columns.\n    - 'division': divides the values in the first column by those in the second. only two columns are allowed in this case, and the second column should not contain zero values.\n- `columns` (list of strings): the names of the existing columns to be interacted. the list should contain at least two column names.\n\nfeature selection\n~~~~~~~~~~~~~~~~~\n\nthis class selects top 'k' features based on a selection method.\n\n- `method` (string): the feature selection method. supported values are:\n    - 'univariate': selects features based on statistical tests.\n    - 'rfe': selects features using recursive feature elimination.\n    - 'lasso': selects features based on l1 regularization using lasso.\n    - 'random_forest': selects features based on their importance in a trained random forest model.\n    - 'pearson_correlation': selects features based on pearson correlation with the target.\n    - 'spearman_correlation': selects features based on spearman correlation with the target.\n    - 'box_cox': selects features based on box-cox transformation.\n\n- `k` (integer): the number of features to select.\n\nin addition to the common inputs mentioned earlier, some feature selection methods in the `featureselection` class require additional parameters:\n\n- `correlation` methods (inlcudes `pearson` and `spearman`):\n    - `correlation_threshold` (float): the threshold for selecting features based on their correlation with the target. only features with a correlation above this threshold will be selected. hence `k` is not required\n\n- `box_cox` method:\n    - `box_cox_threshold` (float): the threshold for selecting features based on their skewness using box-cox transformation. only features with a skewness above this threshold will be selected.\n\nusage examples\n==============\n\n\nmathematical transformations\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code-block:: python\n\n   import pandas as pd\n   from strelok import feat\n\n   df = pd.dataframe({'feature1': [1, 2, 3, 10], 'feature2': [2, 3, 4, 5], 'feature3': [1, 1, 1, 0], 'target': [0, 0, 0, 1]})\n   pipeline = feat.pipeline(target_col = 'feature1')\n\n   log_feature = feat.mathematicaltransformation(name='logarithm_of_feature1', transformation_type='logarithm', diff_col='feature2') #diff_col not required, if left undefined target_col will be used\n\n   pipeline.add_feature(log_feature)\n\n   df_new = pipeline.generate_features(data=df)\n\nmissing value imputation\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code-block:: python\n\n   import pandas as pd\n   import numpy as np\n   from strelok import feat\n\n   df = pd.dataframe({'feature1': [1, np.nan, 3, 10], 'feature2': [2, 3, 4, 5], 'feature3': [1, 1, 1, 0], 'target': [0, 0, 0, 1]})\n\n   pipeline = feat.pipeline(target_col = 'feature1')\n   \n   pipeline.add_feature(feat.missingvalueimputationfeature(name='feature1', imputation_strategy='mean'))\n\n   df_new = pipeline.generate_features(data=df)\n\ninteraction feature generation\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code-block:: python\n\n   import pandas as pd\n   from strelok import feat\n\n   df = pd.dataframe({'feature1': [1, 2, 3, 10], 'feature2': [2, 3, 4, 5], 'feature3': [1, 1, 1, 0], 'target': [0, 0, 0, 1]})\n   pipeline = feat.pipeline(target_col = 'feature1')\n   \n   pipeline.add_feature(feat.interactionfeature(method = 'add', columns=['feature1', 'feature2']))\n\n   pipeline.generate_features(data=df)\n\nfeature selection\n~~~~~~~~~~~~~~~~~\n\n.. code-block:: python\n\n   import pandas as pd\n   from strelok import feat\n\n   df = pd.dataframe({'feature1': [1, 2, 3, 10], 'feature2': [2, 3, 4, 5], 'feature3': [1, 1, 1, 0], 'target': [0, 0, 0, 1]})\n   pipeline = feat.pipeline(target_col = 'feature1')\n   \n   pipeline.set_feature_selector(feat.featureselection(method='univariate', k=2), not_x_col=[], y_col=['target'])\n\n   pipeline.generate_features(data=df)\n\ncomplete example pipeline\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code-block:: python\n\n   import pandas as pd\n   from strelok import feat\n\n   df = pd.dataframe({'feature1': [1, np.nan, 3, 4],\n                     'feature2': [5, 6, 7, 8],\n                     'target': [0, 1, 0, 1]})\n\n   pipeline = feat.pipeline(target_col='feature1')\n\n   # add features to the pipeline\n   pipeline.add_feature(feat.missingvalueimputationfeature(name='feature1', imputation_strategy='mean'))\n   pipeline.add_feature(feat.mathematicaltransformationfeature(name='squared', transformation_type='power', power=2))\n   pipeline.add_feature(feat.interactionfeature(method = 'add', columns=['feature1', 'feature2', 'squared']))\n   pipeline.set_feature_selector(feat.featureselection(method='univariate', k=3), not_x_col=[], y_col=['target'])\n\n\n\n   # generate features on the dataset\n   processed_data = pipeline.generate_features(data=df)\n\n   # print the processed data\n   print(processed_data)\n\n",
  "docs_url": null,
  "keywords": "strelok,feature,selection,machine learning",
  "license": "",
  "name": "strelok",
  "package_url": "https://pypi.org/project/strelok/",
  "project_url": "https://pypi.org/project/strelok/",
  "project_urls": null,
  "release_url": "https://pypi.org/project/strelok/0.0.7/",
  "requires_dist": [],
  "requires_python": ">=3.6",
  "summary": "strelok is a simple python package that provides feat, a feature engineering automation toolkit. with a focus on simplicity, it offers user definable pipelines to streamline the feature engineering process and improve the performance of machine learning models",
  "version": "0.0.7",
  "releases": [],
  "developers": [
    "julius.riel@icloud.com",
    "julius_riel"
  ],
  "kwds": "strelok badge feature3 feature_selector selection",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_strelok",
  "homepage": "",
  "release_count": 7,
  "dependency_ids": []
}