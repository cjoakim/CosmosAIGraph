{
  "classifiers": [
    "development status :: 3 - alpha",
    "environment :: console",
    "intended audience :: developers",
    "license :: osi approved :: apache software license",
    "natural language :: english",
    "operating system :: os independent",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: scientific/engineering :: information analysis"
  ],
  "description": "# lightning \u26a1 intel habana\n\n[![lightning](https://img.shields.io/badge/-lightning_2.0+-792ee5?logo=pytorchlightning&logocolor=white)](https://lightning.ai/)\n[![pypi status](https://badge.fury.io/py/lightning-habana.svg)](https://badge.fury.io/py/lightning-habana)\n[![pypi - python version](https://img.shields.io/pypi/pyversions/lightning-habana)](https://pypi.org/project/lightning-habana/)\n[![pypi - downloads](https://img.shields.io/pypi/dm/lightning-habana)](https://pepy.tech/project/lightning-habana)\n[![deploy docs](https://github.com/lightning-ai/lightning-habana/actions/workflows/docs-deploy.yml/badge.svg)](https://lightning-ai.github.io/lightning-habana/)\n\n[![general checks](https://github.com/lightning-ai/lightning-habana/actions/workflows/ci-checks.yml/badge.svg?event=push)](https://github.com/lightning-ai/lightning-habana/actions/workflows/ci-checks.yml)\n[![build status](https://dev.azure.com/lightning-ai/compatibility/_apis/build/status/lightning-ai.lightning-habana?branchname=main)](https://dev.azure.com/lightning-ai/compatibility/_build/latest?definitionid=45&branchname=main)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/lightning-ai/lightning-habana/main.svg)](https://results.pre-commit.ci/latest/github/lightning-ai/lightning-habana/main)\n\n[habana\u00ae gaudi\u00ae ai processor (hpu)](https://habana.ai/) training processors are built on a heterogeneous architecture with a cluster of fully programmable tensor processing cores (tpc) along with its associated development tools and libraries, and a configurable matrix math engine.\n\nthe tpc core is a vliw simd processor with an instruction set and hardware tailored to serve training workloads efficiently.\nthe gaudi memory architecture includes on-die sram and local memories in each tpc and,\ngaudi is the first dl training processor that has integrated rdma over converged ethernet (roce v2) engines on-chip.\n\non the software side, the pytorch habana bridge interfaces between the framework and synapseai software stack to enable the execution of deep learning models on the habana gaudi device.\n\ngaudi provides a significant cost-effective benefit, allowing you to engage in more deep learning training while minimizing expenses.\n\nfor more information, check out [gaudi architecture](https://docs.habana.ai/en/latest/gaudi_overview/gaudi_overview.html) and [gaudi developer docs](https://developer.habana.ai).\n\n______________________________________________________________________\n\n## installing lighting habana\n\nto install lightning habana, run the following command:\n\n```bash\npip install -u lightning lightning-habana\n```\n\n______________________________________________________________________\n\n**note**\n\nensure either of lightning or pytorch-lightning is used when working with the plugin.\nmixing strategies, plugins etc from both packages is not yet validated.\n\n______________________________________________________________________\n\n## using pytorch lighting with hpu\n\nto enable pytorch lightning with hpu accelerator, provide `accelerator=hpuaccelerator()` parameter to the trainer class.\n\n```python\nfrom lightning import trainer\nfrom lightning_habana.pytorch.accelerator import hpuaccelerator\n\n# run on one hpu.\ntrainer = trainer(accelerator=hpuaccelerator(), devices=1)\n# run on multiple hpus.\ntrainer = trainer(accelerator=hpuaccelerator(), devices=8)\n# choose the number of devices automatically.\ntrainer = trainer(accelerator=hpuaccelerator(), devices=\"auto\")\n```\n\nthe `devices=1` parameter with hpus enables the habana accelerator for single card training using `singlehpustrategy`.\n\nthe `devices>1` parameter with hpus enables the habana accelerator for distributed training. it uses `hpuparallelstrategy` which is based on ddp strategy with the integration of habana\u2019s collective communication library (hccl) to support scale-up within a node and scale-out across multiple nodes.\n\n# support matrix\n\n| **synapseai**         | **1.13.0**                                          |\n| --------------------- | --------------------------------------------------- |\n| pytorch               | 2.1.0                                               |\n| (pytorch) lightning\\* | 2.1.x                                               |\n| **lightning habana**  | **1.3.0**                                           |\n| deepspeed\\*\\*         | forked from v0.10.3 of the official deepspeed repo. |\n\n\\* covers both packages [`lightning`](https://pypi.org/project/lightning/) and [`pytorch-lightning`](https://pypi.org/project/pytorch-lightning/)\n\nfor more information, check out [hpu support matrix](https://docs.habana.ai/en/latest/support_matrix/support_matrix.html)\n",
  "docs_url": null,
  "keywords": "deep learning,pytorch,ai",
  "license": "apache-2.0",
  "name": "lightning-habana",
  "package_url": "https://pypi.org/project/lightning-habana/",
  "project_url": "https://pypi.org/project/lightning-habana/",
  "project_urls": {
    "Bug Tracker": "https://github.com/Lightning-AI/lightning-habana/issues",
    "Documentation": "https://lightning-habana.rtfd.io/en/latest/",
    "Download": "https://github.com/Lightning-AI/lightning-habana",
    "Homepage": "https://github.com/Lightning-AI/lightning-habana",
    "Source Code": "https://github.com/Lightning-AI/lightning-habana"
  },
  "release_url": "https://pypi.org/project/lightning-habana/1.3.0/",
  "requires_dist": [
    "lightning-utilities >=0.7.0",
    "lightning-utilities >=0.7.0 ; extra == 'base'",
    "jsonargparse[signatures] ; extra == 'examples'",
    "lightning >=2.1.0 ; extra == 'lightning'",
    "pytorch-lightning >=2.1.0 ; extra == 'pytorch-lightning'",
    "mypy ==1.7.1 ; extra == 'typing'",
    "pytorch-lightning ==2.1.2 ; extra == 'typing'"
  ],
  "requires_python": ">=3.8",
  "summary": "lightning support for intel habana accelerators",
  "version": "1.3.0",
  "releases": [],
  "developers": [
    "lightning",
    "name@lightning.ai"
  ],
  "kwds": "lightning_habana pytorchlightning lightning_2 pytorch lightning",
  "license_kwds": "apache-2.0",
  "libtype": "pypi",
  "id": "pypi_lightning_habana",
  "homepage": "https://github.com/lightning-ai/lightning-habana",
  "release_count": 11,
  "dependency_ids": [
    "pypi_jsonargparse",
    "pypi_lightning",
    "pypi_lightning_utilities",
    "pypi_mypy",
    "pypi_pytorch_lightning"
  ]
}