{
  "classifiers": [
    "license :: osi approved :: mit license",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "# \ud83c\udf3f konoha: simple wrapper of japanese tokenizers\n\n[![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/himkt/konoha/blob/main/example/konoha_example.ipynb)\n<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/5164000/120913279-e7d62380-c6d0-11eb-8d17-6571277cdf27.gif\" width=\"95%\"></p>\n\n[![github stars](https://img.shields.io/github/stars/himkt/konoha?style=social)](https://github.com/himkt/konoha/stargazers)\n\n[![downloads](https://pepy.tech/badge/konoha)](https://pepy.tech/project/konoha)\n[![downloads](https://pepy.tech/badge/konoha/month)](https://pepy.tech/project/konoha/month)\n[![downloads](https://pepy.tech/badge/konoha/week)](https://pepy.tech/project/konoha/week)\n\n[![build status](https://github.com/himkt/konoha/workflows/python%20package/badge.svg?style=flat-square)](https://github.com/himkt/konoha/actions)\n[![documentation status](https://readthedocs.org/projects/konoha/badge/?version=latest)](https://konoha.readthedocs.io/en/latest/?badge=latest)\n![python](https://img.shields.io/badge/python-3.6%20%7c%203.7%20%7c%203.8-blue?logo=python)\n[![pypi](https://img.shields.io/pypi/v/konoha.svg)](https://pypi.python.org/pypi/konoha)\n[![github issues](https://img.shields.io/github/issues/himkt/konoha.svg?cacheseconds=60&color=yellow)](https://github.com/himkt/konoha/issues)\n[![github pull requests](https://img.shields.io/github/issues-pr/himkt/konoha.svg?cacheseconds=60&color=yellow)](https://github.com/himkt/konoha/issues)\n\n`konoha` is a python library for providing easy-to-use integrated interface of various japanese tokenizers,\nwhich enables you to switch a tokenizer and boost your pre-processing.\n\n## supported tokenizers\n\n<a href=\"https://github.com/buruzaemon/natto-py\"><img src=\"https://img.shields.io/badge/mecab-natto--py-ff69b4\"></a>\n<a href=\"https://github.com/chezou/mykytea-python\"><img src=\"https://img.shields.io/badge/kytea-mykytea--python-ff69b4\"></a>\n<a href=\"https://github.com/mocobeta/janome\"><img src=\"https://img.shields.io/badge/janome-janome-ff69b4\"></a>\n<a href=\"https://github.com/worksapplications/sudachipy\"><img src=\"https://img.shields.io/badge/sudachi-sudachipy-ff69b4\"></a>\n<a href=\"https://github.com/google/sentencepiece\"><img src=\"https://img.shields.io/badge/sentencepiece-sentencepiece-ff69b4\"></a>\n<a href=\"https://github.com/taishi-i/nagisa\"><img src=\"https://img.shields.io/badge/nagisa-nagisa-ff69b4\"></a>\n\nalso, `konoha` provides rule-based tokenizers (whitespace, character) and a rule-based sentence splitter.\n\n\n## quick start with docker\n\nsimply run followings on your computer:\n\n```bash\ndocker run --rm -p 8000:8000 -t himkt/konoha  # from dockerhub\n```\n\nor you can build image on your machine:\n\n```bash\ngit clone https://github.com/himkt/konoha  # download konoha\ncd konoha && docker-compose up --build  # build and launch container\n```\n\ntokenization is done by posting a json object to `localhost:8000/api/v1/tokenize`.\nyou can also batch tokenize by passing `texts: [\"\uff11\u3064\u76ee\u306e\u5165\u529b\", \"\uff12\u3064\u76ee\u306e\u5165\u529b\"]` to `localhost:8000/api/v1/batch_tokenize`.\n\n(api documentation is available on `localhost:8000/redoc`, you can check it using your web browser)\n\nsend a request using `curl` on your terminal.\nnote that a path to an endpoint is changed in v4.6.4.\nplease check our release note (https://github.com/himkt/konoha/releases/tag/v4.6.4).\n\n```json\n$ curl localhost:8000/api/v1/tokenize -x post -h \"content-type: application/json\" \\\n    -d '{\"tokenizer\": \"mecab\", \"text\": \"\u3053\u308c\u306f\u30da\u30f3\u3067\u3059\"}'\n\n{\n  \"tokens\": [\n    [\n      {\n        \"surface\": \"\u3053\u308c\",\n        \"part_of_speech\": \"\u540d\u8a5e\"\n      },\n      {\n        \"surface\": \"\u306f\",\n        \"part_of_speech\": \"\u52a9\u8a5e\"\n      },\n      {\n        \"surface\": \"\u30da\u30f3\",\n        \"part_of_speech\": \"\u540d\u8a5e\"\n      },\n      {\n        \"surface\": \"\u3067\u3059\",\n        \"part_of_speech\": \"\u52a9\u52d5\u8a5e\"\n      }\n    ]\n  ]\n}\n```\n\n\n## installation\n\n\ni recommend you to install konoha by `pip install 'konoha[all]'`.\n\n- install konoha with a specific tokenizer: `pip install 'konoha[(tokenizer_name)]`.\n- install konoha with a specific tokenizer and remote file support: `pip install 'konoha[(tokenizer_name),remote]'`\n\nif you want to install konoha with a tokenizer, please install konoha with a specific tokenizer\n(e.g. `konoha[mecab]`, `konoha[sudachi]`, ...etc) or install tokenizers individually.\n\n\n## example\n\n### word level tokenization\n\n```python\nfrom konoha import wordtokenizer\n\nsentence = '\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3092\u52c9\u5f37\u3057\u3066\u3044\u307e\u3059'\n\ntokenizer = wordtokenizer('mecab')\nprint(tokenizer.tokenize(sentence))\n# => [\u81ea\u7136, \u8a00\u8a9e, \u51e6\u7406, \u3092, \u52c9\u5f37, \u3057, \u3066, \u3044, \u307e\u3059]\n\ntokenizer = wordtokenizer('sentencepiece', model_path=\"data/model.spm\")\nprint(tokenizer.tokenize(sentence))\n# => [\u2581, \u81ea\u7136, \u8a00\u8a9e, \u51e6\u7406, \u3092, \u52c9\u5f37, \u3057, \u3066\u3044\u307e\u3059]\n```\n\nfor more detail, please see the `example/` directory.\n\n### remote files\n\nkonoha supports dictionary and model on cloud storage (currently supports amazon s3).\nit requires installing konoha with the `remote` option, see [installation](#installation).\n\n```python\n# download user dictionary from s3\nword_tokenizer = wordtokenizer(\"mecab\", user_dictionary_path=\"s3://abc/xxx.dic\")\nprint(word_tokenizer.tokenize(sentence))\n\n# download system dictionary from s3\nword_tokenizer = wordtokenizer(\"mecab\", system_dictionary_path=\"s3://abc/yyy\")\nprint(word_tokenizer.tokenize(sentence))\n\n# download model file from s3\nword_tokenizer = wordtokenizer(\"sentencepiece\", model_path=\"s3://abc/zzz.model\")\nprint(word_tokenizer.tokenize(sentence))\n```\n\n### sentence level tokenization\n\n```python\nfrom konoha import sentencetokenizer\n\nsentence = \"\u79c1\u306f\u732b\u3060\u3002\u540d\u524d\u306a\u3093\u3066\u3082\u306e\u306f\u306a\u3044\u3002\u3060\u304c\uff0c\u300c\u304b\u308f\u3044\u3044\u3002\u305d\u308c\u3067\u5341\u5206\u3060\u308d\u3046\u300d\u3002\"\n\ntokenizer = sentencetokenizer()\nprint(tokenizer.tokenize(sentence))\n# => ['\u79c1\u306f\u732b\u3060\u3002', '\u540d\u524d\u306a\u3093\u3066\u3082\u306e\u306f\u306a\u3044\u3002', '\u3060\u304c\uff0c\u300c\u304b\u308f\u3044\u3044\u3002\u305d\u308c\u3067\u5341\u5206\u3060\u308d\u3046\u300d\u3002']\n```\n\nyou can change symbols for a sentence splitter and bracket expression.\n\n1. sentence splitter\n\n```python\nsentence = \"\u79c1\u306f\u732b\u3060\u3002\u540d\u524d\u306a\u3093\u3066\u3082\u306e\u306f\u306a\u3044\uff0e\u3060\u304c\uff0c\u300c\u304b\u308f\u3044\u3044\u3002\u305d\u308c\u3067\u5341\u5206\u3060\u308d\u3046\u300d\u3002\"\n\ntokenizer = sentencetokenizer(period=\"\uff0e\")\nprint(tokenizer.tokenize(sentence))\n# => ['\u79c1\u306f\u732b\u3060\u3002\u540d\u524d\u306a\u3093\u3066\u3082\u306e\u306f\u306a\u3044\uff0e', '\u3060\u304c\uff0c\u300c\u304b\u308f\u3044\u3044\u3002\u305d\u308c\u3067\u5341\u5206\u3060\u308d\u3046\u300d\u3002']\n```\n\n2. bracket expression\n\n```python\nsentence = \"\u79c1\u306f\u732b\u3060\u3002\u540d\u524d\u306a\u3093\u3066\u3082\u306e\u306f\u306a\u3044\u3002\u3060\u304c\uff0c\u300e\u304b\u308f\u3044\u3044\u3002\u305d\u308c\u3067\u5341\u5206\u3060\u308d\u3046\u300f\u3002\"\n\ntokenizer = sentencetokenizer(\n    patterns=sentencetokenizer.patterns + [re.compile(r\"\u300e.*?\u300f\")],\n)\nprint(tokenizer.tokenize(sentence))\n# => ['\u79c1\u306f\u732b\u3060\u3002', '\u540d\u524d\u306a\u3093\u3066\u3082\u306e\u306f\u306a\u3044\u3002', '\u3060\u304c\uff0c\u300e\u304b\u308f\u3044\u3044\u3002\u305d\u308c\u3067\u5341\u5206\u3060\u308d\u3046\u300f\u3002']\n```\n\n\n## test\n\n```\npython -m pytest\n```\n\n## article\n\n- [\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u3092\u3044\u3044\u611f\u3058\u306b\u5207\u308a\u66ff\u3048\u308b\u30e9\u30a4\u30d6\u30e9\u30ea konoha \u3092\u4f5c\u3063\u305f](https://qiita.com/klis/items/bb9ffa4d9c886af0f531)\n- [\u65e5\u672c\u8a9e\u89e3\u6790\u30c4\u30fc\u30eb konoha \u306b allennlp \u9023\u643a\u6a5f\u80fd\u3092\u5b9f\u88c5\u3057\u305f](https://qiita.com/klis/items/f1d29cb431d1bf879898)\n\n## acknowledgement\n\nsentencepiece model used in test is provided by @yoheikikuta. thanks!\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "konoha",
  "package_url": "https://pypi.org/project/konoha/",
  "project_url": "https://pypi.org/project/konoha/",
  "project_urls": null,
  "release_url": "https://pypi.org/project/konoha/5.4.0/",
  "requires_dist": [
    "importlib-metadata (>=4.0.0,<7.0.0)",
    "overrides (>=3.0.0,<4.0.0)",
    "janome (>=0.3.10,<0.4.0) ; extra == \"janome\" or extra == \"all\"",
    "natto-py (>=1.0.0,<2.0.0) ; extra == \"mecab\" or extra == \"all\"",
    "kytea (>=0.1.4,<0.2.0) ; extra == \"kytea\" or extra == \"all\"",
    "sentencepiece (>=0.1.85,<0.2.0) ; extra == \"sentencepiece\" or extra == \"all\"",
    "sudachipy (==0.4.9) ; extra == \"sudachi\" or extra == \"all\"",
    "boto3 (>=1.11.0,<2.0.0) ; extra == \"remote\" or extra == \"all\"",
    "fastapi (>=0.54.1,<0.66.0) ; extra == \"server\" or extra == \"all\"",
    "uvicorn (==0.13.4) ; extra == \"server\" or extra == \"all\"",
    "sudachidict-core (>=20200330,<20200331) ; extra == \"sudachi\" or extra == \"all\"",
    "sphinx (>=5.1.1,<6.0.0) ; extra == \"docs\"",
    "pydata-sphinx-theme (>=0.10.1,<0.11.0) ; extra == \"docs\"",
    "nagisa (>=0.2.7,<0.3.0) ; extra == \"nagisa\" or extra == \"all\"",
    "rich (>=10.2.2,<11.0.0) ; extra == \"server\" or extra == \"all\"",
    "requests (>=2.26.0,<3.0.0)"
  ],
  "requires_python": ">=3.7.0,<4.0.0",
  "summary": "a tiny sentence/word tokenizer for japanese text written in python",
  "version": "5.4.0",
  "releases": [],
  "developers": [
    "himkt",
    "himkt@klis.tsukuba.ac.jp"
  ],
  "kwds": "konoha konoha_example tokenizer tokenizers japanese",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_konoha",
  "homepage": "",
  "release_count": 20,
  "dependency_ids": [
    "pypi_boto3",
    "pypi_fastapi",
    "pypi_importlib_metadata",
    "pypi_janome",
    "pypi_kytea",
    "pypi_nagisa",
    "pypi_natto_py",
    "pypi_overrides",
    "pypi_pydata_sphinx_theme",
    "pypi_requests",
    "pypi_rich",
    "pypi_sentencepiece",
    "pypi_sphinx",
    "pypi_sudachidict_core",
    "pypi_sudachipy",
    "pypi_uvicorn"
  ]
}