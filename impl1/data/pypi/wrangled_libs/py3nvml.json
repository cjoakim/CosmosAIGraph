{
  "classifiers": [
    "development status :: 3 - alpha",
    "intended audience :: developers",
    "intended audience :: system administrators",
    "license :: osi approved :: bsd license",
    "operating system :: microsoft :: windows",
    "operating system :: posix :: linux",
    "programming language :: python",
    "programming language :: python :: 3.5",
    "topic :: software development :: libraries :: python modules",
    "topic :: system :: hardware",
    "topic :: system :: systems administration"
  ],
  "description": "py3nvml\n=======\ndocumentation also available at `readthedocs`__.\n\npython 3 compatible bindings to the nvidia management library. can be used to\nquery the state of the gpus on your system. this was ported from the nvidia\nprovided python bindings `nvidia-ml-py`__, which only \nsupported python 2. i have forked from version 7.352.0. the old library was \nitself a wrapper around the `nvidia management library`__.\n\n__ https://py3nvml.readthedocs.io/en/latest/\n__ https://pypi.python.org/pypi/nvidia-ml-py/7.352.0\n__ http://developer.nvidia.com/nvidia-management-library-nvml\n\nin addition to these nvidia functions to query the state of the gpu, i have written\na couple functions/tools to help in using gpus (particularly for a shared\ngpu server). these are:\n\n- a function to 'restrict' the available gpus by setting the `cuda_visible_devices` \n  environment variable. \n- a script for displaying a differently formatted nvidia-smi.\n\nsee the utils section below for more info.\n\nupdates in version 0.2.3\n------------------------\nto try and keep py3nvml somewhat up-to-date with the constantly evolving nvidia\ndrivers, i have done some work to the `py3nvml.py3nvml` module. in particular,\ni have updated all the constants that were missing in py3nvml and existing in the\n`nvidia source`__ as of version 418.43. in addition, i have wrapped all of these \nconstants in enums so it is easier to see what constants go together. finally,\nfor all the functions in `py3nvml.py3nvml` i have copied in the\nc docstring. while this will result in some strange looking docstrings which\nwill be slightly incorrect, they should give good guidance on the scope of the\nfunction, something which was ill-defined before.\n\nfinally, i will remove the `py3nvml.nvidia_smi` module in a future version, as\ni believe it was only ever meant as an example of how to use the nvml functions\nto query the gpus, and is now quite out of date. to get the same functionality,\nyou can call `nvidia-smi -q -x` from python with subprocess.\n\n__ https://github.com/nvidia/nvidia-settings/blob/master/src/nvml.h\n\nrequires\n--------\npython 3.5+.\n\ninstallation \n------------\nfrom pypi::\n\n    $ pip install py3nvml\n\nfrom github::\n\n    $ pip install -e git+https://github.com/fbcotter/py3nvml#egg=py3nvml\n\nor, download and pip install:: \n\n    $ git clone https://github.com/fbcotter/py3nvml\n    $ cd py3nvml\n    $ pip install .\n\n.. _utils-label:\n\nutils \n-----\n(added by me - not ported from nvidia library)\n\ngrab_gpus\n~~~~~~~~~\n\nyou can call the :code:`grab_gpus(num_gpus, gpu_select, gpu_fraction=.95)` function to check the available gpus and set\nthe `cuda_visible_devices` environment variable as need be. it determines if a gpu is available by checking if the\namount of free memory is below memory-usage is above/equal to the gpu_fraction value. the default of .95 allows for some\nsmall amount of memory to be taken before it deems the gpu as being 'used'. \n\ni have found this useful as i have a shared gpu server and like to use tensorflow which is very greedy and calls to\n:code:`tf.session()` grabs all available gpus.\n\ne.g.\n\n.. code:: python\n\n    import py3nvml\n    import tensorflow as tf\n    py3nvml.grab_gpus(3)\n    sess = tf.session() # now we only grab 3 gpus!\n\nor the following will grab 2 gpus from the first 4 (and leave any higher gpus untouched)\n\n.. code:: python\n\n    py3nvml.grab_gpus(num_gpus=2, gpu_select=[0,1,2,3])\n    sess = tf.session() \n\nthis will look for 3 available gpus in the range of gpus from 0 to 3. the range option is not necessary, and it only\nserves to restrict the search space for the grab_gpus. \n\nyou can adjust the memory threshold for determining if a gpu is free/used with the :code:`gpu_fraction` parameter\n(default is 1):\n\n.. code:: python\n\n    # will allocate a gpu if less than 20% of its memory is being used\n    py3nvml.grab_gpus(num_gpus=2, gpu_fraction=0.8)\n    sess = tf.session() \n\nyou can select the graphics card based on its capacity. specify minimal amount of graphics card memory in mib in \norder to exclude the weaker graphics cards.\n\n.. code:: python\n\n    # will allocate a gpu only if it has more than 4000 mib of memory\n    py3nvml.grab_gpus(num_gpus=2, gpu_min_memory=4000)\n    sess = tf.session() \n\nthis function has no return codes but may raise some warnings/exceptions:\n\n- if the method could not connect to any nvidia gpus, it will raise\n  a runtimewarning. \n- if it could connect to the gpus, but there were none available, it will \n  raise a valueerror. \n- if it could connect to the gpus but not enough were available (i.e. more than\n  1 was requested), it will take everything it can and raise a runtimewarning.\n\nget_free_gpus\n~~~~~~~~~~~~~\nthis tool can query the gpu status. unlike the default for `grab_gpus`, which checks the memory usage of a gpu, this\nfunction checks if a process is running on a gpu. for a system with n gpus, returns a list of n booleans, where the nth\nvalue is true if no process was found running on gpu n. an example use is:\n\n.. code:: python\n\n    import py3nvml\n    free_gpus = py3nvml.get_free_gpus()\n    if true not in free_gpus:\n        print('no free gpus found')\n\nget_num_procs\n~~~~~~~~~~~~~\nthis function is called by `get_free_gpus`. it simply returns a list of integers\nwith the number of processes running on each gpu. e.g. if you had 1 process\nrunning on gpu 5 in an 8 gpu system, you would expect to get the following:\n\n.. code:: python\n\n    import py3nvml\n    num_procs = py3nvml.get_num_procs()\n    print(num_proces)\n    >>> [0, 0, 0, 0, 0, 1, 0, 0]\n\npy3smi\n~~~~~~\ni found the default `nvidia-smi` output was missing some useful info, so made use of the\n`py3nvml/nvidia_smi.py` module to query the device and get info on the\ngpus, and then defined my own printout. i have included this as a script in\n`scripts/py3smi`. the print code is horribly messy but the query code is very\nsimple and should be understandable. \n\nrunning pip install will now put this script in your python's\nbin, and you'll be able to run it from the command line. here is a comparison of\nthe two outputs:\n\n.. image:: https://i.imgur.com/tvdfkfe.png\n\n.. image:: https://i.imgur.com/upshr8k.png\n\nfor py3smi, you can specify an update period so it will refresh the feed every\nfew seconds. i.e., similar to :code:`watch -n5 nvidia-smi`, you can run\n:code:`py3smi -l 5`.\n\nyou can also get the full output (very similar to nvidia-smi) by running `py3smi\n-f` (this shows a slightly modified process info pane below).\n\nregular usage \n-------------\nvisit `nvml reference`__ for a list of the\nfunctions available and their help. also the script py3smi is a bit hacky but\nshows examples of me querying the gpus for info. \n\n__ https://docs.nvidia.com/deploy/nvml-api/index.html\n\n(below here is everything ported from pynvml)\n\n.. code:: python\n\n    from py3nvml.py3nvml import *\n    nvmlinit()\n    print(\"driver version: {}\".format(nvmlsystemgetdriverversion()))\n    # e.g. will print:\n    #   driver version: 352.00\n    devicecount = nvmldevicegetcount()\n    for i in range(devicecount):\n        handle = nvmldevicegethandlebyindex(i)\n        print(\"device {}: {}\".format(i, nvmldevicegetname(handle)))\n    # e.g. will print:\n    #  device 0 : tesla k40c\n    #  device 1 : tesla k40c\n\n    nvmlshutdown()\n\nadditionally, see `py3nvml.nvidia_smi.py`. this does the equivalent of the\n`nvidia-smi` command:: \n\n    nvidia-smi -q -x\n\nwith\n\n.. code:: python\n\n    import py3nvml.nvidia_smi as smi\n    print(smi.xmldevicequery())\n\ndifferences from nvml\n~~~~~~~~~~~~~~~~~~~~~\nthe py3nvml library consists of python methods which wrap \nseveral nvml functions, implemented in a c shared library.\neach function's use is the same with the following exceptions:\n\n1. instead of returning error codes, failing error codes are raised as python exceptions. i.e. they should be wrapped with exception handlers.\n\n  .. code:: python\n\n    try:\n        nvmldevicegetcount()\n    except nvmlerror as error:\n        print(error)\n\n\n2. c function output parameters are returned from the corresponding python function as tuples, rather than requiring pointers. eg the c function:\n\n  .. code:: c\n\n    nvmlreturn_t nvmldevicegeteccmode(nvmldevice_t device,\n                                      nvmlenablestate_t *current,\n                                      nvmlenablestate_t *pending);\n\n  becomes\n\n  .. code:: python\n\n    nvmlinit()\n    handle = nvmldevicegethandlebyindex(0)\n    (current, pending) = nvmldevicegeteccmode(handle)\n\n3. c structs are converted into python classes. e.g. the c struct:\n\n  .. code:: c\n\n    nvmlreturn_t decldir nvmldevicegetmemoryinfo(nvmldevice_t device,\n                                                 nvmlmemory_t *memory);\n    typedef struct nvmlmemory_st {\n        unsigned long long total;\n        unsigned long long free;\n        unsigned long long used;\n    } nvmlmemory_t;\n\n  becomes:\n\n  .. code:: python\n\n    info = nvmldevicegetmemoryinfo(handle)\n    print(\"total memory: {}mib\".format(info.total >> 20))\n    # will print:\n    #   total memory: 5375mib\n    print(\"free memory: {}\".format(info.free >> 20))\n    # will print:\n    #   free memory: 5319mib\n    print(\"used memory: \".format(info.used >> 20))\n    # will print:\n    #   used memory: 55mib\n\n4. python handles string buffer creation.  e.g. the c function:\n\n  .. code:: c\n\n    nvmlreturn_t nvmlsystemgetdriverversion(char* version,\n                                            unsigned int length);\n\n  can be called like so:\n\n  .. code:: python\n\n    version = nvmlsystemgetdriverversion()\n    nvmlshutdown()\n\n\n5.  all meaningful nvml constants and enums are exposed in python. e.g. the constant `nvml_temperature_gpu` is available under\n`py3nvml.nvml_temperature_gpu` \n\nthe `nvml_value_not_available` constant is not used.  instead none is mapped to the field.\n\nrelease notes (for pynvml)\n--------------------------\nversion 2.285.0\n\n- added new functions for nvml 2.285.  see nvml documentation for more information.\n- ported to support python 3.0 and python 2.0 syntax.\n- added nvidia_smi.py tool as a sample app.\n\nversion 3.295.0\n\n- added new functions for nvml 3.295.  see nvml documentation for more information.\n- updated nvidia_smi.py tool\n  - includes additional error handling\n\nversion 4.304.0\n\n- added new functions for nvml 4.304.  see nvml documentation for more information.\n- updated nvidia_smi.py tool\n\nversion 4.304.3\n\n- fixing nvmlunitgetdevicecount bug\n\nversion 5.319.0\n\n- added new functions for nvml 5.319.  see nvml documentation for more information.\n\nversion 6.340.0\n\n- added new functions for nvml 6.340.  see nvml documentation for more information.\n\nversion 7.346.0\n\n- added new functions for nvml 7.346.  see nvml documentation for more information.\n\nversion 7.352.0\n\n- added new functions for nvml 7.352.  see nvml documentation for more information.\n\ncopyright\n---------\ncopyright (c) 2011-2015, nvidia corporation.  all rights reserved.\n\nlicense\n-------\nredistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n- redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n- redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n- neither the name of the nvidia corporation nor the names of its contributors\n  may be used to endorse or promote products derived from this software without\n  specific prior written permission.\n\nthis software is provided by the copyright holders and contributors \"as is\" and\nany express or implied warranties, including, but not limited to, the implied\nwarranties of merchantability and fitness for a particular purpose are\ndisclaimed. in no event shall the copyright holder or contributors be liable\nfor any direct, indirect, incidental, special, exemplary, or consequential\ndamages (including, but not limited to, procurement of substitute goods or\nservices; loss of use, data, or profits; or business interruption) however\ncaused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use\nof this software, even if advised of the possibility of such damage.\n\n\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "bsd",
  "name": "py3nvml",
  "package_url": "https://pypi.org/project/py3nvml/",
  "project_url": "https://pypi.org/project/py3nvml/",
  "project_urls": {
    "Download": "https://github.com/fbcotter/py3nvml/archive/0.2.7.tar.gz",
    "Homepage": "https://github.com/fbcotter/py3nvml.git"
  },
  "release_url": "https://pypi.org/project/py3nvml/0.2.7/",
  "requires_dist": [
    "xmltodict"
  ],
  "requires_python": "",
  "summary": "python 3 bindings for the nvidia management library",
  "version": "0.2.7",
  "releases": [],
  "developers": [
    "fbc23@cam.ac.uk",
    "fergal_cotter"
  ],
  "kwds": "grab_gpus nvml_temperature_gpu gpu_select get_free_gpus free_gpus",
  "license_kwds": "bsd",
  "libtype": "pypi",
  "id": "pypi_py3nvml",
  "homepage": "https://github.com/fbcotter/py3nvml.git",
  "release_count": 14,
  "dependency_ids": [
    "pypi_xmltodict"
  ]
}