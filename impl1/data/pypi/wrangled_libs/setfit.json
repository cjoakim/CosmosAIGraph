{
  "classifiers": [
    "development status :: 1 - planning",
    "intended audience :: developers",
    "intended audience :: education",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "operating system :: os independent",
    "programming language :: python :: 3",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "<img src=\"https://raw.githubusercontent.com/huggingface/setfit/main/assets/setfit.png\">\n\n<p align=\"center\">\n    \ud83e\udd17 <a href=\"https://huggingface.co/setfit\" target=\"_blank\">models & datasets</a> | \ud83d\udcd5 <a href=\"https://huggingface.co/docs/setfit\" target=\"_blank\">documentation</a> | \ud83d\udcd6 <a href=\"https://huggingface.co/blog/setfit\" target=\"_blank\">blog</a> | \ud83d\udcc3 <a href=\"https://arxiv.org/abs/2209.11055\" target=\"_blank\">paper</a>\n</p>\n\n# setfit - efficient few-shot learning with sentence transformers\n\nsetfit is an efficient and prompt-free framework for few-shot fine-tuning of [sentence transformers](https://sbert.net/). it achieves high accuracy with little labeled data - for instance, with only 8 labeled examples per class on the customer reviews sentiment dataset, setfit is competitive with fine-tuning roberta large on the full training set of 3k examples \ud83e\udd2f!\n\ncompared to other few-shot learning methods, setfit has several unique features:\n\n* \ud83d\udde3 **no prompts or verbalizers:** current techniques for few-shot fine-tuning require handcrafted prompts or verbalizers to convert examples into a format suitable for the underlying language model. setfit dispenses with prompts altogether by generating rich embeddings directly from text examples.\n* \ud83c\udfce **fast to train:** setfit doesn't require large-scale models like t0 or gpt-3 to achieve high accuracy. as a result, it is typically an order of magnitude (or more) faster to train and run inference with.\n* \ud83c\udf0e **multilingual support**: setfit can be used with any [sentence transformer](https://huggingface.co/models?library=sentence-transformers&sort=downloads) on the hub, which means you can classify text in multiple languages by simply fine-tuning a multilingual checkpoint.\n\ncheck out the [setfit documentation](https://huggingface.co/docs/setfit) for more information!\n\n## installation\n\ndownload and install `setfit` by running:\n\n```bash\npip install setfit\n```\n\nif you want the bleeding-edge version instead, install from source by running:\n\n```bash\npip install git+https://github.com/huggingface/setfit.git\n```\n\n## usage\n\nthe [quickstart](https://huggingface.co/docs/setfit/quickstart) is a good place to learn about training, saving, loading, and performing inference with setfit models. \n\nfor more examples, check out the [`notebooks`](https://github.com/huggingface/setfit/tree/main/notebooks) directory, the [tutorials](https://huggingface.co/docs/setfit/tutorials/overview), or the [how-to guides](https://huggingface.co/docs/setfit/how_to/overview).\n\n\n### training a setfit model\n\n`setfit` is integrated with the [hugging face hub](https://huggingface.co/) and provides two main classes:\n\n* [`setfitmodel`](https://huggingface.co/docs/setfit/reference/main#setfit.setfitmodel): a wrapper that combines a pretrained body from `sentence_transformers` and a classification head from either [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.logisticregression.html) or [`setfithead`](https://huggingface.co/docs/setfit/reference/main#setfit.setfithead) (a differentiable head built upon `pytorch` with similar apis to `sentence_transformers`).\n* [`trainer`](https://huggingface.co/docs/setfit/reference/trainer#setfit.trainer): a helper class that wraps the fine-tuning process of setfit.\n\nhere is a simple end-to-end training example using the default classification head from `scikit-learn`:\n\n\n```python\nfrom datasets import load_dataset\nfrom setfit import setfitmodel, trainer, trainingarguments, sample_dataset\n\n\n# load a dataset from the hugging face hub\ndataset = load_dataset(\"sst2\")\n\n# simulate the few-shot regime by sampling 8 examples per class\ntrain_dataset = sample_dataset(dataset[\"train\"], label_column=\"label\", num_samples=8)\neval_dataset = dataset[\"validation\"].select(range(100))\ntest_dataset = dataset[\"validation\"].select(range(100, len(dataset[\"validation\"])))\n\n# load a setfit model from hub\nmodel = setfitmodel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n\nargs = trainingarguments(\n    batch_size=16,\n    num_epochs=4,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=true,\n)\n\ntrainer = trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    metric=\"accuracy\",\n    column_mapping={\"sentence\": \"text\", \"label\": \"label\"}  # map dataset columns to text/label expected by trainer\n)\n\n# train and evaluate\ntrainer.train()\nmetrics = trainer.evaluate(test_dataset)\nprint(metrics)\n# {'accuracy': 0.8691709844559585}\n\n# push model to the hub\ntrainer.push_to_hub(\"tomaarsen/setfit-paraphrase-mpnet-base-v2-sst2\")\n\n# download from hub\nmodel = setfitmodel.from_pretrained(\"tomaarsen/setfit-paraphrase-mpnet-base-v2-sst2\")\n# run inference\npreds = model.predict([\"i loved the spiderman movie!\", \"pineapple on pizza is the worst \ud83e\udd2e\"])\nprint(preds)\n# tensor([1, 0], dtype=torch.int32)\n```\n\n\n## reproducing the results from the paper\n\nwe provide scripts to reproduce the results for setfit and various baselines presented in table 2 of our paper. check out the setup and training instructions in the [`scripts/`](scripts/) directory.\n\n## developer installation\n\nto run the code in this project, first create a python virtual environment using e.g. conda:\n\n```bash\nconda create -n setfit python=3.9 && conda activate setfit\n```\n\nthen install the base requirements with:\n\n```bash\npip install -e '.[dev]'\n```\n\nthis will install mandatory packages for setfit like `datasets` as well as development packages like `black` and `isort` that we use to ensure consistent code formatting.\n\n### formatting your code\n\nwe use `black` and `isort` to ensure consistent code formatting. after following the installation steps, you can check your code locally by running:\n\n```\nmake style && make quality\n```\n\n## project structure\n\n```\n\u251c\u2500\u2500 license\n\u251c\u2500\u2500 makefile        <- makefile with commands like `make style` or `make tests`\n\u251c\u2500\u2500 readme.md       <- the top-level readme for developers using this project.\n\u251c\u2500\u2500 docs            <- documentation source\n\u251c\u2500\u2500 notebooks       <- jupyter notebooks.\n\u251c\u2500\u2500 final_results   <- model predictions from the paper\n\u251c\u2500\u2500 scripts         <- scripts for training and inference\n\u251c\u2500\u2500 setup.cfg       <- configuration file to define package metadata\n\u251c\u2500\u2500 setup.py        <- make this project pip installable with `pip install -e`\n\u251c\u2500\u2500 src             <- source code for setfit\n\u2514\u2500\u2500 tests           <- unit tests\n```\n\n## related work\n\n* [https://github.com/pmbaumgartner/setfit](https://github.com/pmbaumgartner/setfit) - a scikit-learn api version of setfit.\n* [jxpress/setfit-pytorch-lightning](https://github.com/jxpress/setfit-pytorch-lightning) - a pytorch lightning implementation of setfit.\n* [davidberenstein1957/spacy-setfit](https://github.com/davidberenstein1957/spacy-setfit) - an easy and intuitive approach to use setfit in combination with spacy. \n\n## citation\n\n```bibtex\n@misc{https://doi.org/10.48550/arxiv.2209.11055,\n  doi = {10.48550/arxiv.2209.11055},\n  url = {https://arxiv.org/abs/2209.11055},\n  author = {tunstall, lewis and reimers, nils and jo, unso eun seo and bates, luke and korat, daniel and wasserblat, moshe and pereg, oren},\n  keywords = {computation and language (cs.cl), fos: computer and information sciences, fos: computer and information sciences},\n  title = {efficient few-shot learning without prompts},\n  publisher = {arxiv},\n  year = {2022},\n  copyright = {creative commons attribution 4.0 international}\n}\n```\n",
  "docs_url": null,
  "keywords": "nlp,machine learning,fewshot learning,transformers",
  "license": "apache 2.0",
  "name": "setfit",
  "package_url": "https://pypi.org/project/setfit/",
  "project_url": "https://pypi.org/project/setfit/",
  "project_urls": {
    "Download": "https://github.com/huggingface/setfit/tags",
    "Homepage": "https://github.com/huggingface/setfit"
  },
  "release_url": "https://pypi.org/project/setfit/1.0.1/",
  "requires_dist": [
    "datasets (>=2.3.0)",
    "sentence-transformers (>=2.2.1)",
    "evaluate (>=0.3.0)",
    "huggingface-hub (>=0.13.0)",
    "scikit-learn",
    "spacy ; extra == 'absa'",
    "codecarbon ; extra == 'codecarbon'",
    "datasets (==2.3.0) ; extra == 'compat_tests'",
    "sentence-transformers (==2.2.1) ; extra == 'compat_tests'",
    "evaluate (==0.3.0) ; extra == 'compat_tests'",
    "huggingface-hub (==0.13.0) ; extra == 'compat_tests'",
    "scikit-learn ; extra == 'compat_tests'",
    "pytest ; extra == 'compat_tests'",
    "pytest-cov ; extra == 'compat_tests'",
    "onnxruntime ; extra == 'compat_tests'",
    "onnx ; extra == 'compat_tests'",
    "skl2onnx ; extra == 'compat_tests'",
    "hummingbird-ml (<0.4.9) ; extra == 'compat_tests'",
    "openvino (==2022.3.0) ; extra == 'compat_tests'",
    "spacy ; extra == 'compat_tests'",
    "pandas (<2) ; extra == 'compat_tests'",
    "fsspec (<2023.12.0) ; extra == 'compat_tests'",
    "openvino (==2022.3.0) ; extra == 'dev'",
    "onnx ; extra == 'dev'",
    "onnxruntime ; extra == 'dev'",
    "tabulate ; extra == 'dev'",
    "skl2onnx ; extra == 'dev'",
    "hummingbird-ml (<0.4.9) ; extra == 'dev'",
    "pytest-cov ; extra == 'dev'",
    "spacy ; extra == 'dev'",
    "black ; extra == 'dev'",
    "hf-doc-builder (>=0.3.0) ; extra == 'dev'",
    "codecarbon ; extra == 'dev'",
    "optuna ; extra == 'dev'",
    "pytest ; extra == 'dev'",
    "isort ; extra == 'dev'",
    "flake8 ; extra == 'dev'",
    "hf-doc-builder (>=0.3.0) ; extra == 'docs'",
    "onnxruntime ; extra == 'onnx'",
    "onnx ; extra == 'onnx'",
    "skl2onnx ; extra == 'onnx'",
    "onnxruntime ; extra == 'openvino'",
    "onnx ; extra == 'openvino'",
    "skl2onnx ; extra == 'openvino'",
    "hummingbird-ml (<0.4.9) ; extra == 'openvino'",
    "openvino (==2022.3.0) ; extra == 'openvino'",
    "optuna ; extra == 'optuna'",
    "black ; extra == 'quality'",
    "flake8 ; extra == 'quality'",
    "isort ; extra == 'quality'",
    "tabulate ; extra == 'quality'",
    "pytest ; extra == 'tests'",
    "pytest-cov ; extra == 'tests'",
    "onnxruntime ; extra == 'tests'",
    "onnx ; extra == 'tests'",
    "skl2onnx ; extra == 'tests'",
    "hummingbird-ml (<0.4.9) ; extra == 'tests'",
    "openvino (==2022.3.0) ; extra == 'tests'",
    "spacy ; extra == 'tests'"
  ],
  "requires_python": "",
  "summary": "efficient few-shot learning with sentence transformers",
  "version": "1.0.1",
  "releases": [],
  "developers": [
    "lewis@huggingface.co",
    "lewis_tunstall"
  ],
  "kwds": "sentence_transformers nlp learning fewshot text",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_setfit",
  "homepage": "https://github.com/huggingface/setfit",
  "release_count": 13,
  "dependency_ids": [
    "pypi_black",
    "pypi_codecarbon",
    "pypi_datasets",
    "pypi_evaluate",
    "pypi_flake8",
    "pypi_fsspec",
    "pypi_hf_doc_builder",
    "pypi_huggingface_hub",
    "pypi_hummingbird_ml",
    "pypi_isort",
    "pypi_onnx",
    "pypi_onnxruntime",
    "pypi_openvino",
    "pypi_optuna",
    "pypi_pandas",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_scikit_learn",
    "pypi_sentence_transformers",
    "pypi_skl2onnx",
    "pypi_spacy",
    "pypi_tabulate"
  ]
}