{
  "classifiers": [
    "development status :: 4 - beta",
    "intended audience :: developers",
    "license :: osi approved :: mit license",
    "programming language :: python :: 3.6",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "# embedding_reader\n[![pypi](https://img.shields.io/pypi/v/embedding_reader.svg)](https://pypi.python.org/pypi/embedding_reader)\n[![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rom1504/embedding_reader/blob/master/notebook/embedding_reader_getting_started.ipynb)\n[![try it on gitpod](https://img.shields.io/badge/try-on%20gitpod-brightgreen.svg)](https://gitpod.io/#https://github.com/rom1504/embedding_reader)\n\nembedding reader is a module to make it easy to read efficiently a large collection of embeddings stored in any file system.\n* 400gb of embeddings read in 8min using an nvme drive\n* 400gb of embeddings read in 40min using an hdd drive\n* 400gb of embeddings read in 1.3h from aws s3\n\n## install\n\npip install embedding_reader\n\n## python examples\n\ncheckout these examples to call this as a lib:\n* [example.py](examples/example.py)\n\n### simple example\n\n```python\nfrom embedding_reader import embeddingreader\n\nembedding_reader = embeddingreader(embeddings_folder=\"embedding_folder\", file_format=\"npy\")\n\nprint(\"embedding count\", embedding_reader.count)\nprint(\"dimension\", embedding_reader.dimension)\nprint(\"total size\", embedding_reader.total_size)\nprint(\"byte per item\", embedding_reader.byte_per_item)\n\nfor emb, meta in embedding_reader(batch_size=10 ** 6, start=0, end=embedding_reader.count):\n    print(emb.shape)\n```\n\n### laion5b example\n\nin [laion5b](https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/) you can find 5b vit-l/14 image embeddings, you can read them with that code:\n\n```python\nfrom embedding_reader import embeddingreader\n\nembedding_reader = embeddingreader(embeddings_folder=\"https://mystic.the-eye.eu/public/ai/cah/laion5b/embeddings/laion2b-en/img_emb/\", file_format=\"npy\")\n\nprint(\"embedding count\", embedding_reader.count)\nprint(\"dimension\", embedding_reader.dimension)\nprint(\"total size\", embedding_reader.total_size)\nprint(\"byte per item\", embedding_reader.byte_per_item)\n\nfor emb, meta in embedding_reader(batch_size=10 ** 6, start=0, end=embedding_reader.count):\n    print(emb.shape)\n```\nit takes about 3h to read laion2b-en embeddings at 300mb/s\n\n### numpy & parquet metadata example\n\nthe parquet_npy format supports reading from both a .npy collection and a .parquet collection that are in the same order.\nhere is an example of usage:\n\n```python\nfrom embedding_reader import embeddingreader\n\nembedding_reader = embeddingreader(\n    embeddings_folder=\"embedding_folder\",\n    metadata_folder=\"metadata_folder\",\n    meta_columns=['image_path', 'caption'],\n    file_format=\"parquet_npy\"\n)\n\nfor emb, meta in embedding_reader(batch_size=10 ** 6, start=0, end=embedding_reader.count):\n    print(emb.shape)\n    print(meta[\"image_path\"], meta[\"caption\"])\n```\n`emb` is a numpy array like the previous examples while `meta` is a pandas dataframe with the columns requested in `meta_columns`.\n\n## who is using embedding reader?\n\nsome use cases of embedding reader include:\n* building knn indices in [autofaiss](https://github.com/criteo/autofaiss)\n* computing zero shot attributes using clip\n* running training or inferences of linear layer models on top of embeddings\n\nembeddings are a powerful concept, they allow turning highly complex data into point in a linearly separable space.\nembeddings are also much smaller and more efficient to manipulate than usual data (images, audio, video, text, interaction items, ...)\n\nto learn more about embeddings read [semantic search blogpost](https://rom1504.medium.com/semantic-search-with-embeddings-index-anything-8fb18556443c)\n\n## file system support\n\nthanks to [fsspec](https://filesystem-spec.readthedocs.io/en/latest/), embedding_reader supports reading and writing files in [many file systems](https://github.com/fsspec/filesystem_spec/blob/6233f315548b512ec379323f762b70764efeb92c/fsspec/registry.py#l87).\nto use it, simply use the prefix of your filesystem before the path. for example `hdfs://`, `s3://`, `http://`, or `gcs://`.\nsome of these file systems require installing an additional package (for example s3fs for s3, gcsfs for gcs).\nsee fsspec doc for all the details.\n\n## api\n\nthis module exposes one class:\n\n### embeddingreader(folder, file_format, embedding_column=\"embedding\", meta_columns=none, metadata_folder=none)\n\ninitialize the reader by listing all files and retrieving their metadata\n\n* **folder** embeddings folder. can also be a list of folders. (*required*)\n* **file_format** parquet, npy or parquet_npy. (*required*)\n* **embedding_column** embedding column in parquet. (*default embedding*)\n* **meta_columns** meta columns in parquet. (*default none*)\n* **metadata_folder** metadata folder, used by the parquet_npy reader (*default none*)\n\n#### .embeddings_folder\n\nthe embedding folder\n\n#### .count\n\ntotal number of embedding in this folder\n\n#### .dimension\n\ndimension of one embedding\n\n#### .byte_per_item\n\nsize of one embedding in bytes\n\n#### .total_size\n\nsize in bytes of the collection\n\n#### .nb_files\n\ntotal number of embedding files in this folder\n\n#### .max_file_size\n\nmax size in bytes of the embedding files of the collection\n\n#### __call__(batch_size, start=0, end=none, max_piece_size=none, parallel_pieces=none, show_progress=true, max_ram_usage_in_bytes=2**31)\n\nproduces an iterator that yields tuples (data, meta) with the given batch_size\n\n* **batch_size** amount of embeddings in one batch. (*required*)\n* **start** start of the subset of the collection to read. (default *0*)\n* **end** end of the subset of the collection to read. (default *end of collection*)\n* **max_piece_size** maximum size of a piece. the default value works for most cases. increase or decrease based on your file system performances (default *max(number of embedding for 50mb, batch size in mb)*)\n* **parallel_pieces** number of pieces to read in parallel. increase or decrease depending on your filesystem. (default *~min(round(max_ram_usage_in_bytes/max_piece_size*byte_per_item), 50)*)\n* **show_progress** display a tqdm bar with the number of pieces done. (default *true*)\n* **max_ram_usage_in_bytes** constraint the ram usage of embedding reader. the exact max ram usage is *min(max_ram_usage_in_bytes, size of a batch in bytes)*. (default 4gb)\n\n\n## architecture notes and benchmark\n\nthe main architecture choice of this lib is the `build_pieces` function that builds decently sizes pieces of embedding files (typically 50mb) initially.\nthese pieces metadata can then be used to fetch in parallel these pieces, which are then used to build the embedding batches and provided to the user.\nin order to reach the maximal speed, it is better to read files of equal size. the number of threads used is constrained by the maximum size of your embeddings files: the lower the size, the more threads are used (you can also set a custom number of threads, but ram consumption will be higher).\n\nin practice, it has been observed speed of up to 100mb/s when fetching embeddings from s3, 1gb/s when fetching from an nvme drive.\nthat means reading 400gb of embeddings in 8 minutes (400m embeddings in float16 and dimension 512)\nthe memory usage stays low and flat thanks to the absence of copy. decreasing the batch size decreases the amount of memory consumed, you can also set max_ram_usage_in_bytes to have a better control on the ram usage.\n\n\n## for development\n\neither locally, or in [gitpod](https://gitpod.io/#https://github.com/rom1504/embedding_reader) (do `export pip_user=false` there)\n\nsetup a virtualenv:\n\n```\npython3 -m venv .env\nsource .env/bin/activate\npip install -e .\n```\n\nto run tests:\n```\npip install -r requirements-test.txt\n```\nthen \n```\nmake lint\nmake test\n```\n\nyou can use `make black` to reformat the code\n\n`python -m pytest -x -s -v tests -k \"dummy\"` to run a specific test\n\n\n\n",
  "docs_url": null,
  "keywords": "machine learning",
  "license": "mit",
  "name": "embedding-reader",
  "package_url": "https://pypi.org/project/embedding-reader/",
  "project_url": "https://pypi.org/project/embedding-reader/",
  "project_urls": {
    "Homepage": "https://github.com/rom1504/embedding_reader"
  },
  "release_url": "https://pypi.org/project/embedding-reader/1.5.1/",
  "requires_dist": [
    "tqdm (<5,>=4.62.3)",
    "fsspec (>=2022.1.0)",
    "numpy (<2,>=1.19.5)",
    "pandas (<2,>=1.1.5)",
    "pyarrow (<13,>=6.0.1)"
  ],
  "requires_python": "",
  "summary": "a python template",
  "version": "1.5.1",
  "releases": [],
  "developers": [
    "romain.rom1@gmail.com",
    "romain_beaumont"
  ],
  "kwds": "embedding_reader embedding_reader_getting_started embeddingreader embeddings_folder reader",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_embedding_reader",
  "homepage": "https://github.com/rom1504/embedding_reader",
  "release_count": 21,
  "dependency_ids": [
    "pypi_fsspec",
    "pypi_numpy",
    "pypi_pandas",
    "pypi_pyarrow",
    "pypi_tqdm"
  ]
}