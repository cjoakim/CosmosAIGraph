{
  "classifiers": [
    "development status :: 4 - beta",
    "license :: osi approved :: apache software license",
    "operating system :: os independent",
    "programming language :: python :: 3",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8"
  ],
  "description": "<p align=\"center\">\n  <img src=\"assets/gfpgan_logo.png\" height=130>\n</p>\n\n## <div align=\"center\"><b><a href=\"readme.md\">english</a> | <a href=\"readme_cn.md\">\u7b80\u4f53\u4e2d\u6587</a></b></div>\n\n<div align=\"center\">\n<!-- <a href=\"https://twitter.com/_xintao_\" style=\"text-decoration:none;\">\n    <img src=\"https://user-images.githubusercontent.com/17445847/187162058-c764ced6-952f-404b-ac85-ba95cce18e7b.png\" width=\"4%\" alt=\"\" />\n</a> -->\n\n[![download](https://img.shields.io/github/downloads/tencentarc/gfpgan/total.svg)](https://github.com/tencentarc/gfpgan/releases)\n[![pypi](https://img.shields.io/pypi/v/gfpgan)](https://pypi.org/project/gfpgan/)\n[![open issue](https://img.shields.io/github/issues/tencentarc/gfpgan)](https://github.com/tencentarc/gfpgan/issues)\n[![closed issue](https://img.shields.io/github/issues-closed/tencentarc/gfpgan)](https://github.com/tencentarc/gfpgan/issues)\n[![license](https://img.shields.io/badge/license-apache%202.0-blue.svg)](https://github.com/tencentarc/gfpgan/blob/master/license)\n[![python lint](https://github.com/tencentarc/gfpgan/actions/workflows/pylint.yml/badge.svg)](https://github.com/tencentarc/gfpgan/blob/master/.github/workflows/pylint.yml)\n[![publish-pip](https://github.com/tencentarc/gfpgan/actions/workflows/publish-pip.yml/badge.svg)](https://github.com/tencentarc/gfpgan/blob/master/.github/workflows/publish-pip.yml)\n</div>\n\n1. :boom: **updated** online demo: [![replicate](https://img.shields.io/static/v1?label=demo&message=replicate&color=blue)](https://replicate.com/tencentarc/gfpgan). here is the [backup](https://replicate.com/xinntao/gfpgan).\n1. :boom: **updated** online demo: [![huggingface gradio](https://img.shields.io/static/v1?label=demo&message=huggingface%20gradio&color=orange)](https://huggingface.co/spaces/xintao/gfpgan)\n1. [colab demo](https://colab.research.google.com/drive/1svsobd9ajckixthgtzhgrhrffi6uuyoo) for gfpgan <a href=\"https://colab.research.google.com/drive/1svsobd9ajckixthgtzhgrhrffi6uuyoo\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"google colab logo\"></a>; (another [colab demo](https://colab.research.google.com/drive/1oa1wwkb4m4l1gmr7ctswdvgocoeslcha?usp=sharing) for the original paper model)\n\n<!-- 3. online demo: [replicate.ai](https://replicate.com/xinntao/gfpgan) (may need to sign in, return the whole image)\n4. online demo: [baseten.co](https://app.baseten.co/applications/q04lz0d/operator_views/8qzg6bg) (backed by gpu, returns the whole image)\n5. we provide a *clean* version of gfpgan, which can run without cuda extensions. so that it can run in **windows** or on **cpu mode**. -->\n\n> :rocket: **thanks for your interest in our work. you may also want to check our new updates on the *tiny models* for *anime images and videos* in [real-esrgan](https://github.com/xinntao/real-esrgan/blob/master/docs/anime_video_model.md)** :blush:\n\ngfpgan aims at developing a **practical algorithm for real-world face restoration**.<br>\nit leverages rich and diverse priors encapsulated in a pretrained face gan (*e.g.*, stylegan2) for blind face restoration.\n\n:question: frequently asked questions can be found in [faq.md](faq.md).\n\n:triangular_flag_on_post: **updates**\n\n- :white_check_mark: add [restoreformer](https://github.com/wzhouxiff/restoreformer) inference codes.\n- :white_check_mark: add [v1.4 model](https://github.com/tencentarc/gfpgan/releases/download/v1.3.0/gfpganv1.4.pth), which produces slightly more details and better identity than v1.3.\n- :white_check_mark: add **[v1.3 model](https://github.com/tencentarc/gfpgan/releases/download/v1.3.0/gfpganv1.3.pth)**, which produces **more natural** restoration results, and better results on *very low-quality* / *high-quality* inputs. see more in [model zoo](#european_castle-model-zoo), [comparisons.md](comparisons.md)\n- :white_check_mark: integrated to [huggingface spaces](https://huggingface.co/spaces) with [gradio](https://github.com/gradio-app/gradio). see [gradio web demo](https://huggingface.co/spaces/akhaliq/gfpgan).\n- :white_check_mark: support enhancing non-face regions (background) with [real-esrgan](https://github.com/xinntao/real-esrgan).\n- :white_check_mark: we provide a *clean* version of gfpgan, which does not require cuda extensions.\n- :white_check_mark: we provide an updated model without colorizing faces.\n\n---\n\nif gfpgan is helpful in your photos/projects, please help to :star: this repo or recommend it to your friends. thanks:blush:\nother recommended projects:<br>\n:arrow_forward: [real-esrgan](https://github.com/xinntao/real-esrgan): a practical algorithm for general image restoration<br>\n:arrow_forward: [basicsr](https://github.com/xinntao/basicsr): an open-source image and video restoration toolbox<br>\n:arrow_forward: [facexlib](https://github.com/xinntao/facexlib): a collection that provides useful face-relation functions<br>\n:arrow_forward: [handyview](https://github.com/xinntao/handyview): a pyqt5-based image viewer that is handy for view and comparison<br>\n\n---\n\n### :book: gfp-gan: towards real-world blind face restoration with generative facial prior\n\n> [[paper](https://arxiv.org/abs/2101.04061)] &emsp; [[project page](https://xinntao.github.io/projects/gfpgan)] &emsp; [demo] <br>\n> [xintao wang](https://xinntao.github.io/), [yu li](https://yu-li.github.io/), [honglun zhang](https://scholar.google.com/citations?hl=en&user=kjqlrooaaaaj), [ying shan](https://scholar.google.com/citations?user=4oxbp9uaaaaj&hl=en) <br>\n> applied research center (arc), tencent pcg\n\n<p align=\"center\">\n  <img src=\"https://xinntao.github.io/projects/gfpgan_src/gfpgan_teaser.jpg\">\n</p>\n\n---\n\n## :wrench: dependencies and installation\n\n- python >= 3.7 (recommend to use [anaconda](https://www.anaconda.com/download/#linux) or [miniconda](https://docs.conda.io/en/latest/miniconda.html))\n- [pytorch >= 1.7](https://pytorch.org/)\n- option: nvidia gpu + [cuda](https://developer.nvidia.com/cuda-downloads)\n- option: linux\n\n### installation\n\nwe now provide a *clean* version of gfpgan, which does not require customized cuda extensions. <br>\nif you want to use the original model in our paper, please see [papermodel.md](papermodel.md) for installation.\n\n1. clone repo\n\n    ```bash\n    git clone https://github.com/tencentarc/gfpgan.git\n    cd gfpgan\n    ```\n\n1. install dependent packages\n\n    ```bash\n    # install basicsr - https://github.com/xinntao/basicsr\n    # we use basicsr for both training and inference\n    pip install basicsr\n\n    # install facexlib - https://github.com/xinntao/facexlib\n    # we use face detection and face restoration helper in the facexlib package\n    pip install facexlib\n\n    pip install -r requirements.txt\n    python setup.py develop\n\n    # if you want to enhance the background (non-face) regions with real-esrgan,\n    # you also need to install the realesrgan package\n    pip install realesrgan\n    ```\n\n## :zap: quick inference\n\nwe take the v1.3 version for an example. more models can be found [here](#european_castle-model-zoo).\n\ndownload pre-trained models: [gfpganv1.3.pth](https://github.com/tencentarc/gfpgan/releases/download/v1.3.0/gfpganv1.3.pth)\n\n```bash\nwget https://github.com/tencentarc/gfpgan/releases/download/v1.3.0/gfpganv1.3.pth -p experiments/pretrained_models\n```\n\n**inference!**\n\n```bash\npython inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2\n```\n\n```console\nusage: python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2 [options]...\n\n  -h                   show this help\n  -i input             input image or folder. default: inputs/whole_imgs\n  -o output            output folder. default: results\n  -v version           gfpgan model version. option: 1 | 1.2 | 1.3. default: 1.3\n  -s upscale           the final upsampling scale of the image. default: 2\n  -bg_upsampler        background upsampler. default: realesrgan\n  -bg_tile             tile size for background sampler, 0 for no tile during testing. default: 400\n  -suffix              suffix of the restored faces\n  -only_center_face    only restore the center face\n  -aligned             input are aligned faces\n  -ext                 image extension. options: auto | jpg | png, auto means using the same extension as inputs. default: auto\n```\n\nif you want to use the original model in our paper, please see [papermodel.md](papermodel.md) for installation and inference.\n\n## :european_castle: model zoo\n\n| version | model name  | description |\n| :---: | :---:        |     :---:      |\n| v1.3 | [gfpganv1.3.pth](https://github.com/tencentarc/gfpgan/releases/download/v1.3.0/gfpganv1.3.pth) | based on v1.2; **more natural** restoration results; better results on very low-quality / high-quality inputs. |\n| v1.2 | [gfpgancleanv1-noce-c2.pth](https://github.com/tencentarc/gfpgan/releases/download/v0.2.0/gfpgancleanv1-noce-c2.pth) | no colorization; no cuda extensions are required. trained with more data with pre-processing. |\n| v1 | [gfpganv1.pth](https://github.com/tencentarc/gfpgan/releases/download/v0.1.0/gfpganv1.pth) | the paper model, with colorization. |\n\nthe comparisons are in [comparisons.md](comparisons.md).\n\nnote that v1.3 is not always better than v1.2. you may need to select different models based on your purpose and inputs.\n\n| version | strengths  | weaknesses |\n| :---: | :---:        |     :---:      |\n|v1.3 |  \u2713 natural outputs<br> \u2713better results on very low-quality inputs <br> \u2713 work on relatively high-quality inputs <br>\u2713 can have repeated (twice) restorations | \u2717 not very sharp <br> \u2717 have a slight change on identity |\n|v1.2 |  \u2713 sharper output <br> \u2713 with beauty makeup | \u2717 some outputs are unnatural |\n\nyou can find **more models (such as the discriminators)** here: [[google drive](https://drive.google.com/drive/folders/17rlifzcumoquhlnptdskoleghwwjonhu?usp=sharing)], or [[tencent cloud \u817e\u8baf\u5fae\u4e91](https://share.weiyun.com/shyoccoc)]\n\n## :computer: training\n\nwe provide the training codes for gfpgan (used in our paper). <br>\nyou could improve it according to your own needs.\n\n**tips**\n\n1. more high quality faces can improve the restoration quality.\n2. you may need to perform some pre-processing, such as beauty makeup.\n\n**procedures**\n\n(you can try a simple version ( `options/train_gfpgan_v1_simple.yml`) that does not require face component landmarks.)\n\n1. dataset preparation: [ffhq](https://github.com/nvlabs/ffhq-dataset)\n\n1. download pre-trained models and other data. put them in the `experiments/pretrained_models` folder.\n    1. [pre-trained stylegan2 model: stylegan2_512_cmul1_ffhq_b12g4_scratch_800k.pth](https://github.com/tencentarc/gfpgan/releases/download/v0.1.0/stylegan2_512_cmul1_ffhq_b12g4_scratch_800k.pth)\n    1. [component locations of ffhq: ffhq_eye_mouth_landmarks_512.pth](https://github.com/tencentarc/gfpgan/releases/download/v0.1.0/ffhq_eye_mouth_landmarks_512.pth)\n    1. [a simple arcface model: arcface_resnet18.pth](https://github.com/tencentarc/gfpgan/releases/download/v0.1.0/arcface_resnet18.pth)\n\n1. modify the configuration file `options/train_gfpgan_v1.yml` accordingly.\n\n1. training\n\n> python -m torch.distributed.launch --nproc_per_node=4 --master_port=22021 gfpgan/train.py -opt options/train_gfpgan_v1.yml --launcher pytorch\n\n## :scroll: license and acknowledgement\n\ngfpgan is released under apache license version 2.0.\n\n## bibtex\n\n    @inproceedings{wang2021gfpgan,\n        author = {xintao wang and yu li and honglun zhang and ying shan},\n        title = {towards real-world blind face restoration with generative facial prior},\n        booktitle={the ieee conference on computer vision and pattern recognition (cvpr)},\n        year = {2021}\n    }\n\n## :e-mail: contact\n\nif you have any question, please email `xintao.wang@outlook.com` or `xintaowang@tencent.com`.\n\n\n",
  "docs_url": null,
  "keywords": "computer vision,pytorch,image restoration,super-resolution,face restoration,gan,gfpgan",
  "license": "apache license version 2.0",
  "name": "gfpgan",
  "package_url": "https://pypi.org/project/gfpgan/",
  "project_url": "https://pypi.org/project/gfpgan/",
  "project_urls": {
    "Homepage": "https://github.com/TencentARC/GFPGAN"
  },
  "release_url": "https://pypi.org/project/gfpgan/1.3.8/",
  "requires_dist": [
    "basicsr (>=1.4.2)",
    "facexlib (>=0.2.5)",
    "lmdb",
    "numpy",
    "opencv-python",
    "pyyaml",
    "scipy",
    "tb-nightly",
    "torch (>=1.7)",
    "torchvision",
    "tqdm",
    "yapf"
  ],
  "requires_python": "",
  "summary": "gfpgan aims at developing practical algorithms for real-world face restoration",
  "version": "1.3.8",
  "releases": [],
  "developers": [
    "xintao.wang@outlook.com",
    "xintao_wang"
  ],
  "kwds": "gfpgan_logo stylegan2 stylegan2_512_cmul1_ffhq_b12g4_scratch_800k png gfpgan_src",
  "license_kwds": "apache license version 2.0",
  "libtype": "pypi",
  "id": "pypi_gfpgan",
  "homepage": "https://github.com/tencentarc/gfpgan",
  "release_count": 11,
  "dependency_ids": [
    "pypi_basicsr",
    "pypi_facexlib",
    "pypi_lmdb",
    "pypi_numpy",
    "pypi_opencv_python",
    "pypi_pyyaml",
    "pypi_scipy",
    "pypi_tb_nightly",
    "pypi_torch",
    "pypi_torchvision",
    "pypi_tqdm",
    "pypi_yapf"
  ]
}