{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: developers",
    "license :: osi approved :: apache software license",
    "programming language :: python :: 3",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "# fugue\n\n[![pypi version](https://badge.fury.io/py/fugue.svg)](https://pypi.python.org/pypi/fugue/)\n[![pypi pyversions](https://img.shields.io/pypi/pyversions/fugue.svg)](https://pypi.python.org/pypi/fugue/)\n[![pypi license](https://img.shields.io/pypi/l/fugue.svg)](https://pypi.python.org/pypi/fugue/)\n[![codecov](https://codecov.io/gh/fugue-project/fugue/branch/master/graph/badge.svg?token=zo9yd5n3ia)](https://codecov.io/gh/fugue-project/fugue)\n[![codacy badge](https://app.codacy.com/project/badge/grade/4fa5f2f53e6f48aaa1218a89f4808b91)](https://www.codacy.com/gh/fugue-project/fugue/dashboard?utm_source=github.com&utm_medium=referral&utm_content=fugue-project/fugue&utm_campaign=badge_grade)\n[![downloads](https://static.pepy.tech/badge/fugue)](https://pepy.tech/project/fugue)\n\n| tutorials                                                                                           | api documentation                                                                     | chat with us on slack!                                                                                                   |\n| --------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |\n| [![jupyter book badge](https://jupyterbook.org/badge.svg)](https://fugue-tutorials.readthedocs.io/) | [![doc](https://readthedocs.org/projects/fugue/badge)](https://fugue.readthedocs.org) | [![slack status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](http://slack.fugue.ai) |\n\n\n**fugue is a unified interface for distributed computing that lets users execute python, pandas, and sql code on spark, dask, and ray with minimal rewrites**.\n\nfugue is most commonly used for:\n\n*   **parallelizing or scaling existing python and pandas code** by bringing it to spark, dask, or ray with minimal rewrites.\n*   using [fuguesql](https://fugue-tutorials.readthedocs.io/tutorials/quick_look/ten_minutes_sql.html) to **define end-to-end workflows** on top of pandas, spark, and dask dataframes. fuguesql is an enhanced sql interface that can invoke python code.\n\nto see how fugue compares to other frameworks like dbt, arrow, ibis, pyspark pandas, see the [comparisons](https://fugue-tutorials.readthedocs.io/#how-does-fugue-compare-to)\n\n## [fugue api](https://fugue-tutorials.readthedocs.io/tutorials/quick_look/ten_minutes.html)\n\nthe fugue api is a collection of functions that are capable of running on pandas, spark, dask, and ray. the simplest way to use fugue is the [`transform()` function](https://fugue-tutorials.readthedocs.io/tutorials/beginner/transform.html). this lets users parallelize the execution of a single function by bringing it to spark, dask, or ray. in the example below, the `map_letter_to_food()` function takes in a mapping and applies it on a column. this is just pandas and python so far (without fugue).\n\n```python\nimport pandas as pd\nfrom typing import dict\n\ninput_df = pd.dataframe({\"id\":[0,1,2], \"value\": ([\"a\", \"b\", \"c\"])})\nmap_dict = {\"a\": \"apple\", \"b\": \"banana\", \"c\": \"carrot\"}\n\ndef map_letter_to_food(df: pd.dataframe, mapping: dict[str, str]) -> pd.dataframe:\n    df[\"value\"] = df[\"value\"].map(mapping)\n    return df\n```\n\nnow, the `map_letter_to_food()` function is brought to the spark execution engine by invoking the `transform()` function of fugue. the output `schema` and `params` are passed to the `transform()` call. the `schema` is needed because it's a requirement for distributed frameworks. a schema of `\"*\"` below means all input columns are in the output.\n\n```python\nfrom pyspark.sql import sparksession\nfrom fugue import transform\n\nspark = sparksession.builder.getorcreate()\nsdf = spark.createdataframe(input_df)\n\nout = transform(sdf,\n               map_letter_to_food,\n               schema=\"*\",\n               params=dict(mapping=map_dict),\n               )\n# out is a spark dataframe\nout.show()\n```\n```rst\n+---+------+\n| id| value|\n+---+------+\n|  0| apple|\n|  1|banana|\n|  2|carrot|\n+---+------+\n```\n\n<details>\n  <summary>pyspark equivalent of fugue transform()</summary>\n\n  ```python\nfrom typing import iterator, union\nfrom pyspark.sql.types import structtype\nfrom pyspark.sql import dataframe, sparksession\n\nspark_session = sparksession.builder.getorcreate()\n\ndef mapping_wrapper(dfs: iterator[pd.dataframe], mapping):\n    for df in dfs:\n        yield map_letter_to_food(df, mapping)\n\ndef run_map_letter_to_food(input_df: union[dataframe, pd.dataframe], mapping):\n    # conversion\n    if isinstance(input_df, pd.dataframe):\n        sdf = spark_session.createdataframe(input_df.copy())\n    else:\n        sdf = input_df.copy()\n\n    schema = structtype(list(sdf.schema.fields))\n    return sdf.mapinpandas(lambda dfs: mapping_wrapper(dfs, mapping),\n                            schema=schema)\n\nresult = run_map_letter_to_food(input_df, map_dict)\nresult.show()\n  ```\n</details>\n\nthis syntax is simpler, cleaner, and more maintainable than the pyspark equivalent. at the same time, no edits were made to the original pandas-based function to bring it to spark. it is still usable on pandas dataframes. fugue `transform()` also supports dask and ray as execution engines alongside the default pandas-based engine.\n\nthe fugue api has a broader collection of functions that are also compatible with spark, dask, and ray. for example, we can use `load()` and `save()` to create an end-to-end workflow compatible with spark, dask, and ray. for the full list of functions, see the [top level api](https://fugue.readthedocs.io/en/latest/top_api.html)\n\n```python\nimport fugue.api as fa\n\ndef run(engine=none):\n    with fa.engine_context(engine):\n        df = fa.load(\"/path/to/file.parquet\")\n        out = fa.transform(df, map_letter_to_food, schema=\"*\")\n        fa.save(out, \"/path/to/output_file.parquet\")\n\nrun()                 # runs on pandas\nrun(engine=\"spark\")   # runs on spark\nrun(engine=\"dask\")    # runs on dask\n```\n\nall functions underneath the context will run on the specified backend. this makes it easy to toggle between local execution, and distributed execution.\n\n## [fuguesql](https://fugue-tutorials.readthedocs.io/tutorials/fugue_sql/index.html)\n\nfuguesql is a sql-based language capable of expressing end-to-end data workflows on top of pandas, spark, and dask. the `map_letter_to_food()` function above is used in the sql expression below. this is how to use a python-defined function along with the standard sql `select` statement.\n\n```python\nfrom fugue.api import fugue_sql\nimport json\n\nquery = \"\"\"\n    select id, value\n      from input_df\n    transform using map_letter_to_food(mapping={{mapping}}) schema *\n    \"\"\"\nmap_dict_str = json.dumps(map_dict)\n\n# returns pandas dataframe\nfugue_sql(query,mapping=map_dict_str)\n\n# returns spark dataframe\nfugue_sql(query, mapping=map_dict_str, engine=\"spark\")\n```\n\n## installation\n\nfugue can be installed through pip or conda. for example:\n\n```bash\npip install fugue\n```\n\nin order to use fugue sql, it is strongly recommended to install the `sql` extra:\n\n```bash\npip install fugue[sql]\n```\n\nit also has the following installation extras:\n\n*   **sql**: to support fugue sql. without this extra, the non-sql part still works. before fugue 0.9.0, this extra is included in fugue's core dependency so you don't need to install explicitly. **but for 0,9.0+, this becomes required if you want to use fugue sql.**\n*   **spark**: to support spark as the [executionengine](https://fugue-tutorials.readthedocs.io/tutorials/advanced/execution_engine.html).\n*   **dask**: to support dask as the executionengine.\n*   **ray**: to support ray as the executionengine.\n*   **duckdb**: to support duckdb as the executionengine, read [details](https://fugue-tutorials.readthedocs.io/tutorials/integrations/backends/duckdb.html).\n*   **polars**: to support polars dataframes and extensions using polars.\n*   **ibis**: to enable ibis for fugue workflows, read [details](https://fugue-tutorials.readthedocs.io/tutorials/integrations/backends/ibis.html).\n*   **cpp_sql_parser**: to enable the cpp antlr parser for fugue sql. it can be 50+ times faster than the pure python parser. for the main python versions and platforms, there is already pre-built binaries, but for the remaining, it needs a c++ compiler to build on the fly.\n\nfor example a common use case is:\n\n```bash\npip install \"fugue[duckdb,spark]\"\n```\n\nnote if you already installed spark or duckdb independently, fugue is able to automatically use them without installing the extras.\n\n## [getting started](https://fugue-tutorials.readthedocs.io/)\n\nthe best way to get started with fugue is to work through the 10 minute tutorials:\n\n*   [fugue api in 10 minutes](https://fugue-tutorials.readthedocs.io/tutorials/quick_look/ten_minutes.html)\n*   [fuguesql in 10 minutes](https://fugue-tutorials.readthedocs.io/tutorials/quick_look/ten_minutes_sql.html)\n\nfor the top level api, see:\n\n*   [fugue top level api](https://fugue.readthedocs.io/en/latest/top_api.html)\n\nthe [tutorials](https://fugue-tutorials.readthedocs.io/) can also be run in an interactive notebook environment through binder or docker:\n\n### using binder\n\n[![binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/fugue-project/tutorials/master)\n\n**note it runs slow on binder** because the machine on binder isn't powerful enough for a distributed framework such as spark. parallel executions can become sequential, so some of the performance comparison examples will not give you the correct numbers.\n\n### using docker\n\nalternatively, you should get decent performance by running this docker image on your own machine:\n\n```bash\ndocker run -p 8888:8888 fugueproject/tutorials:latest\n```\n\n\n## jupyter notebook extension\n\nthere is an accompanying [notebook extension](https://pypi.org/project/fugue-jupyter/) for fuguesql that lets users use the `%%fsql` cell magic. the extension also provides syntax highlighting for fuguesql cells. it works for both classic notebook and jupyter lab. more details can be found in the [installation instructions](https://github.com/fugue-project/fugue-jupyter#install).\n\n![fuguesql gif](https://miro.medium.com/max/700/1*6091-rcropyifjtljo0ana.gif)\n\n\n## ecosystem\n\nby being an abstraction layer, fugue can be used with a lot of other open-source projects seamlessly.\n\npython backends:\n\n*   [pandas](https://github.com/pandas-dev/pandas)\n*   [polars](https://www.pola.rs) (dataframes only)\n*   [spark](https://github.com/apache/spark)\n*   [dask](https://github.com/dask/dask)\n*   [ray](http://github.com/ray-project/ray)\n*   [ibis](https://github.com/ibis-project/ibis/)\n\nfuguesql backends:\n\n*   pandas - fuguesql can run on pandas\n*   [duckdb](https://github.com/duckdb/duckdb) - in-process sql olap database management\n*   [dask-sql](https://github.com/dask-contrib/dask-sql) - sql interface for dask\n*   sparksql\n*   [bigquery](https://fugue-tutorials.readthedocs.io/tutorials/integrations/warehouses/bigquery.html)\n*   trino\n\n\nfugue is available as a backend or can integrate with the following projects:\n\n*   [whylogs](https://whylogs.readthedocs.io/en/latest/examples/integrations/fugue_profiling.html?highlight=fugue) - data profiling\n*   [pycaret](https://fugue-tutorials.readthedocs.io/tutorials/integrations/ecosystem/pycaret.html) - low code machine learning\n*   [nixtla](https://fugue-tutorials.readthedocs.io/tutorials/integrations/ecosystem/nixtla.html) - timeseries modelling\n*   [prefect](https://fugue-tutorials.readthedocs.io/tutorials/integrations/ecosystem/prefect.html) - workflow orchestration\n*   [pandera](https://fugue-tutorials.readthedocs.io/tutorials/integrations/ecosystem/pandera.html) - data validation\n*   [datacompy (by capital one)](https://fugue-tutorials.readthedocs.io/tutorials/integrations/ecosystem/datacompy.html) - comparing dataframes\n\nregistered 3rd party extensions (majorly for fugue sql) include:\n\n*   [pandas plot](https://pandas.pydata.org/docs/reference/api/pandas.dataframe.plot.html) - visualize data using matplotlib or plotly\n*   [seaborn](https://seaborn.pydata.org/api.html) - visualize data using seaborn\n*   [whylogs](https://whylogs.readthedocs.io/en/latest/examples/integrations/fugue_profiling.html?highlight=fugue) - visualize data profiling\n*   [vizzu](https://github.com/vizzuhq/ipyvizzu) - visualize data using ipyvizzu\n\n## community and contributing\n\nfeel free to message us on [slack](http://slack.fugue.ai). we also have [contributing instructions](contributing.md).\n\n### case studies\n\n*   [how lyftlearn democratizes distributed compute through kubernetes spark and fugue](https://eng.lyft.com/how-lyftlearn-democratizes-distributed-compute-through-kubernetes-spark-and-fugue-c0875b97c3d9)\n*   [clobotics - large scale image processing with spark through fugue](https://medium.com/fugue-project/large-scale-image-processing-with-spark-through-fugue-e510b9813da8)\n*   [architecture for a data lake rest api using delta lake, fugue & spark (article by bitsofinfo)](https://bitsofinfo.wordpress.com/2023/08/14/data-lake-rest-api-delta-lake-fugue-spark)\n\n### mentioned uses\n\n*   [productionizing data science at interos, inc. (linkedin post by anthony holten)](https://www.linkedin.com/posts/anthony-holten_pandas-spark-dask-activity-7022628193983459328-qvcf)\n*   [multiple time series forecasting with fugue & nixtla at bain & company (linkedin post by fahad akbar)](https://www.linkedin.com/posts/fahadakbar_fugue-datascience-forecasting-activity-7041119034813124608-u08q?utm_source=share&utm_medium=member_desktop)\n\n## further resources\n\nview some of our latest conferences presentations and content. for a more complete list, check the [content](https://fugue-tutorials.readthedocs.io/tutorials/resources/content.html) page in the tutorials.\n\n### blogs\n\n*   [why pandas-like interfaces are sub-optimal for distributed computing](https://towardsdatascience.com/why-pandas-like-interfaces-are-sub-optimal-for-distributed-computing-322dacbce43)\n*   [introducing fuguesql \u2014 sql for pandas, spark, and dask dataframes (towards data science by khuyen tran)](https://towardsdatascience.com/introducing-fuguesql-sql-for-pandas-spark-and-dask-dataframes-63d461a16b27)\n\n### conferences\n\n*   [distributed machine learning at lyft](https://www.youtube.com/watch?v=_ivyiov0lgy)\n*   [comparing the different ways to scale python and pandas code](https://www.youtube.com/watch?v=b3ae0m_xtys)\n*   [large scale data validation with spark and dask (pycon us)](https://www.youtube.com/watch?v=2advbgjo_3q)\n*   [fuguesql - the enhanced sql interface for pandas, spark, and dask dataframes (pydata global)](https://www.youtube.com/watch?v=obpngyjnbbi)\n*   [distributed hybrid parameter tuning](https://www.youtube.com/watch?v=_gbjqskd8qk)\n\n",
  "docs_url": null,
  "keywords": "distributed spark dask ray sql dsl domain specific language",
  "license": "apache-2.0",
  "name": "fugue",
  "package_url": "https://pypi.org/project/fugue/",
  "project_url": "https://pypi.org/project/fugue/",
  "project_urls": {
    "Homepage": "http://github.com/fugue-project/fugue"
  },
  "release_url": "https://pypi.org/project/fugue/0.8.7/",
  "requires_dist": [
    "triad >=0.9.3",
    "adagio >=0.2.4",
    "qpd >=0.4.4",
    "fugue-sql-antlr >=0.1.6",
    "sqlglot",
    "jinja2",
    "sqlglot ; extra == 'all'",
    "jinja2 ; extra == 'all'",
    "fugue-sql-antlr[cpp] >=0.1.6 ; extra == 'all'",
    "pyspark >=3.1.1 ; extra == 'all'",
    "dask[dataframe,distributed] >=2023.5.0 ; extra == 'all'",
    "dask-sql ; extra == 'all'",
    "ray[data] >=2.4.0 ; extra == 'all'",
    "notebook ; extra == 'all'",
    "jupyterlab ; extra == 'all'",
    "ipython >=7.10.0 ; extra == 'all'",
    "duckdb >=0.5.0 ; extra == 'all'",
    "pyarrow >=6.0.1 ; extra == 'all'",
    "pandas >=2.0.2 ; extra == 'all'",
    "ibis-framework <6,>=3.2.0 ; extra == 'all'",
    "polars ; extra == 'all'",
    "fugue-sql-antlr[cpp] >=0.1.6 ; extra == 'cpp_sql_parser'",
    "dask[dataframe,distributed] >=2023.5.0 ; extra == 'dask'",
    "pyarrow >=7.0.0 ; extra == 'dask'",
    "pandas >=2.0.2 ; extra == 'dask'",
    "duckdb >=0.5.0 ; extra == 'duckdb'",
    "numpy ; extra == 'duckdb'",
    "ibis-framework <6,>=3.2.0 ; extra == 'ibis'",
    "notebook ; extra == 'notebook'",
    "jupyterlab ; extra == 'notebook'",
    "ipython >=7.10.0 ; extra == 'notebook'",
    "polars ; extra == 'polars'",
    "ray[data] >=2.4.0 ; extra == 'ray'",
    "duckdb >=0.5.0 ; extra == 'ray'",
    "pyarrow >=6.0.1 ; extra == 'ray'",
    "pyspark >=3.1.1 ; extra == 'spark'",
    "qpd >=0.4.4 ; extra == 'sql'",
    "fugue-sql-antlr >=0.1.6 ; extra == 'sql'",
    "sqlglot ; extra == 'sql'",
    "jinja2 ; extra == 'sql'"
  ],
  "requires_python": ">=3.8",
  "summary": "an abstraction layer for distributed computation",
  "version": "0.8.7",
  "releases": [],
  "developers": [
    "hello@fugue.ai",
    "the_fugue_development_team"
  ],
  "kwds": "sparksql pyspark fuguesql fugue_profiling fugue_sql",
  "license_kwds": "apache-2.0",
  "libtype": "pypi",
  "id": "pypi_fugue",
  "homepage": "http://github.com/fugue-project/fugue",
  "release_count": 110,
  "dependency_ids": [
    "pypi_adagio",
    "pypi_dask",
    "pypi_dask_sql",
    "pypi_duckdb",
    "pypi_fugue_sql_antlr",
    "pypi_ibis_framework",
    "pypi_ipython",
    "pypi_jinja2",
    "pypi_jupyterlab",
    "pypi_notebook",
    "pypi_numpy",
    "pypi_pandas",
    "pypi_polars",
    "pypi_pyarrow",
    "pypi_pyspark",
    "pypi_qpd",
    "pypi_ray",
    "pypi_sqlglot",
    "pypi_triad"
  ]
}