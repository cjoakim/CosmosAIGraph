{
  "classifiers": [
    "development status :: 4 - beta",
    "intended audience :: developers",
    "license :: osi approved :: mit license",
    "operating system :: os independent",
    "programming language :: java",
    "programming language :: python :: 3",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: database :: front-ends"
  ],
  "description": ".. image:: https://img.shields.io/pypi/pyversions/pyathenajdbc.svg\n    :target: https://pypi.org/project/pyathenajdbc/\n\n.. image:: https://github.com/laughingman7743/pyathenajdbc/workflows/test/badge.svg\n    :target: https://github.com/laughingman7743/pyathenajdbc/actions\n\n.. image:: https://codecov.io/gh/laughingman7743/pyathenajdbc/branch/master/graph/badge.svg\n    :target: https://codecov.io/gh/laughingman7743/pyathenajdbc\n\n.. image:: https://img.shields.io/pypi/l/pyathenajdbc.svg\n    :target: https://github.com/laughingman7743/pyathenajdbc/blob/master/license\n\n.. image:: https://pepy.tech/badge/pyathenajdbc/month\n    :target: https://pepy.tech/project/pyathenajdbc\n\n.. image:: https://img.shields.io/badge/code%20style-black-000000.svg\n    :target: https://github.com/psf/black\n\npyathenajdbc\n============\n\npyathenajdbc is an `amazon athena jdbc driver`_ wrapper for the python `db api 2.0 (pep 249)`_.\n\n.. _`db api 2.0 (pep 249)`: https://www.python.org/dev/peps/pep-0249/\n.. _`amazon athena jdbc driver`: https://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html\n\nrequirements\n------------\n\n* python\n\n  - cpython 3.6, 3.7, 3.8, 3.9\n\n* java\n\n  - java >= 8 (jdbc 4.2)\n\njdbc driver compatibility\n-------------------------\n\n+---------------+---------------------+-------------------------------------------------------------------------------+\n| version       | jdbc driver version | vendor                                                                        |\n+===============+=====================+===============================================================================+\n| < 2.0.0       | == 1.1.0            | aws (early released jdbc driver. it is incompatible with simba's jdbc driver) |\n+---------------+---------------------+-------------------------------------------------------------------------------+\n| >= 2.0.0      | >= 2.0.5            | simba                                                                         |\n+---------------+---------------------+-------------------------------------------------------------------------------+\n\ninstallation\n------------\n\n.. code:: bash\n\n    $ pip install pyathenajdbc\n\nextra packages:\n\n+---------------+------------------------------------------+-----------------+\n| package       | install command                          | version         |\n+===============+==========================================+=================+\n| pandas        | ``pip install pyathenajdbc[pandas]``     | >=1.0.0         |\n+---------------+------------------------------------------+-----------------+\n| sqlalchemy    | ``pip install pyathenajdbc[sqlalchemy]`` | >=1.0.0, <2.0.0 |\n+---------------+------------------------------------------+-----------------+\n\nusage\n-----\n\nbasic usage\n~~~~~~~~~~~\n\n.. code:: python\n\n    from pyathenajdbc import connect\n\n    conn = connect(s3outputlocation='s3://your_s3_bucket/path/to/',\n                   awsregion='us-west-2')\n    try:\n        with conn.cursor() as cursor:\n            cursor.execute(\"\"\"\n            select * from one_row\n            \"\"\")\n            print(cursor.description)\n            print(cursor.fetchall())\n    finally:\n        conn.close()\n\ncursor iteration\n~~~~~~~~~~~~~~~~\n\n.. code:: python\n\n    from pyathenajdbc import connect\n\n    conn = connect(s3outputlocation='s3://your_s3_bucket/path/to/',\n                   awsregion='us-west-2')\n    try:\n        with conn.cursor() as cursor:\n            cursor.execute(\"\"\"\n            select * from many_rows limit 10\n            \"\"\")\n            for row in cursor:\n                print(row)\n    finally:\n        conn.close()\n\nquery with parameter\n~~~~~~~~~~~~~~~~~~~~\n\nsupported `db api paramstyle`_ is only ``pyformat``.\n``pyformat`` only supports `named placeholders`_ with old ``%`` operator style and parameters specify dictionary format.\n\n.. code:: python\n\n    from pyathenajdbc import connect\n\n    conn = connect(s3outputlocation='s3://your_s3_bucket/path/to/',\n                   awsregion='us-west-2')\n    try:\n        with conn.cursor() as cursor:\n            cursor.execute(\"\"\"\n            select col_string from one_row_complex\n            where col_string = %(param)s\n            \"\"\", {'param': 'a string'})\n            print(cursor.fetchall())\n    finally:\n        conn.close()\n\nif ``%`` character is contained in your query, it must be escaped with ``%%`` like the following:\n\n.. code:: sql\n\n    select col_string from one_row_complex\n    where col_string = %(param)s or col_string like 'a%%'\n\n.. _`db api paramstyle`: https://www.python.org/dev/peps/pep-0249/#paramstyle\n.. _`named placeholders`: https://pyformat.info/#named_placeholders\n\njvm options\n~~~~~~~~~~~\n\nin the connect method or connection object, you can specify jvm options with a string array.\n\nyou can increase the jvm heap size like the following:\n\n.. code:: python\n\n    from pyathenajdbc import connect\n\n    conn = connect(s3outputlocation='s3://your_s3_bucket/path/to/',\n                   awsregion='us-west-2',\n                   jvm_options=['-xms1024m', '-xmx4096m'])\n    try:\n        with conn.cursor() as cursor:\n            cursor.execute(\"\"\"\n            select * from many_rows\n            \"\"\")\n            print(cursor.fetchall())\n    finally:\n        conn.close()\n\njdbc 4.1\n~~~~~~~~\n\nif you want to use jdbc 4.1, download the corresponding jdbc driver\nand specify the path of the downloaded jdbc driver as the argument ``driver_path`` of the connect method or connection object.\n\n* the `athenajdbc41-2.0.7.jar`_ is compatible with jdbc 4.1 and requires jdk 7.0 or later.\n\n.. _`athenajdbc41-2.0.7.jar`: https://s3.amazonaws.com/athena-downloads/drivers/jdbc/simbaathenajdbc_2.0.7/athenajdbc41_2.0.7.jar\n\n.. code:: python\n\n    from pyathenajdbc import connect\n\n    conn = connect(s3outputlocation='s3://your_s3_bucket/path/to/',\n                   awsregion='us-west-2',\n                   driver_path='/path/to/athenajdbc41_2.0.7.jar')\n\njdbc driver configuration options\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nthe connect method or connection object pass keyword arguments as options to the jdbc driver.\nif you want to change the behavior of the jdbc driver,\nspecify the option as a keyword argument in the connect method or connection object.\n\n.. code:: python\n\n    from pyathenajdbc import connect\n\n    conn = connect(s3outputlocation='s3://your_s3_bucket/path/to/',\n                   awsregion='us-west-2',\n                   logpath='/path/to/pyathenajdbc/log/',\n                   loglevel='6')\n\nfor details of the jdbc driver options refer to the official documentation.\n\n* `jdbc driver installation and configuration guide`_.\n\n.. _`jdbc driver installation and configuration guide`: https://s3.amazonaws.com/athena-downloads/drivers/jdbc/simbaathenajdbc_2.0.7/docs/simba+athena+jdbc+driver+install+and+configuration+guide.pdf\n\nnote: option names and values are case-sensitive. the option value is specified as a character string.\n\nsqlalchemy\n~~~~~~~~~~\n\ninstall sqlalchemy with ``pip install sqlalchemy>=1.0.0`` or ``pip install pyathenajdbc[sqlalchemy]``.\nsupported sqlalchemy is 1.0.0 or higher and less than 2.0.0.\n\n.. code:: python\n\n    import contextlib\n    from urllib.parse import quote_plus\n    from sqlalchemy.engine import create_engine\n    from sqlalchemy.sql.expression import select\n    from sqlalchemy.sql.functions import func\n    from sqlalchemy.sql.schema import table, metadata\n\n    conn_str = 'awsathena+jdbc://{user}:{password}@athena.{awsregion}.amazonaws.com:443/'\\\n               '{schema}?s3outputlocation={s3outputlocation}'\n    engine = create_engine(conn_str.format(\n        user=quote_plus('your_access_key'),\n        password=quote_plus('your_secret_access_key'),\n        awsregion='us-west-2',\n        schema='default',\n        s3outputlocation=quote_plus('s3://your_s3_bucket/path/to/')))\n    try:\n        with contextlib.closing(engine.connect()) as conn:\n            many_rows = table('many_rows', metadata(bind=engine), autoload=true)\n            print(select([func.count('*')], from_obj=many_rows).scalar())\n    finally:\n        engine.dispose()\n\nthe connection string has the following format:\n\n.. code:: text\n\n    awsathena+jdbc://{user}:{password}@athena.{awsregion}.amazonaws.com:443/{schema}?s3outputlocation={s3outputlocation}&driver_path={driver_path}&...\n\nif you do not specify ``user`` (i.e. awsaccesskeyid) and ``password`` (i.e. awssecretaccesskey) using instance profile credentials or credential profiles file:\n\n.. code:: text\n\n    awsathena+jdbc://:@athena.{region}.amazonaws.com:443/{schema}?s3outputlocation={s3outputlocation}&driver_path={driver_path}&...\n\nnote: ``s3outputlocation`` requires quote. if ``user``, ``password`` and other parameter contain special characters, quote is also required.\n\npandas\n~~~~~~\n\nas dataframe\n^^^^^^^^^^^^\n\nyou can use the `pandas.read_sql`_ to handle the query results as a `dataframe object`_.\n\n.. code:: python\n\n    from pyathenajdbc import connect\n    import pandas as pd\n\n    conn = connect(user='your_access_key_id',\n                   password='your_secret_access_key',\n                   s3outputlocation='s3://your_s3_bucket/path/to/',\n                   awsregion='us-west-2',\n                   jvm_path='/path/to/jvm')\n    df = pd.read_sql(\"select * from many_rows limit 10\", conn)\n\nthe ``pyathena.util`` package also has helper methods.\n\n.. code:: python\n\n    import contextlib\n    from pyathenajdbc import connect\n    from pyathenajdbc.util import as_pandas\n\n    with contextlib.closing(\n            connect(s3outputlocation='s3://your_s3_bucket/path/to/'\n                    awsregion='us-west-2'))) as conn:\n        with conn.cursor() as cursor:\n            cursor.execute(\"\"\"\n            select * from many_rows\n            \"\"\")\n            df = as_pandas(cursor)\n    print(df.describe())\n\n.. _`pandas.read_sql`: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\n.. _`dataframe object`: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.dataframe.html\n\nto sql\n^^^^^^\n\nyou can use `pandas.dataframe.to_sql`_ to write records stored in dataframe to amazon athena.\n`pandas.dataframe.to_sql`_ uses `sqlalchemy`_, so you need to install it.\n\n.. code:: python\n\n    import pandas as pd\n    from urllib.parse import quote_plus\n    from sqlalchemy import create_engine\n    conn_str = 'awsathena+jdbc://:@athena.{awsregion}.amazonaws.com:443/'\\\n               '{schema}?s3outputlocation={s3outputlocation}&s3location={s3location}&compression=snappy'\n    engine = create_engine(conn_str.format(\n        awsregion='us-west-2',\n        schema_name='your_schema',\n        s3outputlocation=quote_plus('s3://your_s3_bucket/path/to/'),\n        s3location=quote_plus('s3://your_s3_bucket/path/to/')))\n    df = pd.dataframe({'a': [1, 2, 3, 4, 5]})\n    df.to_sql('your_table', engine, schema=\"your_schema\", index=false, if_exists='replace', method='multi')\n\nthe location of the amazon s3 table is specified by the ``s3location`` parameter in the connection string.\nif ``s3location`` is not specified, ``s3outputlocation`` parameter will be used. the following rules apply.\n\n.. code:: text\n\n    s3://{s3location or s3outputlocation}/{schema}/{table}/\n\nthe data format only supports parquet. the compression format is specified by the ``compression`` parameter in the connection string.\n\n.. _`pandas.dataframe.to_sql`: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.dataframe.to_sql.html\n\ncredential\n----------\n\naws credentials provider chain\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nsee `supplying and retrieving aws credentials`_\n\n    https://docs.aws.amazon.com/awsjavasdk/latest/javadoc/com/amazonaws/auth/defaultawscredentialsproviderchain.html\n\n    aws credentials provider chain that looks for credentials in this order:\n\n        * environment variables - aws_access_key_id and aws_secret_access_key (recommended since they are recognized by all the aws sdks and cli except for .net), or aws_access_key and aws_secret_key (only recognized by java sdk)\n        * java system properties - aws.accesskeyid and aws.secretkey\n        * web identity token credentials from the environment or container\n        * credential profiles file at the default location (~/.aws/credentials) shared by all aws sdks and the aws cli\n        * credentials delivered through the amazon ec2 container service if aws_container_credentials_relative_uri\" environment variable is set and security manager has permission to access the variable,\n        * instance profile credentials delivered through the amazon ec2 metadata service\n\nin the connect method or connection object, you can connect by specifying at least ``s3outputlocation`` and ``awsregion``.\n``user`` and ``password`` are not required if environment variables, credential files, or instance profiles have been set.\n\n.. code:: python\n\n    from pyathenajdbc import connect\n\n    conn = connect(s3outputlocation='s3://your_s3_bucket/path/to/',\n                   awsregion='us-west-2')\n\n.. _`supplying and retrieving aws credentials`: https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/credentials.html\n\ntesting\n-------\n\ndepends on the following environment variables:\n\n.. code:: bash\n\n    $ export aws_access_key_id=your_access_key_id\n    $ export aws_secret_access_key=your_secret_access_key\n    $ export aws_default_region=us-west-2\n    $ export aws_athena_s3_staging_dir=s3://your_s3_bucket/path/to/\n\nand you need to create a workgroup named ``test-pyathena-jdbc``.\n\nrun test\n~~~~~~~~\n\n.. code:: bash\n\n    $ pip install poetry\n    $ poetry install -v\n    $ poetry run scripts/test_data/upload_test_data.sh\n    $ poetry run pytest\n    $ poetry run scripts/test_data/delete_test_data.sh\n\nrun test multiple python versions\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code:: bash\n\n    $ pip install poetry\n    $ poetry install -v\n    $ poetry run scripts/test_data/upload_test_data.sh\n    $ pyenv local 3.9.0 3.8.6 3.7.9 3.6.12\n    $ poetry run tox\n    $ poetry run scripts/test_data/delete_test_data.sh\n\ncode formatting\n---------------\n\nthe code formatting uses `black`_ and `isort`_.\n\nappy format\n~~~~~~~~~~~\n\n.. code:: bash\n\n    $ make fmt\n\ncheck format\n~~~~~~~~~~~~\n\n.. code:: bash\n\n    $ make chk\n\n.. _`black`: https://github.com/psf/black\n.. _`isort`: https://github.com/timothycrosley/isort\n\nlicense\n-------\n\nthe license of all python code except jdbc driver is `mit license`_.\n\n.. _`mit license`: license\n\njdbc driver\n~~~~~~~~~~~\n\nfor the license of jdbc driver, please check the following link.\n\n* `jdbc driver release notes`_\n* `jdbc driver license`_\n* `jdbc driver notices`_\n* `jdbc driver third-party licenses`_\n\n.. _`jdbc driver release notes`: jdbc/release-notes.txt\n.. _`jdbc driver license`: jdbc/license.txt\n.. _`jdbc driver notices`: jdbc/notices.txt\n.. _`jdbc driver third-party licenses`: jdbc/third-party-licenses.txt\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "pyathenajdbc",
  "package_url": "https://pypi.org/project/pyathenajdbc/",
  "project_url": "https://pypi.org/project/pyathenajdbc/",
  "project_urls": {
    "Homepage": "https://github.com/laughingman7743/PyAthenaJDBC/"
  },
  "release_url": "https://pypi.org/project/pyathenajdbc/3.0.1/",
  "requires_dist": [
    "jpype1 (>=1.1.0,<2.0.0)",
    "pandas (>=1.0.0); extra == \"pandas\"",
    "sqlalchemy (>=1.0.0,<2.0.0); extra == \"sqlalchemy\""
  ],
  "requires_python": ">=3.6.1,<4.0.0",
  "summary": "amazon athena jdbc driver wrapper for the python db api 2.0 (pep 249)",
  "version": "3.0.1",
  "releases": [],
  "developers": [
    "laughingman7743",
    "laughingman7743@gmail.com"
  ],
  "kwds": "pyathenajdbc badge svg pyathena image",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_pyathenajdbc",
  "homepage": "https://github.com/laughingman7743/pyathenajdbc/",
  "release_count": 32,
  "dependency_ids": [
    "pypi_jpype1",
    "pypi_pandas",
    "pypi_sqlalchemy"
  ]
}