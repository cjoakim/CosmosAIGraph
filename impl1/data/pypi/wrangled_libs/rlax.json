{
  "classifiers": [
    "development status :: 5 - production/stable",
    "environment :: console",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "operating system :: macos :: macos x",
    "operating system :: microsoft :: windows",
    "operating system :: posix :: linux",
    "programming language :: python :: 3",
    "programming language :: python :: 3.7",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "# rlax\n\n![ci status](https://github.com/deepmind/rlax/workflows/ci/badge.svg)\n![docs](https://readthedocs.org/projects/rlax/badge/?version=latest)\n![pypi](https://img.shields.io/pypi/v/rlax)\n\nrlax (pronounced \"relax\") is a library built on top of jax that exposes\nuseful building blocks for implementing reinforcement learning agents. full\ndocumentation can be found at\n [rlax.readthedocs.io](https://rlax.readthedocs.io/en/latest/index.html).\n\n## installation\n\nyou can install the latest released version of rlax from pypi via:\n\n```sh\npip install rlax\n```\n\nor you can install the latest development version from github:\n\n```sh\npip install git+https://github.com/deepmind/rlax.git\n```\n\nall rlax code may then be just in time compiled for different hardware\n(e.g. cpu, gpu, tpu) using `jax.jit`.\n\nin order to run the `examples/` you will also need to clone the repo and\ninstall the additional requirements:\n[optax](https://github.com/deepmind/optax),\n[haiku](https://github.com/deepmind/haiku), and\n[bsuite](https://github.com/deepmind/bsuite).\n\n## content\n\nthe operations and functions provided are not complete algorithms, but\nimplementations of reinforcement learning specific mathematical operations that\nare needed when building fully-functional agents capable of learning:\n\n* values, including both state and action-values;\n* values for non-linear generalizations of the bellman equations.\n* return distributions, aka distributional value functions;\n* general value functions, for cumulants other than the main reward;\n* policies, via policy-gradients in both continuous and discrete action spaces.\n\nthe library supports both on-policy and off-policy learning (i.e. learning from\ndata sampled from a policy different from the agent's policy).\n\nsee file-level and function-level doc-strings for the documentation of these\nfunctions and for references to the papers that introduced and/or used them.\n\n## usage\n\nsee `examples/` for examples of using some of the functions in rlax to\nimplement a few simple reinforcement learning agents, and demonstrate learning\non bsuite's version of the catch environment (a common unit-test for\nagent development in the reinforcement learning literature):\n\nother examples of jax reinforcement learning agents using `rlax` can be found in\n[bsuite](https://github.com/deepmind/bsuite/tree/master/bsuite/baselines).\n\n## background\n\nreinforcement learning studies the problem of a learning system (the *agent*),\nwhich must learn to interact with the universe it is embedded in (the\n*environment*).\n\nagent and environment interact on discrete steps. on each step the agent selects\nan *action*, and is provided in return a (partial) snapshot of the state of the\nenvironment (the *observation*), and a scalar feedback signal (the *reward*).\n\nthe behaviour of the agent is characterized by a probability distribution over\nactions, conditioned on past observations of the environment (the *policy*). the\nagents seeks a policy that, from any given step, maximises the discounted\ncumulative reward that will be collected from that point onwards (the *return*).\n\noften the agent policy or the environment dynamics itself are stochastic. in\nthis case the return is a random variable, and the optimal agent's policy is\ntypically more precisely specified as a policy that maximises the expectation of\nthe return (the *value*), under the agent's and environment's stochasticity.\n\n## reinforcement learning algorithms\n\nthere are three prototypical families of reinforcement learning algorithms:\n\n1.  those that estimate the value of states and actions, and infer a policy by\n    *inspection* (e.g. by selecting the action with highest estimated value)\n2.  those that learn a model of the environment (capable of predicting the\n    observations and rewards) and infer a policy via *planning*.\n3.  those that parameterize a policy that can be directly *executed*,\n\nin any case, policies, values or models are just functions. in deep\nreinforcement learning such functions are represented by a neural network.\nin this setting, it is common to formulate reinforcement learning updates as\ndifferentiable pseudo-loss functions (analogously to (un-)supervised learning).\nunder automatic differentiation, the original update rule is recovered.\n\nnote however, that in particular, the updates are only valid if the input data\nis sampled in the correct manner. for example, a policy gradient loss is only\nvalid if the input trajectory is an unbiased sample from the current policy;\ni.e. the data are on-policy. the library cannot check or enforce such\nconstraints. links to papers describing how each operation is used are however\nprovided in the functions' doc-strings.\n\n## naming conventions and developer guidelines\n\nwe define functions and operations for agents interacting with a single stream\nof experience. the jax construct `vmap` can be used to apply these same\nfunctions to batches (e.g. to support *replay* and *parallel* data generation).\n\nmany functions consider policies, actions, rewards, values, in consecutive\ntimesteps in order to compute their outputs. in this case the suffix `_t` and\n`tm1` is often to clarify on which step each input was generated, e.g:\n\n*   `q_tm1`: the action value in the `source` state of a transition.\n*   `a_tm1`: the action that was selected in the `source` state.\n*   `r_t`: the resulting rewards collected in the `destination` state.\n*   `discount_t`: the `discount` associated with a transition.\n*   `q_t`: the action values in the `destination` state.\n\nextensive testing is provided for each function. all tests should also verify\nthe output of `rlax` functions when compiled to xla using `jax.jit` and when\nperforming batch operations using `jax.vmap`.\n\n## citing rlax\n\nrlax is part of the [deepmind jax ecosystem], to cite rlax please use\nthe [deepmind jax ecosystem citation].\n\n[deepmind jax ecosystem]: https://deepmind.com/blog/article/using-jax-to-accelerate-our-research \"deepmind jax ecosystem\"\n[deepmind jax ecosystem citation]: https://github.com/deepmind/jax/blob/main/deepmind2020jax.txt \"citation\"\n\n",
  "docs_url": null,
  "keywords": "reinforcement-learning python machine learning",
  "license": "apache 2.0",
  "name": "rlax",
  "package_url": "https://pypi.org/project/rlax/",
  "project_url": "https://pypi.org/project/rlax/",
  "project_urls": {
    "Homepage": "https://github.com/deepmind/rlax"
  },
  "release_url": "https://pypi.org/project/rlax/0.1.6/",
  "requires_dist": [
    "absl-py (>=0.9.0)",
    "chex (>=0.0.8)",
    "distrax (>=0.0.2)",
    "dm-env",
    "jax (>=0.3.0)",
    "jaxlib (>=0.1.37)",
    "numpy (>=1.18.0)"
  ],
  "requires_python": ">=3.9",
  "summary": "a library of reinforcement learning building blocks in jax.",
  "version": "0.1.6",
  "releases": [],
  "developers": [
    "deepmind",
    "rlax-dev@google.com"
  ],
  "kwds": "rlax reinforcement implementations optax rewards",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_rlax",
  "homepage": "https://github.com/deepmind/rlax",
  "release_count": 9,
  "dependency_ids": [
    "pypi_absl_py",
    "pypi_chex",
    "pypi_distrax",
    "pypi_dm_env",
    "pypi_jax",
    "pypi_jaxlib",
    "pypi_numpy"
  ]
}