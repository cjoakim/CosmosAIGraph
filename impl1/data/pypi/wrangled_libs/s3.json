{
  "classifiers": [
    "intended audience :: developers",
    "programming language :: python"
  ],
  "description": "=========\ns3\n=========\n\n.. contents::\n\noverview\n========\n\ns3 is a connector to s3, amazon's simple storage system rest api.\n\nuse it to upload, download, delete, copy, test files for existence in s3, or \nupdate their metadata.\n\ns3 files may have metadata in addition to their content.  \nmetadata is a set of key/value pairs.  \nmetadata may be set when the file is uploaded \nor it can be updated subsequently.\n\ns3 files are stored in s3 buckets.  buckets can be created, listed, \nconfigured, and deleted.  the bucket configuration can be read and the \nbucket contents can be listed.\n\nin addition to the s3 python module, \nthis package contains a command line tool also named s3.  \nthe tool imports the module and offers a command line\ninterface to some of the module's capability.\n\ninstallation\n============\n\nfrom pypi\n::\n\n    $ pip install s3 \n\nfrom source\n::\n\n    $ hg clone ssh://hg@bitbucket.org/prometheus/s3\n    $ pip install -e s3 \n\nthe installation is successful if you can import s3 \nand run the command line tool.  the following commands \nmust produce no errors:\n::\n\n    $ python -c 'import s3'\n    $ s3 --help\n\napi to remote storage\n=====================\n\ns3 buckets\n----------\n\nbuckets store files.  buckets may be created and deleted.  they may be\nlisted, configured, and loaded with files.  the configuration can be read,\nand the files in the bucket can be listed.\n\nbucket names must be unique across s3 so it is best to use a unique prefix on\nall bucket names.  s3 forbids underscores in bucket names, and although\nit allows periods, these confound dns and should be avoided.\nfor example, at prometheus research \nwe prefix all of our bucket names with: **com-prometheus-**\n\nall the bucket configuration options work the same way - the caller\nprovides xml or json data and perhaps headers or params as well.\n\ns3 accepts a python object for the data argument instead of a string.\nthe object will be converted to xml or json as required.\n\nlikewise, s3 returns a python dict instead of the xml or json string\nreturned by s3.  however, that string is readily available if need be,\nbecause the response returned by requests.request() is exposed to the\ncaller.\n\ns3 filenames\n------------\n\nan s3 file name consists of a bucket and a key.  this pair of\nstrings uniquely identifies the file within s3.\n\nthe s3name class is instantiated with a key and a bucket; the key\nis required and the bucket defaults to none.\n\nthe storage class methods take a **remote_name** argument which\ncan be either a string which is the key, or an instance of the\ns3name class.  when no bucket is given (or the bucket is none) then\nthe default_bucket established when the connection is instantiated\nis used.  if no bucket is given (or the bucket is none) and there\nis no default bucket then a valueerror is raised.\n\nin other words, the s3name class provides a means of using a bucket\nother than the default_bucket.\n\ns3 directories\n--------------\n\nalthough s3 storage is flat: buckets contain keys, s3 lets you impose\na directory tree structure on your bucket by using a delimiter in your\nkeys.\n\nfor example, if you name a key 'a/b/f', and use '/' as the delimiter,\nthen s3 will consider that 'a' is a directory, 'b' is a sub-directory\nof 'a', and 'f' is a file in 'b'.\n\n\nheaders and metadata\n--------------------\n\nadditional http headers may be sent using the methods which write\ndata.  these methods accept an optional **headers** argument which\nis a python dict.  the headers control various aspects of how the\nfile may be handled.  s3 supports a variety of headers.  these are\nnot discussed here.  see amazon's s3 documentation for more info\non s3 headers.\n\nthose headers whose key begins with the special prefix:\n**x-amz-meta-** are considered to be metadata headers and are\nused to set the metadata attributes of the file.\n\nthe methods which read files also return the metadata which\nconsists of only those response headers which begin with\n**x-amz-meta-**.\n\npython classes for s3 data\n--------------------------\n\nto facilitate the transfer of data between s3 and applications various\nclasses were defined which correspond to data returned by s3.\n\nall attributes of these classes are strings.\n\n* s3bucket\n    * creation_date\n    * name\n\n* s3key\n    * e_tag\n    * key\n    * last_modified\n    * owner\n    * size\n    * storage_class\n\n* s3owner\n    * display_name\n    * id\n\nxml strings and python objects\n------------------------------\n\nan xml string consists of a series of nested tags.  an xml tag can be\nrepresented in python as an entry in a dict.  an ordereddict from the\ncollections module should be used when the order of the keys is\nimportant.\n\nthe opening tag (everything between the '<' and the '>') is the key and\neverything between the opening tag and the closing tag is the value of\nthe key.\n\nsince every value must be enclosed in a tag, not every python object can\nrepresent xml in this way.  in particular, lists may only contain dicts\nwhich have a single key.\n\nfor example this xml::\n\n    <a xmlns=\"foo\">\n        <b1>\n            <c1> 1 </c1>\n        </b1>\n        <b2>\n            <c2> 2 </c2>\n        </b2>\n    </a>\n\nis equivalent to this object::\n\n    {'a xmlns=\"foo\"': [{'b1': {'c1': 1}}, {'b2': {'c2': 2}}] }\n\nstorage methods\n---------------\n\nthe arguments **remote_source**, **remote_destination**, and\n**remote_name** may be either a string, or an s3name instance.\n\n**local_name** is a string and is the name of the file on the\nlocal system.  this string is passed directly to open().\n\n**bucket** is a string and is the name of the bucket.\n\n**headers** is a python dict used to encode additional request headers.\n\n**params** is either a python dict used to encode the request\nparameters, or a string containing all the text of the url query string\nafter the '?'.\n\n**data** is a string or an object and is the body of the message.  the\nobject will be converted to an xml or json string as appropriate.\n\nall methods return on success or raise storageerror on failure.\n\nupon return **storage.response** contains the raw response object which\nwas returned by the requests module.  so for example,\nstorage.response.headers contains the response headers returned by s3.\nsee\nhttp://docs.python-requests.org/en/latest/api/ for a description of the\nresponse object.\n\nsee http://docs.aws.amazon.com/amazons3/latest/api/restbucketops.html\nfor a description of the available bucket operations and their arguments.\n\n**storage.bucket_create(bucket, headers={}, data=none)**\n    create a bucket named **bucket**.  **headers** may be used to set\n    either acl or explicit access permissions.  **data** may be used to\n    override the default region.  if data is none, data is set as\n    follows::\n\n        data = {\n                'createbucketconfiguration'\n                ' xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"': {\n                        'locationconstraint': self.connection.region}}\n\n**storage.bucket_delete(bucket)**\n    delete a bucket named **bucket**.\n\n**storage.bucket_delete_cors(bucket)**\n    delete cors configuration of bucket named **bucket**.\n\n**storage.bucket_delete_lifecycle(bucket)**\n    delete lifecycle configuration of bucket named **bucket**.\n\n**storage.bucket_delete_policy(bucket)**\n    delete policy of bucket named **bucket**.\n\n**storage.bucket_delete_tagging(bucket)**\n    delete tagging configuration of bucket named **bucket**.\n\n**storage.bucket_delete_website(bucket)**\n    delete website configuration of bucket named **bucket**.\n\n**exists = storage.bucket_exists(bucket)**\n    test if **bucket** exists in storage.\n\n    exists - boolean.\n\n**storage.bucket_get(self, bucket, params={})**\n    gets the next block of keys from the bucket based on params.\n\n**d = storage.bucket_get_acl(bucket)**\n    returns bucket acl configuration as a dict.\n\n**d = storage.bucket_get_cors(bucket)**\n    returns bucket cors configuration as a dict.\n\n**d = storage.bucket_get_lifecycle(bucket)**\n    returns bucket lifecycle as a dict.\n\n**d = storage.bucket_get_location(bucket)**\n    returns bucket location configuration as a dict.\n\n**d = storage.bucket_get_logging(bucket)**\n    returns bucket logging configuration as a dict.\n\n**d = storage.bucket_get_notification(bucket)**\n    returns bucket notification configuration as a dict.\n\n**d = storage.bucket_get_policy(bucket)**\n    returns bucket policy as a dict.\n\n**d = storage.bucket_get_request_payment(bucket)**\n    returns bucket requestpayment configuration as a dict.\n\n**d = storage.bucket_get_tagging(bucket)**\n    returns bucket tagging configuration as a dict.\n\n**d = storage.bucket_get_versioning(bucket)**\n    returns bucket versioning configuration as a dict.\n\n**d = storage.bucket_get_versions(bucket, params={})**\n    returns bucket versions as a dict.\n\n**d = storage.bucket_get_website(bucket)**\n    returns bucket website configuration as a dict.\n\n**for bucket in storage.bucket_list():**\n    returns a generator object which returns all the buckets for the\n    authenticated user's account.  \n\n    each bucket is returned as an s3bucket instance.\n\n**for key in storage.bucket_list_keys(bucket, delimiter=none, prefix=none, params={}):**\n    returns a generator object which returns all the keys in the bucket.\n    \n    each key is returned as an s3key instance.\n\n    * bucket - the name of the bucket to list\n    * delimiter - used to request common prefixes\n    * prefix - used to filter the listing\n    * params - additional parameters.\n\n    when delimiter is used, the keys (i.e. file names) are returned\n    first, followed by the common prefixes (i.e. directory names).\n    each key is returned as an s3key instance.  each common prefix\n    is returned as a string.\n\n    as a convenience, the delimiter and prefix may be\n    provided as either keyword arguments or as keys in params.  if the\n    arguments are provided, they are used to update params.  in any case,\n    params are passed to s3.\n\n    see http://docs.aws.amazon.com/amazons3/latest/api/restbucketget.html\n    for a description of delimiter, prefix, and the other parameters.\n\n**bucket_set_acl(bucket, headers={}, data='')**\n    configure bucket acl using xml data, or request headers.\n\n**bucket_set_cors(bucket, data='')**\n    configure bucket cors with xml data.\n\n**bucket_set_lifecycle(bucket, data='')**\n    configure bucket lifecycle with xml data.\n\n**bucket_set_logging(bucket, data='')**\n    configure bucket logging with xml data.\n\n**bucket_set_notification(bucket, data='')**\n    configure bucket notification with xml data.\n\n**bucket_set_policy(bucket, data='')**\n    configure bucket policy using json data.\n\n**bucket_set_request_payment(bucket, data='')**\n    configure bucket requestpayment with xml data.\n\n**bucket_set_tagging(bucket, data='')**\n    configure bucket tagging with xml data.\n\n**bucket_set_versioning(bucket, headers={}, data='')**\n    configure bucket versioning using xml data and request headers.\n\n**bucket_set_website(bucket, data='')**\n    configure bucket website with xml data.\n\n**storage.copy(remote_source, remote_destination, headers={})**\n    copy **remote_source** to **remote_destination**.\n\n    the destination metadata is copied from **headers** when it\n    contains metadata; otherwise it is copied from the source\n    metadata.\n\n**storage.delete(remote_name)**\n    delete **remote_name** from storage.\n\n**exists, metadata = storage.exists(remote_name)**\n    test if **remote_name** exists in storage, retrieve its\n    metadata if it does.\n\n    exists - boolean, metadata - dict.\n\n**metadata = storage.read(remote_name, local_name)**\n    download **remote_name** from storage, save it locally as\n    **local_name** and retrieve its metadata.\n\n    metadata - dict.\n\n**storage.update_metadata(remote_name, headers)**\n    update (replace) the metadata associated with **remote_name**\n    with the metadata headers in **headers**.\n\n**storage.write(local_name, remote_name, headers={})**\n    upload **local_name** to storage as **remote_name**, and set\n    its metadata if any metadata headers are in **headers**.\n\nstorageerror\n------------\n\nthere are two forms of exceptions.  \n\nthe first form is when a request to s3 completes but fails.  for example a \nread request may fail because the user does not have read permission.  \nin this case a storageerror is raised with:\n\n* msg - the name of the method that was called (e.g. 'read', 'exists', etc.)\n  \n* exception - a detailed error message\n\n* response - the raw response object returned by requests.\n\nthe second form is when any other exception happens.  for example a disk or \nnetwork error.  in this case storageerror is raised with:\n\n* msg - a detailed error message.\n\n* exception - the exception object\n\n* response - none\n\nusage\n=====\n\nconfiguration\n-------------\n\nfirst configure your yaml file.\n\n- **access_key_id** and **secret_access_key** are generated by the s3 \n  account manager.  they are effectively the username and password for the \n  account.\n\n- **default_bucket** is the name of the default bucket to use when referencing\n  s3 files.  bucket names must be unique (on earth) so by convention we use a\n  prefix on all our bucket names: com-prometheus-  (note: amazon forbids\n  underscores in bucket names, and although they allow periods, periods will \n  confound dns - so it is best not to use periods in bucket names.\n  \n- **endpoint** and **region** are the amazon server url to connect to and\n  its associated region.  see \n  http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region for a list\n  of the available endpoints and their associated regions.\n\n- **tls** true => use https://, false => use http://.  default is true.\n\n- **retry** contains values used to retry requests.request().\n  if a request fails with an error listed in `status_codes`,\n  and the `limit` of tries has not been reached, \n  then a retry message is logged,\n  the program sleeps for `interval` seconds, \n  and the request is sent again.   \n  default is::\n\n    retry:\n        limit: 5\n        interval: 2.5\n        status_codes: \n          - 104\n\n  **limit** is the number of times to try to send the request.\n  0 means unlimited retries.\n\n  **interval** is the number of seconds to wait between retries.\n\n  **status_codes** is a list of request status codes (errors) to retry.\n  \nhere is an example s3.yaml\n::\n\n    ---\n    s3: \n        access_key_id: \"xxxxx\"\n        secret_access_key: \"yyyyyyy\"\n        default_bucket: \"zzzzzzz\"\n        endpoint: \"s3-us-west-2.amazonaws.com\"\n        region: \"us-west-2\"\n\nnext configure your s3 bucket permissions.  you can use s3 to create, \nconfigure, and manage your buckets (see the examples below) or you can \nuse amazon's web interface:\n\n- log onto your amazon account.\n- create a bucket or click on an existing bucket.\n- click on properties.\n- click on permissions.\n- click on edit bucket policy.\n\nhere is a example policy with the required permissions:\n::\n\n    {\n\t    \"version\": \"2008-10-17\",\n\t    \"id\": \"policyxxxxxxxxxxxxx\",\n\t    \"statement\": [\n\t\t    {\n\t\t\t    \"sid\": \"stmtxxxxxxxxxxxxx\",\n\t\t\t    \"effect\": \"allow\",\n\t\t\t    \"principal\": {\n\t\t\t\t    \"aws\": \"arn:aws:iam::xxxxxxxxxxxx:user/xxxxxxx\"\n\t\t\t    },\n\t\t\t    \"action\": [\n\t\t\t\t    \"s3:abortmultipartupload\",\n\t\t\t\t    \"s3:getobjectacl\",\n\t\t\t\t    \"s3:getobjectversion\",\n\t\t\t\t    \"s3:deleteobject\",\n\t\t\t\t    \"s3:deleteobjectversion\",\n\t\t\t\t    \"s3:getobject\",\n\t\t\t\t    \"s3:putobjectacl\",\n\t\t\t\t    \"s3:putobjectversionacl\",\n\t\t\t\t    \"s3:listmultipartuploadparts\",\n\t\t\t\t    \"s3:putobject\",\n\t\t\t\t    \"s3:getobjectversionacl\"\n\t\t\t    ],\n\t\t\t    \"resource\": [\n\t\t\t\t    \"arn:aws:s3:::com.prometheus.cgtest-1/*\",\n\t\t\t\t    \"arn:aws:s3:::com.prometheus.cgtest-1\"\n\t\t\t    ]\n\t\t    }\n\t    ]\n    }\n\nexamples\n--------\n\nonce the yaml file is configured you can instantiate a s3connection and \nyou use that connection to instantiate a storage instance.\n::\n\n    import s3\n    import yaml\n    \n    with open('s3.yaml', 'r') as fi:\n        config = yaml.load(fi)\n\n    connection = s3.s3connection(**config['s3'])    \n    storage = s3.storage(connection)\n\nthen you call methods on the storage instance.  \n\nthe following code creates a bucket called \"com-prometheus-my-bucket\" and  \nasserts the bucket exists.  then it deletes the bucket, and asserts the \nbucket does not exist.\n::\n\n    my_bucket_name = 'com-prometheus-my-bucket'\n    storage.bucket_create(my_bucket_name)\n    assert storage.bucket_exists(my_bucket_name)\n    storage.bucket_delete(my_bucket_name)\n    assert not storage.bucket_exists(my_bucket_name)\n\nthe following code lists all the buckets and all the keys in each bucket.\n::\n\n    for bucket in storage.bucket_list():\n        print bucket.name, bucket.creation_date\n        for key in storage.bucket_list_keys(bucket.name):\n            print '\\t', key.key, key.size, key.last_modified, key.owner.display_name\n            \nthe following code uses the default bucket and uploads a file named \"example\" \nfrom the local filesystem as \"example-in-s3\" in s3.  it then checks that \n\"example-in-s3\" exists in storage, downloads the file as \"example-from-s3\", \ncompares the original with the downloaded copy to ensure they are the same, \ndeletes \"example-in-s3\", and finally checks that it is no longer in storage.\n::\n\n    import subprocess\n    try:\n        storage.write(\"example\", \"example-in-s3\")\n        exists, metadata = storage.exists(\"example-in-s3\")\n        assert exists\n        metadata = storage.read(\"example-in-s3\", \"example-from-s3\")\n        assert 0 == subprocess.call(['diff', \"example\", \"example-from-s3\"])\n        storage.delete(\"example-in-s3\")\n        exists, metadata = storage.exists(\"example-in-s3\")\n        assert not exists\n    except storageerror, e:\n        print 'failed:', e\n        \nthe following code again uploads \"example\" as \"example-in-s3\".  this time it \nuses the bucket \"my-other-bucket\" explicitly, and it sets some metadata and \nchecks that the metadata is set correctly.  then it changes the metadata \nand checks that as well.\n::\n\n    headers = {\n        'x-amz-meta-state': 'unprocessed',\n        }\n    remote_name = s3.s3name(\"example-in-s3\", bucket=\"my-other-bucket\")\n    try:\n        storage.write(\"example\", remote_name, headers=headers)\n        exists, metadata = storage.exists(remote_name)\n        assert exists\n        assert metadata == headers\n        headers['x-amz-meta-state'] = 'processed'\n        storage.update_metadata(remote_name, headers)\n        metadata = storage.read(remote_name, \"example-from-s3\")\n        assert metadata == headers\n    except storageerror, e:\n        print 'failed:', e\n\nthe following code configures \"com-prometheus-my-bucket\" with a policy \nthat restricts \"myuser\" to write-only.  myuser can write files but \ncannot read them back, delete them, or even list them.\n::\n\n    storage.bucket_set_policy(\"com-prometheus-my-bucket\", data={\n            \"version\": \"2008-10-17\",\n            \"id\": \"bucketuploadnodelete\",\n            \"statement\": [\n                    {\n                    \"sid\": \"stmt01\",\n                    \"effect\": \"allow\",\n                    \"principal\": {\n                            \"aws\": \"arn:aws:iam::123456789012:user/myuser\"\n                            },\n                    \"action\": [\n                            \"s3:abortmultipartupload\",\n                            \"s3:listmultipartuploadparts\",\n                            \"s3:putobject\",\n                            ],\n                    \"resource\": [\n                            \"arn:aws:s3:::com-prometheus-my-bucket/*\",\n                            \"arn:aws:s3:::com-prometheus-my-bucket\"\n                            ]\n                    }\n                    ]\n            })\n\n\ns3 command line tool\n====================\n\nthis package installs both the s3 python module \nand the s3 command line tool.\n\nthe command line tool provides a convenient way to upload and download \nfiles to and from s3 without writing python code.\n\nas of now the tool supports the put, get, delete, and list commands; \nbut it does not support all the features of the module api.\n\ns3 expects to find ``s3.yaml`` in the current directory.\nif it is not there you must tell s3 where it is using the --config option.\nfor example::\n\n    $ s3 --config /path/to/s3.yaml command [command arguments]\n\nyou must provide a command.  some commands have required arguments \nand/or optional arguments - it depends upon the command.\n\nuse the --help option to see \na list of supported commands and their arguments::\n\n    $ s3 --help\n    usage: s3 [-h] [-c config] [-v] [-b bucket]\n              {get,put,delete,list,create-bucket,delete-bucket,list-buckets} ...\n\n    commands operate on the default bucket unless the --bucket option is used.\n\n    create a bucket\n      create-bucket [bucket_name]\n      the default bucket_name is the default bucket.\n       \n    delete a file from s3\n      delete delete_file\n\n    delete a bucket\n      delete-bucket [bucket_name]\n      the default bucket_name is the default bucket.\n\n    get a file from s3\n      get remote_src [local_dst]\n\n    list all files or list a single file and its metadata.\n      list [list_file]\n\n    list all buckets or list a single bucket.  \n      list-buckets [bucket_name]\n      if bucket_name is given but does not exist, this is printed::\n       \n          '%s not found' % bucket_name\n\n    put a file to s3\n      put local_src [remote_dst]\n\n    arguments:\n      bucket_name\n        the name of the bucket to use.  \n      delete_file\n        the remote file to delete.\n      list_file\n        if present, the file to list (with its metadata),\n        otherwise list all files.\n      local_dst\n        the name of the local file to create (or overwrite).\n        the default is the basename of the remote_src.\n      local_src\n        the name of the local file to put.\n      remote_dst\n        the name of the s3 file to create (or overwrite).\n        the default is the basename of the local_src.\n      remote_src\n        the name of the file in s3 to get.\n\n    positional arguments:\n      {get,put,delete,list,create-bucket,delete-bucket,list-buckets}\n\n    optional arguments:\n      -h, --help            show this help message and exit\n      -c config, --config config\n                            config is the configuration file to use.\n                            default is s3.yaml\n      -v, --verbose         show results of commands.\n      -b bucket, --bucket bucket\n                            use bucket instead of the default bucket.\n\nsee `s3 command line tool`_  in the api reference. \n\n.. _`s3 command line tool`: reference.html#module-bin_s3",
  "docs_url": "https://pythonhosted.org/s3/",
  "keywords": "amazon,aws,s3,upload,download",
  "license": "mit",
  "name": "s3",
  "package_url": "https://pypi.org/project/s3/",
  "project_url": "https://pypi.org/project/s3/",
  "project_urls": {
    "Download": "UNKNOWN",
    "Homepage": "https://bitbucket.org/prometheus/s3/"
  },
  "release_url": "https://pypi.org/project/s3/3.0.0/",
  "requires_dist": [],
  "requires_python": null,
  "summary": "python module which connects to amazon's s3 rest api",
  "version": "3.0.0",
  "releases": [],
  "developers": [
    "paul@prometheusresearch.com",
    "paul_wexler"
  ],
  "kwds": "bin_s3 s3bucket amazons3 s3 s3name",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_s3",
  "homepage": "https://bitbucket.org/prometheus/s3/",
  "release_count": 9,
  "dependency_ids": []
}