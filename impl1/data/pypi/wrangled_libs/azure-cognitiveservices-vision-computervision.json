{
  "classifiers": [
    "development status :: 4 - beta",
    "license :: osi approved :: mit license",
    "programming language :: python",
    "programming language :: python :: 2",
    "programming language :: python :: 2.7",
    "programming language :: python :: 3",
    "programming language :: python :: 3.5",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8"
  ],
  "description": "# azure cognitive services computer vision sdk for python\n\nthe computer vision service provides developers with access to advanced algorithms for processing images and returning information. computer vision algorithms analyze the content of an image in different ways, depending on the visual features you're interested in.\n\nyou can use computer vision in your application to:\n\n- analyze images for insight\n- extract text from images\n- generate thumbnails\n\nlooking for more documentation?\n\n* [sdk reference documentation](https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-computervision/azure.cognitiveservices.vision.computervision)\n* [cognitive services computer vision documentation](https://docs.microsoft.com/azure/cognitive-services/computer-vision/)\n\n## prerequisites\n\n* azure subscription - [create a free account][azure_sub]\n* azure [computer vision resource][computervision_resource]\n* [python 3.6+][python]\n\nif you need a computer vision api account, you can create one with this [azure cli][azure_cli] command:\n\n```bash\nres_region=westeurope\nres_group=<resourcegroup-name>\nacct_name=<computervision-account-name>\n\naz cognitiveservices account create \\\n    --resource-group $res_group \\\n    --name $acct_name \\\n    --location $res_region \\\n    --kind computervision \\\n    --sku s1 \\\n    --yes\n```\n\n## installation\n\ninstall the azure cognitive services computer vision sdk with [pip][pip], optionally within a [virtual environment][venv].\n\n### configure a virtual environment (optional)\n\nalthough not required, you can keep your base system and azure sdk environments isolated from one another if you use a [virtual environment][virtualenv]. execute the following commands to configure and then enter a virtual environment with [venv][venv], such as `cogsrv-vision-env`:\n\n```bash\npython3 -m venv cogsrv-vision-env\nsource cogsrv-vision-env/bin/activate\n```\n\n### install the sdk\n\ninstall the azure cognitive services computer vision sdk for python [package][pypi_computervision] with [pip][pip]:\n\n```bash\npip install azure-cognitiveservices-vision-computervision\n```\n\n## authentication\n\nonce you create your computer vision resource, you need its **region**, and one of its **account keys** to instantiate the client object.\n\nuse these values when you create the instance of the [computervisionclient][ref_computervisionclient] client object.\n\n### get credentials\n\nuse the [azure cli][cloud_shell] snippet below to populate two environment variables with the computer vision account **region** and one of its **keys** (you can also find these values in the [azure portal][azure_portal]). the snippet is formatted for the bash shell.\n\n```bash\nres_group=<resourcegroup-name>\nacct_name=<computervision-account-name>\n\nexport account_region=$(az cognitiveservices account show \\\n    --resource-group $res_group \\\n    --name $acct_name \\\n    --query location \\\n    --output tsv)\n\nexport account_key=$(az cognitiveservices account keys list \\\n    --resource-group $res_group \\\n    --name $acct_name \\\n    --query key1 \\\n    --output tsv)\n```\n\n### create client\n\nonce you've populated the `account_region` and `account_key` environment variables, you can create the [computervisionclient][ref_computervisionclient] client object.\n\n```python\nfrom azure.cognitiveservices.vision.computervision import computervisionclient\nfrom azure.cognitiveservices.vision.computervision.models import visualfeaturetypes\nfrom msrest.authentication import cognitiveservicescredentials\n\nimport os\nregion = os.environ['account_region']\nkey = os.environ['account_key']\n\ncredentials = cognitiveservicescredentials(key)\nclient = computervisionclient(\n    endpoint=\"https://\" + region + \".api.cognitive.microsoft.com/\",\n    credentials=credentials\n)\n```\n\n## usage\n\nonce you've initialized a [computervisionclient][ref_computervisionclient] client object, you can:\n\n* analyze an image: you can analyze an image for certain features such as faces, colors, tags.\n* generate thumbnails: create a custom jpeg image to use as a thumbnail of the original image.\n* get description of an image: get a description of the image based on its subject domain.\n\nfor more information about this service, see [what is computer vision?][computervision_docs].\n\n## examples\n\nthe following sections provide several code snippets covering some of the most common computer vision tasks, including:\n\n* [analyze an image](#analyze-an-image)\n* [get subject domain list](#get-subject-domain-list)\n* [analyze an image by domain](#analyze-an-image-by-domain)\n* [get text description of an image](#get-text-description-of-an-image)\n* [get handwritten text from image](#get-text-from-image)\n* [generate thumbnail](#generate-thumbnail)\n\n### analyze an image\n\nyou can analyze an image for certain features with [`analyze_image`][ref_computervisionclient_analyze_image]. use the [`visual_features`][ref_computervision_model_visualfeatures] property to set the types of analysis to perform on the image. common values are `visualfeaturetypes.tags` and `visualfeaturetypes.description`.\n\n```python\nurl = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/broadway_and_times_square_by_night.jpg/450px-broadway_and_times_square_by_night.jpg\"\n\nimage_analysis = client.analyze_image(url,visual_features=[visualfeaturetypes.tags])\n\nfor tag in image_analysis.tags:\n    print(tag)\n```\n\n### get subject domain list\n\nreview the subject domains used to analyze your image with [`list_models`][ref_computervisionclient_list_models]. these domain names are used when [analyzing an image by domain](#analyze-an-image-by-domain). an example of a domain is `landmarks`.\n\n```python\nmodels = client.list_models()\n\nfor x in models.models_property:\n    print(x)\n```\n\n### analyze an image by domain\n\nyou can analyze an image by subject domain with [`analyze_image_by_domain`][ref_computervisionclient_analyze_image_by_domain]. get the [list of supported subject domains](#get-subject-domain-list) in order to use the correct domain name.\n\n```python\ndomain = \"landmarks\"\nurl = \"https://images.pexels.com/photos/338515/pexels-photo-338515.jpeg\"\nlanguage = \"en\"\n\nanalysis = client.analyze_image_by_domain(domain, url, language)\n\nfor landmark in analysis.result[\"landmarks\"]:\n    print(landmark[\"name\"])\n    print(landmark[\"confidence\"])\n```\n\n### get text description of an image\n\nyou can get a language-based text description of an image with [`describe_image`][ref_computervisionclient_describe_image]. request several descriptions with the `max_description` property if you are doing text analysis for keywords associated with the image. examples of a text description for the following image include `a train crossing a bridge over a body of water`, `a large bridge over a body of water`, and `a train crossing a bridge over a large body of water`.\n\n```python\ndomain = \"landmarks\"\nurl = \"http://www.public-domain-photos.com/free-stock-photos-4/travel/san-francisco/golden-gate-bridge-in-san-francisco.jpg\"\nlanguage = \"en\"\nmax_descriptions = 3\n\nanalysis = client.describe_image(url, max_descriptions, language)\n\nfor caption in analysis.captions:\n    print(caption.text)\n    print(caption.confidence)\n```\n\n### get text from image\n\nyou can get any handwritten or printed text from an image. this requires two calls to the sdk: [`read`][ref_computervisionclient_read] and [`get_read_result`][ref_computervisionclient_get_read_result]. the call to read is asynchronous. in the results of the get_read_result call, you need to check if the first call completed with [`operationstatuscodes`][ref_computervision_model_operationstatuscodes] before extracting the text data. the results include the text as well as the bounding box coordinates for the text.\n\n```python\n# import models\nfrom azure.cognitiveservices.vision.computervision.models import operationstatuscodes\n\nurl = \"https://github.com/azure-samples/cognitive-services-python-sdk-samples/raw/master/samples/vision/images/make_things_happen.jpg\"\nraw = true\nnumberofcharsinoperationid = 36\n\n# sdk call\nrawhttpresponse = client.read(url, language=\"en\", raw=true)\n\n# get id from returned headers\noperationlocation = rawhttpresponse.headers[\"operation-location\"]\nidlocation = len(operationlocation) - numberofcharsinoperationid\noperationid = operationlocation[idlocation:]\n\n# sdk call\nresult = client.get_read_result(operationid)\n\n# get data\nif result.status == operationstatuscodes.succeeded:\n\n    for line in result.analyze_result.read_results[0].lines:\n        print(line.text)\n        print(line.bounding_box)\n```\n\n### generate thumbnail\n\nyou can generate a thumbnail (jpg) of an image with [`generate_thumbnail`][ref_computervisionclient_generate_thumbnail]. the thumbnail does not need to be in the same proportions as the original image.\n\nthis example uses the [pillow][pypi_pillow] package to save the new thumbnail image locally.\n\n```python\nfrom pil import image\nimport io\n\nwidth = 50\nheight = 50\nurl = \"http://www.public-domain-photos.com/free-stock-photos-4/travel/san-francisco/golden-gate-bridge-in-san-francisco.jpg\"\n\nthumbnail = client.generate_thumbnail(width, height, url)\n\nfor x in thumbnail:\n    image = image.open(io.bytesio(x))\n\nimage.save('thumbnail.jpg')\n```\n\n## troubleshooting\n\n### general\n\nwhen you interact with the [computervisionclient][ref_computervisionclient] client object using the python sdk, the [`computervisionerrorresponseexception`][ref_computervision_computervisionerrorexception] class is used to return errors. errors returned by the service correspond to the same http status codes returned for rest api requests.\n\nfor example, if you try to analyze an image with an invalid key, a `401` error is returned. in the following snippet, the [error][ref_httpfailure] is handled gracefully by catching the exception and displaying additional information about the error.\n\n```python\n\ndomain = \"landmarks\"\nurl = \"http://www.public-domain-photos.com/free-stock-photos-4/travel/san-francisco/golden-gate-bridge-in-san-francisco.jpg\"\nlanguage = \"en\"\nmax_descriptions = 3\n\ntry:\n    analysis = client.describe_image(url, max_descriptions, language)\n\n    for caption in analysis.captions:\n        print(caption.text)\n        print(caption.confidence)\nexcept httpfailure as e:\n    if e.status_code == 401:\n        print(\"error unauthorized. make sure your key and region are correct.\")\n    else:\n        raise\n```\n\n### handle transient errors with retries\n\nwhile working with the [computervisionclient][ref_computervisionclient] client, you might encounter transient failures caused by [rate limits][computervision_request_units] enforced by the service, or other transient problems like network outages. for information about handling these types of failures, see [retry pattern][azure_pattern_retry] in the cloud design patterns guide, and the related [circuit breaker pattern][azure_pattern_circuit_breaker].\n\n## next steps\n\n### more sample code\n\nseveral computer vision python sdk samples are available to you in the sdk's github repository. these samples provide example code for additional scenarios commonly encountered while working with computer vision:\n\n* [see sample repo][recognize-text]\n\n### additional documentation\n\nfor more extensive documentation on the computer vision service, see the [azure computer vision documentation][computervision_docs] on docs.microsoft.com.\n\n<!-- links -->\n[pip]: https://pypi.org/project/pip/\n[python]: https://www.python.org/downloads/\n\n[azure_cli]: https://docs.microsoft.com/cli/azure\n[azure_pattern_circuit_breaker]: https://docs.microsoft.com/azure/architecture/patterns/circuit-breaker\n[azure_pattern_retry]: https://docs.microsoft.com/azure/architecture/patterns/retry\n[azure_portal]: https://portal.azure.com\n[azure_sub]: https://azure.microsoft.com/free/\n\n[cloud_shell]: https://docs.microsoft.com/azure/cloud-shell/overview\n\n[venv]: https://docs.python.org/3/library/venv.html\n[virtualenv]: https://virtualenv.pypa.io\n\n[source_code]: https://github.com/azure/azure-sdk-for-python/tree/master/sdk/cognitiveservices/azure-cognitiveservices-vision-computervision\n\n[pypi_computervision]:https://pypi.org/project/azure-cognitiveservices-vision-computervision/\n[pypi_pillow]:https://pypi.org/project/pillow/\n\n[ref_computervision_sdk]: https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-computervision/azure.cognitiveservices.vision.computervision?view=azure-python\n[ref_computervision_computervisionerrorexception]:https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-computervision/azure.cognitiveservices.vision.computervision.models.computervisionerrorresponseexception?view=azure-python\n[ref_httpfailure]: https://docs.microsoft.com/python/api/msrest/msrest.exceptions.httpoperationerror?view=azure-python\n\n\n[computervision_resource]: https://docs.microsoft.com/azure/cognitive-services/computer-vision/vision-api-how-to-topics/howtosubscribe\n\n[computervision_docs]: https://docs.microsoft.com/azure/cognitive-services/computer-vision/home\n\n[ref_computervisionclient]: https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-computervision/azure.cognitiveservices.vision.computervision.computervisionclient?view=azure-python\n\n\n[ref_computervisionclient_analyze_image]: https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-computervision/azure.cognitiveservices.vision.computervision.computervisionclient?view=azure-python#analyze-image-url--visual-features-none--details-none--language--en---custom-headers-none--raw-false----operation-config-\n[ref_computervisionclient_list_models]:https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-computervision/azure.cognitiveservices.vision.computervision.computervisionclient?view=azure-python#list-models-custom-headers-none--raw-false----operation-config-\n[ref_computervisionclient_analyze_image_by_domain]:https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-computervision/azure.cognitiveservices.vision.computervision.computervisionclient?view=azure-python#analyze-image-by-domain-model--url--language--en---custom-headers-none--raw-false----operation-config-\n[ref_computervisionclient_describe_image]:https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-computervision/azure.cognitiveservices.vision.computervision.computervisionclient?view=azure-python#describe-image-url--max-candidates--1---language--en---custom-headers-none--raw-false----operation-config-\n[ref_computervisionclient_read]:https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-computervision/azure.cognitiveservices.vision.computervision.computervisionclient?view=azure-python#read-url--mode--custom-headers-none--raw-false----operation-config-\n[ref_computervisionclient_get_read_result]:https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-computervision/azure.cognitiveservices.vision.computervision.computervisionclient?view=azure-python#get-read-result-operation-id--custom-headers-none--raw-false----operation-config-\n[ref_computervisionclient_generate_thumbnail]:https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-computervision/azure.cognitiveservices.vision.computervision.computervisionclient?view=azure-python#generate-thumbnail-width--height--url--smart-cropping-false--custom-headers-none--raw-false--callback-none----operation-config-\n\n\n[ref_computervision_model_visualfeatures]:https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-computervision/azure.cognitiveservices.vision.computervision.models.visualfeaturetypes?view=azure-python\n\n[ref_computervision_model_operationstatuscodes]:https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-computervision/azure.cognitiveservices.vision.computervision.models.operationstatuscodes?view=azure-python\n\n[computervision_request_units]:https://azure.microsoft.com/pricing/details/cognitive-services/computer-vision/\n\n[recognize-text]:https://github.com/azure-samples/cognitive-services-python-sdk-samples/blob/master/samples/vision/computer_vision_samples.py\n\n# release history\n\n## 0.9.0 (2021-05-31)\n\n**breaking changes**\n\n  - operation computervisionclientoperationsmixin.read has a new signature\n  - operation computervisionclientoperationsmixin.read_in_stream has a new signature\n\n## 0.8.0 (2021-03-22)\n\n**features**\n\n  - model line has a new parameter appearance\n  - model tagresult has a new parameter model_version\n  - model domainmodelresults has a new parameter model_version\n  - model areaofinterestresult has a new parameter model_version\n  - model detectresult has a new parameter model_version\n  - model imageanalysis has a new parameter model_version\n  - model imagedescription has a new parameter model_version\n  - model ocrresult has a new parameter model_version\n  - model computervisionerror has a new parameter innererror\n\n**breaking changes**\n\n  - operation computervisionclientoperationsmixin.analyze_image has a new signature\n  - operation computervisionclientoperationsmixin.analyze_image_by_domain has a new signature\n  - operation computervisionclientoperationsmixin.analyze_image_by_domain_in_stream has a new signature\n  - operation computervisionclientoperationsmixin.analyze_image_in_stream has a new signature\n  - operation computervisionclientoperationsmixin.describe_image has a new signature\n  - operation computervisionclientoperationsmixin.describe_image_in_stream has a new signature\n  - operation computervisionclientoperationsmixin.detect_objects has a new signature\n  - operation computervisionclientoperationsmixin.detect_objects_in_stream has a new signature\n  - operation computervisionclientoperationsmixin.generate_thumbnail has a new signature\n  - operation computervisionclientoperationsmixin.generate_thumbnail_in_stream has a new signature\n  - operation computervisionclientoperationsmixin.get_area_of_interest has a new signature\n  - operation computervisionclientoperationsmixin.get_area_of_interest_in_stream has a new signature\n  - operation computervisionclientoperationsmixin.read_in_stream has a new signature\n  - operation computervisionclientoperationsmixin.recognize_printed_text has a new signature\n  - operation computervisionclientoperationsmixin.recognize_printed_text_in_stream has a new signature\n  - operation computervisionclientoperationsmixin.tag_image has a new signature\n  - operation computervisionclientoperationsmixin.tag_image_in_stream has a new signature\n  - operation computervisionclientoperationsmixin.read has a new signature\n  - model analyzeresults has a new required parameter model_version\n  - model computervisionerror no longer has parameter request_id\n\n## 0.7.0 (2020-10-08)\n\n**features**\n\n  - supports 3.1 service version\n\n## 0.6.0 (2020-05-18)\n\n**features**\n\n  - model line has a new parameter language\n  - added operation computervisionclientoperationsmixin.read\n  - added operation computervisionclientoperationsmixin.get_read_result\n  - added operation computervisionclientoperationsmixin.read_in_stream\n\n**breaking changes**\n\n  - parameter words of model line is now required\n  - parameter bounding_box of model line is now required\n  - parameter text of model line is now required\n  - parameter confidence of model word is now required\n  - removed operation computervisionclientoperationsmixin.get_text_operation_result\n  - removed operation computervisionclientoperationsmixin.get_read_operation_result\n  - removed operation computervisionclientoperationsmixin.recognize_text_in_stream\n  - removed operation computervisionclientoperationsmixin.recognize_text\n  - removed operation computervisionclientoperationsmixin.batch_read_file\n  - removed operation computervisionclientoperationsmixin.batch_read_file_in_stream\n  - model readoperationresult has a new signature\n\n## 0.5.0 (2019-10-01)\n\n**features**\n\n  - model adultinfo has a new parameter is_gory_content\n  - model adultinfo has a new parameter gore_score\n\n**breaking changes**\n\n  - operation computervisionclientoperationsmixin.analyze_image has a\n    new signature\n  - operation\n    computervisionclientoperationsmixin.analyze_image_in_stream has a\n    new signature\n  - operation computervisionclientoperationsmixin.describe_image has a\n    new signature\n  - operation\n    computervisionclientoperationsmixin.describe_image_in_stream has\n    a new signature\n\n## 0.4.0 (2019-06-27)\n\n**breaking changes**\n\n  - \"batch_read_file\" and \"batch_read_file_in_stream\" have no\n    \"mode\" parameter anymore\n\n**bugfix**\n\n  - \"bounding_box\" now supports float numbers\n  - incorrect \"not started\" typo for state reporting\n\n## 0.3.0 (2019-03-11)\n\n**features**\n\n  - model imageanalysis has a new parameter brands\n  - model imageanalysis has a new parameter objects\n  - model word has a new parameter confidence\n\n**breaking changes**\n\n  - client computervisionapi has been renamed computervisionclient\n  - parameter text of model word is now required\n  - parameter bounding_box of model word is now required\n\n## 0.2.0 (2018-06-22)\n\n**features**\n\n  - analyze_image now support 'en', 'es', 'ja', 'pt', 'zh' (including\n    \"in_stream\" version of these operations)\n  - describe_image/tag_image/analyze_image_by_domain now support\n    the language parameter (including \"in_stream\" version of these\n    operations)\n  - client class can be used as a context manager to keep the underlying\n    http session open for performance\n\n**bug fixes**\n\n  - fix several invalid json description, that was raising unexpected\n    exceptions (including ocrresult from bug #2614)\n\n**breaking changes**\n\n  - recognize_text \"detect_handwriting\" boolean is now a \"mode\" str\n    between 'handwritten' and 'printed'\n\n**general breaking changes**\n\nthis version uses a next-generation code generator that *might*\nintroduce breaking changes.\n\n  - model signatures now use only keyword-argument syntax. all\n    positional arguments must be re-written as keyword-arguments. to\n    keep auto-completion in most cases, models are now generated for\n    python 2 and python 3. python 3 uses the \"*\" syntax for\n    keyword-only arguments.\n  - enum types now use the \"str\" mixin (class azureenum(str, enum)) to\n    improve the behavior when unrecognized enum values are encountered.\n    while this is not a breaking change, the distinctions are important,\n    and are documented here:\n    <https://docs.python.org/3/library/enum.html#others> at a glance:\n      - \"is\" should not be used at all.\n      - \"format\" will return the string value, where \"%s\" string\n        formatting will return `nameofenum.stringvalue`. format syntax\n        should be prefered.\n  - new long running operation:\n      - return type changes from\n        `msrestazure.azure_operation.azureoperationpoller` to\n        `msrest.polling.lropoller`. external api is the same.\n      - return type is now **always** a `msrest.polling.lropoller`,\n        regardless of the optional parameters used.\n      - the behavior has changed when using `raw=true`. instead of\n        returning the initial call result as `clientrawresponse`,\n        without polling, now this returns an lropoller. after polling,\n        the final resource will be returned as a `clientrawresponse`.\n      - new `polling` parameter. the default behavior is\n        `polling=true` which will poll using arm algorithm. when\n        `polling=false`, the response of the initial call will be\n        returned without polling.\n      - `polling` parameter accepts instances of subclasses of\n        `msrest.polling.pollingmethod`.\n      - `add_done_callback` will no longer raise if called after\n        polling is finished, but will instead execute the callback right\n        away.\n\n## 0.1.0 (2018-01-23)\n\n  - initial release\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit license",
  "name": "azure-cognitiveservices-vision-computervision",
  "package_url": "https://pypi.org/project/azure-cognitiveservices-vision-computervision/",
  "project_url": "https://pypi.org/project/azure-cognitiveservices-vision-computervision/",
  "project_urls": {
    "Homepage": "https://github.com/Azure/azure-sdk-for-python"
  },
  "release_url": "https://pypi.org/project/azure-cognitiveservices-vision-computervision/0.9.0/",
  "requires_dist": [
    "msrest (>=0.5.0)",
    "azure-common (~=1.1)",
    "azure-cognitiveservices-vision-nspkg ; python_version<'3.0'"
  ],
  "requires_python": "",
  "summary": "microsoft azure cognitive services computer vision client library for python",
  "version": "0.9.0",
  "releases": [],
  "developers": [
    "azpysdkhelp@microsoft.com",
    "microsoft_corporation"
  ],
  "kwds": "azure_cli azure_operation ref_computervisionclient_analyze_image azure ref_computervisionclient_describe_image",
  "license_kwds": "mit license",
  "libtype": "pypi",
  "id": "pypi_azure_cognitiveservices_vision_computervision",
  "homepage": "https://github.com/azure/azure-sdk-for-python",
  "release_count": 9,
  "dependency_ids": [
    "pypi_azure_cognitiveservices_vision_nspkg",
    "pypi_azure_common",
    "pypi_msrest"
  ]
}