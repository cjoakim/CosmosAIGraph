{
  "classifiers": [
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "<br />\n<p align=\"center\">\n    <a href=\"https://github.com/mosaicml/composer#gh-light-mode-only\" class=\"only-light\">\n      <img src=\"https://storage.googleapis.com/docs.mosaicml.com/images/header_light.svg\" width=\"50%\"/>\n    </a>\n    \n</p>\n\n<h2><p align=\"center\">supercharge your model training</p></h2>\n<h3><p align=\"center\">deep learning framework for training at scale</p></h3>\n\n<h4><p align='center'>\n<a href=\"https://www.mosaicml.com\">[website]</a>\n- <a href=\"https://docs.mosaicml.com/projects/composer/en/stable/getting_started/installation.html\">[getting started]</a>\n- <a href=\"https://docs.mosaicml.com/projects/composer/\">[docs]</a>\n- <a href=\"https://www.mosaicml.com/careers\">[we're hiring!]</a>\n</p></h4>\n\n<p align=\"center\">\n    <a href=\"https://pypi.org/project/mosaicml/\">\n        <img alt=\"pypi version\" src=\"https://img.shields.io/pypi/pyversions/mosaicml\">\n    </a>\n    <a href=\"https://pypi.org/project/mosaicml/\">\n        <img alt=\"pypi package version\" src=\"https://img.shields.io/pypi/v/mosaicml\">\n    </a>\n    <a href=\"https://pepy.tech/project/mosaicml/\">\n        <img alt=\"pypi downloads\" src=\"https://static.pepy.tech/personalized-badge/mosaicml?period=month&units=international_system&left_color=grey&right_color=blue&left_text=downloads/month\">\n    </a>\n    <a href=\"https://docs.mosaicml.com/projects/composer/en/stable/\">\n        <img alt=\"documentation\" src=\"https://readthedocs.org/projects/composer/badge/?version=stable\">\n    </a>\n    <a href=\"https://mosaicml.me/slack\">\n        <img alt=\"chat @ slack\" src=\"https://img.shields.io/badge/slack-chat-2eb67d.svg?logo=slack\">\n    </a>\n    <a href=\"https://github.com/mosaicml/composer/blob/dev/license\">\n        <img alt=\"license\" src=\"https://img.shields.io/badge/license-apache%202.0-green.svg?logo=slack\">\n    </a>\n</p>\n<br />\n\n# **\ud83d\udc4b welcome**\n\ncomposer is an open-source deep learning training library by [mosaicml](https://www.mosaicml.com/). built on top of pytorch, the composer library makes it easier to implement distributed training workflows on large-scale clusters.\n\nwe built composer to be **optimized for scalability and usability**, integrating best practices for efficient, multi-node training. by abstracting away low-level complexities like parallelism techniques, distributed data loading, and memory optimization, you can focus on training modern ml models and running experiments without slowing down.\n\nwe recommend using composer to speedup your experimentation workflow if you\u2019re training neural networks of any size, including:\n\n- large language models (llms)\n- diffusion models\n- embedding models (e.g. bert)\n- transformer-based models\n- convolutional neural networks (cnns)\n\ncomposer is heavily used by the mosaicml research team to train state-of-the-art models like mpt, and we open-sourced this library to enable the ml community to do the same. this framework is used by organizations in both the tech industry and the academic sphere and is continually updated with new features, bug fixes, and stability improvements for production workloads.\n\n# **\ud83d\udd11\u00a0key features**\n![composer is to give you better workflows with the ability to maximize scale and customizability.](docs/source/_static/images/key_features.png)\n\nwe designed composer from the ground up for modern deep learning workloads. gone are the days of alexnet and resnet, when state-of-the-art models could be trained on a couple of desktop gpus. today, developing the latest and greatest deep learning models often requires cluster-scale hardware \u2014 but with composer\u2019s help, you\u2019ll hardly notice the difference.\n\nthe heart of composer is our trainer abstraction: a highly optimized pytorch training loop designed to allow both you and your model to iterate faster. our trainer has simple ways for you to configure your parallelization scheme, data loaders, metrics, loggers, and more.\n\n## scalability\n\nwhether you\u2019re training on 1 gpu or 512 gpus, 50mb or 10tb of data - composer is built to keep your workflow simple.\n\n- [**fsdp**](https://docs.mosaicml.com/projects/composer/en/stable/notes/distributed_training.html#fullyshardeddataparallel-fsdp): for large models that are too large to fit on gpus, composer has integrated pytorch [fullyshardeddataparallelism](https://docs.mosaicml.com/projects/composer/en/stable/notes/distributed_training.html#fullyshardeddataparallel-fsdp) into our trainer and made it simple to efficiently parallelize custom models. we\u2019ve found fsdp is competitive performance-wise with much more complex parallelism strategies. alternatively, composer also supports standard pytorch distributed data parallelism (ddp) and deepspeed execution.\n- [**elastic sharded checkpointing**](https://docs.mosaicml.com/projects/composer/en/stable/notes/distributed_training.html#saving-and-loading-sharded-checkpoints-with-fsdp): save on eight gpus, resume on sixteen. composer supports elastic sharded checkpointing, so you never have to worry if your sharded saved state is compatible with your new hardware setup.\n- **data streaming:** working with large datasets? download datasets from cloud blob storage on the fly by integrating with mosaicml [streamingdataset](https://github.com/mosaicml/streaming) during model training.\n\n## customizability\n\nother high-level deep learning trainers provide simplicity at the cost of rigidity. when you want to add your own features, their abstractions get in your way. composer, on the other hand, provides simple ways for you to customize our trainer to your needs.\n\n![composer\u2019s training loop has a series of events that occur at each stage in the training process.](docs/source/_static/images/traning_loop.png)\n\n***fig. 1:** composer\u2019s training loop has a series of events that occur at each stage in the training process. callbacks are functions that users write to run at specific events. for example, our [learning rate monitor callback](https://docs.mosaicml.com/projects/composer/en/stable/api_reference/generated/composer.callbacks.lrmonitor.html#composer.callbacks.lrmonitor) logs the learning rate at every batch_end event.*\n\n- [**callbacks**](https://docs.mosaicml.com/projects/composer/en/stable/trainer/callbacks.html): composer\u2019s callback system allows you to insert custom logic at any point in the training loop. we\u2019ve written callbacks to monitor memory usage, log and visualize images, and estimate your model\u2019s remaining training time, to name a few. this feature is popular among researchers who want to implement and experiment with custom training techniques.\n- [**speedup algorithms**](https://docs.mosaicml.com/projects/composer/en/stable/examples/custom_speedup_methods.html): we draw from the latest research to create a collection of algorithmic speedups. stack these speedups into mosaicml recipes to boost your training speeds. our team has open-sourced the optimal combinations of speedups for different types of models.\n    - **8x speedup: stable diffusion**\n        - $200k original sd2 cost \u2014> $50k ([blog](https://www.mosaicml.com/blog/diffusion))\n    - **7x speedup: resnet-50 on imagenet**\n        - 3h33m \u2014> 25m on 8xa100 ([blog](https://www.mosaicml.com/blog/mosaic-resnet))\n    - **8.8x speedup: bert-base pretraining**\n        - 10h \u2014> 1.13h on 8xa100 ([blog](https://www.mosaicml.com/blog/mosaicbert))\n    - **5.4x speedup: deeplab v3 on ade20k**\n        - 3h30m \u2014> 39m on 8xa100 ([blog](https://www.mosaicml.com/blog/behind-the-scenes))\n\n## better workflows\n\ncomposer is built to automate away low-level pain points and headaches so you can focus on the important (and fun) parts of deep learning and iterate faster.\n\n- [**auto-resumption**](https://docs.mosaicml.com/projects/composer/en/stable/notes/resumption.html): failed training run? have no fear \u2014 just re-run your code, and composer will automatically resume from your latest saved checkpoint.\n- [**cuda oom prevention**](https://docs.mosaicml.com/projects/composer/en/stable/examples/auto_microbatching.html): say goodbye to out-of-memory errors.  set your microbatch size to \u201cauto\u201d, and composer will automatically select the biggest one that fits on your gpus.\n- **[time abstractions](https://docs.mosaicml.com/projects/composer/en/latest/trainer/time.html):** ever messed up your conversion between update steps, epochs, samples, and tokens? specify your training duration with custom units (epochs, batches, samples, and tokens) in your training loop with our `time` class.\n\n## integrations\n\nintegrate with the tools you know and love for experiment tracking and data streaming.\n\n- **cloud integrations**: our checkpointing and logging features have first-class support for remote storage and loading from cloud bucket (oci, gcp, aws s3).\n- **********experiment tracking:********** weights and biases, mlflow, and cometml \u2014 the choice is yours, easily log your data to your favorite platform.\n\n# **\ud83d\ude80 getting started**\n\n## **\ud83d\udccd**prerequisites\n\ncomposer is designed for users who are comfortable with python and have basic familiarity with deep learning fundamentals and pytorch.\n\n**********************************************software requirements:**********************************************  a recent version of pytorch.\n\n**********************************************hardware requirements:**********************************************  system with cuda-compatible gpus (amd + rocm coming soon!). composer can run on cpus, but for full benefits, we recommend using it on hardware accelerators.\n\n## **\ud83d\udcbe installation**\n\ncomposer can be installed with\u00a0`pip`:\n\n<!--pytest.mark.skip-->\n```bash\npip install mosaicml\n```\n\nto simplify the environment setup for composer, we also provide a set of [pre-built docker images](https://docs.mosaicml.com/projects/composer/en/stable/getting_started/installation.html#docker). we *highly recommend* you use our docker images.\n\n## **\ud83c\udfc1\u00a0quick start**\n\nhere is a code snippet demonstrating our trainer on the mnist dataset.\n\n<!--pytest.mark.filterwarnings(r'ignore:some targets have less than 1 total probability:userwarning')-->\n<!--pytest.mark.filterwarnings('ignore:cannot split tensor of length .* into batches of size 128.*:userwarning')-->\n```python\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import dataloader\n\nfrom composer import trainer\nfrom composer.models import mnist_model\nfrom composer.algorithms import labelsmoothing, cutmix, channelslast\n\ntransform = transforms.compose([transforms.totensor()])\ndataset = datasets.mnist(\"data\", train=true, download=true, transform=transform)\ntrain_dataloader = dataloader(dataset, batch_size=128)\n\ntrainer = trainer(\n    model=mnist_model(num_classes=10),\n    train_dataloader=train_dataloader,\n    max_duration=\"2ep\",\n    algorithms=[\n        labelsmoothing(smoothing=0.1),\n        cutmix(alpha=1.0),\n        channelslast(),\n        ]\n)\ntrainer.fit()\n```\n\nnext, check out our [getting started colab](https://colab.research.google.com/github/mosaicml/composer/blob/9f594876f957c912758e540598ac9f47a468c39d/examples/getting_started.ipynb) for a walk-through of composer\u2019s main features. in this tutorial, we will cover the basics of the composer trainer:\n\n- dataloader\n- trainer\n- optimizer and scheduler\n- logging\n- training a baseline model\n- speeding up training\n\n## **\ud83d\udcda learn more**\n\nonce you\u2019ve completed the quick start, you can go through the below tutorials or our [documentation](https://docs.mosaicml.com/projects/composer/en/stable/) to further familiarize yourself with composer.\n\nif you have any questions, please feel free to reach out to us on\u00a0our\u00a0[community slack](https://mosaicml.me/slack)!\n\nhere are some resources actively maintained by the composer community to help you get started:\n<table>\n<thead>\n  <tr>\n      <th><b>resource</b></th>\n      <th><b>details</b></th>\n  </tr>\n</thead>\n<tbody>\n    <tr>\n    <td><a href=\"https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/finetune_huggingface.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">training berts with composer and \ud83e\udd17 </a></td>\n    <td>a colab notebook showing how to train bert models with composer and \ud83e\udd17!</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://github.com/mosaicml/llm-foundry/blob/main/tutorial.md\" target=\"_blank\" rel=\"noopener noreferrer\">pretraining and finetuning an llm tutorial</a></td>\n    <td>a tutorial from mosaicml\u2019s llm foundry, using mosaicml composer, streamingdataset, and mcli on training and evaluating llms.</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://docs.mosaicml.com/projects/composer/en/stable/examples/migrate_from_ptl.html\" target=\"_blank\" rel=\"noopener noreferrer\">migrating from pytorch lightning</a></td>\n    <td>a tutorial is to illustrating a path from working in pytorch lightning to working in composer.</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://docs.mosaicml.com/projects/composer/en/stable/examples/finetune_huggingface.html\" target=\"_blank\" rel=\"noopener noreferrer\">finetuning and pretraining huggingface models</a></td>\n    <td>want to use hugging face models with composer? no problem. here, we\u2019ll walk through using composer to fine-tune a pretrained hugging face bert model.</td>\n  </tr>\n  <tr>\n    <td><a href=\"https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/custom_speedup_methods.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">building speedup methods</a></td>\n    <td>a colab notebook showing how to build new training modifications on top of composer</td>\n  </tr>\n\n</tbody>\n</table>\n\n# \ud83d\udee0\ufe0f\u00a0for best results, use with the mosaicml ecosystem\n\ncomposer can be used on its own, but for the smoothest experience we recommend using it in combination with other components of the mosaicml ecosystem:\n\n![we recommend that you train models with composer, mosaicml streamingdatasets, and the mosaicml platform.](docs/source/_static/images/ecosystem.png)\n\n- [**mosaicml platform**](https://www.mosaicml.com/training) (mcli)- our proprietary command line interface (cli) and python sdk for orchestrating, scaling, and monitoring the gpu nodes and container images executing training and deployment. used by our customers for training their own generative ai models.\n    - **to get started,\u00a0[sign up here](https://www.mosaicml.com/get-started?utm_source=blog&utm_medium=referral&utm_campaign=llama2) to apply for access and check out our [training](https://www.mosaicml.com/training) and [inference](https://www.mosaicml.com/inference) product pages**\n- [**mosaicml llm foundry**](https://github.com/mosaicml/llm-foundry) - this open source repository contains code for training, finetuning, evaluating, and preparing llms for inference with\u00a0[composer](https://github.com/mosaicml/composer). designed to be easy to use, efficient and\u00a0flexible, this codebase is designed to enable rapid experimentation with the latest techniques.\n- [**mosaicml streamingdataset**](https://github.com/mosaicml/streaming) - open-source library for fast, accurate streaming from cloud storage.\n- [**mosaicml diffusion**](https://github.com/mosaicml/diffusion) - open-source code to train your own stable diffusion model on your own data.  learn more via our blogs: ([results](https://www.mosaicml.com/blog/stable-diffusion-2) , [speedup details](https://www.mosaicml.com/blog/diffusion))\n- [**mosaicml examples**](https://github.com/mosaicml/examples) - this repo contains reference examples for using the [mosaicml platform](https://www.notion.so/composer-readme-draft-5d30690d40f04cdf8528f749e98782bf?pvs=21) to train and deploy machine learning models at scale. it's designed to be easily forked/copied and modified.\n\n# **\ud83c\udfc6\u00a0project showcase**\n\nhere are some projects and experiments that used composer. got something to add? share in our\u00a0[community slack](https://mosaicml.me/slack)!\n\n- [**mpt foundation series:**](https://www.mosaicml.com/mpt) commercially usable open source llms, optimized for fast training and inference and trained with composer.\n    - [mpt-7b blog](https://www.mosaicml.com/blog/mpt-7b)\n    - [mpt-7b-8k blog](https://www.mosaicml.com/blog/long-context-mpt-7b-8k)\n    - [mpt-30b blog](https://www.mosaicml.com/blog/mpt-30b)\n- [**mosaic diffusion models**](https://www.mosaicml.com/blog/training-stable-diffusion-from-scratch-costs-160k): see how we trained a stable diffusion model from scratch for <$50k\n- [**replit-code-v1-3b**](https://huggingface.co/replit/replit-code-v1-3b): a 2.7b causal language model focused on\u00a0**code completion,** trained by replit on the mosaicml platform in 10 days.\n- **babyllm:** the first llm to support both arabic and english. this 7b model was trained by metadialog on the world\u2019s largest arabic/english dataset to improve customer support workflows ([blog](https://blogs.nvidia.com/blog/2023/08/31/generative-ai-startups-africa-middle-east/))\n- [**biomedlm**](https://www.mosaicml.com/blog/introducing-pubmed-gpt): a domain-specific llm for bio medicine built by mosaicml and [stanford crfm](https://crfm.stanford.edu/)\n\n# \ud83d\udcab contributors\n\ncomposer is part of the broader machine learning community, and we welcome any contributions, pull requests, or issues!\n\nto start contributing, see our\u00a0[contributing](https://github.com/mosaicml/composer/blob/dev/contributing.md)\u00a0page.\n\np.s.:\u00a0[we're hiring](https://www.mosaicml.com/careers)!\n\n# \u2753faq\n\n- **what is the best tech stack you recommend when training large models?**\n    - we recommend that users combine components of the mosaicml ecosystem for the smoothest experience:\n        - composer\n        - [streamingdataset](https://github.com/mosaicml/streaming)\n        - [mcli](https://www.mosaicml.com/training) (mosaicml platform)\n- **how can i get community support for using composer?**\n    - you can join our [community slack](https://mosaicml.me/slack)!\n- **how does composer compare to other trainers like nemo megatron and pytorch lightning?**\n    - we built composer to be optimized for both simplicity and efficiency. community users have shared that they enjoy composer for its capabilities and ease of use compared to alternative libraries.\n- **how do i use composer to train graph neural networks (gnns), or generative adversarial networks (gans), or models for reinforcement learning (rl)?**\n    - we recommend you use alternative libraries for if you want to train these types of models - a lot of assumptions we made when designing composer are suboptimal for gnns, rl, and gans\n\n# \u270d\ufe0f citation\n```\n@misc{mosaicml2022composer,\n    author = {the mosaic ml team},\n    title = {composer},\n    year = {2021},\n    howpublished = {\\url{https://github.com/mosaicml/composer/}},\n}\n```\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "mosaicml",
  "package_url": "https://pypi.org/project/mosaicml/",
  "project_url": "https://pypi.org/project/mosaicml/",
  "project_urls": {
    "Homepage": "https://github.com/mosaicml/composer"
  },
  "release_url": "https://pypi.org/project/mosaicml/0.17.2/",
  "requires_dist": [
    "pyyaml <7,>=6.0",
    "tqdm <5,>=4.62.3",
    "torchmetrics <1.1,>=0.10.0",
    "torch-optimizer <0.4,>=0.3.0",
    "torchvision <0.17,>=0.13.1",
    "torch <2.1.2,>=1.13.1",
    "requests <3,>=2.26.0",
    "numpy <1.27.0,>=1.21.5",
    "psutil <6,>=5.8.0",
    "coolname <3,>=1.1.0",
    "tabulate ==0.9.0",
    "py-cpuinfo <10,>=8.0.0",
    "packaging <23,>=21.3.0",
    "importlib-metadata <7,>=5.0.0",
    "mosaicml-cli <0.6,>=0.5.25",
    "sphinxext.opengraph ==0.9.0 ; extra == 'all'",
    "pypandoc ==1.12 ; extra == 'all'",
    "monai <1.4,>=0.9.1 ; extra == 'all'",
    "sphinx-panels ==0.6.0 ; extra == 'all'",
    "pandas <3.0,>=2.0.0 ; extra == 'all'",
    "traitlets ==5.13.0 ; extra == 'all'",
    "coverage[toml] ==7.3.0 ; extra == 'all'",
    "pytest-codeblocks ==0.17.0 ; extra == 'all'",
    "pandoc ==2.3 ; extra == 'all'",
    "pynvml <12,>=11.5.0 ; extra == 'all'",
    "timm <0.6,>=0.5.4 ; extra == 'all'",
    "databricks-sdk <1.0,>=0.8.0 ; extra == 'all'",
    "sphinx-argparse ==0.4.0 ; extra == 'all'",
    "pycocotools <3,>=2.0.4 ; extra == 'all'",
    "furo ==2022.9.29 ; extra == 'all'",
    "cryptography ==41.0.5 ; extra == 'all'",
    "wandb <0.17,>=0.13.2 ; extra == 'all'",
    "paramiko <3,>=2.11.0 ; extra == 'all'",
    "protobuf <3.21 ; extra == 'all'",
    "sentencepiece ==0.1.99 ; extra == 'all'",
    "py-cpuinfo <10,>=8.0.0 ; extra == 'all'",
    "sphinxcontrib.katex ==0.9.6 ; extra == 'all'",
    "tensorboard <3.0.0,>=2.9.1 ; extra == 'all'",
    "vit-pytorch ==1.6.1 ; extra == 'all'",
    "junitparser ==3.1.0 ; extra == 'all'",
    "custom-inherit ==2.4.1 ; extra == 'all'",
    "jupyter ==1.0.0 ; extra == 'all'",
    "onnxruntime <2,>=1.12.1 ; extra == 'all'",
    "google-cloud-storage <3.0,>=2.0.0 ; extra == 'all'",
    "sphinx ==4.4.0 ; extra == 'all'",
    "boto3 <2,>=1.21.45 ; extra == 'all'",
    "yamllint ==1.33.0 ; extra == 'all'",
    "sphinxemoji ==0.2.0 ; extra == 'all'",
    "transformers !=4.34.0,<4.36,>=4.11 ; extra == 'all'",
    "setuptools <=59.5.0 ; extra == 'all'",
    "sphinx-markdown-tables ==0.0.17 ; extra == 'all'",
    "ipython ==8.11.0 ; extra == 'all'",
    "pytest ==7.4.3 ; extra == 'all'",
    "pre-commit <4,>=3.4.0 ; extra == 'all'",
    "sphinxcontrib-images ==0.9.4 ; extra == 'all'",
    "mlflow <3.0,>=2.8.1 ; extra == 'all'",
    "toml ==0.10.2 ; extra == 'all'",
    "deepspeed ==0.8.3 ; extra == 'all'",
    "testbook ==0.4.2 ; extra == 'all'",
    "comet-ml <4.0.0,>=3.31.12 ; extra == 'all'",
    "datasets <3,>=2.4 ; extra == 'all'",
    "moto[s3] <5,>=4.0.1 ; extra == 'all'",
    "apache-libcloud <4,>=3.3.1 ; extra == 'all'",
    "recommonmark ==0.7.1 ; extra == 'all'",
    "pydantic <2,>=1.0 ; extra == 'all'",
    "oci <3.0.0,>=2.88.2 ; extra == 'all'",
    "myst-parser ==0.16.1 ; extra == 'all'",
    "GitPython ==3.1.40 ; extra == 'all'",
    "scikit-learn <2,>=1.0.1 ; extra == 'all'",
    "ipykernel ==6.26.0 ; extra == 'all'",
    "fasteners ==0.18 ; extra == 'all'",
    "mosaicml-streaming <1.0 ; extra == 'all'",
    "onnx <2,>=1.12.0 ; extra == 'all'",
    "mock-ssh-server ==0.9.1 ; extra == 'all'",
    "docutils ==0.17.1 ; extra == 'all'",
    "slack-sdk <4,>=3.19.5 ; extra == 'all'",
    "sphinx-copybutton ==0.5.2 ; extra == 'all'",
    "nbsphinx ==0.9.1 ; extra == 'all'",
    "pytest-httpserver <1.1,>=1.0.4 ; extra == 'all'",
    "pycocotools <3,>=2.0.4 ; extra == 'coco'",
    "comet-ml <4.0.0,>=3.31.12 ; extra == 'comet_ml'",
    "databricks-sdk <1.0,>=0.8.0 ; extra == 'databricks'",
    "deepspeed ==0.8.3 ; extra == 'deepspeed'",
    "pydantic <2,>=1.0 ; extra == 'deepspeed'",
    "custom-inherit ==2.4.1 ; extra == 'dev'",
    "junitparser ==3.1.0 ; extra == 'dev'",
    "coverage[toml] ==7.3.0 ; extra == 'dev'",
    "fasteners ==0.18 ; extra == 'dev'",
    "pytest ==7.4.3 ; extra == 'dev'",
    "toml ==0.10.2 ; extra == 'dev'",
    "ipython ==8.11.0 ; extra == 'dev'",
    "ipykernel ==6.26.0 ; extra == 'dev'",
    "jupyter ==1.0.0 ; extra == 'dev'",
    "yamllint ==1.33.0 ; extra == 'dev'",
    "recommonmark ==0.7.1 ; extra == 'dev'",
    "sphinx ==4.4.0 ; extra == 'dev'",
    "pre-commit <4,>=3.4.0 ; extra == 'dev'",
    "docutils ==0.17.1 ; extra == 'dev'",
    "sphinx-markdown-tables ==0.0.17 ; extra == 'dev'",
    "sphinx-argparse ==0.4.0 ; extra == 'dev'",
    "sphinxcontrib.katex ==0.9.6 ; extra == 'dev'",
    "sphinxext.opengraph ==0.9.0 ; extra == 'dev'",
    "sphinxemoji ==0.2.0 ; extra == 'dev'",
    "furo ==2022.9.29 ; extra == 'dev'",
    "sphinx-copybutton ==0.5.2 ; extra == 'dev'",
    "testbook ==0.4.2 ; extra == 'dev'",
    "myst-parser ==0.16.1 ; extra == 'dev'",
    "sphinx-panels ==0.6.0 ; extra == 'dev'",
    "sphinxcontrib-images ==0.9.4 ; extra == 'dev'",
    "pytest-codeblocks ==0.17.0 ; extra == 'dev'",
    "traitlets ==5.13.0 ; extra == 'dev'",
    "nbsphinx ==0.9.1 ; extra == 'dev'",
    "pandoc ==2.3 ; extra == 'dev'",
    "pypandoc ==1.12 ; extra == 'dev'",
    "GitPython ==3.1.40 ; extra == 'dev'",
    "moto[s3] <5,>=4.0.1 ; extra == 'dev'",
    "mock-ssh-server ==0.9.1 ; extra == 'dev'",
    "cryptography ==41.0.5 ; extra == 'dev'",
    "pytest-httpserver <1.1,>=1.0.4 ; extra == 'dev'",
    "setuptools <=59.5.0 ; extra == 'dev'",
    "google-cloud-storage <3.0,>=2.0.0 ; extra == 'gcs'",
    "pynvml <12,>=11.5.0 ; extra == 'health_checker'",
    "apache-libcloud <4,>=3.3.1 ; extra == 'libcloud'",
    "mlflow <3.0,>=2.8.1 ; extra == 'mlflow'",
    "py-cpuinfo <10,>=8.0.0 ; extra == 'mlperf'",
    "transformers !=4.34.0,<4.36,>=4.11 ; extra == 'nlp'",
    "datasets <3,>=2.4 ; extra == 'nlp'",
    "oci <3.0.0,>=2.88.2 ; extra == 'oci'",
    "onnx <2,>=1.12.0 ; extra == 'onnx'",
    "onnxruntime <2,>=1.12.1 ; extra == 'onnx'",
    "pandas <3.0,>=2.0.0 ; extra == 'pandas'",
    "protobuf <3.21 ; extra == 'sentencepiece'",
    "sentencepiece ==0.1.99 ; extra == 'sentencepiece'",
    "slack-sdk <4,>=3.19.5 ; extra == 'slack'",
    "mosaicml-streaming <1.0 ; extra == 'streaming'",
    "boto3 <2,>=1.21.45 ; extra == 'streaming'",
    "paramiko <3,>=2.11.0 ; extra == 'streaming'",
    "pynvml <12,>=11.5.0 ; extra == 'system_metrics_monitor'",
    "tensorboard <3.0.0,>=2.9.1 ; extra == 'tensorboard'",
    "timm <0.6,>=0.5.4 ; extra == 'timm'",
    "monai <1.4,>=0.9.1 ; extra == 'unet'",
    "scikit-learn <2,>=1.0.1 ; extra == 'unet'",
    "vit-pytorch ==1.6.1 ; extra == 'vit'",
    "wandb <0.17,>=0.13.2 ; extra == 'wandb'"
  ],
  "requires_python": ">=3.8",
  "summary": "composer is a pytorch library that enables you to train neural networks faster, at lower cost, and to higher accuracy.",
  "version": "0.17.2",
  "releases": [],
  "developers": [
    "mosaicml",
    "team@mosaicml.com"
  ],
  "kwds": "mosaicml mosaicml2022composer mosaic tutorial header_light",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_mosaicml",
  "homepage": "https://github.com/mosaicml/composer",
  "release_count": 38,
  "dependency_ids": [
    "pypi_apache_libcloud",
    "pypi_boto3",
    "pypi_comet_ml",
    "pypi_coolname",
    "pypi_coverage",
    "pypi_cryptography",
    "pypi_custom_inherit",
    "pypi_databricks_sdk",
    "pypi_datasets",
    "pypi_deepspeed",
    "pypi_docutils",
    "pypi_fasteners",
    "pypi_furo",
    "pypi_gitpython",
    "pypi_google_cloud_storage",
    "pypi_importlib_metadata",
    "pypi_ipykernel",
    "pypi_ipython",
    "pypi_junitparser",
    "pypi_jupyter",
    "pypi_mlflow",
    "pypi_mock_ssh_server",
    "pypi_monai",
    "pypi_mosaicml_cli",
    "pypi_mosaicml_streaming",
    "pypi_moto",
    "pypi_myst_parser",
    "pypi_nbsphinx",
    "pypi_numpy",
    "pypi_oci",
    "pypi_onnx",
    "pypi_onnxruntime",
    "pypi_packaging",
    "pypi_pandas",
    "pypi_pandoc",
    "pypi_paramiko",
    "pypi_pre_commit",
    "pypi_protobuf",
    "pypi_psutil",
    "pypi_py_cpuinfo",
    "pypi_pycocotools",
    "pypi_pydantic",
    "pypi_pynvml",
    "pypi_pypandoc",
    "pypi_pytest",
    "pypi_pytest_codeblocks",
    "pypi_pytest_httpserver",
    "pypi_pyyaml",
    "pypi_recommonmark",
    "pypi_requests",
    "pypi_scikit_learn",
    "pypi_sentencepiece",
    "pypi_setuptools",
    "pypi_slack_sdk",
    "pypi_sphinx",
    "pypi_sphinx_argparse",
    "pypi_sphinx_copybutton",
    "pypi_sphinx_markdown_tables",
    "pypi_sphinx_panels",
    "pypi_sphinxcontrib_images",
    "pypi_sphinxcontrib.katex",
    "pypi_sphinxemoji",
    "pypi_sphinxext.opengraph",
    "pypi_tabulate",
    "pypi_tensorboard",
    "pypi_testbook",
    "pypi_timm",
    "pypi_toml",
    "pypi_torch",
    "pypi_torch_optimizer",
    "pypi_torchmetrics",
    "pypi_torchvision",
    "pypi_tqdm",
    "pypi_traitlets",
    "pypi_transformers",
    "pypi_vit_pytorch",
    "pypi_wandb",
    "pypi_yamllint"
  ]
}