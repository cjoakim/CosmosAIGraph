{
  "classifiers": [
    "development status :: 4 - beta",
    "intended audience :: developers",
    "intended audience :: system administrators",
    "license :: osi approved :: apache software license",
    "programming language :: python",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "programming language :: python :: implementation :: cpython"
  ],
  "description": "fastparquet\n===========\n\n.. image:: https://github.com/dask/fastparquet/actions/workflows/main.yaml/badge.svg\n    :target: https://github.com/dask/fastparquet/actions/workflows/main.yaml\n\n.. image:: https://readthedocs.org/projects/fastparquet/badge/?version=latest\n    :target: https://fastparquet.readthedocs.io/en/latest/\n\nfastparquet is a python implementation of the `parquet\nformat <https://github.com/apache/parquet-format>`_, aiming integrate\ninto python-based big data work-flows. it is used implicitly by\nthe projects dask, pandas and intake-parquet.\n\nwe offer a high degree of support for the features of the parquet format, and\nvery competitive performance, in a small install size and codebase.\n\ndetails of this project, how to use it and comparisons to other work can be found in the documentation_.\n\n.. _documentation: https://fastparquet.readthedocs.io\n\nrequirements\n------------\n\n(all development is against recent versions in the default anaconda channels\nand/or conda-forge)\n\nrequired:\n\n- numpy\n- pandas\n- cython >= 0.29.23 (if building from pyx files)\n- cramjam\n- fsspec\n\nsupported compression algorithms:\n\n- available by default:\n\n  - gzip\n  - snappy\n  - brotli\n  - lz4\n  - zstandard\n\n- optionally supported\n  \n  - `lzo <https://github.com/jd-boyd/python-lzo>`_\n\n\ninstallation\n------------\n\ninstall using conda, to get the latest compiled version::\n\n   conda install -c conda-forge fastparquet\n\nor install from pypi::\n\n   pip install fastparquet\n\nyou may wish to install numpy first, to help pip's resolver.\nthis may install an appropriate wheel, or compile from source. for the latter,\nyou will need a suitable c compiler toolchain on your system.\n\nyou can also install latest version from github::\n\n   pip install git+https://github.com/dask/fastparquet\n\nin which case you should also have ``cython`` to be able to rebuild the c files.\n\nusage\n-----\n\nplease refer to the documentation_.\n\n*reading*\n\n.. code-block:: python\n\n    from fastparquet import parquetfile\n    pf = parquetfile('myfile.parq')\n    df = pf.to_pandas()\n    df2 = pf.to_pandas(['col1', 'col2'], categories=['col1'])\n\nyou may specify which columns to load, which of those to keep as categoricals\n(if the data uses dictionary encoding). the file-path can be a single file,\na metadata file pointing to other data files, or a directory (tree) containing\ndata files. the latter is what is typically output by hive/spark.\n\n*writing*\n\n.. code-block:: python\n\n    from fastparquet import write\n    write('outfile.parq', df)\n    write('outfile2.parq', df, row_group_offsets=[0, 10000, 20000],\n          compression='gzip', file_scheme='hive')\n\nthe default is to produce a single output file with a single row-group\n(i.e., logical segment) and no compression. at the moment, only simple\ndata-types and plain encoding are supported, so expect performance to be\nsimilar to *numpy.savez*.\n\nhistory\n-------\n\nthis project forked in october 2016 from `parquet-python`_, which was not designed\nfor vectorised loading of big data or parallel access.\n\n.. _parquet-python: https://github.com/jcrobak/parquet-python\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "apache license 2.0",
  "name": "fastparquet",
  "package_url": "https://pypi.org/project/fastparquet/",
  "project_url": "https://pypi.org/project/fastparquet/",
  "project_urls": {
    "Homepage": "https://github.com/dask/fastparquet/"
  },
  "release_url": "https://pypi.org/project/fastparquet/2023.10.1/",
  "requires_dist": [],
  "requires_python": ">=3.8",
  "summary": "python support for parquet file format",
  "version": "2023.10.1",
  "releases": [],
  "developers": [
    "martin_durant",
    "mdurant@anaconda.com"
  ],
  "kwds": "fastparquet parquetfile parquet yaml _parquet",
  "license_kwds": "apache license 2.0",
  "libtype": "pypi",
  "id": "pypi_fastparquet",
  "homepage": "https://github.com/dask/fastparquet/",
  "release_count": 47,
  "dependency_ids": []
}