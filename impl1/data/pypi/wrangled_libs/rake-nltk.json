{
  "classifiers": [
    "development status :: 3 - alpha",
    "intended audience :: developers",
    "intended audience :: education",
    "license :: osi approved :: mit license",
    "operating system :: posix",
    "programming language :: python :: 3",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: software development :: build tools",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "rake-nltk\n=========\n\n|pypiv| |pyv| |licence| |build status| |coverage status|\n\nrake short for rapid automatic keyword extraction algorithm, is a domain\nindependent keyword extraction algorithm which tries to determine key\nphrases in a body of text by analyzing the frequency of word appearance\nand its co-occurance with other words in the text.\n\n|demo|\n\nfeatures\n--------\n\n* ridiculously simple interface.\n* configurable word and sentence tokenizers, language based stop words etc\n* configurable ranking metric.\n\nsetup\n-----\n\nusing pip\n~~~~~~~~~\n\n.. code:: bash\n\n    pip install rake-nltk\n\ndirectly from the repository\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code:: bash\n\n    git clone https://github.com/csurfer/rake-nltk.git\n    python rake-nltk/setup.py install\n\nquick start\n-----------\n\n.. code:: python\n\n    from rake_nltk import rake\n\n    # uses stopwords for english from nltk, and all puntuation characters by\n    # default\n    r = rake()\n\n    # extraction given the text.\n    r.extract_keywords_from_text(<text to process>)\n\n    # extraction given the list of strings where each string is a sentence.\n    r.extract_keywords_from_sentences(<list of sentences>)\n\n    # to get keyword phrases ranked highest to lowest.\n    r.get_ranked_phrases()\n\n    # to get keyword phrases ranked highest to lowest with scores.\n    r.get_ranked_phrases_with_scores()\n\ndebugging setup\n---------------\n\nif you see a stopwords error, it means that you do not have the corpus\n`stopwords` downloaded from nltk. you can download it using command below.\n\n.. code:: bash\n\n    python -c \"import nltk; nltk.download('stopwords')\"\n\nreferences\n----------\n\nthis is a python implementation of the algorithm as mentioned in paper\n`automatic keyword extraction from individual documents by stuart rose,\ndave engel, nick cramer and wendy cowley`_\n\nwhy i chose to implement it myself?\n-----------------------------------\n\n-  it is extremely fun to implement algorithms by reading papers. it is\n   the digital equivalent of diy kits.\n-  there are some rather popular implementations out there, in python(\\ `aneesha/rake`_) and\n   node(\\ `waseem18/node-rake`_) but neither seemed to use the power of `nltk`_. by making nltk\n   an integral part of the implementation i get the flexibility and power to extend it in other\n   creative ways, if i see fit later, without having to implement everything myself.\n-  i plan to use it in my other pet projects to come and wanted it to be\n   modular and tunable and this way i have complete control.\n\ncontributing\n------------\n\nbug reports and feature requests\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nplease use `issue tracker`_ for reporting bugs or feature requests.\n\ndevelopment\n~~~~~~~~~~~\n\n1. checkout the repository.\n2. make your changes and add/update relavent tests.\n3. install **`poetry`** using **`pip install poetry`**.\n4. run **`poetry install`** to create project's virtual environment.\n5. run tests using **`poetry run tox`** (any python versions which you don't have checked out will fail this). fix failing tests and repeat.\n6. make documentation changes that are relavant.\n7. install **`pre-commit`** using **`pip install pre-commit`** and run **`pre-commit run --all-files`** to do lint checks.\n8. generate documentation using **`poetry run sphinx-build -b html docs/ docs/_build/html`**.\n9. generate **`requirements.txt`** for automated testing using **`poetry export --dev --without-hashes -f requirements.txt > requirements.txt`**.\n10. commit the changes and raise a pull request.\n\nbuy the developer a cup of coffee!\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nif you found the utility helpful you can buy me a cup of coffee using\n\n|donate|\n\n.. |donate| image:: https://www.paypalobjects.com/webstatic/en_us/i/btn/png/silver-pill-paypal-44px.png\n   :target: https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=3bsbw7d45c4yn&lc=us&currency_code=usd&bn=pp%2ddonationsbf%3abtn_donate_sm%2egif%3anonhosted\n\n.. _automatic keyword extraction from individual documents by stuart rose, dave engel, nick cramer and wendy cowley: https://www.researchgate.net/profile/stuart_rose/publication/227988510_automatic_keyword_extraction_from_individual_documents/links/55071c570cf27e990e04c8bb.pdf\n.. _aneesha/rake: https://github.com/aneesha/rake\n.. _waseem18/node-rake: https://github.com/waseem18/node-rake\n.. _nltk: http://www.nltk.org/\n.. _issue tracker: https://github.com/csurfer/rake-nltk/issues\n\n.. |build status| image:: https://github.com/csurfer/rake-nltk/actions/workflows/pytest.yml/badge.svg\n   :target: https://github.com/csurfer/rake-nltk/actions\n.. |licence| image:: https://img.shields.io/badge/license-mit-blue.svg\n   :target: https://raw.githubusercontent.com/csurfer/rake-nltk/master/license\n.. |coverage status| image:: https://codecov.io/gh/csurfer/rake-nltk/branch/master/graph/badge.svg?token=ghrhwvec9x\n   :target: https://codecov.io/gh/csurfer/rake-nltk\n.. |demo| image:: http://i.imgur.com/wvozu7y.gif\n.. |pypiv| image:: https://img.shields.io/pypi/v/rake-nltk.svg\n   :target: https://pypi.python.org/pypi/rake-nltk\n.. |pyv| image:: https://img.shields.io/pypi/pyversions/rake-nltk.svg\n   :target: https://pypi.python.org/pypi/rake-nltk\n",
  "docs_url": null,
  "keywords": "nlp,text-mining,algorithms,development",
  "license": "mit",
  "name": "rake-nltk",
  "package_url": "https://pypi.org/project/rake-nltk/",
  "project_url": "https://pypi.org/project/rake-nltk/",
  "project_urls": {
    "Homepage": "https://csurfer.github.io/rake-nltk",
    "Repository": "https://github.com/csurfer/rake-nltk"
  },
  "release_url": "https://pypi.org/project/rake-nltk/1.0.6/",
  "requires_dist": [
    "nltk (>=3.6.2,<4.0.0)"
  ],
  "requires_python": ">=3.6,<4.0",
  "summary": "rake short for rapid automatic keyword extraction algorithm, is a domain independent keyword extraction algorithm which tries to determine key phrases in a body of text by analyzing the frequency of word appearance and its co-occurance with other words in the text.",
  "version": "1.0.6",
  "releases": [],
  "developers": [
    "csurfer",
    "sharma.vishwas88@gmail.com"
  ],
  "kwds": "extract_keywords_from_text extract_keywords_from_sentences 227988510_automatic_keyword_extraction_from_individual_documents rake_nltk get_ranked_phrases",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_rake_nltk",
  "homepage": "https://csurfer.github.io/rake-nltk",
  "release_count": 7,
  "dependency_ids": [
    "pypi_nltk"
  ]
}