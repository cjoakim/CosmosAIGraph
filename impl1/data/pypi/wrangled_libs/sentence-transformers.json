{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "programming language :: python :: 3.6",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "<!--- badges: start --->\r\n[![github - license](https://img.shields.io/github/license/ukplab/sentence-transformers?logo=github&style=flat&color=green)][#github-license]\r\n[![pypi - python version](https://img.shields.io/pypi/pyversions/sentence-transformers?logo=pypi&style=flat&color=blue)][#pypi-package]\r\n[![pypi - package version](https://img.shields.io/pypi/v/sentence-transformers?logo=pypi&style=flat&color=orange)][#pypi-package]\r\n[![conda - platform](https://img.shields.io/conda/pn/conda-forge/sentence-transformers?logo=anaconda&style=flat)][#conda-forge-package]\r\n[![conda (channel only)](https://img.shields.io/conda/vn/conda-forge/sentence-transformers?logo=anaconda&style=flat&color=orange)][#conda-forge-package]\r\n[![docs - github.io](https://img.shields.io/static/v1?logo=github&style=flat&color=pink&label=docs&message=sentence-transformers)][#docs-package]\r\n<!--- \r\n[![pypi - downloads](https://img.shields.io/pypi/dm/sentence-transformers?logo=pypi&style=flat&color=green)][#pypi-package]\r\n[![conda](https://img.shields.io/conda/dn/conda-forge/sentence-transformers?logo=anaconda)][#conda-forge-package] \r\n--->\r\n\r\n[#github-license]: https://github.com/ukplab/sentence-transformers/blob/master/license\r\n[#pypi-package]: https://pypi.org/project/sentence-transformers/\r\n[#conda-forge-package]: https://anaconda.org/conda-forge/sentence-transformers\r\n[#docs-package]: https://www.sbert.net/\r\n<!--- badges: end --->\r\n\r\n# sentence transformers: multilingual sentence, paragraph, and image embeddings using bert & co.\r\n\r\nthis framework provides an easy method to compute dense vector representations for **sentences**, **paragraphs**, and **images**. the models are based on transformer networks like bert / roberta / xlm-roberta etc. and achieve state-of-the-art performance in various task. text is embedding in vector space such that similar text is close and can efficiently be found using cosine similarity.\r\n\r\nwe provide an increasing number of **[state-of-the-art pretrained models](https://www.sbert.net/docs/pretrained_models.html)** for more than 100 languages, fine-tuned for various use-cases.\r\n\r\nfurther, this framework allows an easy  **[fine-tuning of custom embeddings models](https://www.sbert.net/docs/training/overview.html)**, to achieve maximal performance on your specific task.\r\n\r\nfor the **full documentation**, see **[www.sbert.net](https://www.sbert.net)**.\r\n\r\nthe following publications are integrated in this framework:\r\n\r\n- [sentence-bert: sentence embeddings using siamese bert-networks](https://arxiv.org/abs/1908.10084) (emnlp 2019)\r\n- [making monolingual sentence embeddings multilingual using knowledge distillation](https://arxiv.org/abs/2004.09813) (emnlp 2020)\r\n- [augmented sbert: data augmentation method for improving bi-encoders for pairwise sentence scoring tasks](https://arxiv.org/abs/2010.08240) (naacl 2021)\r\n- [the curse of dense low-dimensional information retrieval for large index sizes](https://arxiv.org/abs/2012.14210) (arxiv 2020)\r\n- [tsdae: using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning](https://arxiv.org/abs/2104.06979) (arxiv 2021)\r\n- [beir: a heterogenous benchmark for zero-shot evaluation of information retrieval models](https://arxiv.org/abs/2104.08663) (arxiv 2021)\r\n\r\n## installation\r\n\r\nwe recommend **python 3.6** or higher, **[pytorch 1.6.0](https://pytorch.org/get-started/locally/)** or higher and **[transformers v4.6.0](https://github.com/huggingface/transformers)** or higher. the code does **not** work with python 2.7.\r\n\r\n**install with pip**\r\n\r\ninstall the *sentence-transformers* with `pip`:\r\n\r\n```\r\npip install -u sentence-transformers\r\n```\r\n\r\n**install with conda**\r\n\r\nyou can install the *sentence-transformers* with `conda`:\r\n\r\n```\r\nconda install -c conda-forge sentence-transformers\r\n```\r\n\r\n**install from sources**\r\n\r\nalternatively, you can also clone the latest version from the [repository](https://github.com/ukplab/sentence-transformers) and install it directly from the source code:\r\n\r\n````\r\npip install -e .\r\n```` \r\n\r\n**pytorch with cuda**\r\n\r\nif you want to use a gpu / cuda, you must install pytorch with the matching cuda version. follow\r\n[pytorch - get started](https://pytorch.org/get-started/locally/) for further details how to install pytorch.\r\n\r\n## getting started\r\n\r\nsee [quickstart](https://www.sbert.net/docs/quickstart.html) in our documenation.\r\n\r\n[this example](https://github.com/ukplab/sentence-transformers/tree/master/examples/applications/computing-embeddings/computing_embeddings.py) shows you how to use an already trained sentence transformer model to embed sentences for another task.\r\n\r\nfirst download a pretrained model.\r\n\r\n````python\r\nfrom sentence_transformers import sentencetransformer\r\nmodel = sentencetransformer('all-minilm-l6-v2')\r\n````\r\n\r\nthen provide some sentences to the model.\r\n\r\n````python\r\nsentences = ['this framework generates embeddings for each input sentence',\r\n    'sentences are passed as a list of string.', \r\n    'the quick brown fox jumps over the lazy dog.']\r\nsentence_embeddings = model.encode(sentences)\r\n````\r\n\r\nand that's it already. we now have a list of numpy arrays with the embeddings.\r\n\r\n````python\r\nfor sentence, embedding in zip(sentences, sentence_embeddings):\r\n    print(\"sentence:\", sentence)\r\n    print(\"embedding:\", embedding)\r\n    print(\"\")\r\n````\r\n\r\n## pre-trained models\r\n\r\nwe provide a large list of [pretrained models](https://www.sbert.net/docs/pretrained_models.html) for more than 100 languages. some models are general purpose models, while others produce embeddings for specific use cases. pre-trained models can be loaded by just passing the model name: `sentencetransformer('model_name')`.\r\n\r\n[\u00bb  full list of pretrained models](https://www.sbert.net/docs/pretrained_models.html)\r\n\r\n## training\r\n\r\nthis framework allows you to fine-tune your own sentence embedding methods, so that you get task-specific sentence embeddings. you have various options to choose from in order to get perfect sentence embeddings for your specific task. \r\n\r\nsee [training overview](https://www.sbert.net/docs/training/overview.html) for an introduction how to train your own embedding models. we provide [various examples](https://github.com/ukplab/sentence-transformers/tree/master/examples/training) how to train models on various datasets.\r\n\r\nsome highlights are:\r\n- support of various transformer networks including bert, roberta, xlm-r, distilbert, electra, bart, ...\r\n- multi-lingual and multi-task learning\r\n- evaluation during training to find optimal model\r\n- [10+ loss-functions](https://www.sbert.net/docs/package_reference/losses.html) allowing to tune models specifically for semantic search, paraphrase mining, semantic similarity comparison, clustering, triplet loss, contrastive loss.\r\n\r\n## performance\r\n\r\nour models are evaluated extensively on 15+ datasets including challening domains like tweets, reddit, emails. they achieve by far the **best performance** from all available sentence embedding methods. further, we provide several **smaller models** that are **optimized for speed**.\r\n\r\n[\u00bb full list of pretrained models](https://www.sbert.net/docs/pretrained_models.html)\r\n\r\n## application examples\r\n\r\nyou can use this framework for:\r\n\r\n- [computing sentence embeddings](https://www.sbert.net/examples/applications/computing-embeddings/readme.html)\r\n- [semantic textual similarity](https://www.sbert.net/docs/usage/semantic_textual_similarity.html)\r\n- [clustering](https://www.sbert.net/examples/applications/clustering/readme.html)\r\n- [paraphrase mining](https://www.sbert.net/examples/applications/paraphrase-mining/readme.html)\r\n - [translated sentence mining](https://www.sbert.net/examples/applications/parallel-sentence-mining/readme.html)\r\n - [semantic search](https://www.sbert.net/examples/applications/semantic-search/readme.html)\r\n - [retrieve & re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/readme.html) \r\n - [text summarization](https://www.sbert.net/examples/applications/text-summarization/readme.html) \r\n- [multilingual image search, clustering & duplicate detection](https://www.sbert.net/examples/applications/image-search/readme.html)\r\n\r\nand many more use-cases.\r\n\r\nfor all examples, see [examples/applications](https://github.com/ukplab/sentence-transformers/tree/master/examples/applications).\r\n\r\n## citing & authors\r\n\r\nif you find this repository helpful, feel free to cite our publication [sentence-bert: sentence embeddings using siamese bert-networks](https://arxiv.org/abs/1908.10084):\r\n\r\n```bibtex \r\n@inproceedings{reimers-2019-sentence-bert,\r\n    title = \"sentence-bert: sentence embeddings using siamese bert-networks\",\r\n    author = \"reimers, nils and gurevych, iryna\",\r\n    booktitle = \"proceedings of the 2019 conference on empirical methods in natural language processing\",\r\n    month = \"11\",\r\n    year = \"2019\",\r\n    publisher = \"association for computational linguistics\",\r\n    url = \"https://arxiv.org/abs/1908.10084\",\r\n}\r\n```\r\n\r\nif you use one of the multilingual models, feel free to cite our publication [making monolingual sentence embeddings multilingual using knowledge distillation](https://arxiv.org/abs/2004.09813):\r\n\r\n```bibtex\r\n@inproceedings{reimers-2020-multilingual-sentence-bert,\r\n    title = \"making monolingual sentence embeddings multilingual using knowledge distillation\",\r\n    author = \"reimers, nils and gurevych, iryna\",\r\n    booktitle = \"proceedings of the 2020 conference on empirical methods in natural language processing\",\r\n    month = \"11\",\r\n    year = \"2020\",\r\n    publisher = \"association for computational linguistics\",\r\n    url = \"https://arxiv.org/abs/2004.09813\",\r\n}\r\n```\r\n\r\nplease have a look at [publications](https://www.sbert.net/docs/publications.html) for our different publications that are integrated into sentencetransformers.\r\n\r\ncontact person: [nils reimers](https://www.nils-reimers.de), [info@nils-reimers.de](mailto:info@nils-reimers.de)\r\n\r\nhttps://www.ukp.tu-darmstadt.de/\r\n\r\ndon't hesitate to send us an e-mail or report an issue, if something is broken (and it shouldn't be) or if you have further questions.\r\n\r\n> this repository contains experimental software and is published for the sole purpose of giving additional background details on the respective publication.\r\n\r\n\r\n",
  "docs_url": null,
  "keywords": "transformer networks bert xlnet sentence embedding pytorch nlp deep learning",
  "license": "apache license 2.0",
  "name": "sentence-transformers",
  "package_url": "https://pypi.org/project/sentence-transformers/",
  "project_url": "https://pypi.org/project/sentence-transformers/",
  "project_urls": {
    "Download": "https://github.com/UKPLab/sentence-transformers/archive/v2.2.1.zip",
    "Homepage": "https://github.com/UKPLab/sentence-transformers"
  },
  "release_url": "https://pypi.org/project/sentence-transformers/2.2.2/",
  "requires_dist": [],
  "requires_python": ">=3.6.0",
  "summary": "multilingual text embeddings",
  "version": "2.2.2",
  "releases": [],
  "developers": [
    "info@nils-reimers.de",
    "nils_reimers"
  ],
  "kwds": "sentence_transformers sentence_embeddings bert sentencetransformer sentencetransformers",
  "license_kwds": "apache license 2.0",
  "libtype": "pypi",
  "id": "pypi_sentence_transformers",
  "homepage": "https://github.com/ukplab/sentence-transformers",
  "release_count": 43,
  "dependency_ids": []
}