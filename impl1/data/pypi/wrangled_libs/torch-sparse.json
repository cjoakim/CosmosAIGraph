{
  "classifiers": [
    "development status :: 5 - production/stable",
    "license :: osi approved :: mit license",
    "programming language :: python",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "[pypi-image]: https://badge.fury.io/py/torch-sparse.svg\n[pypi-url]: https://pypi.python.org/pypi/torch-sparse\n[testing-image]: https://github.com/rusty1s/pytorch_sparse/actions/workflows/testing.yml/badge.svg\n[testing-url]: https://github.com/rusty1s/pytorch_sparse/actions/workflows/testing.yml\n[linting-image]: https://github.com/rusty1s/pytorch_sparse/actions/workflows/linting.yml/badge.svg\n[linting-url]: https://github.com/rusty1s/pytorch_sparse/actions/workflows/linting.yml\n[coverage-image]: https://codecov.io/gh/rusty1s/pytorch_sparse/branch/master/graph/badge.svg\n[coverage-url]: https://codecov.io/github/rusty1s/pytorch_sparse?branch=master\n\n# pytorch sparse\n\n[![pypi version][pypi-image]][pypi-url]\n[![testing status][testing-image]][testing-url]\n[![linting status][linting-image]][linting-url]\n[![code coverage][coverage-image]][coverage-url]\n\n--------------------------------------------------------------------------------\n\nthis package consists of a small extension library of optimized sparse matrix operations with autograd support.\nthis package currently consists of the following methods:\n\n* **[coalesce](#coalesce)**\n* **[transpose](#transpose)**\n* **[sparse dense matrix multiplication](#sparse-dense-matrix-multiplication)**\n* **[sparse sparse matrix multiplication](#sparse-sparse-matrix-multiplication)**\n\nall included operations work on varying data types and are implemented both for cpu and gpu.\nto avoid the hazzle of creating [`torch.sparse_coo_tensor`](https://pytorch.org/docs/stable/torch.html?highlight=sparse_coo_tensor#torch.sparse_coo_tensor), this package defines operations on sparse tensors by simply passing `index` and `value` tensors as arguments ([with same shapes as defined in pytorch](https://pytorch.org/docs/stable/sparse.html)).\nnote that only `value` comes with autograd support, as `index` is discrete and therefore not differentiable.\n\n## installation\n\n### anaconda\n\n**update:** you can now install `pytorch-sparse` via [anaconda](https://anaconda.org/pyg/pytorch-sparse) for all major os/pytorch/cuda combinations \ud83e\udd17\ngiven that you have [`pytorch >= 1.8.0` installed](https://pytorch.org/get-started/locally/), simply run\n\n```\nconda install pytorch-sparse -c pyg\n```\n\n### binaries\n\nwe alternatively provide pip wheels for all major os/pytorch/cuda combinations, see [here](https://data.pyg.org/whl).\n\n#### pytorch 2.1\n\nto install the binaries for pytorch 2.1.0, simply run\n\n```\npip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+${cuda}.html\n```\n\nwhere `${cuda}` should be replaced by either `cpu`, `cu118`, or `cu121` depending on your pytorch installation.\n\n|             | `cpu` | `cu118` | `cu121` |\n|-------------|-------|---------|---------|\n| **linux**   | \u2705    | \u2705      | \u2705      |\n| **windows** | \u2705    | \u2705      | \u2705      |\n| **macos**   | \u2705    |         |         |\n\n#### pytorch 2.0\n\nto install the binaries for pytorch 2.0.0, simply run\n\n```\npip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+${cuda}.html\n```\n\nwhere `${cuda}` should be replaced by either `cpu`, `cu117`, or `cu118` depending on your pytorch installation.\n\n|             | `cpu` | `cu117` | `cu118` |\n|-------------|-------|---------|---------|\n| **linux**   | \u2705    | \u2705      | \u2705      |\n| **windows** | \u2705    | \u2705      | \u2705      |\n| **macos**   | \u2705    |         |         |\n\n**note:** binaries of older versions are also provided for pytorch 1.4.0, pytorch 1.5.0, pytorch 1.6.0, pytorch 1.7.0/1.7.1, pytorch 1.8.0/1.8.1, pytorch 1.9.0, pytorch 1.10.0/1.10.1/1.10.2, pytorch 1.11.0, pytorch 1.12.0/1.12.1 and pytorch 1.13.0/1.13.1 (following the same procedure).\nfor older versions, you need to explicitly specify the latest supported version number or install via `pip install --no-index` in order to prevent a manual installation from source.\nyou can look up the latest supported version number [here](https://data.pyg.org/whl).\n\n### from source\n\nensure that at least pytorch 1.7.0 is installed and verify that `cuda/bin` and `cuda/include` are in your `$path` and `$cpath` respectively, *e.g.*:\n\n```\n$ python -c \"import torch; print(torch.__version__)\"\n>>> 1.7.0\n\n$ echo $path\n>>> /usr/local/cuda/bin:...\n\n$ echo $cpath\n>>> /usr/local/cuda/include:...\n```\n\nif you want to additionally build `torch-sparse` with metis support, *e.g.* for partioning, please download and install the [metis library](https://web.archive.org/web/20211119110155/http://glaros.dtc.umn.edu/gkhome/metis/metis/download) by following the instructions in the `install.txt` file.\nnote that metis needs to be installed with 64 bit `idxtypewidth` by changing `include/metis.h`.\nafterwards, set the environment variable `with_metis=1`.\n\nthen run:\n\n```\npip install torch-scatter torch-sparse\n```\n\nwhen running in a docker container without nvidia driver, pytorch needs to evaluate the compute capabilities and may fail.\nin this case, ensure that the compute capabilities are set via `torch_cuda_arch_list`, *e.g.*:\n\n```\nexport torch_cuda_arch_list=\"6.0 6.1 7.2+ptx 7.5+ptx\"\n```\n\n## functions\n\n### coalesce\n\n```\ntorch_sparse.coalesce(index, value, m, n, op=\"add\") -> (torch.longtensor, torch.tensor)\n```\n\nrow-wise sorts `index` and removes duplicate entries.\nduplicate entries are removed by scattering them together.\nfor scattering, any operation of [`torch_scatter`](https://github.com/rusty1s/pytorch_scatter) can be used.\n\n#### parameters\n\n* **index** *(longtensor)* - the index tensor of sparse matrix.\n* **value** *(tensor)* - the value tensor of sparse matrix.\n* **m** *(int)* - the first dimension of sparse matrix.\n* **n** *(int)* - the second dimension of sparse matrix.\n* **op** *(string, optional)* - the scatter operation to use. (default: `\"add\"`)\n\n#### returns\n\n* **index** *(longtensor)* - the coalesced index tensor of sparse matrix.\n* **value** *(tensor)* - the coalesced value tensor of sparse matrix.\n\n#### example\n\n```python\nimport torch\nfrom torch_sparse import coalesce\n\nindex = torch.tensor([[1, 0, 1, 0, 2, 1],\n                      [0, 1, 1, 1, 0, 0]])\nvalue = torch.tensor([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]])\n\nindex, value = coalesce(index, value, m=3, n=2)\n```\n\n```\nprint(index)\ntensor([[0, 1, 1, 2],\n        [1, 0, 1, 0]])\nprint(value)\ntensor([[6.0, 8.0],\n        [7.0, 9.0],\n        [3.0, 4.0],\n        [5.0, 6.0]])\n```\n\n### transpose\n\n```\ntorch_sparse.transpose(index, value, m, n) -> (torch.longtensor, torch.tensor)\n```\n\ntransposes dimensions 0 and 1 of a sparse matrix.\n\n#### parameters\n\n* **index** *(longtensor)* - the index tensor of sparse matrix.\n* **value** *(tensor)* - the value tensor of sparse matrix.\n* **m** *(int)* - the first dimension of sparse matrix.\n* **n** *(int)* - the second dimension of sparse matrix.\n* **coalesced** *(bool, optional)* - if set to `false`, will not coalesce the output. (default: `true`)\n\n#### returns\n\n* **index** *(longtensor)* - the transposed index tensor of sparse matrix.\n* **value** *(tensor)* - the transposed value tensor of sparse matrix.\n\n#### example\n\n```python\nimport torch\nfrom torch_sparse import transpose\n\nindex = torch.tensor([[1, 0, 1, 0, 2, 1],\n                      [0, 1, 1, 1, 0, 0]])\nvalue = torch.tensor([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]])\n\nindex, value = transpose(index, value, 3, 2)\n```\n\n```\nprint(index)\ntensor([[0, 0, 1, 1],\n        [1, 2, 0, 1]])\nprint(value)\ntensor([[7.0, 9.0],\n        [5.0, 6.0],\n        [6.0, 8.0],\n        [3.0, 4.0]])\n```\n\n### sparse dense matrix multiplication\n\n```\ntorch_sparse.spmm(index, value, m, n, matrix) -> torch.tensor\n```\n\nmatrix product of a sparse matrix with a dense matrix.\n\n#### parameters\n\n* **index** *(longtensor)* - the index tensor of sparse matrix.\n* **value** *(tensor)* - the value tensor of sparse matrix.\n* **m** *(int)* - the first dimension of sparse matrix.\n* **n** *(int)* - the second dimension of sparse matrix.\n* **matrix** *(tensor)* - the dense matrix.\n\n#### returns\n\n* **out** *(tensor)* - the dense output matrix.\n\n#### example\n\n```python\nimport torch\nfrom torch_sparse import spmm\n\nindex = torch.tensor([[0, 0, 1, 2, 2],\n                      [0, 2, 1, 0, 1]])\nvalue = torch.tensor([1, 2, 4, 1, 3])\nmatrix = torch.tensor([[1, 4], [2, 5], [3, 6]])\n\nout = spmm(index, value, 3, 3, matrix)\n```\n\n```\nprint(out)\ntensor([[7.0, 16.0],\n        [8.0, 20.0],\n        [7.0, 19.0]])\n```\n\n### sparse sparse matrix multiplication\n\n```\ntorch_sparse.spspmm(indexa, valuea, indexb, valueb, m, k, n) -> (torch.longtensor, torch.tensor)\n```\n\nmatrix product of two sparse tensors.\nboth input sparse matrices need to be **coalesced** (use the `coalesced` attribute to force).\n\n#### parameters\n\n* **indexa** *(longtensor)* - the index tensor of first sparse matrix.\n* **valuea** *(tensor)* - the value tensor of first sparse matrix.\n* **indexb** *(longtensor)* - the index tensor of second sparse matrix.\n* **valueb** *(tensor)* - the value tensor of second sparse matrix.\n* **m** *(int)* - the first dimension of first sparse matrix.\n* **k** *(int)* - the second dimension of first sparse matrix and first dimension of second sparse matrix.\n* **n** *(int)* - the second dimension of second sparse matrix.\n* **coalesced** *(bool, optional)*: if set to `true`, will coalesce both input sparse matrices. (default: `false`)\n\n#### returns\n\n* **index** *(longtensor)* - the output index tensor of sparse matrix.\n* **value** *(tensor)* - the output value tensor of sparse matrix.\n\n#### example\n\n```python\nimport torch\nfrom torch_sparse import spspmm\n\nindexa = torch.tensor([[0, 0, 1, 2, 2], [1, 2, 0, 0, 1]])\nvaluea = torch.tensor([1, 2, 3, 4, 5])\n\nindexb = torch.tensor([[0, 2], [1, 0]])\nvalueb = torch.tensor([2, 4])\n\nindexc, valuec = spspmm(indexa, valuea, indexb, valueb, 3, 3, 2)\n```\n\n```\nprint(indexc)\ntensor([[0, 1, 2],\n        [0, 1, 1]])\nprint(valuec)\ntensor([8.0, 6.0, 8.0])\n```\n\n## running tests\n\n```\npytest\n```\n\n## c++ api\n\n`torch-sparse` also offers a c++ api that contains c++ equivalent of python models.\nfor this, we need to add `torchlib` to the `-dcmake_prefix_path` (*e.g.*, it may exists in `{conda}/lib/python{x.x}/site-packages/torch` if installed via `conda`):\n\n```\nmkdir build\ncd build\n# add -dwith_cuda=on support for cuda support\ncmake -dcmake_prefix_path=\"...\" ..\nmake\nmake install\n```\n\n\n",
  "docs_url": null,
  "keywords": "pytorch,sparse,sparse-matrices,autograd",
  "license": "",
  "name": "torch-sparse",
  "package_url": "https://pypi.org/project/torch-sparse/",
  "project_url": "https://pypi.org/project/torch-sparse/",
  "project_urls": {
    "Download": "https://github.com/rusty1s/pytorch_sparse/archive/0.6.18.tar.gz",
    "Homepage": "https://github.com/rusty1s/pytorch_sparse"
  },
  "release_url": "https://pypi.org/project/torch-sparse/0.6.18/",
  "requires_dist": [],
  "requires_python": ">=3.8",
  "summary": "pytorch extension library of optimized autograd sparse matrix operations",
  "version": "0.6.18",
  "releases": [],
  "developers": [
    "matthias.fey@tu-dortmund.de",
    "matthias_fey"
  ],
  "kwds": "pytorch_sparse torch_sparse sparse sparse_coo_tensor pytorch",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_torch_sparse",
  "homepage": "https://github.com/rusty1s/pytorch_sparse",
  "release_count": 30,
  "dependency_ids": []
}