{
  "classifiers": [
    "development status :: 4 - beta",
    "intended audience :: developers",
    "license :: osi approved :: apache software license",
    "license :: osi approved :: mit license",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "emcache\n#######\n\na high performance asynchronous python client for `memcached <https://memcached.org/>`_ with full batteries included\n\n.. image:: https://readthedocs.org/projects/emcache/badge/?version=latest\n  :target: https://emcache.readthedocs.io/en/latest/?badge=latest\n\n.. image:: https://github.com/emcache/emcache/workflows/ci/badge.svg\n  :target: https://github.com/emcache/emcache/workflows/ci/badge.svg\n\n.. image:: https://github.com/emcache/emcache/workflows/pypi%20release/badge.svg\n  :target: https://github.com/emcache/emcache/workflows/pypi%20release/badge.svg\n\nemcache stands on the giant's shoulders and implements most of the characteristics that are desired for a memcached client based\non the experience of other memcached clients, providing the following main characteristics:\n\n- support for many memcached hosts, distributing traffic around them by using the `rendezvous hashing <https://emcache.readthedocs.io/en/latest/advanced_topics.html#hashing-algorithm>`_ algorithm.\n- support for different commands and different flag behaviors like ``noreply``, ``exptime`` or ``flags``.\n- support for ssl/tls protocol.\n- support for `autodiscovery <https://emcache.readthedocs.io/en/latest/client.html#autodiscovery>`_, which should work with aws and gcp memcached clusters.\n- adaptative `connection pool <https://emcache.readthedocs.io/en/latest/advanced_topics.html#connection-pool>`_, which increases the number of connections per memcache host depending on the traffic.\n- `node healthiness <https://emcache.readthedocs.io/en/latest/advanced_topics.html#healthy-and-unhealthy-nodes>`_ traceability and an optional flag for disabling unhealthy for participating in the commands.\n- metrics for `operations and connections <https://emcache.readthedocs.io/en/latest/cluster_managment.html#connection-pool-metrics>`_, send them to your favourite ts database for knowing how the emcache driver is behaving.\n- listen to the most significant `cluster events <https://emcache.readthedocs.io/en/latest/advanced_topics.html#cluster-events>`_, for example for knowing when a node has been marked as unhealthy.\n- speed, emcache is fast. see the benchmark section.\n\nusage\n==========\n\nfor installing\n\n.. code-block:: bash\n\n    pip install emcache\n\nthe following snippet shows the minimal stuff that would be needed for creating a new client and saving a new key and retrieving later the value.\n\n.. code-block:: python\n\n    import asyncio\n    import emcache\n    async def main():\n        client = await emcache.create_client([emcache.memcachedhostaddress('localhost', 11211)])\n        await client.set(b'key', b'value')\n        item = await client.get(b'key')\n        print(item.value)\n        await client.close()\n    asyncio.run(main())\n\nemcache has currently support, among many of them, for the following commands:\n\n- **get** used for retrieving a specific key.\n- **gets** cas version that returns also the case token of a specific key.\n- **get_many** many keys get version.\n- **gets_many** many keys + case token gets version.\n- **set** set a new key and value\n- **add** add a new key and value, if and only if it does not exist.\n- **replace** update a value of a key, if an only if the key does exist.\n- **append** append a value to the current one for a specific key, if and only if the key does exist.\n- **prepend** prepend a value to the current one for a specific key, if and only if the key does exist.\n- **cas** update a value for a key if and only if token as provided matches with the ones stored in the memcached server.\n\ntake a look at the documentation for getting a list of all of the `operations <https://emcache.readthedocs.io/en/latest/operations.html>`_ that are currently supported.\n\nsome of the commands have support for the following behavior flags:\n\n- ``noreply`` for storage commands like **set** we do not wait for an explicit response from the memcached server. sacrifice the explicit ack from the memcached server for speed.\n- ``flags`` for storage we can save an int16 value that can be retrieved later on by fetch commands.\n- ``exptime`` for storage commands this provides a way of configuring an expiration time, once that time is reached keys will be automatically evicted by the memcached server\n\nfor more information about usage, `read the docs <https://emcache.readthedocs.io/en/latest/>`_.\n\n\nbenchmarks\n===========\n\nthe following table shows how fast - operations per second - emcache can be compared to the other two memcached python clients,\n`aiomcache <https://github.com/aio-libs/aiomcache>`_ and `pymemcache <https://github.com/pinterest/pymemcache>`_.\nfor that specific benchmark two nodes were used, one for the client and one for the memcached server, using 32 tcp connections\nand using 32 concurrent asyncio tasks - threads for the use case of pymemcache. for emcache and aiomcache\n`uvloop <https://github.com/magicstack/uvloop>`_ was used as a default loop.\n\nin the first part of the benchmark, the client tried to run as mucha **set** operations it could, and in a second step the same was\ndone but using **get** operations.\n\n+------------------------+---------------+---------------+-------------------+--------------------+------------------+\n| client                 | concurrency   | sets ops/sec  | sets latency avg  |  gets ops/sec      | gets latency avg |\n+========================+===============+===============+===================+====================+==================+\n| aiomcache              |            32 |         33872 |           0.00094 |              34183 |          0.00093 |\n+------------------------+---------------+---------------+-------------------+--------------------+------------------+\n| pymemcache             |            32 |         32792 |           0.00097 |              32961 |          0.00096 |\n+------------------------+---------------+---------------+-------------------+--------------------+------------------+\n| emcache                |            32 |         49410 |           0.00064 |              49212 |          0.00064 |\n+------------------------+---------------+---------------+-------------------+--------------------+------------------+\n| emcache (autobatching) |            32 |         49410 |           0.00064 |              89052 |          0.00035 |\n+------------------------+---------------+---------------+-------------------+--------------------+------------------+\n\nemcache performed better than the other two implementations reaching almost 50k ops/sec for get and set operations. one autobatching is used\nit can boost the throughtput x2 (more info about autobatching below)\n\nanother benchmark was performed for comparing how each implementation will behave in case of having to deal with more than 1 node, a new\nbenchmark was performed with different cluster sizes but using the same methodology as the previous test by first, performing as many set\noperations it could and later as many get operations it could. for this specific use test with aiomemcahce could not be used since it\ndoes not support multiple nodes.\n\n+-------------+-------------+---------------+---------------+------------------+--------------+------------------+\n| client      | concurrency | memcahed nodes| sets ops/sec  | sets latency avg | gets ops/sec | gets latency avg |\n+=============+=============+===============+===============+==================+==============+==================+\n| pymemcache  |          32 |             2 |         21260 |          0.00150 |        21583 |          0.00148 |\n+-------------+-------------+---------------+---------------+------------------+--------------+------------------+\n| emcache     |          32 |             2 |         42245 |          0.00075 |        48079 |          0.00066 |\n+-------------+-------------+---------------+---------------+------------------+--------------+------------------+\n| pymemcache  |          32 |             4 |         15334 |          0.00208 |        15458 |          0.00207 |\n+-------------+-------------+---------------+---------------+------------------+--------------+------------------+\n| emcache     |          32 |             4 |         39786 |          0.00080 |        47603 |          0.00067 |\n+-------------+-------------+---------------+---------------+------------------+--------------+------------------+\n| pymemcache  |          32 |             8 |          9903 |          0.00323 |         9970 |          0.00322 |\n+-------------+-------------+---------------+---------------+------------------+--------------+------------------+\n| emcache     |          32 |             8 |         42167 |          0.00075 |        46472 |          0.00068 |\n+-------------+-------------+---------------+---------------+------------------+--------------+------------------+\n\nthe addition of new nodes did not add almost degradation for emcache, in the last test with 8 nodes emcache reached 42k\nget ops/sec and 46k set ops/sec. on the other hand, pymemcached suffered substantial degradation making emcache ~x5 times.\nfaster.\n\nautobatching\n============\n\nautobatching provides you a way for fetching multiple keys using a single command, batching happens transparently behind the scenes\nwithout bothering the caller.\n\nfor start using the autobatching feature you must provide the parameter `autobatching` as true, hereby all usages of the `get` and `gets` \ncommand will send batched requests behind the scenes.\n\nget\u00b4s are piled up until the next loop iteration. once the next loop iteration is reached all get\u00b4s are transmitted using the\nsame memcached operation.\n\nautobatching can boost up the throughput of your application x2/x3.\n\ndevelopment\n===========\n\nclone the repository and its murmur3 submodule\n\n.. code-block:: bash\n\n    git clone --recursive git@github.com:emcache/emcache\n\ncompile murmur3\n\n.. code-block:: bash\n\n    pushd vendor/murmur3\n    make static\n    popd\n\ninstall emcache with dev dependencies\n\n.. code-block:: bash\n\n    make install-dev\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "emcache",
  "package_url": "https://pypi.org/project/emcache/",
  "project_url": "https://pypi.org/project/emcache/",
  "project_urls": {
    "Homepage": "http://github.com/emcache/emcache"
  },
  "release_url": "https://pypi.org/project/emcache/1.1.0/",
  "requires_dist": [
    "Cython ==0.29.32 ; extra == 'dev'",
    "pytest ==6.2.5 ; extra == 'dev'",
    "pytest-mock ==3.1.0 ; extra == 'dev'",
    "pytest-asyncio ==0.11.0 ; extra == 'dev'",
    "pytest-cov ==2.8.1 ; extra == 'dev'",
    "black ==22.3.0 ; extra == 'dev'",
    "isort ==4.3.21 ; extra == 'dev'",
    "flake8 ==6.1.0 ; extra == 'dev'"
  ],
  "requires_python": ">=3.8",
  "summary": "a high performance asynchronous python client for memcached with full batteries included",
  "version": "1.1.0",
  "releases": [],
  "developers": [
    "pau_freixes",
    "pfreixes@gmail.com"
  ],
  "kwds": "badge emcache memcache memcached aiomcache",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_emcache",
  "homepage": "http://github.com/emcache/emcache",
  "release_count": 15,
  "dependency_ids": [
    "pypi_black",
    "pypi_cython",
    "pypi_flake8",
    "pypi_isort",
    "pypi_pytest",
    "pypi_pytest_asyncio",
    "pypi_pytest_cov",
    "pypi_pytest_mock"
  ]
}