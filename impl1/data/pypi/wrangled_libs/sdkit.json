{
  "classifiers": [
    "license :: osi approved :: mit license",
    "operating system :: macos",
    "operating system :: microsoft :: windows :: windows 10",
    "operating system :: microsoft :: windows :: windows 11",
    "operating system :: posix :: linux",
    "programming language :: python :: 3"
  ],
  "description": "# sdkit\n**sdkit** (**s**table **d**iffusion **kit**) is an easy-to-use library for using stable diffusion in your ai art projects. it is fast, feature-packed, and memory-efficient.\n\n[![discord server](https://img.shields.io/discord/1014774730907209781?label=discord)](https://discord.com/invite/u9yhsfmekb)\n\n*new: stable diffusion xl, controlnets, loras and embeddings are now supported!*\n\nthis is a community project, so please feel free to contribute (and use in your project)!\n\n![t2i](https://raw.githubusercontent.com/stability-ai/stablediffusion/main/assets/stable-samples/txt2img/768/merged-0006.png)\n\n# why?\nthe goal is to let you be productive quickly (at your ai art project), so it bundles stable diffusion along with commonly-used features (like controlnets, loras, textual inversion embeddings, gfpgan and codeformer for face restoration, realesrgan for upscaling, k-samplers, support for loading custom vaes, nsfw filter etc).\n\nadvanced features include a model-downloader (with a database of commonly used models), support for running in parallel on multiple gpus, auto-scanning for malicious models etc. [full list of features](https://github.com/easydiffusion/sdkit/wiki/features)\n\n# installation\ntested with python 3.8. supports windows, linux and mac.\n\n**windows/linux:**\n1. `pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu116`\n2. run `pip install sdkit`\n\n**mac:**\n1. run `pip install sdkit`\n\n# example\n### local model\na simple example for generating an image from a stable diffusion model file (already present on the disk):\n```python\nimport sdkit\nfrom sdkit.models import load_model\nfrom sdkit.generate import generate_images\nfrom sdkit.utils import log\n\ncontext = sdkit.context()\n\n# set the path to the model file on the disk (.ckpt or .safetensors file)\ncontext.model_paths['stable-diffusion'] = 'd:\\\\path\\\\to\\\\512-base-ema.ckpt'\nload_model(context, 'stable-diffusion')\n\n# generate the image\nimages = generate_images(context, prompt='photograph of an astronaut riding a horse', seed=42, width=512, height=512)\n\n# save the image\nimages[0].save(\"image.png\") # images is a list of pil.image\n\nlog.info(\"generated images!\")\n```\n\n### auto-download a known model\na simple example for automatically downloading a known stable diffusion model file:\n```python\nimport sdkit\nfrom sdkit.models import download_models, resolve_downloaded_model_path, load_model\nfrom sdkit.generate import generate_images\nfrom sdkit.utils import save_images\n\ncontext = sdkit.context()\n\ndownload_models(context, models={'stable-diffusion': '1.5-pruned-emaonly'}) # downloads the known \"sd 1.5-pruned-emaonly\" model\n\ncontext.model_paths['stable-diffusion'] = resolve_downloaded_model_path(context, 'stable-diffusion', '1.5-pruned-emaonly')\nload_model(context, 'stable-diffusion')\n\nimages = generate_images(context, prompt='photograph of an astronaut riding a horse', seed=42, width=512, height=512)\nsave_images(images, dir_path='d:\\\\path\\\\to\\\\images\\\\directory')\n```\n\nplease see the list of [examples](https://github.com/easydiffusion/sdkit/tree/main/examples), to learn how to use the other features (like filters, vae, controlnet, embeddings, lora, memory optimizations, running on multiple gpus etc).\n\n# api\nplease see the [api reference](https://github.com/easydiffusion/sdkit/wiki/api) page for a detailed summary.\n\nbroadly, the api contains 5 modules:\n```python\nsdkit.models # load/unloading models, downloading known models, scanning models\nsdkit.generate # generating images\nsdkit.filter # face restoration, upscaling\nsdkit.train # model merge, and (in the future) more training methods\nsdkit.utils\n```\n\nand a `sdkit.context` object is passed around, which encapsulates the data related to the runtime (e.g. `device` and `vram_optimizations`) as well as references to the loaded model files and paths. `context` is a thread-local object.\n\n# models db\nclick here to see the [list of known models](https://github.com/easydiffusion/sdkit/tree/main/sdkit/models/models_db).\n\nsdkit includes a database of known models and their configurations. this lets you download a known model with a single line of code. you can customize where it saves the downloaded model.\n\nadditionally, sdkit will attempt to automatically determine the configuration for a given model (when loading from disk). for e.g. if an sd 2.1 model is being loaded, sdkit will automatically know to use `fp32` for `attn_precision`. if an sd 2.0 v-type model is being loaded, sdkit will automatically know to use the `v2-inference-v.yaml` configuration. it does this by matching the quick-hash of the given model file, with the list of known quick-hashes.\n\nfor models that don't match a known hash (e.g. custom models), or if you want to override the config file, you can set the path to the config file in `context.model_paths`. e.g. `context.model_paths['stable-diffusion'] = 'path/to/config.yaml'`\n\n# faq\n## does it have all the cool features?\nit has a lot of features! it was born out of a popular stable diffusion ui, splitting out the battle-tested core engine into `sdkit`.\n\n**features include:** sd 2.1, sdxl, controlnet, loras, embeddings, txt2img, img2img, inpainting, nsfw filter, multiple gpu support, mac support, gfpgan and codeformer (fix faces), realesrgan (upscale), 16 samplers (including k-samplers and unipc), custom vae, low-memory optimizations, model merging, safetensor support, picklescan, etc. [click here to see the full list of features](https://github.com/easydiffusion/sdkit/wiki/features).\n\n\ud83d\udce2 we're looking to add support for *lycoris*, *amd*, *pix2pix*, and *outpainting*. we'd love code contributions for these!\n\n## is it fast?\nit is pretty fast, and close to the fastest. for the same image, `sdkit` took 5.5 seconds, while `automatic1111` webui took 4.95 seconds. \ud83d\udce2 we're looking for code contributions to make `sdkit` even faster!\n\n`xformers` is supported experimentally, which will make `sdkit` even faster.\n\n**details of the benchmark:**\n\nwindows 11, nvidia 3060 12gb, 512x512 image, sd-v1-4.ckpt, euler_a sampler, number of steps: 25, seed 42, guidance scale 7.5.\n\nno xformers. no vram optimizations for low-memory usage.\n\n| | time taken | iterations/sec | peak vram usage |\n| --- | --- | --- | --- |\n| `sdkit` | 5.5 sec | 6.0 it/s | 5.1 gb |\n| `automatic1111` webui | 4.95 sec | 6.15 it/s | 5.1 gb |\n\n## does it work on lower-end gpus, or without gpus?\nyes. it works on nvidia/mac gpus with atleast 2gb of vram. for pcs without a compatible gpu, it can run entirely on the cpu. running on the cpu will be *very* slow, but atleast you'll be able to try it out!\n\n\ud83d\udce2 we don't support amd yet on windows (it'll run in cpu-mode, or in linux), but we're looking for code contributions for amd support!\n\n## why not just use diffusers?\nyou can certainly use diffusers. `sdkit` is infact using `diffusers` internally, so you can think of `sdkit` as a convenient api and a collection of tools, focused on stable diffusion projects.\n\n`sdkit`:\n1. is a simple, lightweight toolkit for stable diffusion projects.\n2. natively includes frequently-used projects like gfpgan, codeformer and realesrgan.\n3. works with the popular `.ckpt` and `.safetensors` model format.\n4. includes memory optimizations for low-end gpus.\n5. built-in support for running on multiple gpus.\n6. can download models from any server.\n7. auto-scans for malicious models.\n8. includes 16 samplers (including k-samplers).\n9. born out of the needs of the new stable diffusion ai art scene, starting aug 2022.\n\n# who is using sdkit?\n* [easy diffusion (cmdr2 ui)](https://github.com/easydiffusion/easydiffusion) for stable diffusion.\n* [arthemy ai](https://arthemy.ai/)\n\nif your project is using sdkit, you can add it to this list. please feel free to open a pull request (or let us know at our [discord community](https://discord.com/invite/u9yhsfmekb)).\n\n# contributing\nwe'd love to accept code contributions. please feel free to drop by our [discord community](https://discord.com/invite/u9yhsfmekb)!\n\n\ud83d\udce2 we're looking for code contributions for these features (or anything else you'd like to work on):\n- lycoris.\n- outpainting.\n- pix2pix.\n- amd support.\n\nif you'd like to set up a developer version on your pc (to contribute code changes), please follow [these instructions](https://github.com/easydiffusion/sdkit/blob/main/contributing.md).\n\ninstructions for running automated tests: [running tests](tests/readme.md).\n\n# credits\n* stable diffusion: https://github.com/stability-ai/stablediffusion\n* codeformer: https://github.com/sczhou/codeformer (license: https://github.com/sczhou/codeformer/blob/master/license)\n* gfpgan: https://github.com/tencentarc/gfpgan\n* realesrgan: https://github.com/xinntao/real-esrgan\n* k-diffusion: https://github.com/crowsonkb/k-diffusion\n* code contributors and artists on easy diffusion (cmdr2 ui): https://github.com/easydiffusion/easydiffusion and discord (https://discord.com/invite/u9yhsfmekb)\n* lots of contributors on the internet\n\n# disclaimer\nthe authors of this project are not responsible for any content generated using this project.\n\nthe license of this software forbids you from sharing any content that:\n- violates any laws.\n- produces any harm to a person or persons.\n- disseminates (spreads) any personal information that would be meant for harm.\n- spreads misinformation.\n- target vulnerable groups. \n\nfor the full list of restrictions please read [the license](https://github.com/easydiffusion/sdkit/blob/main/license). you agree to these terms by using this software.\n",
  "docs_url": null,
  "keywords": "stable diffusion,ai,art",
  "license": "",
  "name": "sdkit",
  "package_url": "https://pypi.org/project/sdkit/",
  "project_url": "https://pypi.org/project/sdkit/",
  "project_urls": {
    "Bug Tracker": "https://github.com/easydiffusion/sdkit/issues",
    "Homepage": "https://github.com/easydiffusion/sdkit"
  },
  "release_url": "https://pypi.org/project/sdkit/2.0.15/",
  "requires_dist": [
    "stable-diffusion-sdkit ==2.1.5",
    "gfpgan",
    "piexif",
    "realesrgan",
    "requests",
    "picklescan",
    "safetensors ==0.3.3",
    "k-diffusion ==0.0.12",
    "diffusers ==0.20.2",
    "compel ==2.0.1",
    "accelerate ==0.23.0",
    "controlnet-aux ==0.0.6",
    "invisible-watermark ==0.2.0"
  ],
  "requires_python": ">=3.8.5",
  "summary": "sdkit (stable diffusion kit) is an easy-to-use library for using stable diffusion in your ai art projects. it is fast, feature-packed, and memory-efficient. it bundles stable diffusion along with commonly-used features (like sdxl, controlnet, lora, embeddings, gfpgan, realesrgan, k-samplers, custom vae etc). it also includes a model-downloader with a database of commonly used models, and advanced features like running in parallel on multiple gpus, auto-scanning for malicious models etc.",
  "version": "2.0.15",
  "releases": [],
  "developers": [
    "secondary.cmdr2@gmail.com"
  ],
  "kwds": "diffusion stablediffusion stability ai stable",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_sdkit",
  "homepage": "",
  "release_count": 198,
  "dependency_ids": [
    "pypi_accelerate",
    "pypi_compel",
    "pypi_controlnet_aux",
    "pypi_diffusers",
    "pypi_gfpgan",
    "pypi_invisible_watermark",
    "pypi_k_diffusion",
    "pypi_picklescan",
    "pypi_piexif",
    "pypi_realesrgan",
    "pypi_requests",
    "pypi_safetensors",
    "pypi_stable_diffusion_sdkit"
  ]
}