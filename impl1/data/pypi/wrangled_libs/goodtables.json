{
  "classifiers": [
    "development status :: 4 - beta",
    "environment :: web environment",
    "intended audience :: developers",
    "license :: osi approved :: mit license",
    "operating system :: os independent",
    "programming language :: python :: 2",
    "programming language :: python :: 2.7",
    "programming language :: python :: 3",
    "programming language :: python :: 3.4",
    "programming language :: python :: 3.5",
    "programming language :: python :: 3.6",
    "topic :: internet :: www/http :: dynamic content",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "# goodtables-py\n\n[![travis](https://img.shields.io/travis/frictionlessdata/goodtables-py/master.svg)](https://travis-ci.org/frictionlessdata/goodtables-py)\n[![coveralls](http://img.shields.io/coveralls/frictionlessdata/goodtables-py.svg?branch=master)](https://coveralls.io/r/frictionlessdata/goodtables-py?branch=master)\n[![pypi](https://img.shields.io/pypi/v/goodtables.svg)](https://pypi.python.org/pypi/goodtables)\n[![github](https://img.shields.io/badge/github-master-brightgreen)](https://github.com/frictionlessdata/goodtables-py)\n[![gitter](https://img.shields.io/gitter/room/frictionlessdata/chat.svg)](https://gitter.im/frictionlessdata/chat)\n\ngoodtables is a framework to validate tabular data. it can check the structure\nof your data (e.g. all rows have the same number of columns), and its contents\n(e.g. all dates are valid).\n\n> **[important notice]** `goodtables` was renamed to `frictionless` since version 3. the framework got various improvements and was extended to be a complete data solution. the change in not breaking for the existing software so no actions are required. please read the [migration guide](https://framework.frictionlessdata.io/docs/development/migration#from-goodtables) to start working with frictionless for python.\n> - we continue to bug-fix `goodtables@2.x` in this [branch](https://github.com/frictionlessdata/goodtables-py/tree/goodtables) as well as it's available on [pypi](https://pypi.org/project/goodtables/) as it was before\n> - please note that `frictionless@3.x` version's api, we're working on at the moment, is not stable\n> - we will release `frictionless@4.x` by the end of 2020 to be the first semver/stable version\n\n## features\n\n* **structural checks**: ensure that there are no empty rows, no blank headers, etc.\n* **content checks**: ensure that the values have the correct types (\"string\", \"number\", \"date\", etc.), that their format is valid (\"string must be an e-mail\"), and that they respect the constraints (\"age must be a number greater than 18\").\n* **support for multiple tabular formats**: csv, excel files, libreoffice, data package, etc.\n* **parallelized validations for multi-table datasets**\n* **command line interface**\n\n## contents\n\n<!--toc-->\n\n  - [getting started](#getting-started)\n    - [installing](#installing)\n    - [running on cli](#running-on-cli)\n    - [running on python](#running-on-python)\n  - [documentation](#documentation)\n    - [report](#report)\n    - [checks](#checks)\n    - [presets](#presets)\n    - [data quality errors](#data-quality-errors)\n    - [frequently asked questions](#frequently-asked-questions)\n  - [api reference](#api-reference)\n    - [`cli`](#cli)\n    - [`validate`](#validate)\n    - [`preset`](#preset)\n    - [`check`](#check)\n    - [`error`](#error)\n    - [`spec`](#spec)\n    - [`goodtablesexception`](#goodtablesexception)\n  - [contributing](#contributing)\n  - [changelog](#changelog)\n\n<!--toc-->\n\n## getting started\n\n> for faster goodtables-combatible pandas dataframes validation take a look at https://github.com/ezwelty/goodtables-pandas-py\n\n### installing\n\n```\npip install goodtables\npip install goodtables[ods]  # if you need libreoffice's ods file support\n```\n\n### running on cli\n\n```\ngoodtables data.csv\n```\n\nuse `goodtables --help` to see the different options.\n\n### running on python\n\n```python\nfrom goodtables import validate\n\nreport = validate('invalid.csv')\nreport['valid'] # false\nreport['table-count'] # 1\nreport['error-count'] # 3\nreport['tables'][0]['valid'] # false\nreport['tables'][0]['source'] # 'invalid.csv'\nreport['tables'][0]['errors'][0]['code'] # 'blank-header'\n```\n\nyou can read a more in depth explanation on using goodtables with python on\nthe [developer documentation](#developer-documentation) section. check also\nthe [examples](examples) folder for other examples.\n\n## documentation\n\ngoodtables validates your tabular dataset to find structural and content\nerrors. consider you have a file named `invalid.csv`. let's validate it:\n\n```python\nreport = validate('invalid.csv')\n```\n\nwe could also pass a remote uri instead of a local path. it supports csv, xls,\nxlsx, ods, json, and all other formats supported by the [tabulator][tabulator]\nlibrary.\n\n### report\n\n> the validation report follows the json schema defined on [goodtables/schemas/report.json][validation-jsonschema].\n\nthe output of the `validate()` method is a report dictionary. it includes\ninformation if the data was valid, count of errors, list of table reports, which\nindividual checks failed, etc. a report will be looking like this:\n\n```json\n{\n    \"time\": 0.009,\n    \"error-count\": 1,\n    \"warnings\": [\n        \"table \\\"data/invalid.csv\\\" inspection has reached 1 error(s) limit\"\n    ],\n    \"preset\": \"table\",\n    \"valid\": false,\n    \"tables\": [\n        {\n            \"errors\": [\n                {\n                    \"row-number\": null,\n                    \"message\": \"header in column 3 is blank\",\n                    \"row\": null,\n                    \"column-number\": 3,\n                    \"code\": \"blank-header\"\n                }\n            ],\n            \"error-count\": 1,\n            \"headers\": [\n                \"id\",\n                \"name\",\n                \"\",\n                \"name\"\n            ],\n            \"scheme\": \"file\",\n            \"row-count\": 2,\n            \"valid\": false,\n            \"encoding\": \"utf-8\",\n            \"time\": 0.007,\n            \"schema\": null,\n            \"format\": \"csv\",\n            \"source\": \"data/invalid\"\n        }\n    ],\n    \"table-count\": 1\n}\n```\n\nthe errors are divided in one of the following categories:\n\n- `source` - data can't be loaded or parsed\n- `structure` - general tabular errors like duplicate headers\n- `schema` - error of checks against [table schema](http://specs.frictionlessdata.io/table-schema/)\n- `custom` - custom checks errors\n\n### checks\n\ncheck is a main validation actor in goodtables. the list of enabled checks can\nbe changed using `checks` and `skip_checks` arguments. let's explore the options\non an example:\n\n```python\nreport = validate('data.csv') # by default structure and schema (if available) checks\nreport = validate('data.csv', checks=['structure']) # only structure checks\nreport = validate('data.csv', checks=['schema']) # only schema (if available) checks\nreport = validate('data.csv', checks=['bad-headers']) # check only 'bad-headers'\nreport = validate('data.csv', skip_checks=['bad-headers']) # exclude 'bad-headers'\n```\n\nby default a dataset will be validated against all available data quality spec\nerrors. some checks can be unavailable for validation. for example, if the\nschema isn't provided, only the `structure` checks will be done.\n\n### presets\n\ngoodtables support different formats of tabular datasets. they're called\npresets. a tabular dataset is some data that can be split in a list of data\ntables, as:\n\n![dataset](data/dataset.png)\n\nwe can change the preset using the `preset` argument for `validate()`. by\ndefault, it'll be inferred from the source, falling back to `table`. to validate\na [data package][datapackage], we can do:\n\n```python\nreport = validate('datapackage.json') # implicit preset\nreport = validate('datapackage.json', preset='datapackage') # explicit preset\n```\n\nthis will validate all tabular resources in the datapackage.\n\nit's also possible to validate a list of files using the \"nested\" preset. to do\nso, the first argument to `validate()` should be a list of dictionaries, where\neach key in the dictionary is named after a parameter on `validate()`. for example:\n\n```python\nreport = validate([{'source': 'data1.csv'}, {'source': 'data2.csv'}]) # implicit preset\nreport = validate([{'source': 'data1.csv'}, {'source': 'data2.csv'}], preset='nested') # explicit preset\n```\n\nis similar to:\n\n```python\nreport_data1 = validate('data1.csv')\nreport_data2 = validate('data2.csv')\n```\n\nthe difference is that goodtables validates multiple tables in parallel, so\ncalling using the \"nested\" preset should run faster.\n\n### data quality errors\n\nbase report errors are standardized and described in\n[data quality spec](https://github.com/frictionlessdata/data-quality-spec/blob/master/spec.json).\n\n#### source errors\n\nthe basic checks can't be disabled, as they deal with goodtables being able to read the files.\n\n| check | description |\n| --- | --- |\n| io-error | data reading error because of io error. |\n| http-error | data reading error because of http error. |\n| source-error | data reading error because of not supported or inconsistent contents. |\n| scheme-error | data reading error because of incorrect scheme. |\n| format-error | data reading error because of incorrect format. |\n| encoding-error | data reading error because of an encoding problem. |\n\n#### structure errors\n\nthese checks validate that the structure of the file are valid.\n\n| check | description |\n| --- | --- |\n| blank-header | there is a blank header name. all cells in the header row must have a value. |\n| duplicate-header | there are multiple columns with the same name. all column names must be unique. |\n| blank-row | rows must have at least one non-blank cell. |\n| duplicate-row | rows can't be duplicated. |\n| extra-value | a row has more columns than the header. |\n| missing-value | a row has less columns than the header. |\n\n#### schema errors\n\nthese checks validate the contents of the file. to use them, you need to pass a [table schema][tableschema]. if you don't have a schema, goodtables can infer it if you use the `infer_schema` option.\n\nif your schema only covers part of the data, you can use the `infer_fields` to infer the remaining fields.\n\nlastly, if the order of the fields in the data is different than in your schema, enable the `order_fields` option.\n\n| check | description |\n| --- | --- |\n| schema-error | schema is not valid. |\n| non-matching-header | the header's name in the schema is different from what's in the data. |\n| extra-header | the data contains a header not defined in the schema. |\n| missing-header | the data doesn't contain a header defined in the schema. |\n| type-or-format-error | the value can\u2019t be cast based on the schema type and format for this field. |\n| required-constraint | this field is a required field, but it contains no value. |\n| pattern-constraint | this field value's should conform to the defined pattern. |\n| unique-constraint | this field is a unique field but it contains a value that has been used in another row. |\n| enumerable-constraint | this field value should be equal to one of the values in the enumeration constraint. |\n| minimum-constraint | this field value should be greater or equal than constraint value. |\n| maximum-constraint | this field value should be less or equal than constraint value. |\n| minimum-length-constraint | a length of this field value should be greater or equal than schema constraint value. |\n| maximum-length-constraint | a length of this field value should be less or equal than schema constraint value. |\n\n#### custom errors\n\n| check | description |\n| --- | --- |\n| [blacklisted-value](#blacklisted-value) | ensure there are no cells with the blacklisted values. |\n| [deviated-value](#deviated-value) | ensure numbers are within a number of standard deviations from the average. |\n| [foreign-key](#foreign-key) | ensure foreign keys are valid within a data package |\n| [sequential-value](#sequential-value) | ensure numbers are sequential. |\n| [truncated-value](#truncated-value) | detect values that were potentially truncated. |\n| [custom-constraint](#custom-constraint) | defines a constraint based on the values of other columns (e.g. `value * quantity == total`). |\n\n##### blacklisted-value\n\nsometimes we have to check for some values we don't want to have in out dataset. it accepts following options:\n\n| option | type | description |\n| --- | --- | --- |\n| column | int/str | column number or name |\n| blacklist | list of str | list of blacklisted values |\n\nconsider the following csv file:\n\n```csv\nid,name\n1,john\n2,bug\n3,bad\n5,alex\n```\n\nlet's check that the `name` column doesn't contain rows with `bug` or `bad`:\n\n```python\nfrom goodtables import validate\n\nreport = validate('data.csv', checks=[\n    {'blacklisted-value': {'column': 'name', 'blacklist': ['bug', 'bad']}},\n])\n# error on row 3 with code \"blacklisted-value\"\n# error on row 4 with code \"blacklisted-value\"\n```\n\n##### deviated-value\n\nthis check helps to find outlines in a column containing positive numbers. it accepts following options:\n\n| option | type | description |\n| --- | --- | --- |\n| column | int/str | column number or name |\n| average | str | average type, either \"mean\", \"median\" or \"mode\" |\n| interval | int | values must be inside range `average \u00b1 standard deviation * interval` |\n\nconsider the following csv file:\n\n```csv\ntemperature\n1\n-2\n7\n0\n1\n2\n5\n-4\n100\n8\n3\n```\n\nwe use `median` to get an average of the column values and allow interval of 3 standard deviations. for our case median is `2.0` and standard deviation is `29.73` so all valid values must be inside the `[-87.19, 91.19]` interval.\n\n```python\nreport = validate('data.csv', checks=[\n    {'deviated-value': {'column': 'temperature', 'average': 'median', 'interval': 3}},\n])\n# error on row 10 with code \"deviated-value\"\n```\n\n##### foreign-key\n\n> we support here relative paths. it must be used only for trusted data sources.\n\nthis check validate foreign keys within a data package. consider we have a data package defined below:\n\n```python\ndescriptor = {\n  'resources': [\n    {\n      'name': 'cities',\n      'data': [\n        ['id', 'name', 'next_id'],\n        [1, 'london', 2],\n        [2, 'paris', 3],\n        [3, 'rome', 4],\n        # [4, 'rio', none],\n      ],\n      'schema': {\n        'fields': [\n          {'name': 'id', 'type': 'integer'},\n          {'name': 'name', 'type': 'string'},\n          {'name': 'next_id', 'type': 'integer'},\n        ],\n        'foreignkeys': [\n          {\n            'fields': 'next_id',\n            'reference': {'resource': '', 'fields': 'id'},\n          },\n          {\n            'fields': 'id',\n            'reference': {'resource': 'people', 'fields': 'label'},\n          },\n        ],\n      },\n    }, {\n      'name': 'people',\n      'data': [\n        ['label', 'population'],\n        [1, 8],\n        [2, 2],\n        # [3, 3],\n        # [4, 6],\n      ],\n    },\n  ],\n}\n```\n\nrunning `goodtables` on it will raise a few `foreign-key` errors because we have commented some rows in the data package's data:\n\n```python\nreport = validate(descriptor, checks=['structure', 'schema', 'foreign-key'])\nprint(report)\n```\n\n```\n{'error-count': 2,\n 'preset': 'datapackage',\n 'table-count': 2,\n 'tables': [{'datapackage': '...',\n             'error-count': 2,\n             'errors': [{'code': 'foreign-key',\n                         'message': 'foreign key \"[\\'next_id\\']\" violation in '\n                                    'row 4',\n                         'message-data': {'fields': ['next_id']},\n                         'row-number': 4},\n                        {'code': 'foreign-key',\n                         'message': 'foreign key \"[\\'id\\']\" violation in row 4',\n                         'message-data': {'fields': ['id']},\n                         'row-number': 4}],\n             'format': 'inline',\n             'headers': ['id', 'name', 'next_id'],\n             'resource-name': 'cities',\n             'row-count': 4,\n             'schema': 'table-schema',\n             'source': 'inline',\n             'time': 0.031,\n             'valid': false},\n            {'datapackage': '...',\n             'error-count': 0,\n             'errors': [],\n             'format': 'inline',\n             'headers': ['label', 'population'],\n             'resource-name': 'people',\n             'row-count': 3,\n             'source': 'inline',\n             'time': 0.038,\n             'valid': true}],\n 'time': 0.117,\n 'valid': false,\n 'warnings': []}\n```\n\nit experimetally supports external resource checks, for example, for a `foreignkey` definition like these:\n\n```json\n{\"package\": \"../people/datapackage.json\", \"resource\": \"people\", \"fields\": \"label\"}\n{\"package\": \"http:/example.com/datapackage.json\", \"resource\": \"people\", \"fields\": \"label\"}\n```\n\n##### sequential-value\n\nthis checks is for pretty common case when a column should have integers that sequentially increment.  it accepts following options:\n\n| option | type | description |\n| --- | --- | --- |\n| column | int/str | column number or name |\n\nconsider the following csv file:\n\n```csv\nid,name\n1,one\n2,two\n3,three\n5,five\n```\n\nlet's check if the `id` column contains sequential integers:\n\n```python\nfrom goodtables import validate\n\nreport = validate('data.csv', checks=[\n    {'sequential-value': {'column': 'id'}},\n])\n# error on row 5 with code \"sequential-value\"\n```\n\n##### truncated-value\n\nsome database or spreadsheet software (like mysql or excel) could cutoff values on saving. there are some well-known heuristics to find this bad values. see https://github.com/propublica/guides/blob/master/data-bulletproofing.md for more detailed information.\n\nconsider the following csv file:\n\n```csv\nid,amount,comment\n1,14000000,good\n2,2147483647,bad\n3,32767,bad\n4,234234234,bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbad\n```\n\nto detect all probably truncated values we could use `truncated-value` check:\n\n```python\nreport = validate('data.csv', checks=[\n    'truncated-value',\n])\n# error on row 3 with code \"truncated-value\"\n# error on row 4 with code \"truncated-value\"\n# error on row 5 with code \"truncated-value\"\n```\n\n##### custom-constraint\n\nwith table schema we could create constraints for an individual field but sometimes it's not enough. with a custom constraint check every row could be checked against given limited python expression in which variable names resolve to column values. see list of [available operators]( https://github.com/danthedeckie/simpleeval#operators). it accepts following options:\n\n<dl>\n  <dt>constraint (str)</dt>\n  <dd>constraint definition (e.g. <code>col1 + col2 == col3</code>)</dd>\n</dl>\n\nconsider csv file like this:\n\n```csv\nid,name,salary,bonus\n1,alex,1000,200\n2,sam,2500,500\n3,ray,1350,500\n4,john,5000,1000\n```\n\nlet's say our business rule is to be shy on bonuses:\n\n```python\nreport = validate('data.csv', checks=[\n    {'custom-constraint': {'constraint': 'salary > bonus * 4'}},\n])\n# error on row 4 with code \"custom-constraint\"\n```\n\n### frequently asked questions\n\n#### how can i add a new custom check?\n\nto create a custom check user could use a `check` decorator. this way the builtin check could be overridden (use the spec error code like `duplicate-row`) or could be added a check for a custom error (use `type`, `context` and `position` arguments):\n\n```python\nfrom goodtables import validate, check, error\n\n@check('custom-check', type='custom', context='body')\ndef custom_check(cells):\n    errors = []\n    for cell in cells:\n        message = 'custom error on column {column_number} and row {row_number}'\n        error = error(\n            'custom-error',\n            cell,\n            message\n        )\n        errors.append(error)\n    return errors\n\nreport = validate('data.csv', checks=['custom-check'])\n```\n\nrecommended steps:\n- let's discuss in comment proposed checks first\n- select name for a new check like `possible-noise-text`\n- copy https://github.com/frictionlessdata/goodtables-py/blob/master/goodtables/contrib/checks/blacklisted_value.py to new check module\n- add new check module to configuration - https://github.com/frictionlessdata/goodtables-py/blob/master/goodtables/config.py\n- write actual code for the new check\n- write tests and readme for the new check\n\n#### how can i add support for a new tabular file type?\n\nto create a custom preset user could use a `preset` decorator. this way the builtin preset could be overridden or could be added a custom preset.\n\n```python\nfrom tabulator import stream\nfrom tableschema import schema\nfrom goodtables import validate\n\n@preset('custom-preset')\ndef custom_preset(source, **options):\n    warnings = []\n    tables = []\n    for table in source:\n        try:\n            tables.append({\n                'source':  str(source),\n                'stream':  stream(...),\n                'schema': schema(...),\n                'extra': {...},\n            })\n        except exception:\n            warnings.append('warning message')\n    return warnings, tables\n\nreport = validate(source, preset='custom-preset')\n```\n\nfor now this documentation section is incomplete. please see builtin presets to learn more about the dataset extraction protocol.\n\n## api reference\n\n### `cli`\n```python\ncli()\n```\ncommand-line interface\n\n```\nusage: cli.py [options] command [args]...\n\noptions:\n  --version  show the version and exit.\n  --help     show this message and exit.\n\ncommands:\n  validate*  validate tabular files (default).\n  init       init data package from list of files.\n```\n\n\n### `validate`\n```python\nvalidate(source, **options)\n```\nvalidates a source file and returns a report.\n\n__arguments__\n\n- __source (union[str, dict, list[dict], io])__:\n        the source to be validated.\n        it can be a local file path, url, dict, list of dicts, or a\n        file-like object. if it's a list of dicts and the `preset` is\n        \"nested\", each of the dict key's will be used as if it was passed\n        as a keyword argument to this method.\n\n        the file can be a csv, xls, json, and any other format supported by\n        `tabulator`_.\n- __checks (list[str])__:\n        list of checks names to be enabled. they can be\n        individual check names (e.g. `blank-headers`), or check types (e.g.\n        `structure`).\n- __skip_checks (list[str])__:\n        list of checks names to be skipped. they can\n        be individual check names (e.g. `blank-headers`), or check types\n        (e.g.  `structure`).\n- __infer_schema (bool)__:\n        infer schema if one wasn't passed as an argument.\n- __infer_fields (bool)__:\n        infer schema for columns not present in the received schema.\n- __order_fields (bool)__:\n        order source columns based on schema fields order.\n        this is useful when you don't want to validate that the data\n        columns' order is the same as the schema's.\n- __error_limit (int)__:\n        stop validation if the number of errors per table exceeds this value.\n- __table_limit (int)__:\n        maximum number of tables to validate.\n- __row_limit (int)__:\n        maximum number of rows to validate.\n- __preset (str)__:\n        dataset type could be `table` (default), `datapackage`,\n        `nested` or custom. usually, the preset can be inferred from the\n        source, so you don't need to define it.\n- __any (any)__:\n        any additional arguments not defined here will be passed on,\n        depending on the chosen `preset`. if the `preset` is `table`, the\n        extra arguments will be passed on to `tabulator`_, if it is\n        `datapackage`, they will be passed on to the `datapackage`_\n        constructor.\n\n__raises__\n- `goodtablesexception`: raised on any non-tabular error.\n\n__returns__\n\n`dict`: the validation report.\n\n\n### `preset`\n```python\npreset(name)\n```\nregister a custom preset (decorator)\n\n__example__\n\n\n```python\n@preset('custom-preset')\ndef custom_preset(source, **options):\n    # ...\n```\n\n__arguments__\n- __name (str)__: preset name\n\n\n### `check`\n```python\ncheck(name, type=none, context=none, position=none)\n```\nregister a custom check (decorator)\n\n__example__\n\n\n```python\n@check('custom-check', type='custom', context='body')\ndef custom_check(cells):\n    # ...\n```\n\n__arguments__\n- __name (str)__: preset name\n- __type (str)__: has to be `custom`\n- __context (str)__: has to be `head` or `body`\n- __position (str)__: has to be `before:<check-name>` or `after:<check-name>`\n\n\n### `error`\n```python\nerror(self, code, cell=none, row_number=none, message=none, message_substitutions=none)\n```\ndescribes a validation check error\n\n__arguments__\n- __code (str)__: the error code. must be one in the spec.\n- __cell (dict, optional)__: the cell where the error occurred.\n- __row_number (int, optional)__: the row number where the error occurs.\n- __message (str, optional)__:\n        the error message. defaults to the message from the data quality spec.\n- __message_substitutions (dict, optional)__:\n        dictionary with substitutions to be used when\n        generating the error message and description.\n\n__raises__\n- `keyerror`: raised if the error code isn't known.\n\n\n### `spec`\ndict() -> new empty dictionary\ndict(mapping) -> new dictionary initialized from a mapping object's\n    (key, value) pairs\ndict(iterable) -> new dictionary initialized as if via:\n    d = {}\n    for k, v in iterable:\n        d[k] = v\ndict(**kwargs) -> new dictionary initialized with the name=value pairs\n    in the keyword argument list.  for example:  dict(one=1, two=2)\n### `goodtablesexception`\n```python\ngoodtablesexception(self, /, *args, **kwargs)\n```\nbase goodtables exception\n\n## contributing\n\n> the project follows the [open knowledge international coding standards](https://github.com/okfn/coding-standards).\n\nrecommended way to get started is to create and activate a project virtual environment.\nto install package and development dependencies into active environment:\n\n```bash\n$ make install\n```\n\nto run tests with linting and coverage:\n\n```bash\n$ make test\n```\n\n## changelog\n\nhere described only breaking and the most important changes. the full changelog and documentation for all released versions could be found in nicely formatted [commit history](https://github.com/frictionlessdata/goodtables-py/commits/master).\n\n##### v2.5\n\n- added `check.check_headers_hook` to support headers check for body-contexted checks (see https://github.com/frictionlessdata/goodtables-py/tree/v3 for native support)\n\n##### v2.4\n\n- added integrity checks for data packages. if `resource.bytes` or `resource.hash` (sha256) is provided it will be verified against actual values\n\n##### v2.3\n\n- added a [foreign keys check](#foreign-key)\n\n##### v2.2\n\n- improved missing/non-matching-headers detection ([#298](https://github.com/frictionlessdata/goodtables-py/issues/298))\n\n##### v2.1\n\n- a new key added to the `error.to_dict` return: `message-data`\n\n##### v2.0\n\nbreaking changes:\n\n- checks method signature now only receives the current row's `cells` list\n- checks raise errors by returning an array of `error` objects\n- cells have the row number in the `row-number` key\n- files with zip extension are presumed to be datapackages, so `goodtables mydatapackage.zip` works\n- improvements to goodtables cli ([#233](https://github.com/frictionlessdata/goodtables-py/issues/233))\n- new `goodtables init <data paths>` command to create a new `datapackage.json` with the files passed as parameters and their inferred schemas.\n\nbug fixes:\n- fix bug with `truncated-values` check on date fields ([#250](https://github.com/frictionlessdata/goodtables-py/issues/250))\n\n##### v1.5\n\nnew api added:\n- validation `source` now could be a `pathlib.path`\n\n##### v1.4\n\nimproved behaviour:\n- rebased on data quality spec v1\n- rebased on data package spec v1\n- rebased on table schema spec v1\n- treat primary key as required/unique field\n\n##### v1.3\n\nnew advanced checks added:\n- `blacklisted-value`\n- `custom-constraint`\n- `deviated-value`\n- `sequential-value`\n- `truncated-value`\n\n##### v1.2\n\nnew api added:\n- `report.preset`\n- `report.tables[].schema`\n\n##### v1.1\n\nnew api added:\n- `report.tables[].scheme`\n- `report.tables[].format`\n- `report.tables[].encoding`\n\n##### v1.0\n\nthis version includes various big changes. a migration guide is under development and will be published here.\n\n##### v0.6\n\nfirst version of `goodtables`.\n\n[tableschema]: https://specs.frictionlessdata.io/table-schema/\n[tabulator]: https://github.com/frictionlessdata/tabulator-py/\n[datapackage]: https://specs.frictionlessdata.io/data-package/ \"data package specification\"\n[semver]: https://semver.org/ \"semantic versioning\"\n[validation-jsonschema]: goodtables/schemas/report.json \"validation report json schema\"\n\n",
  "docs_url": null,
  "keywords": "data validation,frictionless data,open data,json schema,json table schema,data package,tabular data package",
  "license": "mit",
  "name": "goodtables",
  "package_url": "https://pypi.org/project/goodtables/",
  "project_url": "https://pypi.org/project/goodtables/",
  "project_urls": {
    "Homepage": "https://github.com/frictionlessdata/goodtables"
  },
  "release_url": "https://pypi.org/project/goodtables/2.5.4/",
  "requires_dist": [
    "six (>=1.9)",
    "click (>=6.6)",
    "click-default-group",
    "requests (>=2.10)",
    "simpleeval (>=0.9)",
    "statistics (>=1.0)",
    "tabulator (>=1.40)",
    "tableschema (>=1.16.4)",
    "datapackage (>=1.10)",
    "mock ; extra == 'develop'",
    "pylama ; extra == 'develop'",
    "pytest ; extra == 'develop'",
    "pytest-cov ; extra == 'develop'",
    "pyyaml ; extra == 'develop'",
    "ezodf (>=0.3) ; extra == 'ods'",
    "lxml (>=3.0) ; extra == 'ods'"
  ],
  "requires_python": "",
  "summary": "goodtables is a framework to inspect tabular data.",
  "version": "2.5.4",
  "releases": [],
  "developers": [
    "info@okfn.org",
    "open_knowledge_international"
  ],
  "kwds": "goodtables frictionlessdata tableschema goodtablesexception tables",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_goodtables",
  "homepage": "https://github.com/frictionlessdata/goodtables",
  "release_count": 100,
  "dependency_ids": [
    "pypi_click",
    "pypi_click_default_group",
    "pypi_datapackage",
    "pypi_ezodf",
    "pypi_lxml",
    "pypi_mock",
    "pypi_pylama",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_pyyaml",
    "pypi_requests",
    "pypi_simpleeval",
    "pypi_six",
    "pypi_statistics",
    "pypi_tableschema",
    "pypi_tabulator"
  ]
}