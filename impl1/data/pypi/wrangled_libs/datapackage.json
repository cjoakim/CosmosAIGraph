{
  "classifiers": [
    "development status :: 4 - beta",
    "intended audience :: developers",
    "intended audience :: information technology",
    "license :: osi approved :: mit license",
    "programming language :: python :: 2.7",
    "programming language :: python :: 3.4",
    "programming language :: python :: 3.5",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "topic :: utilities"
  ],
  "description": "# datapackage-py\n\n[![travis](https://travis-ci.org/frictionlessdata/datapackage-py.svg?branch=master)](https://travis-ci.org/frictionlessdata/datapackage-py)\n[![coveralls](https://coveralls.io/repos/github/frictionlessdata/datapackage-py/badge.svg?branch=master)](https://coveralls.io/github/frictionlessdata/datapackage-py?branch=master)\n[![pypi](https://img.shields.io/pypi/v/datapackage.svg)](https://pypi.python.org/pypi/datapackage)\n[![github](https://img.shields.io/badge/github-master-brightgreen)](https://github.com/frictionlessdata/datapackage-py)\n[![gitter](https://img.shields.io/gitter/room/frictionlessdata/chat.svg)](https://gitter.im/frictionlessdata/chat)\n\na library for working with [data packages](http://specs.frictionlessdata.io/data-package/).\n\n> **[important notice]** we have released [frictionless framework](https://github.com/frictionlessdata/frictionless-py). this framework provides improved `datapackage` functionality extended to be a complete data solution. the change in not breaking for the existing software so no actions are required. please read the [migration guide](https://framework.frictionlessdata.io/docs/development/migration) from `datapackage` to frictionless framework.\n> - we continue to bug-fix `datapackage@1.x` in this [repository](https://github.com/frictionlessdata/datapackage-py) as well as it's available on [pypi](https://pypi.org/project/datapackage/) as it was before\n> - please note that `frictionless@3.x` version's api, we're working on at the moment, is not stable\n> - we will release `frictionless@4.x` by the end of 2020 to be the first semver/stable version\n\n## features\n\n - `package` class for working with data packages\n - `resource` class for working with data resources\n - `profile` class for working with profiles\n - `validate` function for validating data package descriptors\n - `infer` function for inferring data package descriptors\n\n## contents\n\n<!--toc-->\n\n  - [getting started](#getting-started)\n    - [installation](#installation)\n  - [documentation](#documentation)\n    - [introduction](#introduction)\n    - [working with package](#working-with-package)\n    - [working with resource](#working-with-resource)\n    - [working with group](#working-with-group)\n    - [working with profile](#working-with-profile)\n    - [working with foreign keys](#working-with-foreign-keys)\n    - [working with validate/infer](#working-with-validateinfer)\n    - [frequently asked questions](#frequently-asked-questions)\n  - [api reference](#api-reference)\n    - [`cli`](#cli)\n    - [`package`](#package)\n    - [`resource`](#resource)\n    - [`group`](#group)\n    - [`profile`](#profile)\n    - [`validate`](#validate)\n    - [`infer`](#infer)\n    - [`datapackageexception`](#datapackageexception)\n    - [`tableschemaexception`](#tableschemaexception)\n    - [`loaderror`](#loaderror)\n    - [`casterror`](#casterror)\n    - [`integrityerror`](#integrityerror)\n    - [`relationerror`](#relationerror)\n    - [`storageerror`](#storageerror)\n  - [contributing](#contributing)\n  - [changelog](#changelog)\n\n<!--toc-->\n\n## getting started\n\n### installation\n\nthe package use semantic versioning. it means that major versions  could include breaking changes. it's highly recommended to specify `datapackage` version range in your `setup/requirements` file e.g. `datapackage>=1.0,<2.0`.\n\n```bash\n$ pip install datapackage\n```\n\n#### osx 10.14+\nif you receive an error about the `cchardet` package when installing datapackage on mac osx 10.14 (mojave) or higher, follow these steps:\n1. make sure you have the latest x-code by running the following in terminal: `xcode-select --install`\n2. then go to [https://developer.apple.com/download/more/](https://developer.apple.com/download/more/) and download the `command line tools`. note, this requires an apple id.\n3. then, in terminal, run `open /library/developer/commandlinetools/packages/macos_sdk_headers_for_macos_10.14.pkg`\nyou can read more about these steps in this [post.](https://stackoverflow.com/questions/52509602/cant-compile-c-program-on-a-mac-after-upgrade-to-mojave)\n\n## documentation\n\n### introduction\n\nlet's start with a simple example:\n\n```python\nfrom datapackage import package\n\npackage = package('datapackage.json')\npackage.get_resource('resource').read()\n```\n\n### working with package\n\na class for working with data packages. it provides various capabilities like loading local or remote data package, inferring a data package descriptor, saving a data package descriptor and many more.\n\nconsider we have some local csv files in a `data` directory. let's create a data package based on this data using a `package` class:\n\n> data/cities.csv\n\n```csv\ncity,location\nlondon,\"51.50,-0.11\"\nparis,\"48.85,2.30\"\nrome,\"41.89,12.51\"\n```\n\n> data/population.csv\n\n```csv\ncity,year,population\nlondon,2017,8780000\nparis,2017,2240000\nrome,2017,2860000\n```\n\nfirst we create a blank data package:\n\n```python\npackage = package()\n```\n\nnow we're ready to infer a data package descriptor based on data files we have. because we have two csv files we use glob pattern `**/*.csv`:\n\n```python\npackage.infer('**/*.csv')\npackage.descriptor\n#{ profile: 'tabular-data-package',\n#  resources:\n#   [ { path: 'data/cities.csv',\n#       profile: 'tabular-data-resource',\n#       encoding: 'utf-8',\n#       name: 'cities',\n#       format: 'csv',\n#       mediatype: 'text/csv',\n#       schema: [object] },\n#     { path: 'data/population.csv',\n#       profile: 'tabular-data-resource',\n#       encoding: 'utf-8',\n#       name: 'population',\n#       format: 'csv',\n#       mediatype: 'text/csv',\n#       schema: [object] } ] }\n```\n\nan `infer` method has found all our files and inspected it to extract useful metadata like profile, encoding, format, table schema etc. let's tweak it a little bit:\n\n```python\npackage.descriptor['resources'][1]['schema']['fields'][1]['type'] = 'year'\npackage.commit()\npackage.valid # true\n```\n\nbecause our resources are tabular we could read it as a tabular data:\n\n```python\npackage.get_resource('population').read(keyed=true)\n#[ { city: 'london', year: 2017, population: 8780000 },\n#  { city: 'paris', year: 2017, population: 2240000 },\n#  { city: 'rome', year: 2017, population: 2860000 } ]\n```\n\nlet's save our descriptor on the disk as a zip-file:\n\n```python\npackage.save('datapackage.zip')\n```\n\nto continue the work with the data package we just load it again but this time using local `datapackage.zip`:\n\n```python\npackage = package('datapackage.zip')\n# continue the work\n```\n\nit was onle basic introduction to the `package` class. to learn more let's take a look on `package` class api reference.\n\n### working with resource\n\na class for working with data resources. you can read or iterate tabular resources using the `iter/read` methods and all resource as bytes using `row_iter/row_read` methods.\n\nconsider we have some local csv file. it could be inline data or remote link - all supported by `resource` class (except local files for in-brower usage of course). but say it's `data.csv` for now:\n\n```csv\ncity,location\nlondon,\"51.50,-0.11\"\nparis,\"48.85,2.30\"\nrome,n/a\n```\n\nlet's create and read a resource. because resource is tabular we could use `resource.read` method with a `keyed` option to get an array of keyed rows:\n\n```python\nresource = resource({path: 'data.csv'})\nresource.tabular # true\nresource.read(keyed=true)\n# [\n#   {city: 'london', location: '51.50,-0.11'},\n#   {city: 'paris', location: '48.85,2.30'},\n#   {city: 'rome', location: 'n/a'},\n# ]\nresource.headers\n# ['city', 'location']\n# (reading has to be started first)\n```\n\nas we could see our locations are just a strings. but it should be geopoints. also rome's location is not available but it's also just a `n/a` string instead of python `none`. first we have to infer resource metadata:\n\n```python\nresource.infer()\nresource.descriptor\n#{ path: 'data.csv',\n#  profile: 'tabular-data-resource',\n#  encoding: 'utf-8',\n#  name: 'data',\n#  format: 'csv',\n#  mediatype: 'text/csv',\n# schema: { fields: [ [object], [object] ], missingvalues: [ '' ] } }\nresource.read(keyed=true)\n# fails with a data validation error\n```\n\nlet's fix not available location. there is a `missingvalues` property in table schema specification. as a first try we set `missingvalues` to `n/a` in `resource.descriptor.schema`. resource descriptor could be changed in-place but all changes should be commited by `resource.commit()`:\n\n```python\nresource.descriptor['schema']['missingvalues'] = 'n/a'\nresource.commit()\nresource.valid # false\nresource.errors\n# [<validationerror: \"'n/a' is not of type 'array'\">]\n```\n\nas a good citiziens we've decided to check out recource descriptor validity. and it's not valid! we should use an array for `missingvalues` property. also don't forget to have an empty string as a missing value:\n\n```python\nresource.descriptor['schema']['missingvalues'] = ['', 'n/a']\nresource.commit()\nresource.valid # true\n```\n\nall good. it looks like we're ready to read our data again:\n\n```python\nresource.read(keyed=true)\n# [\n#   {city: 'london', location: [51.50,-0.11]},\n#   {city: 'paris', location: [48.85,2.30]},\n#   {city: 'rome', location: null},\n# ]\n```\n\nnow we see that:\n- locations are arrays with numeric lattide and longitude\n- rome's location is a native javascript `null`\n\nand because there are no errors on data reading we could be sure that our data is valid againt our schema. let's save our resource descriptor:\n\n```python\nresource.save('dataresource.json')\n```\n\nlet's check newly-crated `dataresource.json`. it contains path to our data file, inferred metadata and our `missingvalues` tweak:\n\n```json\n{\n    \"path\": \"data.csv\",\n    \"profile\": \"tabular-data-resource\",\n    \"encoding\": \"utf-8\",\n    \"name\": \"data\",\n    \"format\": \"csv\",\n    \"mediatype\": \"text/csv\",\n    \"schema\": {\n        \"fields\": [\n            {\n                \"name\": \"city\",\n                \"type\": \"string\",\n                \"format\": \"default\"\n            },\n            {\n                \"name\": \"location\",\n                \"type\": \"geopoint\",\n                \"format\": \"default\"\n            }\n        ],\n        \"missingvalues\": [\n            \"\",\n            \"n/a\"\n        ]\n    }\n}\n```\n\nif we decide to improve it even more we could update the `dataresource.json` file and then open it again using local file name:\n\n```python\nresource = resource('dataresource.json')\n# continue the work\n```\n\nit was onle basic introduction to the `resource` class. to learn more let's take a look on `resource` class api reference.\n\n### working with group\n\na class representing a group of tabular resources. groups can be used to read multiple resource as one or to export them, for example, to a database as one table. to define a group add the `group: <name>` field to corresponding resources. the group's metadata will be created from the \"leading\" resource's metadata (the first resource with the group name).\n\nconsider we have a data package with two tables partitioned by a year and a shared schema stored separately:\n\n>  cars-2017.csv\n\n```csv\nname,value\nbmw,2017\ntesla,2017\nnissan,2017\n```\n\n>  cars-2018.csv\n\n```csv\nname,value\nbmw,2018\ntesla,2018\nnissan,2018\n```\n\n> cars.schema.json\n\n```json\n{\n    \"fields\": [\n        {\n            \"name\": \"name\",\n            \"type\": \"string\"\n        },\n        {\n            \"name\": \"value\",\n            \"type\": \"integer\"\n        }\n    ]\n}\n```\n\n> datapackage.json\n\n```json\n{\n    \"name\": \"datapackage\",\n    \"resources\": [\n        {\n            \"group\": \"cars\",\n            \"name\": \"cars-2017\",\n            \"path\": \"cars-2017.csv\",\n            \"profile\": \"tabular-data-resource\",\n            \"schema\": \"cars.schema.json\"\n        },\n        {\n            \"group\": \"cars\",\n            \"name\": \"cars-2018\",\n            \"path\": \"cars-2018.csv\",\n            \"profile\": \"tabular-data-resource\",\n            \"schema\": \"cars.schema.json\"\n        }\n    ]\n}\n```\n\nlet's read the resources separately:\n\n```python\npackage = package('datapackage.json')\npackage.get_resource('cars-2017').read(keyed=true) == [\n    {'name': 'bmw', 'value': 2017},\n    {'name': 'tesla', 'value': 2017},\n    {'name': 'nissan', 'value': 2017},\n]\npackage.get_resource('cars-2018').read(keyed=true) == [\n    {'name': 'bmw', 'value': 2018},\n    {'name': 'tesla', 'value': 2018},\n    {'name': 'nissan', 'value': 2018},\n]\n```\n\non the other hand, these resources defined with a `group: cars` field. it means we can treat them as a group:\n\n```python\npackage = package('datapackage.json')\npackage.get_group('cars').read(keyed=true) == [\n    {'name': 'bmw', 'value': 2017},\n    {'name': 'tesla', 'value': 2017},\n    {'name': 'nissan', 'value': 2017},\n    {'name': 'bmw', 'value': 2018},\n    {'name': 'tesla', 'value': 2018},\n    {'name': 'nissan', 'value': 2018},\n]\n```\n\nwe can use this approach when we need to save the data package to a storage, for example, to a sql database. there is the `merge_groups` flag to enable groupping behaviour:\n\n```python\npackage = package('datapackage.json')\npackage.save(storage='sql', engine=engine)\n# sql tables:\n# - cars-2017\n# - cars-2018\npackage.save(storage='sql', engine=engine, merge_groups=true)\n# sql tables:\n# - cars\n```\n\n### working with profile\n\na component to represent json schema profile from [profiles registry]( https://specs.frictionlessdata.io/schemas/registry.json):\n\n```python\nprofile = profile('data-package')\n\nprofile.name # data-package\nprofile.jsonschema # json schema contents\n\ntry:\n   valid = profile.validate(descriptor)\nexcept exceptions.validationerror as exception:\n   for error in exception.errors:\n       # handle individual error\n```\n\n### working with foreign keys\n\nthe library supports foreign keys described in the [table schema](http://specs.frictionlessdata.io/table-schema/#foreign-keys) specification. it means if your data package descriptor use `resources[].schema.foreignkeys` property for some resources a data integrity will be checked on reading operations.\n\nconsider we have a data package:\n\n```python\ndescriptor = {\n  'resources': [\n    {\n      'name': 'teams',\n      'data': [\n        ['id', 'name', 'city'],\n        ['1', 'arsenal', 'london'],\n        ['2', 'real', 'madrid'],\n        ['3', 'bayern', 'munich'],\n      ],\n      'schema': {\n        'fields': [\n          {'name': 'id', 'type': 'integer'},\n          {'name': 'name', 'type': 'string'},\n          {'name': 'city', 'type': 'string'},\n        ],\n        'foreignkeys': [\n          {\n            'fields': 'city',\n            'reference': {'resource': 'cities', 'fields': 'name'},\n          },\n        ],\n      },\n    }, {\n      'name': 'cities',\n      'data': [\n        ['name', 'country'],\n        ['london', 'england'],\n        ['madrid', 'spain'],\n      ],\n    },\n  ],\n}\n```\n\nlet's check relations for a `teams` resource:\n\n```python\nfrom datapackage import package\n\npackage = package(descriptor)\nteams = package.get_resource('teams')\nteams.check_relations()\n# tableschema.exceptions.relationerror: foreign key \"['city']\" violation in row \"4\"\n```\n\nas we could see there is a foreign key violation. that's because our lookup table `cities` doesn't have a city of `munich` but we have a team from there. we need to fix it in `cities` resource:\n\n```python\npackage.descriptor['resources'][1]['data'].append(['munich', 'germany'])\npackage.commit()\nteams = package.get_resource('teams')\nteams.check_relations()\n# true\n```\n\nfixed! but not only a check operation is available. we could use `relations` argument for `resource.iter/read` methods to dereference a resource relations:\n\n```python\nteams.read(keyed=true, relations=true)\n#[{'id': 1, 'name': 'arsenal', 'city': {'name': 'london', 'country': 'england}},\n# {'id': 2, 'name': 'real', 'city': {'name': 'madrid', 'country': 'spain}},\n# {'id': 3, 'name': 'bayern', 'city': {'name': 'munich', 'country': 'germany}}]\n```\n\ninstead of plain city name we've got a dictionary containing a city data. these `resource.iter/read` methods will fail with the same as `resource.check_relations` error if there is an integrity issue. but only if `relations=true` flag is passed.\n\n### working with validate/infer\n\na standalone function to validate a data package descriptor:\n\n```python\nfrom datapackage import validate, exceptions\n\ntry:\n    valid = validate(descriptor)\nexcept exceptions.validationerror as exception:\n   for error in exception.errors:\n       # handle individual error\n```\n\na standalone function to infer a data package descriptor.\n\n```python\ndescriptor = infer('**/*.csv')\n#{ profile: 'tabular-data-resource',\n#  resources:\n#   [ { path: 'data/cities.csv',\n#       profile: 'tabular-data-resource',\n#       encoding: 'utf-8',\n#       name: 'cities',\n#       format: 'csv',\n#       mediatype: 'text/csv',\n#       schema: [object] },\n#     { path: 'data/population.csv',\n#       profile: 'tabular-data-resource',\n#       encoding: 'utf-8',\n#       name: 'population',\n#       format: 'csv',\n#       mediatype: 'text/csv',\n#       schema: [object] } ] }\n```\n\n### frequently asked questions\n\n#### accessing data behind a proxy server?\n\nbefore the `package = package(\"https://xxx.json\")` call set these environment variables:\n\n```python\nimport os\n\nos.environ[\"http_proxy\"] = 'xxx'\nos.environ[\"https_proxy\"] = 'xxx'\n```\n\n## api reference\n\n### `cli`\n```python\ncli()\n```\ncommand-line interface\n\n```\nusage: datapackage [options] command [args]...\n\noptions:\n  --version  show the version and exit.\n  --help     show this message and exit.\n\ncommands:\n  infer\n  validate\n```\n\n\n### `package`\n```python\npackage(self,\n        descriptor=none,\n        base_path=none,\n        strict=false,\n        unsafe=false,\n        storage=none,\n        schema=none,\n        default_base_path=none,\n        **options)\n```\npackage representation\n\n__arguments__\n- __descriptor (str/dict)__: data package descriptor as local path, url or object\n- __base_path (str)__: base path for all relative paths\n- __strict (bool)__: strict flag to alter validation behavior.\n        setting it to `true` leads to throwing errors\n        on any operation with invalid descriptor\n- __unsafe (bool)__:\n        if `true` unsafe paths will be allowed. for more inforamtion\n        https://specs.frictionlessdata.io/data-resource/#data-location.\n        default to `false`\n- __storage (str/tableschema.storage)__: storage name like `sql` or storage instance\n- __options (dict)__: storage options to use for storage creation\n\n__raises__\n- `datapackageexception`: raises error if something goes wrong\n\n\n\n#### `package.base_path`\npackage's base path\n\n__returns__\n\n`str/none`: returns the data package base path\n\n\n\n#### `package.descriptor`\npackage's descriptor\n\n__returns__\n\n`dict`: descriptor\n\n\n\n#### `package.errors`\nvalidation errors\n\nalways empty in strict mode.\n\n__returns__\n\n`exception[]`: validation errors\n\n\n\n#### `package.profile`\npackage's profile\n\n__returns__\n\n`profile`: an instance of `profile` class\n\n\n\n#### `package.resource_names`\npackage's resource names\n\n__returns__\n\n`str[]`: returns an array of resource names\n\n\n\n#### `package.resources`\npackage's resources\n\n__returns__\n\n`resource[]`: returns an array of `resource` instances\n\n\n\n#### `package.valid`\nvalidation status\n\nalways true in strict mode.\n\n__returns__\n\n`bool`: validation status\n\n\n\n#### `package.get_resource`\n```python\npackage.get_resource(name)\n```\nget data package resource by name.\n\n__arguments__\n- __name (str)__: data resource name\n\n__returns__\n\n`resource/none`: returns `resource` instances or null if not found\n\n\n\n#### `package.add_resource`\n```python\npackage.add_resource(descriptor)\n```\nadd new resource to data package.\n\nthe data package descriptor will be validated with newly added resource descriptor.\n\n__arguments__\n- __descriptor (dict)__: data resource descriptor\n\n__raises__\n- `datapackageexception`: raises error if something goes wrong\n\n__returns__\n\n`resource/none`: returns added `resource` instance or null if not added\n\n\n\n#### `package.remove_resource`\n```python\npackage.remove_resource(name)\n```\nremove data package resource by name.\n\nthe data package descriptor will be validated after resource descriptor removal.\n\n__arguments__\n- __name (str)__: data resource name\n\n__raises__\n- `datapackageexception`: raises error if something goes wrong\n\n__returns__\n\n`resource/none`: returns removed `resource` instances or null if not found\n\n\n\n#### `package.get_group`\n```python\npackage.get_group(name)\n```\nreturns a group of tabular resources by name.\n\nfor more information about groups see [group](#group).\n\n__arguments__\n- __name (str)__: name of a group of resources\n\n__raises__\n- `datapackageexception`: raises error if something goes wrong\n\n__returns__\n\n`group/none`: returns a `group` instance or null if not found\n\n\n\n#### `package.infer`\n```python\npackage.infer(pattern=false)\n```\ninfer a data package metadata.\n\n> argument `pattern` works only for local files\n\nif `pattern` is not provided only existent resources will be inferred\n(added metadata like encoding, profile etc). if `pattern` is provided\nnew resoures with file names mathing the pattern will be added and inferred.\nit commits changes to data package instance.\n\n__arguments__\n- __pattern (str)__: glob pattern for new resources\n\n__returns__\n\n`dict`: returns data package descriptor\n\n\n\n#### `package.commit`\n```python\npackage.commit(strict=none)\n```\nupdate data package instance if there are in-place changes in the descriptor.\n\n__example__\n\n\n```python\npackage = package({\n    'name': 'package',\n    'resources': [{'name': 'resource', 'data': ['data']}]\n})\n\npackage.name # package\npackage.descriptor['name'] = 'renamed-package'\npackage.name # package\npackage.commit()\npackage.name # renamed-package\n```\n\n__arguments__\n- __strict (bool)__: alter `strict` mode for further work\n\n__raises__\n- `datapackageexception`: raises error if something goes wrong\n\n__returns__\n\n`bool`: returns true on success and false if not modified\n\n\n\n#### `package.save`\n```python\npackage.save(target=none,\n             storage=none,\n             merge_groups=false,\n             to_base_path=false,\n             **options)\n```\nsaves this data package\n\nit saves it to storage if `storage` argument is passed or\nsaves this data package's descriptor to json file if `target` arguments\nends with `.json` or saves this data package to zip file otherwise.\n\n__example__\n\n\nit creates a zip file into ``file_or_path`` with the contents\nof this data package and its resources. every resource which content\nlives in the local filesystem will be copied to the zip file.\nconsider the following data package descriptor:\n\n```json\n{\n    \"name\": \"gdp\",\n    \"resources\": [\n        {\"name\": \"local\", \"format\": \"csv\", \"path\": \"data.csv\"},\n        {\"name\": \"inline\", \"data\": [4, 8, 15, 16, 23, 42]},\n        {\"name\": \"remote\", \"url\": \"http://someplace.com/data.csv\"}\n    ]\n}\n```\n\nthe final structure of the zip file will be:\n\n```\n./datapackage.json\n./data/local.csv\n```\n\nwith the contents of `datapackage.json` being the same as\nreturned `datapackage.descriptor`. the resources' file names are generated\nbased on their `name` and `format` fields if they exist.\nif the resource has no `name`, it'll be used `resource-x`,\nwhere `x` is the index of the resource in the `resources` list (starting at zero).\nif the resource has `format`, it'll be lowercased and appended to the `name`,\nbecoming \"`name.format`\".\n\n__arguments__\n- __target (string/filelike)__:\n        the file path or a file-like object where\n        the contents of this data package will be saved into.\n- __storage (str/tableschema.storage)__:\n        storage name like `sql` or storage instance\n- __merge_groups (bool)__:\n        save all the group's tabular resoruces into one bucket\n        if a storage is provided (for example into one sql table).\n        read more about [group](#group).\n- __to_base_path (bool)__:\n        save the package to the package's base path\n        using the \"<base_path>/<target>\" route\n- __options (dict)__:\n        storage options to use for storage creation\n\n__raises__\n- `datapackageexception`: raises if there was some error writing the package\n\n__returns__\n\n`bool/storage`: on success return true or a `storage` instance\n\n### `resource`\n```python\nresource(self,\n         descriptor={},\n         base_path=none,\n         strict=false,\n         unsafe=false,\n         storage=none,\n         package=none,\n         **options)\n```\nresource represenation\n\n__arguments__\n- __descriptor (str/dict)__: data resource descriptor as local path, url or object\n- __base_path (str)__: base path for all relative paths\n- __strict (bool)__:\n        strict flag to alter validation behavior.  setting it to `true`\n        leads to throwing errors on any operation with invalid descriptor\n- __unsafe (bool)__:\n        if `true` unsafe paths will be allowed. for more inforamtion\n        https://specs.frictionlessdata.io/data-resource/#data-location.\n        default to `false`\n- __storage (str/tableschema.storage)__: storage name like `sql` or storage instance\n- __options (dict)__: storage options to use for storage creation\n\n__raises__\n- `datapackageexception`: raises error if something goes wrong\n\n\n\n#### `resource.data`\nreturn resource data\n\n\n#### `resource.descriptor`\npackage's descriptor\n\n__returns__\n\n`dict`: descriptor\n\n\n\n#### `resource.errors`\nvalidation errors\n\nalways empty in strict mode.\n\n__returns__\n\n`exception[]`: validation errors\n\n\n\n#### `resource.group`\ngroup name\n\n__returns__\n\n`str`: group name\n\n\n\n#### `resource.headers`\nresource's headers\n\n> only for tabular resources (reading has to be started first or it's `none`)\n\n__returns__\n\n`str[]/none`: returns data source headers\n\n\n\n#### `resource.inline`\nwhether resource inline\n\n__returns__\n\n`bool`: returns true if resource is inline\n\n\n\n#### `resource.local`\nwhether resource local\n\n__returns__\n\n`bool`: returns true if resource is local\n\n\n\n#### `resource.multipart`\nwhether resource multipart\n\n__returns__\n\n`bool`: returns true if resource is multipart\n\n\n\n#### `resource.name`\nresource name\n\n__returns__\n\n`str`: name\n\n\n\n#### `resource.package`\npackage instance if the resource belongs to some package\n\n__returns__\n\n`package/none`: a package instance if available\n\n\n\n#### `resource.profile`\nresource's profile\n\n__returns__\n\n`profile`: an instance of `profile` class\n\n\n\n#### `resource.remote`\nwhether resource remote\n\n__returns__\n\n`bool`: returns true if resource is remote\n\n\n\n#### `resource.schema`\nresource's schema\n\n> only for tabular resources\n\nfor tabular resources it returns `schema` instance to interact with data schema.\nread api documentation - [tableschema.schema](https://github.com/frictionlessdata/tableschema-py#schema).\n\n__returns__\n\n`tableschema.schema`: schema\n\n\n\n#### `resource.source`\nresource's source\n\ncombination of `resource.source` and `resource.inline/local/remote/multipart`\nprovides predictable interface to work with resource data.\n\n__returns__\n\n`list/str`: returns `data` or `path` property\n\n\n\n#### `resource.table`\nreturn resource table\n\n\n#### `resource.tabular`\nwhether resource tabular\n\n__returns__\n\n`bool`: returns true if resource is tabular\n\n\n\n#### `resource.valid`\nvalidation status\n\nalways true in strict mode.\n\n__returns__\n\n`bool`: validation status\n\n\n\n#### `resource.iter`\n```python\nresource.iter(integrity=false, relations=false, **options)\n```\niterates through the resource data and emits rows cast based on table schema.\n\n> only for tabular resources\n\n__arguments__\n\n\n    keyed (bool):\n        yield keyed rows in a form of `{header1: value1, header2: value2}`\n        (default is false; the form of rows is `[value1, value2]`)\n\n    extended (bool):\n        yield extended rows in a for of `[rownumber, [header1, header2], [value1, value2]]`\n        (default is false; the form of rows is `[value1, value2]`)\n\n    cast (bool):\n        disable data casting if false\n        (default is true)\n\n    integrity (bool):\n        if true actual size in bytes and sha256 hash of the file\n        will be checked against `descriptor.bytes` and `descriptor.hash`\n        (other hashing algorithms are not supported and will be skipped silently)\n\n    relations (bool):\n        if true foreign key fields will be checked and resolved to its references\n\n    foreign_keys_values (dict):\n        three-level dictionary of foreign key references optimized\n        to speed up validation process in a form of\n        `{resource1: {(fk_field1, fk_field2): {(value1, value2): {one_keyedrow}, ... }}}`.\n        if not provided but relations is true, it will be created\n        before the validation process by *index_foreign_keys_values* method\n\n    exc_handler (func):\n        optional custom exception handler callable.\n        can be used to defer raising errors (i.e. \"fail late\"), e.g.\n        for data validation purposes. must support the signature below\n\n__custom exception handler__\n\n\n```python\ndef exc_handler(exc, row_number=none, row_data=none, error_data=none):\n    '''custom exception handler (example)\n\n    # arguments:\n        exc(exception):\n            deferred exception instance\n        row_number(int):\n            data row number that triggers exception exc\n        row_data(ordereddict):\n            invalid data row source data\n        error_data(ordereddict):\n            data row source data field subset responsible for the error, if\n            applicable (e.g. invalid primary or foreign key fields). may be\n            identical to row_data.\n    '''\n    # ...\n```\n\n__raises__\n- `datapackageexception`: base class of any error\n- `casterror`: data cast error\n- `integrityerror`: integrity checking error\n- `uniquekeyerror`: unique key constraint violation\n- `unresolvedfkerror`: unresolved foreign key reference error\n\n__returns__\n\n`iterator[list]`: yields rows\n\n\n\n#### `resource.read`\n```python\nresource.read(integrity=false,\n              relations=false,\n              foreign_keys_values=false,\n              **options)\n```\nread the whole resource and return as array of rows\n\n> only for tabular resources\n> it has the same api as `resource.iter` except for\n\n__arguments__\n- __limit (int)__: limit count of rows to read and return\n\n__returns__\n\n`list[]`: returns rows\n\n\n\n#### `resource.check_integrity`\n```python\nresource.check_integrity()\n```\nchecks resource integrity\n\n> only for tabular resources\n\nit checks size in bytes and sha256 hash of the file\nagainst `descriptor.bytes` and `descriptor.hash`\n(other hashing algorithms are not supported and will be skipped silently).\n\n__raises__\n- `exceptions.integrityerror`: raises if there are integrity issues\n\n__returns__\n\n`bool`: returns true if no issues\n\n\n\n#### `resource.check_relations`\n```python\nresource.check_relations(foreign_keys_values=false)\n```\ncheck relations\n\n> only for tabular resources\n\nit checks foreign keys and raises an exception if there are integrity issues.\n\n__raises__\n- `exceptions.relationerror`: raises if there are relation issues\n\n__returns__\n\n`bool`: returns true if no issues\n\n\n\n#### `resource.drop_relations`\n```python\nresource.drop_relations()\n```\ndrop relations\n\n> only for tabular resources\n\nremove relations data from memory\n\n__returns__\n\n`bool`: returns true\n\n\n\n#### `resource.raw_iter`\n```python\nresource.raw_iter(stream=false)\n```\niterate over data chunks as bytes.\n\nif `stream` is true file-like object will be returned.\n\n__arguments__\n- __stream (bool)__: file-like object will be returned\n\n__returns__\n\n`bytes[]/filelike`: returns bytes[]/filelike\n\n\n\n#### `resource.raw_read`\n```python\nresource.raw_read()\n```\nreturns resource data as bytes.\n\n__returns__\n\n`bytes`: returns resource data in bytes\n\n\n\n#### `resource.infer`\n```python\nresource.infer(**options)\n```\ninfer resource metadata\n\nlike name, format, mediatype, encoding, schema and profile.\nit commits this changes into resource instance.\n\n__arguments__\n- __options__:\n        options will be passed to `tableschema.infer` call,\n        for more control on results (e.g. for setting `limit`, `confidence` etc.).\n\n__returns__\n\n`dict`: returns resource descriptor\n\n\n\n#### `resource.commit`\n```python\nresource.commit(strict=none)\n```\nupdate resource instance if there are in-place changes in the descriptor.\n\n__arguments__\n- __strict (bool)__: alter `strict` mode for further work\n\n__raises__\n- `datapackageexception`: raises error if something goes wrong\n\n__returns__\n\n`bool`: returns true on success and false if not modified\n\n\n\n#### `resource.save`\n```python\nresource.save(target, storage=none, to_base_path=false, **options)\n```\nsaves this resource\n\ninto storage if `storage` argument is passed or\nsaves this resource's descriptor to json file otherwise.\n\n__arguments__\n- __target (str)__:\n        path where to save a resource\n- __storage (str/tableschema.storage)__:\n        storage name like `sql` or storage instance\n- __to_base_path (bool)__:\n        save the resource to the resource's base path\n        using the \"<base_path>/<target>\" route\n- __options (dict)__:\n        storage options to use for storage creation\n\n__raises__\n- `datapackageexception`: raises error if something goes wrong\n\n__returns__\n\n`bool`: returns true on success\nbuilding index...\nstarted generating documentation...\n\n### `group`\n```python\ngroup(self, resources)\n```\ngroup representation\n\n__arguments__\n- __resource[]__: list of tabular resources\n\n\n\n#### `group.headers`\ngroup's headers\n\n__returns__\n\n`str[]/none`: returns headers\n\n\n\n#### `group.name`\ngroup name\n\n__returns__\n\n`str`: name\n\n\n\n#### `group.schema`\nresource's schema\n\n__returns__\n\n`tableschema.schema`: schema\n\n\n\n#### `group.iter`\n```python\ngroup.iter(**options)\n```\niterates through the group data and emits rows cast based on table schema.\n\n> it concatenates all the resources and has the same api as `resource.iter`\n\n\n\n#### `group.read`\n```python\ngroup.read(limit=none, **options)\n```\nread the whole group and return as array of rows\n\n> it concatenates all the resources and has the same api as `resource.read`\n\n\n\n#### `group.check_relations`\n```python\ngroup.check_relations()\n```\ncheck group's relations\n\nthe same as `resource.check_relations` but without the optional\nargument *foreign_keys_values*.  this method will test foreignkeys of the\nwhole group at once otpimizing the process by creating the foreign_key_values\nhashmap only once before testing the set of resources.\n\n\n### `profile`\n```python\nprofile(self, profile)\n```\nprofile representation\n\n__arguments__\n- __profile (str)__: profile name in registry or url to json schema\n\n__raises__\n- `datapackageexception`: raises error if something goes wrong\n\n\n\n#### `profile.jsonschema`\njsonschema content\n\n__returns__\n\n`dict`: returns profile's json schema contents\n\n\n\n#### `profile.name`\nprofile name\n\n__returns__\n\n`str/none`: name if available\n\n\n\n#### `profile.validate`\n```python\nprofile.validate(descriptor)\n```\nvalidate a data package `descriptor` against the profile.\n\n__arguments__\n- __descriptor (dict)__: retrieved and dereferenced data package descriptor\n\n__raises__\n- `validationerror`: raises if not valid\n__returns__\n\n`bool`: returns true if valid\n\n\n### `validate`\n```python\nvalidate(descriptor)\n```\nvalidate a data package descriptor.\n\n__arguments__\n- __descriptor (str/dict)__: package descriptor (one of):\n      - local path\n      - remote url\n      - object\n\n__raises__\n- `validationerror`: raises on invalid\n\n__returns__\n\n`bool`: returns true on valid\n\n\n### `infer`\n```python\ninfer(pattern, base_path=none)\n```\ninfer a data package descriptor.\n\n> argument `pattern` works only for local files\n\n__arguments__\n- __pattern (str)__: glob file pattern\n\n__returns__\n\n`dict`: returns data package descriptor\n\n\n### `datapackageexception`\n```python\ndatapackageexception(self, message, errors=[])\n```\nbase class for all datapackage/tableschema exceptions.\n\nif there are multiple errors, they can be read from the exception object:\n\n```python\ntry:\n    # lib action\nexcept datapackageexception as exception:\n    if exception.multiple:\n        for error in exception.errors:\n            # handle error\n```\n\n\n\n#### `datapackageexception.errors`\nlist of nested errors\n\n__returns__\n\n`datapackageexception[]`: list of nested errors\n\n\n\n#### `datapackageexception.multiple`\nwhether it's a nested exception\n\n__returns__\n\n`bool`: whether it's a nested exception\n\n\n\n### `tableschemaexception`\n```python\ntableschemaexception(self, message, errors=[])\n```\nbase class for all tableschema exceptions.\n\n\n### `loaderror`\n```python\nloaderror(self, message, errors=[])\n```\nall loading errors.\n\n\n### `casterror`\n```python\ncasterror(self, message, errors=[])\n```\nall value cast errors.\n\n\n### `integrityerror`\n```python\nintegrityerror(self, message, errors=[])\n```\nall integrity errors.\n\n\n### `relationerror`\n```python\nrelationerror(self, message, errors=[])\n```\nall relations errors.\n\n\n### `storageerror`\n```python\nstorageerror(self, message, errors=[])\n```\nall storage errors.\n\n\n## contributing\n\n> the project follows the [open knowledge international coding standards](https://github.com/okfn/coding-standards).\n\nrecommended way to get started is to create and activate a project virtual environment.\nto install package and development dependencies into active environment:\n\n```bash\n$ make install\n```\n\nto run tests with linting and coverage:\n\n```bash\n$ make test\n```\n\n## changelog\n\nhere described only breaking and the most important changes. the full changelog and documentation for all released versions could be found in nicely formatted [commit history](https://github.com/frictionlessdata/datapackage-py/commits/master).\n\n#### v1.15\n\n> warning: it can be breaking for some setups, please read the discussions below\n\n- fixed header management according to the specs:\n    - https://github.com/frictionlessdata/datapackage-py/pull/257\n    - https://github.com/frictionlessdata/datapackage-py/issues/256\n    - https://github.com/frictionlessdata/forum/issues/1\n\n#### v1.14\n\n- add experimental options for pick/skiping fileds/rows\n\n#### v1.13\n\n- add `unsafe` option to package and resource (#262)\n\n#### v1.12\n\n- use `chardet` for encoding deteciton by default. for `cchardet`: `pip install datapackage[cchardet]`\n\n#### v1.11\n\n- `resource/package.save` now accept a `to_base_path` argument (#254)\n- `package.save` now returns a `storage` instance if available\n\n#### v1.10\n\n- added an ability to check tabular resource's integrity\n\n#### v1.9\n\n- added `resource.package` property\n\n#### v1.8\n\n- added support for [groups of resources](#group)\n\n#### v1.7\n\n- added support for [compression of resources](https://frictionlessdata.io/specs/patterns/#compression-of-resources)\n\n#### v1.6\n\n- added support for custom request session\n\n#### v1.5\n\nupdated behaviour:\n- added support for python 3.7\n\n#### v1.4\n\nnew api added:\n- added `skip_rows` support to the resource descriptor\n\n#### v1.3\n\nnew api added:\n- property `package.base_path` is now publicly available\n\n#### v1.2\n\nupdated behaviour:\n- cli command `$ datapackage infer` now outputs only a json-formatted data package descriptor.\n\n#### v1.1\n\nnew api added:\n- added an integration between `package/resource` and the `tableschema.storage` - https://github.com/frictionlessdata/tableschema-py#storage. it allows to load and save data package from/to different storages like sql/bigquery/etc.\n\n",
  "docs_url": null,
  "keywords": "frictionless data,open data,json schema,table schema,data package,tabular data package",
  "license": "mit",
  "name": "datapackage",
  "package_url": "https://pypi.org/project/datapackage/",
  "project_url": "https://pypi.org/project/datapackage/",
  "project_urls": {
    "Homepage": "https://github.com/frictionlessdata/datapackage-py"
  },
  "release_url": "https://pypi.org/project/datapackage/1.15.2/",
  "requires_dist": [
    "six (>=1.10)",
    "click (>=6.7)",
    "chardet (>=3.0)",
    "requests (>=2.8)",
    "jsonschema (>=2.5)",
    "unicodecsv (>=0.14)",
    "jsonpointer (>=1.10)",
    "tableschema (>=1.12.1)",
    "tabulator (>=1.29)",
    "cchardet (>=2.0) ; extra == 'cchardet'",
    "mock ; extra == 'develop'",
    "pylama ; extra == 'develop'",
    "pytest ; extra == 'develop'",
    "pytest-cov ; extra == 'develop'",
    "httpretty ; extra == 'develop'",
    "tableschema-sql ; extra == 'develop'"
  ],
  "requires_python": "",
  "summary": "utilities to work with data packages as defined on specs.frictionlessdata.io",
  "version": "1.15.2",
  "releases": [],
  "developers": [
    "info@okfn.org",
    "open_knowledge_foundation"
  ],
  "kwds": "datapackage frictionlessdata datapackageexception row_data packages",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_datapackage",
  "homepage": "https://github.com/frictionlessdata/datapackage-py",
  "release_count": 92,
  "dependency_ids": [
    "pypi_cchardet",
    "pypi_chardet",
    "pypi_click",
    "pypi_httpretty",
    "pypi_jsonpointer",
    "pypi_jsonschema",
    "pypi_mock",
    "pypi_pylama",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_requests",
    "pypi_six",
    "pypi_tableschema",
    "pypi_tableschema_sql",
    "pypi_tabulator",
    "pypi_unicodecsv"
  ]
}