{
  "classifiers": [
    "license :: osi approved :: mit license",
    "operating system :: os independent",
    "programming language :: python :: 3"
  ],
  "description": "wrapper for the official [stable diffusion](https://github.com/stability-ai/stablediffusion) repository, to allow installing via `pip`. please see the installation section below for more details.\n\n# stable diffusion version 2\n![t2i](https://github.com/stability-ai/stablediffusion/raw/main/assets/stable-samples/txt2img/768/merged-0006.png)\n![t2i](https://github.com/stability-ai/stablediffusion/raw/main/assets/stable-samples/txt2img/768/merged-0002.png)\n![t2i](https://github.com/stability-ai/stablediffusion/raw/main/assets/stable-samples/txt2img/768/merged-0005.png)\n\nthis repository contains [stable diffusion](https://github.com/compvis/stable-diffusion) models trained from scratch and will be continuously updated with\nnew checkpoints. the following list provides an overview of all currently available models. more coming soon.\n\n# installation\n1. on windows and linux only: `pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu116`\n2. run `pip install stable-diffusion-sdkit`\n\nstep 1 is not necessary on mac.\n\nthis will install the `ldm` package, which contains the stable diffusion code.\n\n## news\n\n**december 7, 2022**\n\n*version 2.1*\n\n- new stable diffusion model (_stable diffusion 2.1-v_, [huggingface](https://huggingface.co/stabilityai/stable-diffusion-2-1)) at 768x768 resolution and (_stable diffusion 2.1-base_, [huggingface](https://huggingface.co/stabilityai/stable-diffusion-2-1-base)) at 512x512 resolution, both based on the same number of parameters and architecture as 2.0 and fine-tuned on 2.0, on a less restrictive nsfw filtering of the [laion-5b](https://laion.ai/blog/laion-5b/) dataset.\nper default, the attention operation of the model is evaluated at full precision when `xformers` is not installed. to enable fp16 (which can cause numerical instabilities with the vanilla attention module on the v2.1 model) , run your script with `attn_precision=fp16 python <thescript.py>`\n\n**november 24, 2022**\n\n*version 2.0*\n\n- new stable diffusion model (_stable diffusion 2.0-v_) at 768x768 resolution. same number of parameters in the u-net as 1.5, but uses [openclip-vit/h](https://github.com/mlfoundations/open_clip) as the text encoder and is trained from scratch. _sd 2.0-v_ is a so-called [v-prediction](https://arxiv.org/abs/2202.00512) model. \n- the above model is finetuned from _sd 2.0-base_, which was trained as a standard noise-prediction model on 512x512 images and is also made available.\n- added a [x4 upscaling latent text-guided diffusion model](#image-upscaling-with-stable-diffusion).\n- new [depth-guided stable diffusion model](#depth-conditional-stable-diffusion), finetuned from _sd 2.0-base_. the model is conditioned on monocular depth estimates inferred via [midas](https://github.com/isl-org/midas) and can be used for structure-preserving img2img and shape-conditional synthesis.\n\n  ![d2i](https://github.com/stability-ai/stablediffusion/raw/main/assets/stable-samples/depth2img/depth2img01.png)\n- a [text-guided inpainting model](#image-inpainting-with-stable-diffusion), finetuned from sd _2.0-base_.\n\nwe follow the [original repository](https://github.com/compvis/stable-diffusion) and provide basic inference scripts to sample from the models.\n\n________________\n*the original stable diffusion model was created in a collaboration with [compvis](https://arxiv.org/abs/2202.00512) and [runwayml](https://runwayml.com/) and builds upon the work:*\n\n[**high-resolution image synthesis with latent diffusion models**](https://ommer-lab.com/research/latent-diffusion-models/)<br/>\n[robin rombach](https://github.com/rromb)\\*,\n[andreas blattmann](https://github.com/ablattmann)\\*,\n[dominik lorenz](https://github.com/qp-qp)\\,\n[patrick esser](https://github.com/pesser),\n[bj\u00f6rn ommer](https://hci.iwr.uni-heidelberg.de/staff/bommer)<br/>\n_[cvpr '22 oral](https://openaccess.thecvf.com/content/cvpr2022/html/rombach_high-resolution_image_synthesis_with_latent_diffusion_models_cvpr_2022_paper.html) |\n[github](https://github.com/compvis/latent-diffusion) | [arxiv](https://arxiv.org/abs/2112.10752) | [project page](https://ommer-lab.com/research/latent-diffusion-models/)_\n\nand [many others](#shout-outs).\n\nstable diffusion is a latent text-to-image diffusion model.\n________________________________\n  \n## requirements\n\nyou can update an existing [latent diffusion](https://github.com/compvis/latent-diffusion) environment by running\n\n```\nconda install pytorch==1.12.1 torchvision==0.13.1 -c pytorch\npip install transformers==4.19.2 diffusers invisible-watermark\npip install -e .\n``` \n#### xformers efficient attention\nfor more efficiency and speed on gpus, \nwe highly recommended installing the [xformers](https://github.com/facebookresearch/xformers)\nlibrary.\n\ntested on a100 with cuda 11.4.\ninstallation needs a somewhat recent version of nvcc and gcc/g++, obtain those, e.g., via \n```commandline\nexport cuda_home=/usr/local/cuda-11.4\nconda install -c nvidia/label/cuda-11.4.0 cuda-nvcc\nconda install -c conda-forge gcc\nconda install -c conda-forge gxx_linux-64==9.5.0\n```\n\nthen, run the following (compiling takes up to 30 min).\n\n```commandline\ncd ..\ngit clone https://github.com/facebookresearch/xformers.git\ncd xformers\ngit submodule update --init --recursive\npip install -r requirements.txt\npip install -e .\ncd ../stablediffusion\n```\nupon successful installation, the code will automatically default to [memory efficient attention](https://github.com/facebookresearch/xformers)\nfor the self- and cross-attention layers in the u-net and autoencoder.\n\n## general disclaimer\nstable diffusion models are general text-to-image diffusion models and therefore mirror biases and (mis-)conceptions that are present\nin their training data. although efforts were made to reduce the inclusion of explicit pornographic material, **we do not recommend using the provided weights for services or products without additional safety mechanisms and considerations.\nthe weights are research artifacts and should be treated as such.**\ndetails on the training procedure and data, as well as the intended use of the model can be found in the corresponding [model card](https://huggingface.co/stabilityai/stable-diffusion-2).\nthe weights are available via [the stabilityai organization at hugging face](https://huggingface.co/stabilityai) under the [creativeml open rail++-m license](license-model). \n\n\n\n## stable diffusion v2\n\nstable diffusion v2 refers to a specific configuration of the model\narchitecture that uses a downsampling-factor 8 autoencoder with an 865m unet\nand openclip vit-h/14 text encoder for the diffusion model. the _sd 2-v_ model produces 768x768 px outputs. \n\nevaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 ddim sampling steps show the relative improvements of the checkpoints:\n\n![sd evaluation results](https://github.com/stability-ai/stablediffusion/raw/main/assets/model-variants.jpg)\n\n\n\n### text-to-image\n![txt2img-stable2](https://github.com/stability-ai/stablediffusion/raw/main/assets/stable-samples/txt2img/merged-0003.png)\n![txt2img-stable2](https://github.com/stability-ai/stablediffusion/raw/main/assets/stable-samples/txt2img/merged-0001.png)\n\nstable diffusion 2 is a latent diffusion model conditioned on the penultimate text embeddings of a clip vit-h/14 text encoder.\nwe provide a [reference script for sampling](#reference-sampling-script).\n#### reference sampling script\n\nthis script incorporates an [invisible watermarking](https://github.com/shieldmnt/invisible-watermark) of the outputs, to help viewers [identify the images as machine-generated](scripts/tests/test_watermark.py).\nwe provide the configs for the _sd2-v_ (768px) and _sd2-base_ (512px) model.\n\nfirst, download the weights for [_sd2.1-v_](https://huggingface.co/stabilityai/stable-diffusion-2-1) and [_sd2.1-base_](https://huggingface.co/stabilityai/stable-diffusion-2-1-base). \n\nto sample from the _sd2.1-v_ model, run the following:\n\n```\npython scripts/txt2img.py --prompt \"a professional photograph of an astronaut riding a horse\" --ckpt <path/to/768model.ckpt/> --config configs/stable-diffusion/v2-inference-v.yaml --h 768 --w 768  \n```\nor try out the web demo: [![hugging face spaces](https://img.shields.io/badge/%f0%9f%a4%97%20hugging%20face-spaces-blue)](https://huggingface.co/spaces/stabilityai/stable-diffusion).\n\nto sample from the base model, use\n```\npython scripts/txt2img.py --prompt \"a professional photograph of an astronaut riding a horse\" --ckpt <path/to/model.ckpt/> --config <path/to/config.yaml/>  \n```\n\nby default, this uses the [ddim sampler](https://arxiv.org/abs/2010.02502), and renders images of size 768x768 (which it was trained on) in 50 steps. \nempirically, the v-models can be sampled with higher guidance scales.\n\nnote: the inference config for all model versions is designed to be used with ema-only checkpoints. \nfor this reason `use_ema=false` is set in the configuration, otherwise the code will try to switch from\nnon-ema to ema weights. \n\n### image modification with stable diffusion\n\n![depth2img-stable2](https://github.com/stability-ai/stablediffusion/raw/main/assets/stable-samples/depth2img/merged-0000.png)\n#### depth-conditional stable diffusion\n\nto augment the well-established [img2img](https://github.com/compvis/stable-diffusion#image-modification-with-stable-diffusion) functionality of stable diffusion, we provide a _shape-preserving_ stable diffusion model.\n\n\nnote that the original method for image modification introduces significant semantic changes w.r.t. the initial image.\nif that is not desired, download our [depth-conditional stable diffusion](https://huggingface.co/stabilityai/stable-diffusion-2-depth) model and the `dpt_hybrid` midas [model weights](https://github.com/intel-isl/dpt/releases/download/1_0/dpt_hybrid-midas-501f0c75.pt), place the latter in a folder `midas_models` and sample via \n```\npython scripts/gradio/depth2img.py configs/stable-diffusion/v2-midas-inference.yaml <path-to-ckpt>\n```\n\nor\n\n```\nstreamlit run scripts/streamlit/depth2img.py configs/stable-diffusion/v2-midas-inference.yaml <path-to-ckpt>\n```\n\nthis method can be used on the samples of the base model itself.\nfor example, take [this sample](https://github.com/stability-ai/stablediffusion/raw/main/assets/stable-samples/depth2img/old_man.png) generated by an anonymous discord user.\nusing the [gradio](https://gradio.app) or [streamlit](https://streamlit.io/) script `depth2img.py`, the midas model first infers a monocular depth estimate given this input, \nand the diffusion model is then conditioned on the (relative) depth output.\n\n<p align=\"center\">\n<b> depth2image </b><br/>\n<img src=https://github.com/stability-ai/stablediffusion/raw/main/assets/stable-samples/depth2img/d2i.gif>\n</p>\n\nthis model is particularly useful for a photorealistic style; see the [examples](https://github.com/stability-ai/stablediffusion/raw/main/assets/stable-samples/depth2img).\nfor a maximum strength of 1.0, the model removes all pixel-based information and only relies on the text prompt and the inferred monocular depth estimate.\n\n![depth2img-stable3](https://github.com/stability-ai/stablediffusion/raw/main/assets/stable-samples/depth2img/merged-0005.png)\n\n#### classic img2img\n\nfor running the \"classic\" img2img, use\n```\npython scripts/img2img.py --prompt \"a fantasy landscape, trending on artstation\" --init-img <path-to-img.jpg> --strength 0.8 --ckpt <path/to/model.ckpt>\n```\nand adapt the checkpoint and config paths accordingly.\n\n### image upscaling with stable diffusion\n![upscaling-x4](https://github.com/stability-ai/stablediffusion/raw/main/assets/stable-samples/upscaling/merged-dog.png)\nafter [downloading the weights](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler), run\n```\npython scripts/gradio/superresolution.py configs/stable-diffusion/x4-upscaling.yaml <path-to-checkpoint>\n```\n\nor\n\n```\nstreamlit run scripts/streamlit/superresolution.py -- configs/stable-diffusion/x4-upscaling.yaml <path-to-checkpoint>\n```\n\nfor a gradio or streamlit demo of the text-guided x4 superresolution model.  \nthis model can be used both on real inputs and on synthesized examples. for the latter, we recommend setting a higher \n`noise_level`, e.g. `noise_level=100`.\n\n### image inpainting with stable diffusion\n\n![inpainting-stable2](https://github.com/stability-ai/stablediffusion/raw/main/assets/stable-inpainting/merged-leopards.png)\n\n[download the sd 2.0-inpainting checkpoint](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting) and run\n\n```\npython scripts/gradio/inpainting.py configs/stable-diffusion/v2-inpainting-inference.yaml <path-to-checkpoint>\n```\n\nor\n\n```\nstreamlit run scripts/streamlit/inpainting.py -- configs/stable-diffusion/v2-inpainting-inference.yaml <path-to-checkpoint>\n```\n\nfor a gradio or streamlit demo of the inpainting model. \nthis scripts adds invisible watermarking to the demo in the [runwayml](https://github.com/runwayml/stable-diffusion/blob/main/scripts/inpaint_st.py) repository, but both should work interchangeably with the checkpoints/configs.  \n\n\n\n## shout-outs\n- thanks to [hugging face](https://huggingface.co/) and in particular [apolin\u00e1rio](https://github.com/apolinario)  for support with our model releases!\n- stable diffusion would not be possible without [laion](https://laion.ai/) and their efforts to create open, large-scale datasets.\n- the [deepfloyd team](https://twitter.com/deepfloydai) at stability ai, for creating the subset of [laion-5b](https://laion.ai/blog/laion-5b/) dataset used to train the model.\n- stable diffusion 2.0 uses [openclip](https://laion.ai/blog/large-openclip/), trained by [romain beaumont](https://github.com/rom1504).  \n- our codebase for the diffusion models builds heavily on [openai's adm codebase](https://github.com/openai/guided-diffusion)\nand [https://github.com/lucidrains/denoising-diffusion-pytorch](https://github.com/lucidrains/denoising-diffusion-pytorch). \nthanks for open-sourcing!\n- [compvis](https://github.com/compvis/stable-diffusion) initial stable diffusion release\n- [patrick](https://github.com/pesser)'s [implementation](https://github.com/runwayml/stable-diffusion/blob/main/scripts/inpaint_st.py) of the streamlit demo for inpainting.\n- `img2img` is an application of [sdedit](https://arxiv.org/abs/2108.01073) by [chenlin meng](https://cs.stanford.edu/~chenlin/) from the [stanford ai lab](https://cs.stanford.edu/~ermon/website/). \n- [kat's implementation]((https://github.com/compvis/latent-diffusion/pull/51)) of the [plms](https://arxiv.org/abs/2202.09778) sampler, and [more](https://github.com/crowsonkb/k-diffusion).\n- [dpmsolver](https://arxiv.org/abs/2206.00927) [integration](https://github.com/compvis/stable-diffusion/pull/440) by [cheng lu](https://github.com/luchengthu).\n- facebook's [xformers](https://github.com/facebookresearch/xformers) for efficient attention computation.\n- [midas](https://github.com/isl-org/midas) for monocular depth estimation.\n\n\n## license\n\nthe code in this repository is released under the mit license.\n\nthe weights are available via [the stabilityai organization at hugging face](https://huggingface.co/stabilityai), and released under the [creativeml open rail++-m license](license-model) license.\n\n## bibtex\n\n```\n@misc{rombach2021highresolution,\n      title={high-resolution image synthesis with latent diffusion models}, \n      author={robin rombach and andreas blattmann and dominik lorenz and patrick esser and bj\u00f6rn ommer},\n      year={2021},\n      eprint={2112.10752},\n      archiveprefix={arxiv},\n      primaryclass={cs.cv}\n}\n```\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "stable-diffusion-sdkit",
  "package_url": "https://pypi.org/project/stable-diffusion-sdkit/",
  "project_url": "https://pypi.org/project/stable-diffusion-sdkit/",
  "project_urls": {
    "Bug Tracker": "https://github.com/Stability-AI/stablediffusion/issues",
    "Homepage": "https://github.com/Stability-AI/stablediffusion"
  },
  "release_url": "https://pypi.org/project/stable-diffusion-sdkit/2.1.5/",
  "requires_dist": [
    "albumentations ==1.3.0",
    "opencv-python ==4.6.0.66",
    "pytorch-lightning ==1.4.2",
    "omegaconf ==2.1.1",
    "test-tube >=0.7.5",
    "einops ==0.3.0",
    "transformers ==4.33.2",
    "kornia ==0.6",
    "open-clip-torch ==2.0.2",
    "torchmetrics ==0.6.0",
    "tqdm",
    "huggingface-hub >=0.10.0"
  ],
  "requires_python": ">=3.8",
  "summary": "high-resolution image synthesis with latent diffusion models. this is a wrapper around the original repo, to allow installing via pip.",
  "version": "2.1.5",
  "releases": [],
  "developers": [],
  "kwds": "resolution_image_synthesis_with_latent_diffusion_models_cvpr_2022_paper stable2 stablediffusion t2i pip",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_stable_diffusion_sdkit",
  "homepage": "",
  "release_count": 6,
  "dependency_ids": [
    "pypi_albumentations",
    "pypi_einops",
    "pypi_huggingface_hub",
    "pypi_kornia",
    "pypi_omegaconf",
    "pypi_open_clip_torch",
    "pypi_opencv_python",
    "pypi_pytorch_lightning",
    "pypi_test_tube",
    "pypi_torchmetrics",
    "pypi_tqdm",
    "pypi_transformers"
  ]
}