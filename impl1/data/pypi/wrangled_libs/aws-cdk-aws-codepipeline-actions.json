{
  "classifiers": [
    "development status :: 7 - inactive",
    "framework :: aws cdk",
    "framework :: aws cdk :: 1",
    "intended audience :: developers",
    "license :: osi approved",
    "operating system :: os independent",
    "programming language :: javascript",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "typing :: typed"
  ],
  "description": "# aws codepipeline actions\n\n<!--begin stability banner-->---\n\n\n![end-of-support](https://img.shields.io/badge/end--of--support-critical.svg?style=for-the-badge)\n\n> aws cdk v1 has reached end-of-support on 2023-06-01.\n> this package is no longer being updated, and users should migrate to aws cdk v2.\n>\n> for more information on how to migrate, see the [*migrating to aws cdk v2* guide](https://docs.aws.amazon.com/cdk/v2/guide/migrating-v2.html).\n\n---\n<!--end stability banner-->\n\nthis package contains actions that can be used in a codepipeline.\n\n```python\nimport aws_cdk.aws_codepipeline as codepipeline\nimport aws_cdk.aws_codepipeline_actions as codepipeline_actions\n```\n\n## sources\n\n### aws codecommit\n\nto use a codecommit repository in a codepipeline:\n\n```python\nrepo = codecommit.repository(self, \"repo\",\n    repository_name=\"myrepo\"\n)\n\npipeline = codepipeline.pipeline(self, \"mypipeline\",\n    pipeline_name=\"mypipeline\"\n)\nsource_output = codepipeline.artifact()\nsource_action = codepipeline_actions.codecommitsourceaction(\n    action_name=\"codecommit\",\n    repository=repo,\n    output=source_output\n)\npipeline.add_stage(\n    stage_name=\"source\",\n    actions=[source_action]\n)\n```\n\nif you want to use existing role which can be used by on commit event rule.\nyou can specify the role object in eventrole property.\n\n```python\n# repo: codecommit.repository\nevent_role = iam.role.from_role_arn(self, \"event-role\", \"rolearn\")\nsource_action = codepipeline_actions.codecommitsourceaction(\n    action_name=\"codecommit\",\n    repository=repo,\n    output=codepipeline.artifact(),\n    event_role=event_role\n)\n```\n\nif you want to clone the entire codecommit repository (only available for codebuild actions),\nyou can set the `codebuildcloneoutput` property to `true`:\n\n```python\n# project: codebuild.pipelineproject\n# repo: codecommit.repository\n\nsource_output = codepipeline.artifact()\nsource_action = codepipeline_actions.codecommitsourceaction(\n    action_name=\"codecommit\",\n    repository=repo,\n    output=source_output,\n    code_build_clone_output=true\n)\n\nbuild_action = codepipeline_actions.codebuildaction(\n    action_name=\"codebuild\",\n    project=project,\n    input=source_output,  # the build action must use the codecommitsourceaction output as input.\n    outputs=[codepipeline.artifact()]\n)\n```\n\nthe codecommit source action emits variables:\n\n```python\n# project: codebuild.pipelineproject\n# repo: codecommit.repository\n\nsource_output = codepipeline.artifact()\nsource_action = codepipeline_actions.codecommitsourceaction(\n    action_name=\"codecommit\",\n    repository=repo,\n    output=source_output,\n    variables_namespace=\"mynamespace\"\n)\n\n# later:\n\ncodepipeline_actions.codebuildaction(\n    action_name=\"codebuild\",\n    project=project,\n    input=source_output,\n    environment_variables={\n        \"commit_id\": codebuild.buildenvironmentvariable(\n            value=source_action.variables.commit_id\n        )\n    }\n)\n```\n\n### github\n\nif you want to use a github repository as the source, you must create:\n\n* a [github access token](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line),\n  with scopes **repo** and **admin:repo_hook**.\n* a [secrets manager secret](https://docs.aws.amazon.com/secretsmanager/latest/userguide/manage_create-basic-secret.html)\n  with the value of the **github access token**. pick whatever name you want (for example `my-github-token`).\n  this token can be stored either as plaintext or as a secret key/value.\n  if you stored the token as plaintext,\n  set `secretvalue.secretsmanager('my-github-token')` as the value of `oauthtoken`.\n  if you stored it as a secret key/value,\n  you must set `secretvalue.secretsmanager('my-github-token', { jsonfield : 'my-github-token' })` as the value of `oauthtoken`.\n\nto use github as the source of a codepipeline:\n\n```python\n# read the secret from secrets manager\npipeline = codepipeline.pipeline(self, \"mypipeline\")\nsource_output = codepipeline.artifact()\nsource_action = codepipeline_actions.githubsourceaction(\n    action_name=\"github_source\",\n    owner=\"awslabs\",\n    repo=\"aws-cdk\",\n    oauth_token=secretvalue.secrets_manager(\"my-github-token\"),\n    output=source_output,\n    branch=\"develop\"\n)\npipeline.add_stage(\n    stage_name=\"source\",\n    actions=[source_action]\n)\n```\n\nthe github source action emits variables:\n\n```python\n# source_output: codepipeline.artifact\n# project: codebuild.pipelineproject\n\n\nsource_action = codepipeline_actions.githubsourceaction(\n    action_name=\"github_source\",\n    output=source_output,\n    owner=\"my-owner\",\n    repo=\"my-repo\",\n    oauth_token=secretvalue.secrets_manager(\"my-github-token\"),\n    variables_namespace=\"mynamespace\"\n)\n\n# later:\n\ncodepipeline_actions.codebuildaction(\n    action_name=\"codebuild\",\n    project=project,\n    input=source_output,\n    environment_variables={\n        \"commit_url\": codebuild.buildenvironmentvariable(\n            value=source_action.variables.commit_url\n        )\n    }\n)\n```\n\n### bitbucket\n\ncodepipeline can use a bitbucket git repository as a source:\n\n**note**: you have to manually connect codepipeline through the aws console with your bitbucket account.\nthis is a one-time operation for a given aws account in a given region.\nthe simplest way to do that is to either start creating a new codepipeline,\nor edit an existing one, while being logged in to bitbucket.\nchoose bitbucket as the source,\nand grant codepipeline permissions to your bitbucket account.\ncopy & paste the connection arn that you get in the console,\nor use the [`codestar-connections list-connections` aws cli operation](https://docs.aws.amazon.com/cli/latest/reference/codestar-connections/list-connections.html)\nto find it.\nafter that, you can safely abort creating or editing the pipeline -\nthe connection has already been created.\n\n```python\nsource_output = codepipeline.artifact()\nsource_action = codepipeline_actions.codestarconnectionssourceaction(\n    action_name=\"bitbucket_source\",\n    owner=\"aws\",\n    repo=\"aws-cdk\",\n    output=source_output,\n    connection_arn=\"arn:aws:codestar-connections:us-east-1:123456789012:connection/12345678-abcd-12ab-34cdef5678gh\"\n)\n```\n\nyou can also use the `codestarconnectionssourceaction` to connect to github, in the same way\n(you just have to select github as the source when creating the connection in the console).\n\nsimilarly to `githubsourceaction`, `codestarconnectionssourceaction` also emits the variables:\n\n```python\n# project: codebuild.project\n\n\nsource_output = codepipeline.artifact()\nsource_action = codepipeline_actions.codestarconnectionssourceaction(\n    action_name=\"bitbucket_source\",\n    owner=\"aws\",\n    repo=\"aws-cdk\",\n    output=source_output,\n    connection_arn=\"arn:aws:codestar-connections:us-east-1:123456789012:connection/12345678-abcd-12ab-34cdef5678gh\",\n    variables_namespace=\"somespace\"\n)\n\n# later:\n\ncodepipeline_actions.codebuildaction(\n    action_name=\"codebuild\",\n    project=project,\n    input=source_output,\n    environment_variables={\n        \"commit_id\": codebuild.buildenvironmentvariable(\n            value=source_action.variables.commit_id\n        )\n    }\n)\n```\n\n### aws s3 source\n\nto use an s3 bucket as a source in codepipeline:\n\n```python\nsource_bucket = s3.bucket(self, \"mybucket\",\n    versioned=true\n)\n\npipeline = codepipeline.pipeline(self, \"mypipeline\")\nsource_output = codepipeline.artifact()\nsource_action = codepipeline_actions.s3sourceaction(\n    action_name=\"s3source\",\n    bucket=source_bucket,\n    bucket_key=\"path/to/file.zip\",\n    output=source_output\n)\npipeline.add_stage(\n    stage_name=\"source\",\n    actions=[source_action]\n)\n```\n\nthe region of the action will be determined by the region the bucket itself is in.\nwhen using a newly created bucket,\nthat region will be taken from the stack the bucket belongs to;\nfor an imported bucket,\nyou can specify the region explicitly:\n\n```python\nsource_bucket = s3.bucket.from_bucket_attributes(self, \"sourcebucket\",\n    bucket_name=\"my-bucket\",\n    region=\"ap-southeast-1\"\n)\n```\n\nby default, the pipeline will poll the bucket to detect changes.\nyou can change that behavior to use cloudwatch events by setting the `trigger`\nproperty to `s3trigger.events` (it's `s3trigger.poll` by default).\nif you do that, make sure the source bucket is part of an aws cloudtrail trail -\notherwise, the cloudwatch events will not be emitted,\nand your pipeline will not react to changes in the bucket.\nyou can do it through the cdk:\n\n```python\nimport aws_cdk.aws_cloudtrail as cloudtrail\n\n# source_bucket: s3.bucket\n\nsource_output = codepipeline.artifact()\nkey = \"some/key.zip\"\ntrail = cloudtrail.trail(self, \"cloudtrail\")\ntrail.add_s3_event_selector([cloudtrail.s3eventselector(\n    bucket=source_bucket,\n    object_prefix=key\n)],\n    read_write_type=cloudtrail.readwritetype.write_only\n)\nsource_action = codepipeline_actions.s3sourceaction(\n    action_name=\"s3source\",\n    bucket_key=key,\n    bucket=source_bucket,\n    output=source_output,\n    trigger=codepipeline_actions.s3trigger.events\n)\n```\n\nthe s3 source action emits variables:\n\n```python\n# source_bucket: s3.bucket\n\n# later:\n# project: codebuild.pipelineproject\nkey = \"some/key.zip\"\nsource_output = codepipeline.artifact()\nsource_action = codepipeline_actions.s3sourceaction(\n    action_name=\"s3source\",\n    bucket_key=key,\n    bucket=source_bucket,\n    output=source_output,\n    variables_namespace=\"mynamespace\"\n)\ncodepipeline_actions.codebuildaction(\n    action_name=\"codebuild\",\n    project=project,\n    input=source_output,\n    environment_variables={\n        \"version_id\": codebuild.buildenvironmentvariable(\n            value=source_action.variables.version_id\n        )\n    }\n)\n```\n\n### aws ecr\n\nto use an ecr repository as a source in a pipeline:\n\n```python\nimport aws_cdk.aws_ecr as ecr\n\n# ecr_repository: ecr.repository\n\npipeline = codepipeline.pipeline(self, \"mypipeline\")\nsource_output = codepipeline.artifact()\nsource_action = codepipeline_actions.ecrsourceaction(\n    action_name=\"ecr\",\n    repository=ecr_repository,\n    image_tag=\"some-tag\",  # optional, default: 'latest'\n    output=source_output\n)\npipeline.add_stage(\n    stage_name=\"source\",\n    actions=[source_action]\n)\n```\n\nthe ecr source action emits variables:\n\n```python\nimport aws_cdk.aws_ecr as ecr\n# ecr_repository: ecr.repository\n\n# later:\n# project: codebuild.pipelineproject\n\n\nsource_output = codepipeline.artifact()\nsource_action = codepipeline_actions.ecrsourceaction(\n    action_name=\"source\",\n    output=source_output,\n    repository=ecr_repository,\n    variables_namespace=\"mynamespace\"\n)\ncodepipeline_actions.codebuildaction(\n    action_name=\"codebuild\",\n    project=project,\n    input=source_output,\n    environment_variables={\n        \"image_uri\": codebuild.buildenvironmentvariable(\n            value=source_action.variables.image_uri\n        )\n    }\n)\n```\n\n## build & test\n\n### aws codebuild\n\nexample of a codebuild project used in a pipeline, alongside codecommit:\n\n```python\n# project: codebuild.pipelineproject\n\nrepository = codecommit.repository(self, \"myrepository\",\n    repository_name=\"myrepository\"\n)\nproject = codebuild.pipelineproject(self, \"myproject\")\n\nsource_output = codepipeline.artifact()\nsource_action = codepipeline_actions.codecommitsourceaction(\n    action_name=\"codecommit\",\n    repository=repository,\n    output=source_output\n)\nbuild_action = codepipeline_actions.codebuildaction(\n    action_name=\"codebuild\",\n    project=project,\n    input=source_output,\n    outputs=[codepipeline.artifact()],  # optional\n    execute_batch_build=true,  # optional, defaults to false\n    combine_batch_build_artifacts=true\n)\n\ncodepipeline.pipeline(self, \"mypipeline\",\n    stages=[codepipeline.stageprops(\n        stage_name=\"source\",\n        actions=[source_action]\n    ), codepipeline.stageprops(\n        stage_name=\"build\",\n        actions=[build_action]\n    )\n    ]\n)\n```\n\nthe default category of the codebuild action is `build`;\nif you want a `test` action instead,\noverride the `type` property:\n\n```python\n# project: codebuild.pipelineproject\n\nsource_output = codepipeline.artifact()\ntest_action = codepipeline_actions.codebuildaction(\n    action_name=\"integrationtest\",\n    project=project,\n    input=source_output,\n    type=codepipeline_actions.codebuildactiontype.test\n)\n```\n\n#### multiple inputs and outputs\n\nwhen you want to have multiple inputs and/or outputs for a project used in a\npipeline, instead of using the `secondarysources` and `secondaryartifacts`\nproperties of the `project` class, you need to use the `extrainputs` and\n`outputs` properties of the codebuild codepipeline\nactions. example:\n\n```python\n# repository1: codecommit.repository\n# repository2: codecommit.repository\n\n# project: codebuild.pipelineproject\n\nsource_output1 = codepipeline.artifact()\nsource_action1 = codepipeline_actions.codecommitsourceaction(\n    action_name=\"source1\",\n    repository=repository1,\n    output=source_output1\n)\nsource_output2 = codepipeline.artifact(\"source2\")\nsource_action2 = codepipeline_actions.codecommitsourceaction(\n    action_name=\"source2\",\n    repository=repository2,\n    output=source_output2\n)\nbuild_action = codepipeline_actions.codebuildaction(\n    action_name=\"build\",\n    project=project,\n    input=source_output1,\n    extra_inputs=[source_output2\n    ],\n    outputs=[\n        codepipeline.artifact(\"artifact1\"),  # for better buildspec readability - see below\n        codepipeline.artifact(\"artifact2\")\n    ]\n)\n```\n\n**note**: when a codebuild action in a pipeline has more than one output, it\nonly uses the `secondary-artifacts` field of the buildspec, never the\nprimary output specification directly under `artifacts`. because of that, it\npays to explicitly name all output artifacts of that action, like we did\nabove, so that you know what name to use in the buildspec.\n\nexample buildspec for the above project:\n\n```python\nproject = codebuild.pipelineproject(self, \"myproject\",\n    build_spec=codebuild.buildspec.from_object({\n        \"version\": \"0.2\",\n        \"phases\": {\n            \"build\": {\n                \"commands\": []\n            }\n        },\n        \"artifacts\": {\n            \"secondary-artifacts\": {\n                \"artifact1\": {},\n                \"artifact2\": {}\n            }\n        }\n    })\n)\n```\n\n#### variables\n\nthe codebuild action emits variables.\nunlike many other actions, the variables are not static,\nbut dynamic, defined in the buildspec,\nin the 'exported-variables' subsection of the 'env' section.\nexample:\n\n```python\n# later:\n# project: codebuild.pipelineproject\nsource_output = codepipeline.artifact()\nbuild_action = codepipeline_actions.codebuildaction(\n    action_name=\"build1\",\n    input=source_output,\n    project=codebuild.pipelineproject(self, \"project\",\n        build_spec=codebuild.buildspec.from_object({\n            \"version\": \"0.2\",\n            \"env\": {\n                \"exported-variables\": [\"my_var\"\n                ]\n            },\n            \"phases\": {\n                \"build\": {\n                    \"commands\": \"export my_var=\\\"some value\\\"\"\n                }\n            }\n        })\n    ),\n    variables_namespace=\"mynamespace\"\n)\ncodepipeline_actions.codebuildaction(\n    action_name=\"codebuild\",\n    project=project,\n    input=source_output,\n    environment_variables={\n        \"myvar\": codebuild.buildenvironmentvariable(\n            value=build_action.variable(\"my_var\")\n        )\n    }\n)\n```\n\n### jenkins\n\nin order to use jenkins actions in the pipeline,\nyou first need to create a `jenkinsprovider`:\n\n```python\njenkins_provider = codepipeline_actions.jenkinsprovider(self, \"jenkinsprovider\",\n    provider_name=\"myjenkinsprovider\",\n    server_url=\"http://my-jenkins.com:8080\",\n    version=\"2\"\n)\n```\n\nif you've registered a jenkins provider in a different cdk app,\nor outside the cdk (in the codepipeline aws console, for example),\nyou can import it:\n\n```python\njenkins_provider = codepipeline_actions.jenkinsprovider.from_jenkins_provider_attributes(self, \"jenkinsprovider\",\n    provider_name=\"myjenkinsprovider\",\n    server_url=\"http://my-jenkins.com:8080\",\n    version=\"2\"\n)\n```\n\nnote that a jenkins provider\n(identified by the provider name-category(build/test)-version tuple)\nmust always be registered in the given account, in the given aws region,\nbefore it can be used in codepipeline.\n\nwith a `jenkinsprovider`,\nwe can create a jenkins action:\n\n```python\n# jenkins_provider: codepipeline_actions.jenkinsprovider\n\nbuild_action = codepipeline_actions.jenkinsaction(\n    action_name=\"jenkinsbuild\",\n    jenkins_provider=jenkins_provider,\n    project_name=\"myproject\",\n    type=codepipeline_actions.jenkinsactiontype.build\n)\n```\n\n## deploy\n\n### aws cloudformation\n\nthis module contains actions that allows you to deploy to cloudformation from aws codepipeline.\n\nfor example, the following code fragment defines a pipeline that automatically deploys a cloudformation template\ndirectly from a codecommit repository, with a manual approval step in between to confirm the changes:\n\n```python\n# source stage: read from repository\nrepo = codecommit.repository(stack, \"templaterepo\",\n    repository_name=\"template-repo\"\n)\nsource_output = codepipeline.artifact(\"sourceartifact\")\nsource = cpactions.codecommitsourceaction(\n    action_name=\"source\",\n    repository=repo,\n    output=source_output,\n    trigger=cpactions.codecommittrigger.poll\n)\nsource_stage = {\n    \"stage_name\": \"source\",\n    \"actions\": [source]\n}\n\n# deployment stage: create and deploy changeset with manual approval\nstack_name = \"ourstack\"\nchange_set_name = \"stagedchangeset\"\n\nprod_stage = {\n    \"stage_name\": \"deploy\",\n    \"actions\": [\n        cpactions.cloudformationcreatereplacechangesetaction(\n            action_name=\"preparechanges\",\n            stack_name=stack_name,\n            change_set_name=change_set_name,\n            admin_permissions=true,\n            template_path=source_output.at_path(\"template.yaml\"),\n            run_order=1\n        ),\n        cpactions.manualapprovalaction(\n            action_name=\"approvechanges\",\n            run_order=2\n        ),\n        cpactions.cloudformationexecutechangesetaction(\n            action_name=\"executechanges\",\n            stack_name=stack_name,\n            change_set_name=change_set_name,\n            run_order=3\n        )\n    ]\n}\n\ncodepipeline.pipeline(stack, \"pipeline\",\n    stages=[source_stage, prod_stage\n    ]\n)\n```\n\nsee [the aws documentation](https://docs.aws.amazon.com/awscloudformation/latest/userguide/continuous-delivery-codepipeline.html)\nfor more details about using cloudformation in codepipeline.\n\n#### actions for updating individual cloudformation stacks\n\nthis package contains the following cloudformation actions:\n\n* **cloudformationcreateupdatestackaction** - deploy a cloudformation template directly from the pipeline. the indicated stack is created,\n  or updated if it already exists. if the stack is in a failure state, deployment will fail (unless `replaceonfailure`\n  is set to `true`, in which case it will be destroyed and recreated).\n* **cloudformationdeletestackaction** - delete the stack with the given name.\n* **cloudformationcreatereplacechangesetaction** - prepare a change set to be applied later. you will typically use change sets if you want\n  to manually verify the changes that are being staged, or if you want to separate the people (or system) preparing the\n  changes from the people (or system) applying the changes.\n* **cloudformationexecutechangesetaction** - execute a change set prepared previously.\n\n#### actions for deploying cloudformation stacksets to multiple accounts\n\nyou can use cloudformation stacksets to deploy the same cloudformation template to multiple\naccounts in a managed way. if you use aws organizations, stacksets can be deployed to\nall accounts in a particular organizational unit (ou), and even automatically to new\naccounts as soon as they are added to a particular ou. for more information, see\nthe [working with stacksets](https://docs.aws.amazon.com/awscloudformation/latest/userguide/what-is-cfnstacksets.html)\nsection of the cloudformation developer guide.\n\nthe actions available for updating stacksets are:\n\n* **cloudformationdeploystacksetaction** - create or update a cloudformation stackset directly from the pipeline, optionally\n  immediately create and update stack instances as well.\n* **cloudformationdeploystackinstancesaction** - update outdated stack instaces using the current version of the stackset.\n\nhere's an example of using both of these actions:\n\n```python\n# pipeline: codepipeline.pipeline\n# source_output: codepipeline.artifact\n\n\npipeline.add_stage(\n    stage_name=\"deploystacksets\",\n    actions=[\n        # first, update the stackset itself with the newest template\n        codepipeline_actions.cloudformationdeploystacksetaction(\n            action_name=\"updatestackset\",\n            run_order=1,\n            stack_set_name=\"mystackset\",\n            template=codepipeline_actions.stacksettemplate.from_artifact_path(source_output.at_path(\"template.yaml\")),\n\n            # change this to 'stacksetdeploymentmodel.organizations()' if you want to deploy to ous\n            deployment_model=codepipeline_actions.stacksetdeploymentmodel.self_managed(),\n            # this deploys to a set of accounts\n            stack_instances=codepipeline_actions.stackinstances.in_accounts([\"111111111111\"], [\"us-east-1\", \"eu-west-1\"])\n        ),\n\n        # afterwards, update/create additional instances in other accounts\n        codepipeline_actions.cloudformationdeploystackinstancesaction(\n            action_name=\"addmoreinstances\",\n            run_order=2,\n            stack_set_name=\"mystackset\",\n            stack_instances=codepipeline_actions.stackinstances.in_accounts([\"222222222222\", \"333333333333\"], [\"us-east-1\", \"eu-west-1\"])\n        )\n    ]\n)\n```\n\n#### lambda deployed through codepipeline\n\nif you want to deploy your lambda through codepipeline,\nand you don't use assets (for example, because your cdk code and lambda code are separate),\nyou can use a special lambda `code` class, `cfnparameterscode`.\nnote that your lambda must be in a different stack than your pipeline.\nthe lambda itself will be deployed, alongside the entire stack it belongs to,\nusing a cloudformation codepipeline action. example:\n\n```python\nlambda_stack = cdk.stack(app, \"lambdastack\")\nlambda_code = lambda_.code.from_cfn_parameters()\nlambda_.function(lambda_stack, \"lambda\",\n    code=lambda_code,\n    handler=\"index.handler\",\n    runtime=lambda_.runtime.nodejs_14_x\n)\n# other resources that your lambda needs, added to the lambdastack...\n\npipeline_stack = cdk.stack(app, \"pipelinestack\")\npipeline = codepipeline.pipeline(pipeline_stack, \"pipeline\")\n\n# add the source code repository containing this code to your pipeline,\n# and the source code of the lambda function, if they're separate\ncdk_source_output = codepipeline.artifact()\ncdk_source_action = codepipeline_actions.codecommitsourceaction(\n    repository=codecommit.repository(pipeline_stack, \"cdkcoderepo\",\n        repository_name=\"cdkcoderepo\"\n    ),\n    action_name=\"cdkcode_source\",\n    output=cdk_source_output\n)\nlambda_source_output = codepipeline.artifact()\nlambda_source_action = codepipeline_actions.codecommitsourceaction(\n    repository=codecommit.repository(pipeline_stack, \"lambdacoderepo\",\n        repository_name=\"lambdacoderepo\"\n    ),\n    action_name=\"lambdacode_source\",\n    output=lambda_source_output\n)\npipeline.add_stage(\n    stage_name=\"source\",\n    actions=[cdk_source_action, lambda_source_action]\n)\n\n# synthesize the lambda cdk template, using codebuild\n# the below values are just examples, assuming your cdk code is in typescript/javascript -\n# adjust the build environment and/or commands accordingly\ncdk_build_project = codebuild.project(pipeline_stack, \"cdkbuildproject\",\n    environment=codebuild.buildenvironment(\n        build_image=codebuild.linuxbuildimage.ubuntu_14_04_nodejs_10_1_0\n    ),\n    build_spec=codebuild.buildspec.from_object({\n        \"version\": \"0.2\",\n        \"phases\": {\n            \"install\": {\n                \"commands\": \"npm install\"\n            },\n            \"build\": {\n                \"commands\": [\"npm run build\", \"npm run cdk synth lambdastack -- -o .\"\n                ]\n            }\n        },\n        \"artifacts\": {\n            \"files\": \"lambdastack.template.yaml\"\n        }\n    })\n)\ncdk_build_output = codepipeline.artifact()\ncdk_build_action = codepipeline_actions.codebuildaction(\n    action_name=\"cdk_build\",\n    project=cdk_build_project,\n    input=cdk_source_output,\n    outputs=[cdk_build_output]\n)\n\n# build your lambda code, using codebuild\n# again, this example assumes your lambda is written in typescript/javascript -\n# make sure to adjust the build environment and/or commands if they don't match your specific situation\nlambda_build_project = codebuild.project(pipeline_stack, \"lambdabuildproject\",\n    environment=codebuild.buildenvironment(\n        build_image=codebuild.linuxbuildimage.ubuntu_14_04_nodejs_10_1_0\n    ),\n    build_spec=codebuild.buildspec.from_object({\n        \"version\": \"0.2\",\n        \"phases\": {\n            \"install\": {\n                \"commands\": \"npm install\"\n            },\n            \"build\": {\n                \"commands\": \"npm run build\"\n            }\n        },\n        \"artifacts\": {\n            \"files\": [\"index.js\", \"node_modules/**/*\"\n            ]\n        }\n    })\n)\nlambda_build_output = codepipeline.artifact()\nlambda_build_action = codepipeline_actions.codebuildaction(\n    action_name=\"lambda_build\",\n    project=lambda_build_project,\n    input=lambda_source_output,\n    outputs=[lambda_build_output]\n)\n\npipeline.add_stage(\n    stage_name=\"build\",\n    actions=[cdk_build_action, lambda_build_action]\n)\n\n# finally, deploy your lambda stack\npipeline.add_stage(\n    stage_name=\"deploy\",\n    actions=[\n        codepipeline_actions.cloudformationcreateupdatestackaction(\n            action_name=\"lambda_cfn_deploy\",\n            template_path=cdk_build_output.at_path(\"lambdastack.template.yaml\"),\n            stack_name=\"lambdastackdeployedname\",\n            admin_permissions=true,\n            parameter_overrides=lambda_code.assign(lambda_build_output.s3_location),\n            extra_inputs=[lambda_build_output\n            ]\n        )\n    ]\n)\n```\n\n#### cross-account actions\n\nif you want to update stacks in a different account,\npass the `account` property when creating the action:\n\n```python\nsource_output = codepipeline.artifact()\ncodepipeline_actions.cloudformationcreateupdatestackaction(\n    action_name=\"cloudformationcreateupdate\",\n    stack_name=\"mystackname\",\n    admin_permissions=true,\n    template_path=source_output.at_path(\"template.yaml\"),\n    account=\"123456789012\"\n)\n```\n\nthis will create a new stack, called `<pipelinestackname>-support-123456789012`, in your `app`,\nthat will contain the role that the pipeline will assume in account 123456789012 before executing this action.\nthis support stack will automatically be deployed before the stack containing the pipeline.\n\nyou can also pass a role explicitly when creating the action -\nin that case, the `account` property is ignored,\nand the action will operate in the same account the role belongs to:\n\n```python\nfrom aws_cdk.core import physicalname\n\n# in stack for account 123456789012...\n# other_account_stack: stack\n\naction_role = iam.role(other_account_stack, \"actionrole\",\n    assumed_by=iam.accountprincipal(\"123456789012\"),\n    # the role has to have a physical name set\n    role_name=physicalname.generate_if_needed\n)\n\n# in the pipeline stack...\nsource_output = codepipeline.artifact()\ncodepipeline_actions.cloudformationcreateupdatestackaction(\n    action_name=\"cloudformationcreateupdate\",\n    stack_name=\"mystackname\",\n    admin_permissions=true,\n    template_path=source_output.at_path(\"template.yaml\"),\n    role=action_role\n)\n```\n\n### aws codedeploy\n\n#### server deployments\n\nto use codedeploy for ec2/on-premise deployments in a pipeline:\n\n```python\n# deployment_group: codedeploy.serverdeploymentgroup\npipeline = codepipeline.pipeline(self, \"mypipeline\",\n    pipeline_name=\"mypipeline\"\n)\n\n# add the source and build stages to the pipeline...\nbuild_output = codepipeline.artifact()\ndeploy_action = codepipeline_actions.codedeployserverdeployaction(\n    action_name=\"codedeploy\",\n    input=build_output,\n    deployment_group=deployment_group\n)\npipeline.add_stage(\n    stage_name=\"deploy\",\n    actions=[deploy_action]\n)\n```\n\n##### lambda deployments\n\nto use codedeploy for blue-green lambda deployments in a pipeline:\n\n```python\nlambda_code = lambda_.code.from_cfn_parameters()\nfunc = lambda_.function(self, \"lambda\",\n    code=lambda_code,\n    handler=\"index.handler\",\n    runtime=lambda_.runtime.nodejs_14_x\n)\n# used to make sure each cdk synthesis produces a different version\nversion = func.current_version\nalias = lambda_.alias(self, \"lambdaalias\",\n    alias_name=\"prod\",\n    version=version\n)\n\ncodedeploy.lambdadeploymentgroup(self, \"deploymentgroup\",\n    alias=alias,\n    deployment_config=codedeploy.lambdadeploymentconfig.linear_10percent_every_1minute\n)\n```\n\nthen, you need to create your pipeline stack,\nwhere you will define your pipeline,\nand deploy the `lambdastack` using a cloudformation codepipeline action\n(see above for a complete example).\n\n### ecs\n\ncodepipeline can deploy an ecs service.\nthe deploy action receives one input artifact which contains the [image definition file](https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-create.html#pipelines-create-image-definitions):\n\n```python\nimport aws_cdk.aws_ecs as ecs\n\n# service: ecs.fargateservice\n\npipeline = codepipeline.pipeline(self, \"mypipeline\")\nbuild_output = codepipeline.artifact()\ndeploy_stage = pipeline.add_stage(\n    stage_name=\"deploy\",\n    actions=[\n        codepipeline_actions.ecsdeployaction(\n            action_name=\"deployaction\",\n            service=service,\n            # if your file is called imagedefinitions.json,\n            # use the `input` property,\n            # and leave out the `imagefile` property\n            input=build_output,\n            # if your file name is _not_ imagedefinitions.json,\n            # use the `imagefile` property,\n            # and leave out the `input` property\n            image_file=build_output.at_path(\"imagedef.json\"),\n            deployment_timeout=duration.minutes(60)\n        )\n    ]\n)\n```\n\n#### deploying ecs applications to existing services\n\ncodepipeline can deploy to an existing ecs service which uses the\n[ecs service arn format that contains the cluster name](https://docs.aws.amazon.com/amazonecs/latest/developerguide/ecs-account-settings.html#ecs-resource-ids).\nthis also works if the service is in a different account and/or region than the pipeline:\n\n```python\nimport aws_cdk.aws_ecs as ecs\n\n\nservice = ecs.baseservice.from_service_arn_with_cluster(self, \"ecsservice\", \"arn:aws:ecs:us-east-1:123456789012:service/myclustername/myservicename\")\npipeline = codepipeline.pipeline(self, \"mypipeline\")\nbuild_output = codepipeline.artifact()\n# add source and build stages to the pipeline as usual...\ndeploy_stage = pipeline.add_stage(\n    stage_name=\"deploy\",\n    actions=[\n        codepipeline_actions.ecsdeployaction(\n            action_name=\"deployaction\",\n            service=service,\n            input=build_output\n        )\n    ]\n)\n```\n\nwhen deploying across accounts, especially in a cdk pipelines self-mutating pipeline,\nit is recommended to provide the `role` property to the `ecsdeployaction`.\nthe role will need to have permissions assigned to it for ecs deployment.\nsee [the codepipeline documentation](https://docs.aws.amazon.com/codepipeline/latest/userguide/how-to-custom-role.html#how-to-update-role-new-services)\nfor the permissions needed.\n\n#### deploying ecs applications stored in a separate source code repository\n\nthe idiomatic cdk way of deploying an ecs application is to have your dockerfiles and your cdk code in the same source code repository,\nleveraging [docker assets](https://docs.aws.amazon.com/cdk/latest/guide/assets.html#assets_types_docker),\nand use the [cdk pipelines module](https://docs.aws.amazon.com/cdk/api/latest/docs/pipelines-readme.html).\n\nhowever, if you want to deploy a docker application whose source code is kept in a separate version control repository than the cdk code,\nyou can use the `tagparametercontainerimage` class from the ecs module.\nhere's an example:\n\n```python\n#\n# this is the stack containing a simple ecs service that uses the provided containerimage.\n#\nclass ecsappstack(cdk.stack):\n    def __init__(self, scope, id, *, image, description=none, env=none, stackname=none, tags=none, synthesizer=none, terminationprotection=none, analyticsreporting=none):\n        super().__init__(scope, id, image=image, description=description, env=env, stackname=stackname, tags=tags, synthesizer=synthesizer, terminationprotection=terminationprotection, analyticsreporting=analyticsreporting)\n\n        task_definition = ecs.taskdefinition(self, \"taskdefinition\",\n            compatibility=ecs.compatibility.fargate,\n            cpu=\"1024\",\n            memory_mi_b=\"2048\"\n        )\n        task_definition.add_container(\"appcontainer\",\n            image=image\n        )\n        ecs.fargateservice(self, \"ecsservice\",\n            task_definition=task_definition,\n            cluster=ecs.cluster(self, \"cluster\",\n                vpc=ec2.vpc(self, \"vpc\",\n                    max_azs=1\n                )\n            )\n        )\n\n#\n# this is the stack containing the codepipeline definition that deploys an ecs service.\n#\nclass pipelinestack(cdk.stack):\n\n    def __init__(self, scope, id, *, description=none, env=none, stackname=none, tags=none, synthesizer=none, terminationprotection=none, analyticsreporting=none):\n        super().__init__(scope, id, description=description, env=env, stackname=stackname, tags=tags, synthesizer=synthesizer, terminationprotection=terminationprotection, analyticsreporting=analyticsreporting)\n\n        # ********* ecs part ****************\n\n        # this is the ecr repository where the built docker image will be pushed\n        app_ecr_repo = ecr.repository(self, \"ecsdeployrepository\")\n        # the build that creates the docker image, and pushes it to the ecr repo\n        app_code_docker_build = codebuild.pipelineproject(self, \"appcodedockerimagebuildandpushproject\",\n            environment=codebuild.buildenvironment(\n                # we need to run docker\n                privileged=true\n            ),\n            build_spec=codebuild.buildspec.from_object({\n                \"version\": \"0.2\",\n                \"phases\": {\n                    \"build\": {\n                        \"commands\": [\"$(aws ecr get-login --region $aws_default_region --no-include-email)\", \"docker build -t $repository_uri:$codebuild_resolved_source_version .\"\n                        ]\n                    },\n                    \"post_build\": {\n                        \"commands\": [\"docker push $repository_uri:$codebuild_resolved_source_version\", \"export imagetag=$codebuild_resolved_source_version\"\n                        ]\n                    }\n                },\n                \"env\": {\n                    # save the imagetag environment variable as a codepipeline variable\n                    \"exported-variables\": [\"imagetag\"\n                    ]\n                }\n            }),\n            environment_variables={\n                \"repository_uri\": codebuild.buildenvironmentvariable(\n                    value=app_ecr_repo.repository_uri\n                )\n            }\n        )\n        # needed for `docker push`\n        app_ecr_repo.grant_pull_push(app_code_docker_build)\n        # create the containerimage used for the ecs application stack\n        self.tag_parameter_container_image = ecs.tagparametercontainerimage(app_ecr_repo)\n\n        cdk_code_build = codebuild.pipelineproject(self, \"cdkcodebuildproject\",\n            build_spec=codebuild.buildspec.from_object({\n                \"version\": \"0.2\",\n                \"phases\": {\n                    \"install\": {\n                        \"commands\": [\"npm install\"\n                        ]\n                    },\n                    \"build\": {\n                        \"commands\": [\"npx cdk synth --verbose\"\n                        ]\n                    }\n                },\n                \"artifacts\": {\n                    # store the entire cloud assembly as the output artifact\n                    \"base-directory\": \"cdk.out\",\n                    \"files\": \"**/*\"\n                }\n            })\n        )\n\n        # ********* pipeline part ****************\n\n        app_code_source_output = codepipeline.artifact()\n        cdk_code_source_output = codepipeline.artifact()\n        cdk_code_build_output = codepipeline.artifact()\n        app_code_build_action = codepipeline_actions.codebuildaction(\n            action_name=\"appcodedockerimagebuildandpush\",\n            project=app_code_docker_build,\n            input=app_code_source_output\n        )\n        codepipeline.pipeline(self, \"codepipelinedeployingecsapplication\",\n            artifact_bucket=s3.bucket(self, \"artifactbucket\",\n                removal_policy=cdk.removalpolicy.destroy\n            ),\n            stages=[codepipeline.stageprops(\n                stage_name=\"source\",\n                actions=[\n                    # this is the action that takes the source of your application code\n                    codepipeline_actions.codecommitsourceaction(\n                        action_name=\"appcodesource\",\n                        repository=codecommit.repository(self, \"appcodesourcerepository\", repository_name=\"appcodesourcerepository\"),\n                        output=app_code_source_output\n                    ),\n                    # this is the action that takes the source of your cdk code\n                    # (which would probably include this pipeline code as well)\n                    codepipeline_actions.codecommitsourceaction(\n                        action_name=\"cdkcodesource\",\n                        repository=codecommit.repository(self, \"cdkcodesourcerepository\", repository_name=\"cdkcodesourcerepository\"),\n                        output=cdk_code_source_output\n                    )\n                ]\n            ), codepipeline.stageprops(\n                stage_name=\"build\",\n                actions=[app_code_build_action,\n                    codepipeline_actions.codebuildaction(\n                        action_name=\"cdkcodebuildandsynth\",\n                        project=cdk_code_build,\n                        input=cdk_code_source_output,\n                        outputs=[cdk_code_build_output]\n                    )\n                ]\n            ), codepipeline.stageprops(\n                stage_name=\"deploy\",\n                actions=[\n                    codepipeline_actions.cloudformationcreateupdatestackaction(\n                        action_name=\"cfn_deploy\",\n                        stack_name=\"sampleecsstackdeployedfromcodepipeline\",\n                        # this name has to be the same name as used below in the cdk code for the application stack\n                        template_path=cdk_code_build_output.at_path(\"ecsstackdeployedinpipeline.template.json\"),\n                        admin_permissions=true,\n                        parameter_overrides={\n                            # read the tag pushed to the ecr repository from the codepipeline variable saved by the application build step,\n                            # and pass it as the cloudformation parameter for the tag\n                            \"self.tag_parameter_container_image.tag_parameter_name\": app_code_build_action.variable(\"imagetag\")\n                        }\n                    )\n                ]\n            )\n            ]\n        )\n\napp = cdk.app()\n\n# the codepipeline stack needs to be created first\npipeline_stack = pipelinestack(app, \"aws-cdk-pipeline-ecs-separate-sources\")\n# we supply the image to the ecs application stack from the codepipeline stack\necsappstack(app, \"ecsstackdeployedinpipeline\",\n    image=pipeline_stack.tag_parameter_container_image\n)\n```\n\n### aws s3 deployment\n\nto use an s3 bucket as a deployment target in codepipeline:\n\n```python\nsource_output = codepipeline.artifact()\ntarget_bucket = s3.bucket(self, \"mybucket\")\n\npipeline = codepipeline.pipeline(self, \"mypipeline\")\ndeploy_action = codepipeline_actions.s3deployaction(\n    action_name=\"s3deploy\",\n    bucket=target_bucket,\n    input=source_output\n)\ndeploy_stage = pipeline.add_stage(\n    stage_name=\"deploy\",\n    actions=[deploy_action]\n)\n```\n\n#### invalidating the cloudfront cache when deploying to s3\n\nthere is currently no native support in codepipeline for invalidating a cloudfront cache after deployment.\none workaround is to add another build step after the deploy step,\nand use the aws cli to invalidate the cache:\n\n```python\n# create a cloudfront web distribution\nimport aws_cdk.aws_cloudfront as cloudfront\n# distribution: cloudfront.distribution\n\n\n# create the build project that will invalidate the cache\ninvalidate_build_project = codebuild.pipelineproject(self, \"invalidateproject\",\n    build_spec=codebuild.buildspec.from_object({\n        \"version\": \"0.2\",\n        \"phases\": {\n            \"build\": {\n                \"commands\": [\"aws cloudfront create-invalidation --distribution-id ${cloudfront_id} --paths \\\"/*\\\"\"\n                ]\n            }\n        }\n    }),\n    environment_variables={\n        \"cloudfront_id\": codebuild.buildenvironmentvariable(value=distribution.distribution_id)\n    }\n)\n\n# add cloudfront invalidation permissions to the project\ndistribution_arn = f\"arn:aws:cloudfront::{this.account}:distribution/{distribution.distributionid}\"\ninvalidate_build_project.add_to_role_policy(iam.policystatement(\n    resources=[distribution_arn],\n    actions=[\"cloudfront:createinvalidation\"\n    ]\n))\n\n# create the pipeline (here only the s3 deploy and invalidate cache build)\ndeploy_bucket = s3.bucket(self, \"deploybucket\")\ndeploy_input = codepipeline.artifact()\ncodepipeline.pipeline(self, \"pipeline\",\n    stages=[codepipeline.stageprops(\n        stage_name=\"deploy\",\n        actions=[\n            codepipeline_actions.s3deployaction(\n                action_name=\"s3deploy\",\n                bucket=deploy_bucket,\n                input=deploy_input,\n                run_order=1\n            ),\n            codepipeline_actions.codebuildaction(\n                action_name=\"invalidatecache\",\n                project=invalidate_build_project,\n                input=deploy_input,\n                run_order=2\n            )\n        ]\n    )\n    ]\n)\n```\n\n### alexa skill\n\nyou can deploy to alexa using codepipeline with the following action:\n\n```python\n# read the secrets from parameterstore\nclient_id = secretvalue.secrets_manager(\"alexaclientid\")\nclient_secret = secretvalue.secrets_manager(\"alexaclientsecret\")\nrefresh_token = secretvalue.secrets_manager(\"alexarefreshtoken\")\n\n# add deploy action\nsource_output = codepipeline.artifact()\ncodepipeline_actions.alexaskilldeployaction(\n    action_name=\"deployskill\",\n    run_order=1,\n    input=source_output,\n    client_id=client_id.to_string(),\n    client_secret=client_secret,\n    refresh_token=refresh_token,\n    skill_id=\"amzn1.ask.skill.12345678-1234-1234-1234-123456789012\"\n)\n```\n\nif you need manifest overrides you can specify them as `parameteroverridesartifact` in the action:\n\n```python\n# deploy some cfn change set and store output\nexecute_output = codepipeline.artifact(\"cloudformation\")\nexecute_change_set_action = codepipeline_actions.cloudformationexecutechangesetaction(\n    action_name=\"executechangestest\",\n    run_order=2,\n    stack_name=\"mystack\",\n    change_set_name=\"mychangeset\",\n    output_file_name=\"overrides.json\",\n    output=execute_output\n)\n\n# provide cfn output as manifest overrides\nclient_id = secretvalue.secrets_manager(\"alexaclientid\")\nclient_secret = secretvalue.secrets_manager(\"alexaclientsecret\")\nrefresh_token = secretvalue.secrets_manager(\"alexarefreshtoken\")\nsource_output = codepipeline.artifact()\ncodepipeline_actions.alexaskilldeployaction(\n    action_name=\"deployskill\",\n    run_order=1,\n    input=source_output,\n    parameter_overrides_artifact=execute_output,\n    client_id=client_id.to_string(),\n    client_secret=client_secret,\n    refresh_token=refresh_token,\n    skill_id=\"amzn1.ask.skill.12345678-1234-1234-1234-123456789012\"\n)\n```\n\n### aws service catalog\n\nyou can deploy a cloudformation template to an existing service catalog product with the following action:\n\n```python\ncdk_build_output = codepipeline.artifact()\nservice_catalog_deploy_action = codepipeline_actions.servicecatalogdeployactionbeta1(\n    action_name=\"servicecatalogdeploy\",\n    template_path=cdk_build_output.at_path(\"sample.template.json\"),\n    product_version_name=\"version - \" + date.now.to_string,\n    product_version_description=\"this is a version from the pipeline with a new description.\",\n    product_id=\"prod-xxxxxxxx\"\n)\n```\n\n## approve & invoke\n\n### manual approval action\n\nthis package contains an action that stops the pipeline until someone manually clicks the approve button:\n\n```python\nimport aws_cdk.aws_sns as sns\n\n\npipeline = codepipeline.pipeline(self, \"mypipeline\")\napprove_stage = pipeline.add_stage(stage_name=\"approve\")\nmanual_approval_action = codepipeline_actions.manualapprovalaction(\n    action_name=\"approve\",\n    notification_topic=sns.topic(self, \"topic\"),  # optional\n    notify_emails=[\"some_email@example.com\"\n    ],  # optional\n    additional_information=\"additional info\"\n)\napprove_stage.add_action(manual_approval_action)\n```\n\nif the `notificationtopic` has not been provided,\nbut `notifyemails` were,\na new sns topic will be created\n(and accessible through the `notificationtopic` property of the action).\n\nif you want to grant a principal permissions to approve the changes,\nyou can invoke the method `grantmanualapproval` passing it a `igrantable`:\n\n```python\npipeline = codepipeline.pipeline(self, \"mypipeline\")\napprove_stage = pipeline.add_stage(stage_name=\"approve\")\nmanual_approval_action = codepipeline_actions.manualapprovalaction(\n    action_name=\"approve\"\n)\napprove_stage.add_action(manual_approval_action)\n\nrole = iam.role.from_role_arn(self, \"admin\", arn.format(arncomponents(service=\"iam\", resource=\"role\", resource_name=\"admin\"), self))\nmanual_approval_action.grant_manual_approval(role)\n```\n\n### aws lambda\n\nthis module contains an action that allows you to invoke a lambda function in a pipeline:\n\n```python\n# fn: lambda.function\n\npipeline = codepipeline.pipeline(self, \"mypipeline\")\nlambda_action = codepipeline_actions.lambdainvokeaction(\n    action_name=\"lambda\",\n    lambda_=fn\n)\npipeline.add_stage(\n    stage_name=\"lambda\",\n    actions=[lambda_action]\n)\n```\n\nthe lambda action can have up to 5 inputs,\nand up to 5 outputs:\n\n```python\n# fn: lambda.function\n\nsource_output = codepipeline.artifact()\nbuild_output = codepipeline.artifact()\nlambda_action = codepipeline_actions.lambdainvokeaction(\n    action_name=\"lambda\",\n    inputs=[source_output, build_output\n    ],\n    outputs=[\n        codepipeline.artifact(\"out1\"),\n        codepipeline.artifact(\"out2\")\n    ],\n    lambda_=fn\n)\n```\n\nthe lambda action supports custom user parameters that pipeline\nwill pass to the lambda function:\n\n```python\n# fn: lambda.function\n\n\npipeline = codepipeline.pipeline(self, \"mypipeline\")\nlambda_action = codepipeline_actions.lambdainvokeaction(\n    action_name=\"lambda\",\n    lambda_=fn,\n    user_parameters={\n        \"foo\": \"bar\",\n        \"baz\": \"qux\"\n    },\n    # or\n    user_parameters_string=\"my-parameter-string\"\n)\n```\n\nthe lambda invoke action emits variables.\nunlike many other actions, the variables are not static,\nbut dynamic, defined by the function calling the `putjobsuccessresult`\napi with the `outputvariables` property filled with the map of variables\nexample:\n\n```python\n# later:\n# project: codebuild.pipelineproject\nlambda_invoke_action = codepipeline_actions.lambdainvokeaction(\n    action_name=\"lambda\",\n    lambda_=lambda_.function(self, \"func\",\n        runtime=lambda_.runtime.nodejs_14_x,\n        handler=\"index.handler\",\n        code=lambda_.code.from_inline(\"\"\"\n                    const aws = require('aws-sdk');\n\n                    exports.handler = async function(event, context) {\n                        const codepipeline = new aws.codepipeline();\n                        await codepipeline.putjobsuccessresult({\n                            jobid: event['codepipeline.job'].id,\n                            outputvariables: {\n                                my_var: \"some value\",\n                            },\n                        }).promise();\n                    }\n                \"\"\")\n    ),\n    variables_namespace=\"mynamespace\"\n)\nsource_output = codepipeline.artifact()\ncodepipeline_actions.codebuildaction(\n    action_name=\"codebuild\",\n    project=project,\n    input=source_output,\n    environment_variables={\n        \"myvar\": codebuild.buildenvironmentvariable(\n            value=lambda_invoke_action.variable(\"my_var\")\n        )\n    }\n)\n```\n\nsee [the aws documentation](https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-invoke-lambda-function.html)\non how to write a lambda function invoked from codepipeline.\n\n### aws step functions\n\nthis module contains an action that allows you to invoke a step function in a pipeline:\n\n```python\nimport aws_cdk.aws_stepfunctions as stepfunctions\n\npipeline = codepipeline.pipeline(self, \"mypipeline\")\nstart_state = stepfunctions.pass(self, \"startstate\")\nsimple_state_machine = stepfunctions.statemachine(self, \"simplestatemachine\",\n    definition=start_state\n)\nstep_function_action = codepipeline_actions.stepfunctioninvokeaction(\n    action_name=\"invoke\",\n    state_machine=simple_state_machine,\n    state_machine_input=codepipeline_actions.statemachineinput.literal({\"ishelloworldexample\": true})\n)\npipeline.add_stage(\n    stage_name=\"stepfunctions\",\n    actions=[step_function_action]\n)\n```\n\nthe `statemachineinput` can be created with one of 2 static factory methods:\n`literal`, which takes an arbitrary map as its only argument, or `filepath`:\n\n```python\nimport aws_cdk.aws_stepfunctions as stepfunctions\n\n\npipeline = codepipeline.pipeline(self, \"mypipeline\")\ninput_artifact = codepipeline.artifact()\nstart_state = stepfunctions.pass(self, \"startstate\")\nsimple_state_machine = stepfunctions.statemachine(self, \"simplestatemachine\",\n    definition=start_state\n)\nstep_function_action = codepipeline_actions.stepfunctioninvokeaction(\n    action_name=\"invoke\",\n    state_machine=simple_state_machine,\n    state_machine_input=codepipeline_actions.statemachineinput.file_path(input_artifact.at_path(\"assets/input.json\"))\n)\npipeline.add_stage(\n    stage_name=\"stepfunctions\",\n    actions=[step_function_action]\n)\n```\n\nsee [the aws documentation](https://docs.aws.amazon.com/codepipeline/latest/userguide/action-reference-stepfunctions.html)\nfor information on action structure reference.\n",
  "docs_url": null,
  "keywords": "",
  "license": "apache-2.0",
  "name": "aws-cdk.aws-codepipeline-actions",
  "package_url": "https://pypi.org/project/aws-cdk.aws-codepipeline-actions/",
  "project_url": "https://pypi.org/project/aws-cdk.aws-codepipeline-actions/",
  "project_urls": {
    "Homepage": "https://github.com/aws/aws-cdk",
    "Source": "https://github.com/aws/aws-cdk.git"
  },
  "release_url": "https://pypi.org/project/aws-cdk.aws-codepipeline-actions/1.204.0/",
  "requires_dist": [
    "aws-cdk.aws-cloudformation (==1.204.0)",
    "aws-cdk.aws-codebuild (==1.204.0)",
    "aws-cdk.aws-codecommit (==1.204.0)",
    "aws-cdk.aws-codedeploy (==1.204.0)",
    "aws-cdk.aws-codepipeline (==1.204.0)",
    "aws-cdk.aws-ec2 (==1.204.0)",
    "aws-cdk.aws-ecr (==1.204.0)",
    "aws-cdk.aws-ecs (==1.204.0)",
    "aws-cdk.aws-events-targets (==1.204.0)",
    "aws-cdk.aws-events (==1.204.0)",
    "aws-cdk.aws-iam (==1.204.0)",
    "aws-cdk.aws-kms (==1.204.0)",
    "aws-cdk.aws-lambda (==1.204.0)",
    "aws-cdk.aws-s3 (==1.204.0)",
    "aws-cdk.aws-sns-subscriptions (==1.204.0)",
    "aws-cdk.aws-sns (==1.204.0)",
    "aws-cdk.aws-stepfunctions (==1.204.0)",
    "aws-cdk.core (==1.204.0)",
    "constructs (<4.0.0,>=3.3.69)",
    "jsii (<2.0.0,>=1.84.0)",
    "publication (>=0.0.3)",
    "typeguard (~=2.13.3)"
  ],
  "requires_python": "~=3.7",
  "summary": "concrete actions for aws code pipeline",
  "version": "1.204.0",
  "releases": [],
  "developers": [
    "amazon_web_services"
  ],
  "kwds": "aws_cdk aws_codepipeline_actions aws_codepipeline aws_stepfunctions cdk_source_action",
  "license_kwds": "apache-2.0",
  "libtype": "pypi",
  "id": "pypi_aws_cdk.aws_codepipeline_actions",
  "homepage": "https://github.com/aws/aws-cdk",
  "release_count": 257,
  "dependency_ids": [
    "pypi_aws_cdk.aws_cloudformation",
    "pypi_aws_cdk.aws_codebuild",
    "pypi_aws_cdk.aws_codecommit",
    "pypi_aws_cdk.aws_codedeploy",
    "pypi_aws_cdk.aws_codepipeline",
    "pypi_aws_cdk.aws_ec2",
    "pypi_aws_cdk.aws_ecr",
    "pypi_aws_cdk.aws_ecs",
    "pypi_aws_cdk.aws_events",
    "pypi_aws_cdk.aws_events_targets",
    "pypi_aws_cdk.aws_iam",
    "pypi_aws_cdk.aws_kms",
    "pypi_aws_cdk.aws_lambda",
    "pypi_aws_cdk.aws_s3",
    "pypi_aws_cdk.aws_sns",
    "pypi_aws_cdk.aws_sns_subscriptions",
    "pypi_aws_cdk.aws_stepfunctions",
    "pypi_aws_cdk.core",
    "pypi_constructs",
    "pypi_jsii",
    "pypi_publication",
    "pypi_typeguard"
  ]
}