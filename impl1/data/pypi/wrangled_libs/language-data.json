{
  "classifiers": [
    "license :: osi approved :: mit license",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "# language_data: a supplement to [langcodes][]\n\n[langcodes]: https://github.com/luminosoinsight/langcodes\n\nthis package is not meant to be used on its own. please see [langcodes][] for documentation.\n\n`language_data` is a supplement to the langcodes module, for working with standardized codes for human languages. it stores the more bulky and hard-to-index data about languages, particularly what they are named in various languages.\n\nfor example, this stores the data that tell you that the code \"en\" means \"english\" in english, or that \"franc\u00e9s\" is the spanish (es) name for french (fr).\n\nthe functions and test cases for working with this data are in [langcodes][], because working with the data correctly requires parsing language codes.\n\n## data\n\nthe data included in this package is:\n\n- the names of various languages, in various languages\n- the estimated population that speaks each language\n- the estimated population that writes each language\n\nthese are all extracted from the unicode [cldr][] data package, version 40, plus a few additional language names that fill in gaps in cldr.\n\n[cldr]: http://cldr.unicode.org/\n\n## caveats\n\n- the estimates for \"writing population\" are often overestimates, as described in the [cldr documentation on territory data][overestimates]. in most cases, they are derived from published data about literacy rates in the places where those languages are spoken. this doesn't take into account that many literate people around the world speak a language that isn't typically written, and write in a _different_ language.\n\n- the writing systems of chinese erase most (but not all) of the distinctions between spoken chinese languages. you'll see separate estimates of the writing population for cantonese, mandarin, wu, and so on, even though you'll likely consider these all to be `zh` when written.\n\n- cldr doesn't have language population data for sign languages. sign languages end up with a `speaking_population()` and `writing_population()` of 0, and i suppose that is literally true, but there's no data from which we could provide a `signing_population()` method.\n\n[overestimates]: https://unicode-org.github.io/cldr-staging/charts/38.1/supplemental/territory_language_information.html\n\n## dependencies\n\n`language_data` has a dependency on the `marisa-trie` package so that it can load a compact, efficient data structure for looking up language names.\n\n## installation\n\n`language_data` is usually installed as a dependency of `langcodes`, and doesn't make much sense without it. you can `pip install language_data` anyway if you want.\n\nto install the `language_data` package in editable mode, run `poetry install` in the package root. (this is the equivalent of `pip install -e .`, which will hopefully become compatible again soon via pep 660.)\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "language-data",
  "package_url": "https://pypi.org/project/language-data/",
  "project_url": "https://pypi.org/project/language-data/",
  "project_urls": {
    "Homepage": "https://github.com/rspeer/language_data"
  },
  "release_url": "https://pypi.org/project/language-data/1.1/",
  "requires_dist": [
    "marisa-trie (>=0.7.7,<0.8.0)"
  ],
  "requires_python": ">=3.6",
  "summary": "supplementary data about languages used by the langcodes module",
  "version": "1.1",
  "releases": [],
  "developers": [
    "elia_robyn_speer",
    "rspeer@arborelia.net"
  ],
  "kwds": "language_data langcodes territory_language_information languages language",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_language_data",
  "homepage": "https://github.com/rspeer/language_data",
  "release_count": 3,
  "dependency_ids": [
    "pypi_marisa_trie"
  ]
}