{
  "classifiers": [
    "license :: osi approved :: mit license",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "======\nsyntok\n======\n\n(a.k.a. segtok_ v2)\n\n.. image:: https://img.shields.io/pypi/v/syntok.svg\n    :alt: syntok version\n    :target: https://pypi.python.org/pypi/syntok\n\n.. image:: https://github.com/fnl/syntok/actions/workflows/tox.yaml/badge.svg\n    :alt: tox build\n    :target: https://github.com/fnl/syntok/actions/workflows/tox.yaml\n\n-------------------------------------------\nsentence segmentation and word tokenization\n-------------------------------------------\n\nthe syntok package provides two modules, ``syntok.segmenter`` and ``syntok.tokenizer``.\nthe tokenizer provides functionality for splitting (indo-european) text into words and symbols (collectively called *tokens*).\nthe segmenter provides functionality for splitting (indo-european) token streams (from the tokenizer) into sentences and for pre-processing documents by splitting them into paragraphs.\nboth modules can also be used from the command-line to split either a given text file (argument) or by reading from stdin.\nwhile other indo-european languages could work, it has only been designed with the languages spanish, english, and german in mind (the author's main languages).\n\n``segtok``\n==========\n\nsyntok is the successor of an earlier, very similar tool, segtok_, but has evolved significantly in terms of providing better segmentation and tokenization performance and throughput (syntok can segment documents at a rate of about 100k tokens per second without problems).\nfor example, if a sentence terminal marker is not followed by a spacing character, segtok is unable to detect that as a terminal marker, while syntok has no problem segmenting that case (as it uses tokenization first, and does segmentation afterwards).\nin fact, i feel confident enough to just boldly claim syntok is the world's best sentence segmenter for at least english, spanish, and german.\n\ninstallation\n============\n\nto use this package, you minimally should have python 3.6 or installed.\nas it uses the typing package, earlier versions are not supported.\nthe easiest way to get ``syntok`` installed is using ``pip`` or any other package manager that works with pypi::\n\n    pip3 install syntok\n\n*important*: if you are on **linux** and have problems installing the ``regex`` dependency of ``syntok``, make sure you have the ``python-dev`` and/or ``python3-dev`` packages installed to get the necessary headers to compile that package.\n\nthen try the command line tools on some plain-text files (e.g., this readme) to see if ``syntok`` meets your needs::\n\n    python3 -m syntok.segmenter readme.rst\n    python3 -m syntok.tokenizer readme.rst\n\ndevelopment\n===========\n\n``syntok`` uses poetry_ as the build tool, and expects pyenv_ to provide the python versions to test with tox_.\ntherefore, to develop ``syntok``, it is recommended that you install poetry and pyenv, first.\ninstall the python versions defined in ``tox.ini`` (see ``envlist``) with pyenv, and set all of them as your local versions, for example: ``pyenv local 3.6.15 3.9.9``\n\nto run the full test suite, you have to install flake8_, pytest_, and mypy_ (via ``poetry install``).\nthe tests in the proper python version & environment are run via ``tox`` or by calling the three commands by hand::\n\n   poetry run tox\n\n   # or, manually:\n   poetry shell\n   flake8 syntok\n   mypy syntok\n   pytest syntok\n\nusage\n=====\n\nfor details, please refer to the code documentation; this readme only provides an overview of the provided functionality.\n\ncommand-line\n------------\n\nafter installing the package, two command-line usages will be available, ``python -m syntok.segmenter`` and ``python -m syntok.tokenizer``.\neach takes [utf-8 encoded] plain-text files (or stdin until eof (ctrl-d)) as input and transforms that into newline-separated sentences or space-separated tokens, respectively.\nyou can control python3's file ``open`` encoding by `configuring the environment variable`_ ``pythonioencoding`` to your needs (e.g. ``export pythonioencoding=\"utf-16-be\"``).\nthe tokenizer produces single-space separated tokens for each input line.\nthe segmenter produces line-segmented sentences for each input file (or after stdin closes).\n\n``syntok.tokenizer``\n--------------------\n\nthis module provides the ``tokenizer`` class to tokenize input text into words and symbols (**value** tokens), prefixed with (possibly empty) **spacing** strings, while recording their **offset** positions.\nthe tokenizer comes with utility static functions, to join hyphenated words across line-breaks, and to reproduce the original string from a sequence of tokens.\nthe tokenizer considers camelcase words as individual tokens (here: camel and case) and by default considers underscores and unicode hyphens *inside* words as spacing characters (not token values).\nit does not split numeric tokens (without letters) if they contain symbols (e.g. maintaining \"2018-11-11\", \"12:30:21\", \"1_000_000\", \"1,000.00\", or \"1..3\" all as single tokens)\nfinally, as it splits english negation contractions (such as \"don't\") into their root and \"not\" (here: do and not), it can be configured to refrain from replacing this special \"n't\" token with \"not\", and instead emit the actual \"n't\" value.\n\nto track the spacing and offset of tokens, the module contains the ``token`` class, which is a ``str`` wrapper class where the token **value** itself is available from the ``value`` property and adding a ``spacing`` and a ``offset`` property that will hold the **spacing** prefix and the **offset** position of the token, respectively.\n\nbasic example::\n\n   from syntok.tokenizer import tokenizer\n\n   document = open('readme.rst').read()\n   tok = tokenizer()  # optional: keep \"n't\" contractions and \"-\", \"_\" inside words as tokens\n\n   for token in tok.tokenize(document):\n       print(repr(token))\n\n``syntok.segmenter``\n--------------------\n\nthis module provides several functions to segment documents into iterators over paragraphs, sentences, and tokens (functions ``analyze`` and ``process``) or simply sentences and tokens (functions ``split`` and ``segment``).\nthe analytic segmenter can even keep track of the original offset of each token in the document while processing (but does not join hyphen-separated words across line-breaks).\nall segmenter functions accept arbitrary token streams as input (typically as generated by the ``tokenizer.tokenize`` method).\ndue to how ``syntok.tokenizer.token`` objects \"work\", it is possible to establish the exact sentence content (with the original spacing between the tokens).\nthe pre-processing functions and paragraph-based segmentation splits paragraphs, i.e., chunks of text separated by at least two consecutive linebreaks (``\\\\r?\\\\n``).\n\nbasic example::\n\n   import syntok.segmenter as segmenter\n\n   document = open('readme.rst').read()\n\n   # choose the segmentation function you need/prefer\n\n   for paragraph in segmenter.process(document):\n       for sentence in paragraph:\n           for token in sentence:\n               # roughly reproduce the input,\n               # except for hyphenated word-breaks\n               # and replacing \"n't\" contractions with \"not\",\n               # separating tokens by single spaces\n               print(token.value, end=' ')\n           print()  # print one sentence per line\n       print()  # separate paragraphs with newlines\n\n   for paragraph in segmenter.analyze(document):\n       for sentence in paragraph:\n           for token in sentence:\n               # exactly reproduce the input\n               # and do not remove \"imperfections\"\n               print(token.spacing, token.value, sep='', end='')\n       print(\"\\n\")  # reinsert paragraph separators\n\nlegal\n=====\n\nlicense: `mit <http://opensource.org/licenses/mit>`_\n\ncopyright (c) 2017-2022, florian leitner. all rights reserved.\n\ncontributors\n============\n\n- arjen p. de vries, @arjenpdevries, http://www.cs.ru.nl/~arjen/\n- bastian zimmermann, @bastianzim\n- p\u00e9ter l\u00e1ng, @peter-lang-dealogic\n- koen dercksen, @kdercksen, https://koendercksen.com/\n- sergiusz bleja, @svenski\n\nthank you!\n\nhistory\n=======\n\n- **1.4.4** bug fixes: support for single letter consontant abbreviations, \"min.\", and \"sen.\" `#26`_ and german weekday abbreviations\n- **1.4.3** bug fixes: under-splitting at month abbreviations `#22`_ and over-splitting at \"no.\" abbreviations `#21`_\n- **1.4.2** improved handling of parenthesis at start of sentences and bugfix for citations at end of texts `#19`_\n- **1.4.1** support citations at sentence begin (e.g., bible quotes) `#12`_\n- **1.4.0** migrated to pyproject.toml and tox.ini, dropped makefile builds and py3.5 support\n- **1.3.3** splitting tokens around the zero-width space characater u+200b `#18`_\n- **1.3.2** bugfix for offset of not contractions; discussion in issue `#15`_\n- **1.3.1** segmenting now occurs at semi-colons, too; discussion in issue `#9`_\n- **1.2.2** bugfix for offsets in multi-nonword prefix tokens; issue `#6`_\n- **1.2.1** added a generic rule for catching more uncommon uses of \".\" without space suffix as abbreviation marker\n- **1.2.0** added support for skipping and handling text in brackets (e.g., citations)\n- **1.1.1** fixed non-trivial segmentation in sci. text and refactored splitting logic to one place only\n- **1.1.0** added support for ellipses (back - from segtok) in\n- **1.0.2** hyphen joining only should happen when letters are present; squash escape warnings\n- **1.0.1** fixing segmenter.analyze to preserve \"n't\" contractions, and improved the readme and tokenizer constructor api\n- **1.0.0** initial release\n\n.. _configuring the environment variable: https://docs.python.org/3/using/cmdline.html\n.. _flake8: https://flake8.pycqa.org/en/latest/\n.. _poetry: https://python-poetry.org/\n.. _segtok: https://github.com/fnl/segtok\n.. _mypy: http://mypy-lang.org/\n.. _pyenv: https://github.com/pyenv/pyenv\n.. _pytest: https://docs.pytest.org/en/latest/\n.. _tox: https://tox.wiki/en/latest/\n.. _#6: https://github.com/fnl/syntok/issues/6\n.. _#9: https://github.com/fnl/syntok/issues/9\n.. _#12: https://github.com/fnl/syntok/pull/12\n.. _#15: https://github.com/fnl/syntok/issues/15\n.. _#18: https://github.com/fnl/syntok/pull/18\n.. _#19: https://github.com/fnl/syntok/issues/19\n.. _#21: https://github.com/fnl/syntok/issues/21\n.. _#22: https://github.com/fnl/syntok/issues/22\n.. _#26: https://github.com/fnl/syntok/issues/26\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "syntok",
  "package_url": "https://pypi.org/project/syntok/",
  "project_url": "https://pypi.org/project/syntok/",
  "project_urls": null,
  "release_url": "https://pypi.org/project/syntok/1.4.4/",
  "requires_dist": [
    "regex (>2016)"
  ],
  "requires_python": ">=3.6,<4.0",
  "summary": "text tokenization and sentence segmentation (segtok v2).",
  "version": "1.4.4",
  "releases": [],
  "developers": [
    "florian_leitner",
    "me@fnl.es"
  ],
  "kwds": "syntok tokenizer tokenization tokenize token",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_syntok",
  "homepage": "",
  "release_count": 16,
  "dependency_ids": [
    "pypi_regex"
  ]
}