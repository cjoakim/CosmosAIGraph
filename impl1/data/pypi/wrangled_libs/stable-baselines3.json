{
  "classifiers": [
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "\n\n# stable baselines3\n\nstable baselines3 is a set of reliable implementations of reinforcement learning algorithms in pytorch. it is the next major version of [stable baselines](https://github.com/hill-a/stable-baselines).\n\nthese algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. we expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. we also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.\n\n\n## links\n\nrepository:\nhttps://github.com/dlr-rm/stable-baselines3\n\nblog post:\nhttps://araffin.github.io/post/sb3/\n\ndocumentation:\nhttps://stable-baselines3.readthedocs.io/en/master/\n\nrl baselines3 zoo:\nhttps://github.com/dlr-rm/rl-baselines3-zoo\n\nsb3 contrib:\nhttps://github.com/stable-baselines-team/stable-baselines3-contrib\n\n## quick example\n\nmost of the library tries to follow a sklearn-like syntax for the reinforcement learning algorithms using gym.\n\nhere is a quick example of how to train and run ppo on a cartpole environment:\n\n```python\nimport gymnasium\n\nfrom stable_baselines3 import ppo\n\nenv = gymnasium.make(\"cartpole-v1\")\n\nmodel = ppo(\"mlppolicy\", env, verbose=1)\nmodel.learn(total_timesteps=10_000)\n\nvec_env = model.get_env()\nobs = vec_env.reset()\nfor i in range(1000):\n    action, _states = model.predict(obs, deterministic=true)\n    obs, reward, done, info = vec_env.step(action)\n    vec_env.render()\n    # vecenv resets automatically\n    # if done:\n    #   obs = vec_env.reset()\n\n```\n\nor just train a model with a one liner if [the environment is registered in gymnasium](https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/) and if [the policy is registered](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html):\n\n```python\nfrom stable_baselines3 import ppo\n\nmodel = ppo(\"mlppolicy\", \"cartpole-v1\").learn(10_000)\n```\n\n",
  "docs_url": null,
  "keywords": "reinforcement-learning-algorithms reinforcement-learning machine-learning gymnasium gym openai stable baselines toolbox python data-science",
  "license": "mit",
  "name": "stable-baselines3",
  "package_url": "https://pypi.org/project/stable-baselines3/",
  "project_url": "https://pypi.org/project/stable-baselines3/",
  "project_urls": {
    "Changelog": "https://stable-baselines3.readthedocs.io/en/master/misc/changelog.html",
    "Code": "https://github.com/DLR-RM/stable-baselines3",
    "Documentation": "https://stable-baselines3.readthedocs.io/",
    "Homepage": "https://github.com/DLR-RM/stable-baselines3",
    "RL-Zoo": "https://github.com/DLR-RM/rl-baselines3-zoo",
    "SB3-Contrib": "https://github.com/Stable-Baselines-Team/stable-baselines3-contrib",
    "SBX": "https://github.com/araffin/sbx"
  },
  "release_url": "https://pypi.org/project/stable-baselines3/2.2.1/",
  "requires_dist": [
    "gymnasium <0.30,>=0.28.1",
    "numpy >=1.20",
    "torch >=1.13",
    "cloudpickle",
    "pandas",
    "matplotlib",
    "sphinx <8,>=5 ; extra == 'docs'",
    "sphinx-autobuild ; extra == 'docs'",
    "sphinx-rtd-theme >=1.3.0 ; extra == 'docs'",
    "sphinxcontrib.spelling ; extra == 'docs'",
    "sphinx-copybutton ; extra == 'docs'",
    "opencv-python ; extra == 'extra'",
    "pygame ; extra == 'extra'",
    "tensorboard >=2.9.1 ; extra == 'extra'",
    "psutil ; extra == 'extra'",
    "tqdm ; extra == 'extra'",
    "rich ; extra == 'extra'",
    "shimmy[atari] ~=1.3.0 ; extra == 'extra'",
    "pillow ; extra == 'extra'",
    "autorom[accept-rom-license] ~=0.6.1 ; extra == 'extra'",
    "opencv-python ; extra == 'extra_no_roms'",
    "pygame ; extra == 'extra_no_roms'",
    "tensorboard >=2.9.1 ; extra == 'extra_no_roms'",
    "psutil ; extra == 'extra_no_roms'",
    "tqdm ; extra == 'extra_no_roms'",
    "rich ; extra == 'extra_no_roms'",
    "shimmy[atari] ~=1.3.0 ; extra == 'extra_no_roms'",
    "pillow ; extra == 'extra_no_roms'",
    "pytest ; extra == 'tests'",
    "pytest-cov ; extra == 'tests'",
    "pytest-env ; extra == 'tests'",
    "pytest-xdist ; extra == 'tests'",
    "mypy ; extra == 'tests'",
    "ruff >=0.0.288 ; extra == 'tests'",
    "black <24,>=23.9.1 ; extra == 'tests'"
  ],
  "requires_python": ">=3.8",
  "summary": "pytorch version of stable baselines, implementations of reinforcement learning algorithms.",
  "version": "2.2.1",
  "releases": [],
  "developers": [
    "antonin.raffin@dlr.de",
    "antonin_raffin"
  ],
  "kwds": "reinforcement stable_baselines3 baselines3 tools baselines",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_stable_baselines3",
  "homepage": "https://github.com/dlr-rm/stable-baselines3",
  "release_count": 80,
  "dependency_ids": [
    "pypi_autorom",
    "pypi_black",
    "pypi_cloudpickle",
    "pypi_gymnasium",
    "pypi_matplotlib",
    "pypi_mypy",
    "pypi_numpy",
    "pypi_opencv_python",
    "pypi_pandas",
    "pypi_pillow",
    "pypi_psutil",
    "pypi_pygame",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_pytest_env",
    "pypi_pytest_xdist",
    "pypi_rich",
    "pypi_ruff",
    "pypi_shimmy",
    "pypi_sphinx",
    "pypi_sphinx_autobuild",
    "pypi_sphinx_copybutton",
    "pypi_sphinx_rtd_theme",
    "pypi_sphinxcontrib.spelling",
    "pypi_tensorboard",
    "pypi_torch",
    "pypi_tqdm"
  ]
}