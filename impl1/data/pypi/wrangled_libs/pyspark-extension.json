{
  "classifiers": [
    "development status :: 5 - production/stable",
    "license :: osi approved :: apache software license",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "programming language :: python :: implementation :: cpython",
    "programming language :: python :: implementation :: pypy",
    "typing :: typed"
  ],
  "description": "# spark extension\n\nthis project provides extensions to the [apache spark project](https://spark.apache.org/) in scala and python:\n\n**diff:** a `diff` transformation and application for `dataset`s that computes the differences between\ntwo datasets, i.e. which rows to _add_, _delete_ or _change_ to get from one dataset to the other.\n\n**global row number:** a `withrownumbers` transformation that provides the global row number w.r.t.\nthe current order of the dataset, or any given order. in contrast to the existing sql function `row_number`, which\nrequires a window spec, this transformation provides the row number across the entire dataset without scaling problems.\n\n**inspect parquet files:** the structure of parquet files (the metadata, not the data stored in parquet) can be inspected similar to [parquet-tools](https://pypi.org/project/parquet-tools/)\nor [parquet-cli](https://pypi.org/project/parquet-cli/) by reading from a simple spark data source.\nthis simplifies identifying why some parquet files cannot be split by spark into scalable partitions.\n\n**.net datetime.ticks:** convert .net (c#, f#, visual basic) `datetime.ticks` into spark timestamps, seconds and nanoseconds.\n\n<details>\n<summary>available methods:</summary>\n\n```scala\n// scala\ndotnettickstotimestamp(column): column       // returns timestamp as timestamptype\ndotnettickstounixepoch(column): column       // returns unix epoch seconds as decimaltype\ndotnettickstounixepochnanos(column): column  // returns unix epoch nanoseconds as longtype\n```\n\nthe reverse is provided by (all return `longtype` .net ticks):\n```scala\n// scala\ntimestamptodotnetticks(column): column\nunixepochtodotnetticks(column): column\nunixepochnanostodotnetticks(column): column\n```\n\nthese methods are also available in python:\n```python\n# python\ndotnet_ticks_to_timestamp(column_or_name)         # returns timestamp as timestamptype\ndotnet_ticks_to_unix_epoch(column_or_name)        # returns unix epoch seconds as decimaltype\ndotnet_ticks_to_unix_epoch_nanos(column_or_name)  # returns unix epoch nanoseconds as longtype\n\ntimestamp_to_dotnet_ticks(column_or_name)\nunix_epoch_to_dotnet_ticks(column_or_name)\nunix_epoch_nanos_to_dotnet_ticks(column_or_name)\n```\n</details>\n\n**spark job description:** set spark job description for all spark jobs within a context:\n\n```python\nfrom gresearch.spark import job_description, append_job_description\n\nwith job_description(\"parquet file\"):\n    df = spark.read.parquet(\"data.parquet\")\n    with append_job_description(\"count\"):\n        count = df.count\n    with append_job_description(\"write\"):\n        df.write.csv(\"data.csv\")\n```\n\nfor details, see the [readme.md](https://github.com/g-research/spark-extension#spark-extension) at the project homepage.\n\n## using spark extension\n\n#### pypi package (local spark cluster only)\n\nyou may want to install the `pyspark-extension` python package from pypi into your development environment.\nthis provides you code completion, typing and test capabilities during your development phase.\n\nrunning your python application on a spark cluster will still require one of the ways below\nto add the scala package to the spark environment.\n\n```shell script\npip install pyspark-extension==2.10.0.3.4\n```\n\nnote: pick the right spark version (here 3.4) depending on your pyspark version.\n\n#### pyspark api\n\nstart a pyspark session with the spark extension dependency (version \u22651.1.0) as follows:\n\n```python\nfrom pyspark.sql import sparksession\n\nspark = sparksession \\\n    .builder \\\n    .config(\"spark.jars.packages\", \"uk.co.gresearch.spark:spark-extension_2.12:2.10.0-3.4\") \\\n    .getorcreate()\n```\n\nnote: pick the right scala version (here 2.12) and spark version (here 3.4) depending on your pyspark version.\n\n#### pyspark repl\n\nlaunch the python spark repl with the spark extension dependency (version \u22651.1.0) as follows:\n\n```shell script\npyspark --packages uk.co.gresearch.spark:spark-extension_2.12:2.10.0-3.4\n```\n\nnote: pick the right scala version (here 2.12) and spark version (here 3.4) depending on your pyspark version.\n\n#### pyspark `spark-submit`\n\nrun your python scripts that use pyspark via `spark-submit`:\n\n```shell script\nspark-submit --packages uk.co.gresearch.spark:spark-extension_2.12:2.10.0-3.4 [script.py]\n```\n\nnote: pick the right scala version (here 2.12) and spark version (here 3.4) depending on your spark version.\n\n### your favorite data science notebook\n\nthere are plenty of [data science notebooks](https://datasciencenotebook.org/) around. to use this library,\nadd **a jar dependency** to your notebook using these **maven coordinates**:\n\n    uk.co.gresearch.spark:spark-extension_2.12:2.10.0-3.4\n\nor [download the jar](https://mvnrepository.com/artifact/uk.co.gresearch.spark/spark-extension) and place it\non a filesystem where it is accessible by the notebook, and reference that jar file directly.\n\ncheck the documentation of your favorite notebook to learn how to add jars to your spark environment.\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "http://www.apache.org/licenses/license-2.0.html",
  "name": "pyspark-extension",
  "package_url": "https://pypi.org/project/pyspark-extension/",
  "project_url": "https://pypi.org/project/pyspark-extension/",
  "project_urls": {
    "Homepage": "https://github.com/G-Research/spark-extension"
  },
  "release_url": "https://pypi.org/project/pyspark-extension/2.10.0.3.5/",
  "requires_dist": [
    "pyspark <4,>=3.5",
    "py4j"
  ],
  "requires_python": ">=3.7",
  "summary": "a library that provides useful extensions to apache spark.",
  "version": "2.10.0.3.5",
  "releases": [],
  "developers": [
    "enrico_minack",
    "github@enrico.minack.dev"
  ],
  "kwds": "row_number pyspark parquet withrownumbers scala",
  "license_kwds": "http://www.apache.org/licenses/license-2.0.html",
  "libtype": "pypi",
  "id": "pypi_pyspark_extension",
  "homepage": "https://github.com/g-research/spark-extension",
  "release_count": 33,
  "dependency_ids": [
    "pypi_py4j",
    "pypi_pyspark"
  ]
}