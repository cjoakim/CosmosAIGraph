{
  "classifiers": [
    "development status :: 5 - production/stable",
    "environment :: console",
    "intended audience :: developers",
    "license :: osi approved :: apache software license",
    "natural language :: english",
    "operating system :: os independent",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: scientific/engineering :: image recognition",
    "topic :: scientific/engineering :: information analysis"
  ],
  "description": "<div align=\"center\">\n\n<img src=\"https://pl-public-data.s3.amazonaws.com/assets_lightning/pytorch-lightning.png\" width=\"400px\">\n\n**the lightweight pytorch wrapper for high-performance ai research.\nscale your models, not the boilerplate.**\n\n______________________________________________________________________\n\n<p align=\"center\">\n  <a href=\"https://www.pytorchlightning.ai/\">website</a> \u2022\n  <a href=\"#key-features\">key features</a> \u2022\n  <a href=\"#how-to-use\">how to use</a> \u2022\n  <a href=\"https://lightning.ai/docs/pytorch/stable/\">docs</a> \u2022\n  <a href=\"#examples\">examples</a> \u2022\n  <a href=\"#community\">community</a> \u2022\n  <a href=\"https://lightning.ai/\">lightning ai</a> \u2022\n  <a href=\"https://github.com/lightning-ai/lightning/blob/master/license\">license</a>\n</p>\n\n<!-- do not add conda downloads... readme changes must be approved by eden or will -->\n\n[![pypi - python version](https://img.shields.io/pypi/pyversions/pytorch-lightning)](https://pypi.org/project/pytorch-lightning/)\n[![pypi status](https://badge.fury.io/py/pytorch-lightning.svg)](https://badge.fury.io/py/pytorch-lightning)\n[![pypi - downloads](https://img.shields.io/pypi/dm/pytorch-lightning)](https://pepy.tech/project/pytorch-lightning)\n[![conda](https://img.shields.io/conda/v/conda-forge/pytorch-lightning?label=conda&color=success)](https://anaconda.org/conda-forge/pytorch-lightning)\n[![dockerhub](https://img.shields.io/docker/pulls/pytorchlightning/pytorch_lightning.svg)](https://hub.docker.com/r/pytorchlightning/pytorch_lightning)\n[![codecov](https://codecov.io/gh/lightning-ai/pytorch-lightning/graph/badge.svg?token=smzx8mnkla)](https://codecov.io/gh/lightning-ai/pytorch-lightning)\n\n[![readthedocs](https://readthedocs.org/projects/pytorch-lightning/badge/?version=2.1.3)](https://lightning.ai/docs/pytorch/stable/)[![discord](https://img.shields.io/discord/1077906959069626439?style=plastic)](https://discord.gg/vptpczkgna)\n[![license](https://img.shields.io/badge/license-apache%202.0-blue.svg)](https://github.com/lightning-ai/lightning/blob/master/license)\n\n<!--\n[![codefactor](https://www.codefactor.io/repository/github/lightning-ai/lightning/badge)](https://www.codefactor.io/repository/github/lightning-ai/lightning)\n-->\n\n</div>\n\n###### \\*codecov is > 90%+ but build delays may show less\n\n______________________________________________________________________\n\n## pytorch lightning is just organized pytorch\n\nlightning disentangles pytorch code to decouple the science from the engineering.\n![pt to pl](https://lightning.ai/docs/pytorch/stable/_static/images/general/pl_quick_start_full_compressed.gif)\n\n______________________________________________________________________\n\n## lightning design philosophy\n\nlightning structures pytorch code with these principles:\n\n<div align=\"center\">\n  <img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/philosophies.jpg\" max-height=\"250px\">\n</div>\n\nlightning forces the following structure to your code which makes it reusable and shareable:\n\n- research code (the lightningmodule).\n- engineering code (you delete, and is handled by the trainer).\n- non-essential research code (logging, etc... this goes in callbacks).\n- data (use pytorch dataloaders or organize them into a lightningdatamodule).\n\nonce you do this, you can train on multiple-gpus, tpus, cpus, ipus, hpus and even in 16-bit precision without changing your code!\n\n[get started in just 15 minutes](https://lightning.ai/docs/pytorch/latest/starter/introduction.html)\n\n______________________________________________________________________\n\n## continuous integration\n\nlightning is rigorously tested across multiple cpus, gpus and tpus and against major python and pytorch versions.\n\n<details>\n  <summary>current build statuses</summary>\n\n<center>\n\n|       system / pytorch ver.        |                                                    1.12                                                     | 1.13                                                                                                        | 2.0                                                                                                         | 2.1                                                                                                               |\n| :--------------------------------: | :---------------------------------------------------------------------------------------------------------: | ----------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |\n|        linux py3.9 \\[gpus\\]        |                                                                                                             |                                                                                                             |                                                                                                             | ![build status](https://dev.azure.com/lightning-ai/lightning/_apis/build/status%2fpytorch-lightning%20%28gpus%29) |\n|        linux py3.9 \\[tpus\\]        |                                                                                                             |                                                                                                             | ![test pytorch - tpu](https://github.com/lightning-ai/lightning/actions/workflows/tpu-tests.yml/badge.svg)  |                                                                                                                   |\n|  linux (multiple python versions)  | ![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg) | ![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg) | ![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg) | ![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)       |\n|   osx (multiple python versions)   | ![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg) | ![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg) | ![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg) | ![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)       |\n| windows (multiple python versions) | ![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg) | ![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg) | ![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg) | ![test pytorch](https://github.com/lightning-ai/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)       |\n\n</center>\n</details>\n\n______________________________________________________________________\n\n## how to use\n\n### step 0: install\n\nsimple installation from pypi\n\n```bash\npip install pytorch-lightning\n```\n\n<!--  -->\n\n### step 1: add these imports\n\n```python\nimport os\nimport torch\nfrom torch import nn\nimport torch.nn.functional as f\nfrom torchvision.datasets import mnist\nfrom torch.utils.data import dataloader, random_split\nfrom torchvision import transforms\nimport pytorch_lightning as pl\n```\n\n### step 2: define a lightningmodule (nn.module subclass)\n\na lightningmodule defines a full *system* (ie: a gan, autoencoder, bert or a simple image classifier).\n\n```python\nclass litautoencoder(pl.lightningmodule):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.sequential(nn.linear(28 * 28, 128), nn.relu(), nn.linear(128, 3))\n        self.decoder = nn.sequential(nn.linear(3, 128), nn.relu(), nn.linear(128, 28 * 28))\n\n    def forward(self, x):\n        # in lightning, forward defines the prediction/inference actions\n        embedding = self.encoder(x)\n        return embedding\n\n    def training_step(self, batch, batch_idx):\n        # training_step defines the train loop. it is independent of forward\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = f.mse_loss(x_hat, x)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.adam(self.parameters(), lr=1e-3)\n        return optimizer\n```\n\n**note: training_step defines the training loop. forward defines how the lightningmodule behaves during inference/prediction.**\n\n### step 3: train!\n\n```python\ndataset = mnist(os.getcwd(), download=true, transform=transforms.totensor())\ntrain, val = random_split(dataset, [55000, 5000])\n\nautoencoder = litautoencoder()\ntrainer = pl.trainer()\ntrainer.fit(autoencoder, dataloader(train), dataloader(val))\n```\n\n## advanced features\n\nlightning has over [40+ advanced features](https://lightning.ai/docs/pytorch/stable/common/trainer.html#trainer-flags) designed for professional ai research at scale.\n\nhere are some examples:\n\n<div align=\"center\">\n  <img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/features_2.jpg\" max-height=\"600px\">\n</div>\n\n<details>\n  <summary>highlighted feature code snippets</summary>\n\n```python\n# 8 gpus\n# no code changes needed\ntrainer = trainer(max_epochs=1, accelerator=\"gpu\", devices=8)\n\n# 256 gpus\ntrainer = trainer(max_epochs=1, accelerator=\"gpu\", devices=8, num_nodes=32)\n```\n\n<summary>train on tpus without code changes</summary>\n\n```python\n# no code changes needed\ntrainer = trainer(accelerator=\"tpu\", devices=8)\n```\n\n<summary>16-bit precision</summary>\n\n```python\n# no code changes needed\ntrainer = trainer(precision=16)\n```\n\n<summary>experiment managers</summary>\n\n```python\nfrom pytorch_lightning import loggers\n\n# tensorboard\ntrainer = trainer(logger=tensorboardlogger(\"logs/\"))\n\n# weights and biases\ntrainer = trainer(logger=loggers.wandblogger())\n\n# comet\ntrainer = trainer(logger=loggers.cometlogger())\n\n# mlflow\ntrainer = trainer(logger=loggers.mlflowlogger())\n\n# neptune\ntrainer = trainer(logger=loggers.neptunelogger())\n\n# ... and dozens more\n```\n\n<summary>earlystopping</summary>\n\n```python\nes = earlystopping(monitor=\"val_loss\")\ntrainer = trainer(callbacks=[es])\n```\n\n<summary>checkpointing</summary>\n\n```python\ncheckpointing = modelcheckpoint(monitor=\"val_loss\")\ntrainer = trainer(callbacks=[checkpointing])\n```\n\n<summary>export to torchscript (jit) (production use)</summary>\n\n```python\n# torchscript\nautoencoder = litautoencoder()\ntorch.jit.save(autoencoder.to_torchscript(), \"model.pt\")\n```\n\n<summary>export to onnx (production use)</summary>\n\n```python\nautoencoder = litautoencoder()\ninput_sample = torch.randn((1, 64))\nwith tempfile.namedtemporaryfile(suffix=\".onnx\", delete=false) as tmpfile:\n    autoencoder.to_onnx(tmpfile.name, input_sample, export_params=true)\n```\n\n</details>\n\n### pro-level control of optimization (advanced users)\n\nfor complex/professional level work, you have optional full control of the optimizers.\n\n```python\nclass litautoencoder(pl.lightningmodule):\n    def __init__(self):\n        super().__init__()\n        self.automatic_optimization = false\n\n    def training_step(self, batch, batch_idx):\n        # access your optimizers with use_pl_optimizer=false. default is true\n        opt_a, opt_b = self.optimizers(use_pl_optimizer=true)\n\n        loss_a = ...\n        self.manual_backward(loss_a, opt_a)\n        opt_a.step()\n        opt_a.zero_grad()\n\n        loss_b = ...\n        self.manual_backward(loss_b, opt_b, retain_graph=true)\n        self.manual_backward(loss_b, opt_b)\n        opt_b.step()\n        opt_b.zero_grad()\n```\n\n______________________________________________________________________\n\n## advantages over unstructured pytorch\n\n- models become hardware agnostic\n- code is clear to read because engineering code is abstracted away\n- easier to reproduce\n- make fewer mistakes because lightning handles the tricky engineering\n- keeps all the flexibility (lightningmodules are still pytorch modules), but removes a ton of boilerplate\n- lightning has dozens of integrations with popular machine learning tools.\n- [tested rigorously with every new pr](https://github.com/lightning-ai/lightning/tree/master/tests). we test every combination of pytorch and python supported versions, every os, multi gpus and even tpus.\n- minimal running speed overhead (about 300 ms per epoch compared with pure pytorch).\n\n______________________________________________________________________\n\n## examples\n\n###### self-supervised learning\n\n- [cpc transforms](https://lightning-bolts.readthedocs.io/en/stable/transforms/self_supervised.html#cpc-transforms)\n- [moco v2 transforms](https://lightning-bolts.readthedocs.io/en/stable/transforms/self_supervised.html#moco-v2-transforms)\n- [simclr transforms](https://lightning-bolts.readthedocs.io/en/stable/transforms/self_supervised.html#simclr-transforms)\n\n###### convolutional architectures\n\n- [gpt-2](https://lightning-bolts.readthedocs.io/en/stable/models/convolutional.html#gpt-2)\n- [unet](https://lightning-bolts.readthedocs.io/en/stable/models/convolutional.html#unet)\n\n###### reinforcement learning\n\n- [dqn loss](https://lightning-bolts.readthedocs.io/en/stable/losses.html#dqn-loss)\n- [double dqn loss](https://lightning-bolts.readthedocs.io/en/stable/losses.html#double-dqn-loss)\n- [per dqn loss](https://lightning-bolts.readthedocs.io/en/stable/losses.html#per-dqn-loss)\n\n###### gans\n\n- [basic gan](https://lightning-bolts.readthedocs.io/en/stable/models/gans.html#basic-gan)\n- [dcgan](https://lightning-bolts.readthedocs.io/en/stable/models/gans.html#dcgan)\n\n###### classic ml\n\n- [logistic regression](https://lightning-bolts.readthedocs.io/en/stable/models/classic_ml.html#logistic-regression)\n- [linear regression](https://lightning-bolts.readthedocs.io/en/stable/models/classic_ml.html#linear-regression)\n\n______________________________________________________________________\n\n## community\n\nthe pytorch lightning community is maintained by\n\n- [10+ core contributors](https://lightning.ai/docs/pytorch/stable/community/governance.html) who are all a mix of professional engineers, research scientists, and ph.d. students from top ai labs.\n- 680+ active community contributors.\n\nwant to help us build lightning and reduce boilerplate for thousands of researchers? [learn how to make your first contribution here](https://devblog.pytorchlightning.ai/quick-contribution-guide-86d977171b3a)\n\npytorch lightning is also part of the [pytorch ecosystem](https://pytorch.org/ecosystem/) which requires projects to have solid testing, documentation and support.\n\n### asking for help\n\nif you have any questions please:\n\n1. [read the docs](https://lightning.ai/docs/pytorch/stable).\n1. [search through existing discussions](https://github.com/lightning-ai/lightning/discussions), or [add a new question](https://github.com/lightning-ai/lightning/discussions/new)\n1. [join our discord community](https://discord.gg/vptpczkgna).\n\n\n",
  "docs_url": null,
  "keywords": "deep learning,pytorch,ai",
  "license": "apache-2.0",
  "name": "pytorch-lightning",
  "package_url": "https://pypi.org/project/pytorch-lightning/",
  "project_url": "https://pypi.org/project/pytorch-lightning/",
  "project_urls": {
    "Bug Tracker": "https://github.com/Lightning-AI/lightning/issues",
    "Documentation": "https://pytorch-lightning.rtfd.io/en/latest/",
    "Download": "https://github.com/Lightning-AI/lightning",
    "Homepage": "https://github.com/Lightning-AI/lightning",
    "Source Code": "https://github.com/Lightning-AI/lightning"
  },
  "release_url": "https://pypi.org/project/pytorch-lightning/2.1.3/",
  "requires_dist": [
    "numpy >=1.17.2",
    "torch >=1.12.0",
    "tqdm >=4.57.0",
    "PyYAML >=5.4",
    "fsspec[http] >=2022.5.0",
    "torchmetrics >=0.7.0",
    "packaging >=20.0",
    "typing-extensions >=4.0.0",
    "lightning-utilities >=0.8.0",
    "matplotlib >3.1 ; extra == 'all'",
    "omegaconf >=2.0.5 ; extra == 'all'",
    "hydra-core >=1.0.5 ; extra == 'all'",
    "jsonargparse[signatures] >=4.26.1 ; extra == 'all'",
    "rich >=12.3.0 ; extra == 'all'",
    "tensorboardX >=2.2 ; extra == 'all'",
    "bitsandbytes <=0.41.1 ; extra == 'all'",
    "torchvision >=0.13.0 ; extra == 'all'",
    "gym[classic_control] >=0.17.0 ; extra == 'all'",
    "ipython[all] <8.15.0 ; extra == 'all'",
    "torchmetrics >=0.10.0 ; extra == 'all'",
    "lightning-utilities >=0.8.0 ; extra == 'all'",
    "deepspeed <=0.9.3,>=0.8.2 ; (platform_system != \"Windows\") and extra == 'all'",
    "deepspeed <=0.9.3,>=0.8.2 ; (platform_system != \"Windows\") and extra == 'deepspeed'",
    "matplotlib >3.1 ; extra == 'dev'",
    "omegaconf >=2.0.5 ; extra == 'dev'",
    "hydra-core >=1.0.5 ; extra == 'dev'",
    "jsonargparse[signatures] >=4.26.1 ; extra == 'dev'",
    "rich >=12.3.0 ; extra == 'dev'",
    "tensorboardX >=2.2 ; extra == 'dev'",
    "bitsandbytes <=0.41.1 ; extra == 'dev'",
    "torchvision >=0.13.0 ; extra == 'dev'",
    "gym[classic_control] >=0.17.0 ; extra == 'dev'",
    "ipython[all] <8.15.0 ; extra == 'dev'",
    "torchmetrics >=0.10.0 ; extra == 'dev'",
    "lightning-utilities >=0.8.0 ; extra == 'dev'",
    "coverage ==7.3.1 ; extra == 'dev'",
    "pytest ==7.4.0 ; extra == 'dev'",
    "pytest-cov ==4.1.0 ; extra == 'dev'",
    "pytest-timeout ==2.1.0 ; extra == 'dev'",
    "pytest-rerunfailures ==12.0 ; extra == 'dev'",
    "pytest-random-order ==1.1.0 ; extra == 'dev'",
    "cloudpickle >=1.3 ; extra == 'dev'",
    "scikit-learn >0.22.1 ; extra == 'dev'",
    "onnx >=0.14.0 ; extra == 'dev'",
    "onnxruntime >=0.15.0 ; extra == 'dev'",
    "psutil <5.9.6 ; extra == 'dev'",
    "pandas >1.0 ; extra == 'dev'",
    "fastapi ; extra == 'dev'",
    "uvicorn ; extra == 'dev'",
    "tensorboard >=2.9.1 ; extra == 'dev'",
    "deepspeed <=0.9.3,>=0.8.2 ; (platform_system != \"Windows\") and extra == 'dev'",
    "torchvision >=0.13.0 ; extra == 'examples'",
    "gym[classic_control] >=0.17.0 ; extra == 'examples'",
    "ipython[all] <8.15.0 ; extra == 'examples'",
    "torchmetrics >=0.10.0 ; extra == 'examples'",
    "lightning-utilities >=0.8.0 ; extra == 'examples'",
    "matplotlib >3.1 ; extra == 'extra'",
    "omegaconf >=2.0.5 ; extra == 'extra'",
    "hydra-core >=1.0.5 ; extra == 'extra'",
    "jsonargparse[signatures] >=4.26.1 ; extra == 'extra'",
    "rich >=12.3.0 ; extra == 'extra'",
    "tensorboardX >=2.2 ; extra == 'extra'",
    "bitsandbytes <=0.41.1 ; extra == 'extra'",
    "deepspeed <=0.9.3,>=0.8.2 ; (platform_system != \"Windows\") and extra == 'strategies'",
    "coverage ==7.3.1 ; extra == 'test'",
    "pytest ==7.4.0 ; extra == 'test'",
    "pytest-cov ==4.1.0 ; extra == 'test'",
    "pytest-timeout ==2.1.0 ; extra == 'test'",
    "pytest-rerunfailures ==12.0 ; extra == 'test'",
    "pytest-random-order ==1.1.0 ; extra == 'test'",
    "cloudpickle >=1.3 ; extra == 'test'",
    "scikit-learn >0.22.1 ; extra == 'test'",
    "onnx >=0.14.0 ; extra == 'test'",
    "onnxruntime >=0.15.0 ; extra == 'test'",
    "psutil <5.9.6 ; extra == 'test'",
    "pandas >1.0 ; extra == 'test'",
    "fastapi ; extra == 'test'",
    "uvicorn ; extra == 'test'",
    "tensorboard >=2.9.1 ; extra == 'test'"
  ],
  "requires_python": ">=3.8",
  "summary": "pytorch lightning is the lightweight pytorch wrapper for ml researchers. scale your models. write less boilerplate.",
  "version": "2.1.3",
  "releases": [],
  "developers": [
    "lightning_ai_et_al",
    "pytorch@lightning.ai"
  ],
  "kwds": "pytorchlightning pytorch_lightning pytorch torchscript tensorboard",
  "license_kwds": "apache-2.0",
  "libtype": "pypi",
  "id": "pypi_pytorch_lightning",
  "homepage": "https://github.com/lightning-ai/lightning",
  "release_count": 188,
  "dependency_ids": [
    "pypi_bitsandbytes",
    "pypi_cloudpickle",
    "pypi_coverage",
    "pypi_deepspeed",
    "pypi_fastapi",
    "pypi_fsspec",
    "pypi_gym",
    "pypi_hydra_core",
    "pypi_ipython",
    "pypi_jsonargparse",
    "pypi_lightning_utilities",
    "pypi_matplotlib",
    "pypi_numpy",
    "pypi_omegaconf",
    "pypi_onnx",
    "pypi_onnxruntime",
    "pypi_packaging",
    "pypi_pandas",
    "pypi_psutil",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_pytest_random_order",
    "pypi_pytest_rerunfailures",
    "pypi_pytest_timeout",
    "pypi_pyyaml",
    "pypi_rich",
    "pypi_scikit_learn",
    "pypi_tensorboard",
    "pypi_tensorboardx",
    "pypi_torch",
    "pypi_torchmetrics",
    "pypi_torchvision",
    "pypi_tqdm",
    "pypi_typing_extensions",
    "pypi_uvicorn"
  ]
}