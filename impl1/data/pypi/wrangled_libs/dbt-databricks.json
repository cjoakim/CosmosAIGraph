{
  "classifiers": [
    "development status :: 5 - production/stable",
    "license :: osi approved :: apache software license",
    "operating system :: macos :: macos x",
    "operating system :: microsoft :: windows",
    "operating system :: posix :: linux",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "<p align=\"center\">\n  <img src=\"https://bynder-public-us-west-2.s3.amazonaws.com/styleguide/abb317701ca31cb7f29268e32b303cae-pdf-column-1.png\" alt=\"databricks logo\" width=\"50%\" />\n  <img src=\"https://raw.githubusercontent.com/dbt-labs/dbt/ec7dee39f793aa4f7dd3dae37282cc87664813e4/etc/dbt-logo-full.svg\" alt=\"dbt logo\" width=\"250\"/>\n</p>\n<p align=\"center\">\n  <a href=\"https://github.com/databricks/dbt-databricks/actions/workflows/main.yml\">\n    <img src=\"https://github.com/databricks/dbt-databricks/actions/workflows/main.yml/badge.svg?event=push\" alt=\"unit tests badge\"/>\n  </a>\n  <a href=\"https://github.com/databricks/dbt-databricks/actions/workflows/integration.yml\">\n    <img src=\"https://github.com/databricks/dbt-databricks/actions/workflows/integration.yml/badge.svg?event=push\" alt=\"integration tests badge\"/>\n  </a>\n</p>\n\n**[dbt](https://www.getdbt.com/)** enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications.\n\nthe **[databricks lakehouse](https://www.databricks.com/)** provides one simple platform to unify all your data, analytics and ai workloads.\n\n# dbt-databricks\n\nthe `dbt-databricks` adapter contains all of the code enabling dbt to work with databricks. this adapter is based off the amazing work done in [dbt-spark](https://github.com/dbt-labs/dbt-spark). some key features include:\n\n- **easy setup**. no need to install an odbc driver as the adapter uses pure python apis.\n- **open by default**. for example, it uses the the open and performant [delta](https://delta.io/) table format by default. this has many benefits, including letting you use `merge` as the the default incremental materialization strategy.\n- **support for unity catalog**. dbt-databricks>=1.1.1 supports the 3-level namespace of unity catalog (catalog / schema / relations) so you can organize and secure your data the way you like.\n- **performance**. the adapter generates sql expressions that are automatically accelerated by the native, vectorized [photon](https://databricks.com/product/photon) execution engine.\n\n## choosing between dbt-databricks and dbt-spark\nif you are developing a dbt project on databricks, we recommend using `dbt-databricks` for the reasons noted above.\n\n`dbt-spark` is an actively developed adapter which works with databricks as well as apache spark anywhere it is hosted e.g. on aws emr.\n\n## getting started\n\n### installation\n\ninstall using pip:\n```nofmt\npip install dbt-databricks\n```\n\nupgrade to the latest version\n```nofmt\npip install --upgrade dbt-databricks\n```\n\n### profile setup\n\n```nofmt\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      type: databricks\n      catalog: [optional catalog name, if you are using unity catalog, only available in dbt-databricks>=1.1.1]\n      schema: [database/schema name]\n      host: [your.databrickshost.com]\n      http_path: [/sql/your/http/path]\n      token: [dapixxxxxxxxxxxxxxxxxxxxxxx]\n```\n\n### quick starts\n\nthese following quick starts will get you up and running with the `dbt-databricks` adapter:\n- [developing your first dbt project](https://github.com/databricks/dbt-databricks/blob/main/docs/local-dev.md)\n- using dbt cloud with databricks ([azure](https://docs.microsoft.com/en-us/azure/databricks/integrations/prep/dbt-cloud) | [aws](https://docs.databricks.com/integrations/prep/dbt-cloud.html))\n- [running dbt production jobs on databricks workflows](https://github.com/databricks/dbt-databricks/blob/main/docs/databricks-workflows.md)\n- [using unity catalog with dbt-databricks](https://github.com/databricks/dbt-databricks/blob/main/docs/uc.md)\n- [using github actions for dbt ci/cd on databricks](https://github.com/databricks/dbt-databricks/blob/main/docs/github-actions.md)\n- [loading data from s3 into delta using the databricks_copy_into macro](https://github.com/databricks/dbt-databricks/blob/main/docs/databricks-copy-into-macro-aws.md)\n- [contribute to this repository](contributing.md)\n\n### compatibility\n\nthe `dbt-databricks` adapter has been tested:\n\n- with python 3.7 or above.\n- against `databricks sql` and `databricks runtime releases 9.1 lts` and later.\n\n### tips and tricks\n## choosing compute for a python model\nyou can override the compute used for a specific python model by setting the `http_path` property in model configuration. this can be useful if, for example, you want to run a python model on an all purpose cluster, while running sql models on a sql warehouse. note that this capability is only available for python models.\n\n```\ndef model(dbt, session):\n    dbt.config(\n      http_path=\"sql/protocolv1/...\"\n    )\n```\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "dbt-databricks",
  "package_url": "https://pypi.org/project/dbt-databricks/",
  "project_url": "https://pypi.org/project/dbt-databricks/",
  "project_urls": {
    "Homepage": "https://github.com/databricks/dbt-databricks"
  },
  "release_url": "https://pypi.org/project/dbt-databricks/1.7.3/",
  "requires_dist": [
    "dbt-spark ==1.7.1",
    "databricks-sql-connector <3.0.0,>=2.9.3",
    "databricks-sdk >=0.9.0",
    "keyring >=23.13.0"
  ],
  "requires_python": ">=3.8",
  "summary": "the databricks adapter plugin for dbt",
  "version": "1.7.3",
  "releases": [],
  "developers": [
    "databricks",
    "feedback@databricks.com"
  ],
  "kwds": "databrickshost databricks databricks_copy_into dbt logo",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_dbt_databricks",
  "homepage": "https://github.com/databricks/dbt-databricks",
  "release_count": 93,
  "dependency_ids": [
    "pypi_databricks_sdk",
    "pypi_databricks_sql_connector",
    "pypi_dbt_spark",
    "pypi_keyring"
  ]
}