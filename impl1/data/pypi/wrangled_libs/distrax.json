{
  "classifiers": [
    "development status :: 5 - production/stable",
    "environment :: console",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "operating system :: os independent",
    "programming language :: python",
    "programming language :: python :: 3",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: scientific/engineering :: mathematics",
    "topic :: scientific/engineering :: physics",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "# distrax\n\n![ci status](https://github.com/deepmind/distrax/workflows/tests/badge.svg)\n![pypi](https://img.shields.io/pypi/v/distrax)\n\ndistrax is a lightweight library of probability distributions and bijectors. it\nacts as a jax-native reimplementation of a subset of\n[tensorflow probability](https://www.tensorflow.org/probability) (tfp), with\nsome new features and emphasis on extensibility.\n\n## installation\n\nyou can install the latest released version of distrax from pypi via:\n\n```sh\npip install distrax\n```\n\nor you can install the latest development version from github:\n\n```sh\npip install git+https://github.com/deepmind/distrax.git\n```\n\nto run the tests or\n[examples](https://github.com/deepmind/distrax/tree/master/examples) you will\nneed to install additional [requirements](https://github.com/deepmind/distrax/tree/master/requirements).\n\n## design principles\n\nthe general design principles for the deepmind jax ecosystem are addressed in\n[this blog](https://deepmind.com/blog/article/using-jax-to-accelerate-our-research).\nadditionally, distrax places emphasis on the following:\n\n1. **readability.** distrax implementations are intended to be self-contained\nand read as close to the underlying math as possible.\n2. **extensibility.** we have made it as simple as possible for users to define\ntheir own distribution or bijector. this is useful for example in reinforcement\nlearning, where users may wish to define custom behavior for probabilistic agent\npolicies.\n3. **compatibility.** distrax is not intended as a replacement for tfp, and tfp\ncontains many advanced features that we do not intend to replicate. to this end,\nwe have made the apis for distributions and bijectors as cross-compatible as\npossible, and provide utilities for transforming between equivalent distrax and\ntfp classes.\n\n## features\n\n### distributions\n\ndistributions in distrax are simple to define and use, particularly if you're\nused to tfp. let's compare the two side-by-side:\n\n```python\nimport distrax\nimport jax\nimport jax.numpy as jnp\n\nfrom tensorflow_probability.substrates import jax as tfp\ntfd = tfp.distributions\n\nkey = jax.random.prngkey(1234)\nmu = jnp.array([-1., 0., 1.])\nsigma = jnp.array([0.1, 0.2, 0.3])\n\ndist_distrax = distrax.multivariatenormaldiag(mu, sigma)\ndist_tfp = tfd.multivariatenormaldiag(mu, sigma)\n\nsamples = dist_distrax.sample(seed=key)\n\n# both print 1.775\nprint(dist_distrax.log_prob(samples))\nprint(dist_tfp.log_prob(samples))\n```\n\nin addition to behaving consistently, distrax distributions and tfp\ndistributions are cross-compatible. for example:\n\n```python\nmu_0 = jnp.array([-1., 0., 1.])\nsigma_0 = jnp.array([0.1, 0.2, 0.3])\ndist_distrax = distrax.multivariatenormaldiag(mu_0, sigma_0)\n\nmu_1 = jnp.array([1., 2., 3.])\nsigma_1 = jnp.array([0.2, 0.3, 0.4])\ndist_tfp = tfd.multivariatenormaldiag(mu_1, sigma_1)\n\n# both print 85.237\nprint(dist_distrax.kl_divergence(dist_tfp))\nprint(tfd.kl_divergence(dist_distrax, dist_tfp))\n```\n\ndistrax distributions implement the method `sample_and_log_prob`, which provides\nsamples and their log-probability in one line. for some distributions, this is\nmore efficient than calling separately `sample` and `log_prob`:\n\n```python\nmu = jnp.array([-1., 0., 1.])\nsigma = jnp.array([0.1, 0.2, 0.3])\ndist_distrax = distrax.multivariatenormaldiag(mu, sigma)\n\nsamples = dist_distrax.sample(seed=key, sample_shape=())\nlog_prob = dist_distrax.log_prob(samples)\n\n# a one-line equivalent of the above is:\nsamples, log_prob = dist_distrax.sample_and_log_prob(seed=key, sample_shape=())\n```\n\ntfp distributions can be passed to distrax meta-distributions as inputs. for\nexample:\n\n```python\nkey = jax.random.prngkey(1234)\n\nmu = jnp.array([-1., 0., 1.])\nsigma = jnp.array([0.2, 0.3, 0.4])\ndist_tfp = tfd.normal(mu, sigma)\n\nmetadist_distrax = distrax.independent(dist_tfp, reinterpreted_batch_ndims=1)\nsamples = metadist_distrax.sample(seed=key)\nprint(metadist_distrax.log_prob(samples))  # prints 0.38871175\n```\n\nto use distrax distributions in tfp meta-distributions, distrax provides the\nwrapper `to_tfp`. a wrapped distrax distribution can be directly used in tfp:\n\n```python\nkey = jax.random.prngkey(1234)\n\ndistrax_dist = distrax.normal(0., 1.)\nwrapped_dist = distrax.to_tfp(distrax_dist)\nmetadist_tfp = tfd.sample(wrapped_dist, sample_shape=[3])\n\nsamples = metadist_tfp.sample(seed=key)\nprint(metadist_tfp.log_prob(samples))  # prints -3.3409896\n```\n\n### bijectors\n\na \"bijector\" in distrax is an invertible function that knows how to compute its\njacobian determinant. bijectors can be used to create complex distributions by\ntransforming simpler ones. distrax bijectors are functionally similar to tfp\nbijectors, with a few api differences. here is an example comparing the two:\n\n```python\nimport distrax\nimport jax.numpy as jnp\n\nfrom tensorflow_probability.substrates import jax as tfp\ntfb = tfp.bijectors\ntfd = tfp.distributions\n\n# same distribution.\ndistrax.transformed(distrax.normal(loc=0., scale=1.), distrax.tanh())\ntfd.transformeddistribution(tfd.normal(loc=0., scale=1.), tfb.tanh())\n```\n\nadditionally, distrax bijectors can be composed and inverted:\n\n```python\nbij_distrax = distrax.tanh()\nbij_tfp = tfb.tanh()\n\n# same bijector.\ninv_bij_distrax = distrax.inverse(bij_distrax)\ninv_bij_tfp = tfb.invert(bij_tfp)\n\n# these are both the identity bijector.\ndistrax.chain([bij_distrax, inv_bij_distrax])\ntfb.chain([bij_tfp, inv_bij_tfp])\n```\n\nall tfp bijectors can be passed to distrax, and can be freely composed with\ndistrax bijectors. for example, all of the following will work:\n\n```python\ndistrax.inverse(tfb.tanh())\n\ndistrax.chain([tfb.tanh(), distrax.tanh()])\n\ndistrax.transformed(tfd.normal(loc=0., scale=1.), tfb.tanh())\n```\n\ndistrax bijectors can also be passed to tfp, but first they must be transformed\nwith `to_tfp`:\n\n```python\nbij_distrax = distrax.to_tfp(distrax.tanh())\n\ntfb.invert(bij_distrax)\n\ntfb.chain([tfb.tanh(), bij_distrax])\n\ntfd.transformeddistribution(tfd.normal(loc=0., scale=1.), bij_distrax)\n```\n\ndistrax also comes with `lambda`, a convenient wrapper for turning simple jax\nfunctions into bijectors. here are a few `lambda` examples with their tfp\nequivalents:\n\n```python\ndistrax.lambda(lambda x: x)\n# tfb.identity()\n\ndistrax.lambda(lambda x: 2*x + 3)\n# tfb.chain([tfb.shift(3), tfb.scale(2)])\n\ndistrax.lambda(jnp.sinh)\n# tfb.sinh()\n\ndistrax.lambda(lambda x: jnp.sinh(2*x + 3))\n# tfb.chain([tfb.sinh(), tfb.shift(3), tfb.scale(2)])\n```\n\nunlike tfp, bijectors in distrax do not take `event_ndims` as an argument when\nthey compute the jacobian determinant. instead, distrax assumes that the number\nof event dimensions is statically known to every bijector, and uses\n`block` to lift bijectors to a different number of dimensions. for example:\n\n```python\nx = jnp.zeros([2, 3, 4])\n\n# in tfp, `event_ndims` can be passed to the bijector.\nbij_tfp = tfb.tanh()\nld_1 = bij_tfp.forward_log_det_jacobian(x, event_ndims=0)  # shape = [2, 3, 4]\n\n# distrax assumes `tanh` is a scalar bijector by default.\nbij_distrax = distrax.tanh()\nld_2 = bij_distrax.forward_log_det_jacobian(x)  # ld_1 == ld_2\n\n# with `event_ndims=2`, tfp sums the last 2 dimensions of the log det.\nld_3 = bij_tfp.forward_log_det_jacobian(x, event_ndims=2)  # shape = [2]\n\n# distrax treats the number of dimensions statically.\nbij_distrax = distrax.block(bij_distrax, ndims=2)\nld_4 = bij_distrax.forward_log_det_jacobian(x)  # ld_3 == ld_4\n```\n\ndistrax bijectors implement the method `forward_and_log_det` (some bijectors\nadditionally implement `inverse_and_log_det`), which allows to obtain the\n`forward` mapping and its log jacobian determinant in one line. for some\nbijectors, this is more efficient than calling separately `forward` and\n`forward_log_det_jacobian`. (analogously, when available, `inverse_and_log_det`\ncan be more efficient than `inverse` and `inverse_log_det_jacobian`.)\n\n```python\nx = jnp.zeros([2, 3, 4])\nbij_distrax = distrax.tanh()\n\ny = bij_distrax.forward(x)\nld = bij_distrax.forward_log_det_jacobian(x)\n\n# a one-line equivalent of the above is:\ny, ld = bij_distrax.forward_and_log_det(x)\n```\n\n### jitting distrax\n\ndistrax distributions and bijectors can be passed as arguments to jitted\nfunctions. user-defined distributions and bijectors get this property for free\nby subclassing `distrax.distribution` and `distrax.bijector` respectively. for\nexample:\n\n```python\nmu_0 = jnp.array([-1., 0., 1.])\nsigma_0 = jnp.array([0.1, 0.2, 0.3])\ndist_0 = distrax.multivariatenormaldiag(mu_0, sigma_0)\n\nmu_1 = jnp.array([1., 2., 3.])\nsigma_1 = jnp.array([0.2, 0.3, 0.4])\ndist_1 = distrax.multivariatenormaldiag(mu_1, sigma_1)\n\njitted_kl = jax.jit(lambda d_0, d_1: d_0.kl_divergence(d_1))\n\n# both print 85.237\nprint(jitted_kl(dist_0, dist_1))\nprint(dist_0.kl_divergence(dist_1))\n```\n\n##### a note about `vmap` and `pmap`\n\nthe serialization logic that enables distrax objects to be passed as arguments\nto jitted functions also enables functions to map over them as data using\n`jax.vmap` and `jax.pmap`.\n\nhowever, ***support for this behavior is experimental and incomplete. use\ncaution when applying `jax.vmap` or `jax.pmap` to functions that take distrax\nobjects as arguments, or return distrax objects.***\n\nsimple objects such as `distrax.categorical` may behave correctly under these\ntransformations, but more complex objects such as\n`distrax.multivariatenormaldiag` may generate exceptions when used as inputs or\noutputs of a `vmap`-ed or `pmap`-ed function.\n\n\n### subclassing distributions and bijectors\n\nuser-defined distributions can be created by subclassing `distrax.distribution`.\nthis can be achieved by implementing only a few methods:\n\n```python\nclass mydistribution(distrax.distribution):\n\n  def __init__(self, ...):\n    ...\n\n  def _sample_n(self, key, n):\n    samples = ...\n    return samples\n\n  def log_prob(self, value):\n    log_prob = ...\n    return log_prob\n\n  def event_shape(self):\n    event_shape = ...\n    return event_shape\n\n  def _sample_n_and_log_prob(self, key, n):\n    # optional. only when more efficient implementation is possible.\n    samples, log_prob = ...\n    return samples, log_prob\n```\n\nsimilarly, more complicated bijectors can be created by subclassing\n`distrax.bijector`. this can be achieved by implementing only one or two class\nmethods:\n\n```python\nclass mybijector(distrax.bijector):\n\n  def __init__(self, ...):\n    super().__init__(...)\n\n  def forward_and_log_det(self, x):\n    y = ...\n    logdet = ...\n    return y, logdet\n\n  def inverse_and_log_det(self, y):\n    # optional. can be omitted if inverse methods are not needed.\n    x = ...\n    logdet = ...\n    return x, logdet\n```\n\n## examples\n\nthe `examples` directory contains some representative examples of full programs\nthat use distrax.\n\n`hmm.py` demonstrates how to use `distrax.hmm` to combine distributions that\nmodel the initial states, transitions, and observation distributions of a\nhidden markov model, and infer the latent rates and state transitions in a\nchanging noisy signal.\n\n`vae.py` contains an example implementation of a variational auto-encoder that\nis trained to model the binarized mnist dataset as a joint `distrax.bernoulli`\ndistribution over the pixels.\n\n`flow.py` illustrates a simple example of modelling mnist data using\n`distrax.maskedcoupling` layers to implement a normalizing flow, and training\nthe model with gradient descent.\n\n## acknowledgements\n\nwe greatly appreciate the ongoing support of the tensorflow probability authors\nin assisting with the design and cross-compatibility of distrax.\n\nspecial thanks to aleyna kara and kevin murphy for contributing the code upon\nwhich the hidden markov model and associated example are based.\n\n## citing distrax\n\nthis repository is part of the deepmind jax ecosystem, to cite distrax\nplease use the citation:\n\n```bibtex\n@software{deepmind2020jax,\n  title = {the {d}eep{m}ind {jax} {e}cosystem},\n  author = {deepmind and babuschkin, igor and baumli, kate and bell, alison and bhupatiraju, surya and bruce, jake and buchlovsky, peter and budden, david and cai, trevor and clark, aidan and danihelka, ivo and dedieu, antoine and fantacci, claudio and godwin, jonathan and jones, chris and hemsley, ross and hennigan, tom and hessel, matteo and hou, shaobo and kapturowski, steven and keck, thomas and kemaev, iurii and king, michael and kunesch, markus and martens, lena and merzic, hamza and mikulik, vladimir and norman, tamara and papamakarios, george and quan, john and ring, roman and ruiz, francisco and sanchez, alvaro and sartran, laurent and schneider, rosalia and sezener, eren and spencer, stephen and srinivasan, srivatsan and stanojevi\\'{c}, milo\\v{s} and stokowiec, wojciech and wang, luyu and zhou, guangyao and viola, fabio},\n  url = {http://github.com/deepmind},\n  year = {2020},\n}\n```\n",
  "docs_url": null,
  "keywords": "jax probability distribution python machine learning",
  "license": "apache 2.0",
  "name": "distrax",
  "package_url": "https://pypi.org/project/distrax/",
  "project_url": "https://pypi.org/project/distrax/",
  "project_urls": {
    "Homepage": "https://github.com/deepmind/distrax"
  },
  "release_url": "https://pypi.org/project/distrax/0.1.5/",
  "requires_dist": [
    "absl-py (>=0.9.0)",
    "chex (>=0.1.8)",
    "jax (>=0.1.55)",
    "jaxlib (>=0.1.67)",
    "numpy (>=1.23.0)",
    "tensorflow-probability (>=0.15.0)",
    "setuptools ; python_version >= \"3.12\""
  ],
  "requires_python": ">=3.9",
  "summary": "distrax: probability distributions in jax.",
  "version": "0.1.5",
  "releases": [],
  "developers": [
    "deepmind",
    "distrax-dev@google.com"
  ],
  "kwds": "tensorflow_probability tensorflow distrax dist_distrax distrax_dist",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_distrax",
  "homepage": "https://github.com/deepmind/distrax",
  "release_count": 9,
  "dependency_ids": [
    "pypi_absl_py",
    "pypi_chex",
    "pypi_jax",
    "pypi_jaxlib",
    "pypi_numpy",
    "pypi_setuptools",
    "pypi_tensorflow_probability"
  ]
}