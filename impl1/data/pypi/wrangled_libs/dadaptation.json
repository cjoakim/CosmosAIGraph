{
  "classifiers": [
    "license :: osi approved :: mit license",
    "operating system :: os independent",
    "programming language :: python :: 3"
  ],
  "description": "# d-adaptation\n[![downloads](https://static.pepy.tech/badge/dadaptation)](https://pepy.tech/project/dadaptation) [![downloads](https://static.pepy.tech/badge/dadaptation/month)](https://pepy.tech/project/dadaptation)\n\nlearning rate free learning for sgd, adagrad and adam! \n\n*by aaron defazio and konstantin mishchenko [(arxiv)](https://arxiv.org/abs/2301.07733)*\n\n``` pip install dadaptation ```\n\n**new v3.0 release uses an improved algorithm that may give different results from past versions. the old version is still availiable under experimental/d_adapt_adam_preprint.**\n\n## new: prodigy\nwe have recently released the [prodigy](https://github.com/konstmish/prodigy) method, which grows the adapted learning rate faster than d-adaptation in theory and practice. try it out if d-adaptation is under-estimating the learning rate.\n\n## how to cite\nif you use d-adaptation in a publication, please cite our work as \n```\n@article{defazio2023dadapt,\nauthor = {aaron defazio and konstantin mishchenko},\ntitle = {learning-rate-free learning by d-adaptation},\njournal = {the 40th international conference on machine learning (icml 2023)},\nyear = {2023}\n}\n```\n\n## details\n\nthe provided pytorch optimizer classes are drop-in replacements, either copy into your project or use via pip with dadaptation.dadaptsgd,  dadaptation.dadaptadam or dadaptation.dadaptadagrad.\n\n - **set the lr parameter to 1.0**. this parameter is not ignored. setting it larger to smaller will directly scale up or down the d-adapted learning rate estimate.\n - different per-layer learning rates can be achieved by setting the layer_scale value in each parameter-group. it defaults to 1.0, and scales each layer's learning rate relative to the other layers.\n - **use the same learning rate scheduler you would normally use on the problem.**\n - the adam variant supports adamw style weight decay, just set decouple=true. it is not turned on by default, so if you are replacing your adam implementation, make sure you use decoupled if necessary.\n - it may be necessary to use larger weight decay than you would normally use, try a factor of 2 or 4 bigger if you see overfitting. d-adaptation uses larger learning rates than people typically hand-choose, in some cases that requires more decay.\n - use the log_every setting to see the learning rate being used (d*lr) and the current d bound.\n - only the adagrad version supports sparse gradients. it does not adapt as efficiently as the other variants and should be considered experimental.\n \n## change log\n\n### version 3.2\n - added support for layer-wise scaling to dadaptadam.\n\n### version 3.0\n - major improvements to dadaptadam, improving the performance particularly on transformer models. this variant may behave differently in practice. the old version is availiable under experimental/d_adapt_adam_preprint if you wish to continue to use it.\n - the ip variant is now the main variant of the method.\n - added lion. this is highly experimental. feedback on it's performance is welcome.\n\n### version 2.0\n - added adan - should still be considered experimental.\n - added support for pytorch's fully sharded data parallel. \n - improved support of edge cases such as learning rate zero.\n - improved logging - uses python logging rather than print statements\n\n # experimental results\n\n![vision](figures/dadapt_cifar.png)\n![vision](figures/dadapt_cifar100.png)\n![vision](figures/dadapt_imagenet.png)\n![vision](figures/dadapt_vit.png)\n![vision](figures/dadapt_lstm.png)\n![vision](figures/dadapt_roberta.png)\n![vision](figures/dadapt_gpt.png)\n![vision](figures/dadapt_fastmri.png)\n![vision](figures/dadapt_detectron.png)\n![vision](figures/dadapt_dlrm.png)\n\n# license\nsee the [license file](/license).\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "dadaptation",
  "package_url": "https://pypi.org/project/dadaptation/",
  "project_url": "https://pypi.org/project/dadaptation/",
  "project_urls": {
    "Homepage": "https://github.com/facebookresearch/dadaptation"
  },
  "release_url": "https://pypi.org/project/dadaptation/3.2/",
  "requires_dist": [],
  "requires_python": ">=3.6",
  "summary": "learning rate free learning for adam, sgd and adagrad",
  "version": "3.2",
  "releases": [],
  "developers": [
    "aaron_defazio",
    "adefazio@meta.com"
  ],
  "kwds": "prodigy d_adapt_adam_preprint dadapt_detectron adaptation dadapt_imagenet",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_dadaptation",
  "homepage": "https://github.com/facebookresearch/dadaptation",
  "release_count": 9,
  "dependency_ids": []
}