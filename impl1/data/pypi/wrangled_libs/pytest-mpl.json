{
  "classifiers": [
    "development status :: 4 - beta",
    "framework :: pytest",
    "intended audience :: developers",
    "license :: osi approved :: bsd license",
    "operating system :: os independent",
    "programming language :: python",
    "programming language :: python :: 3",
    "topic :: scientific/engineering :: visualization",
    "topic :: software development :: testing"
  ],
  "description": "about\n-----\n\nthis is a plugin to facilitate image comparison for\n`matplotlib <http://www.matplotlib.org>`__ figures in pytest.\n\nfor each figure to test, the reference image is subtracted from the\ngenerated image, and the rms of the residual is compared to a\nuser-specified tolerance. if the residual is too large, the test will\nfail (this is implemented using helper functions from\n``matplotlib.testing``).\n\nfor more information on how to write tests to do this, see the **using**\nsection below.\n\ninstalling\n----------\n\nthis plugin is compatible with python 3.6 and later, and\nrequires `pytest <http://pytest.org>`__ and\n`matplotlib <http://www.matplotlib.org>`__ to be installed.\n\nto install, you can do::\n\n    pip install pytest-mpl\n\nyou can check that the plugin is registered with pytest by doing::\n\n    pytest --version\n\nwhich will show a list of plugins:\n\n::\n\n    this is pytest version 2.7.1, imported from ...\n    setuptools registered plugins:\n      pytest-mpl-0.1 at ...\n\nusing\n-----\n\nwith baseline images\n^^^^^^^^^^^^^^^^^^^^\n\nto use, you simply need to mark the function where you want to compare\nimages using ``@pytest.mark.mpl_image_compare``, and make sure that the\nfunction returns a matplotlib figure (or any figure object that has a\n``savefig`` method):\n\n.. code:: python\n\n    import pytest\n    import matplotlib.pyplot as plt\n\n    @pytest.mark.mpl_image_compare\n    def test_succeeds():\n        fig = plt.figure()\n        ax = fig.add_subplot(1,1,1)\n        ax.plot([1,2,3])\n        return fig\n\nto generate the baseline images, run the tests with the\n``--mpl-generate-path`` option with the name of the directory where the\ngenerated images should be placed::\n\n    pytest --mpl-generate-path=baseline\n\nif the directory does not exist, it will be created. the directory will\nbe interpreted as being relative to where you are running ``pytest``.\nonce you are happy with the generated images, you should move them to a\nsub-directory called ``baseline`` relative to the test files (this name\nis configurable, see below). you can also generate the baseline image\ndirectly in the right directory.\n\nwith a hash library\n^^^^^^^^^^^^^^^^^^^\n\ninstead of comparing to baseline images, you can instead compare against a json\nlibrary of sha-256 hashes. this has the advantage of not having to check baseline\nimages into the repository with the tests, or download them from a remote\nsource.\n\nthe hash library can be generated with\n``--mpl-generate-hash-library=path_to_file.json``. the hash library to be used\ncan either be specified via the ``--mpl-hash-library=`` command line argument,\nor via the ``hash_library=`` keyword argument to the\n``@pytest.mark.mpl_image_compare`` decorator.\n\nwhen generating a hash library, the tests will also be run as usual against the\nexisting hash library specified by ``--mpl-hash-library`` or the keyword argument.\nhowever, generating baseline images will always result in the tests being skipped.\n\n\nhybrid mode: hashes and images\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nit is possible to configure both hashes and baseline images. in this scenario\nonly the hash comparison can determine the test result. if the hash comparison\nfails, the test will fail, however a comparison to the baseline image will be\ncarried out so the actual difference can be seen. if the hash comparison passes,\nthe comparison to the baseline image is skipped (unless **results always** is\nconfigured).\n\nthis is especially useful if the baseline images are external to the repository\ncontaining the tests, and are accessed via http. in this situation, if the hashes\nmatch, the baseline images won't be retrieved, saving time and bandwidth. also, it\nallows the tests to be modified and the hashes updated to reflect the changes\nwithout having to modify the external images.\n\n\nrunning tests\n^^^^^^^^^^^^^\n\nonce tests are written with baseline images, a hash library, or both to compare\nagainst, the tests can be run with::\n\n    pytest --mpl\n\nand the tests will pass if the images are the same. if you omit the\n``--mpl`` option, the tests will run but will only check that the code\nruns, without checking the output images.\n\n\ngenerating a test summary\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nby specifying the ``--mpl-generate-summary=html`` cli argument, a html summary\npage will be generated showing the test result, log entry and generated result\nimage. when in the (default) image comparison mode, the baseline image, diff\nimage and rms (if any), and tolerance of each test will also be shown.\nwhen in the hash comparison mode, the baseline hash and result hash will\nalso be shown. when in hybrid mode, all of these are included.\n\nwhen generating a html summary, the ``--mpl-results-always`` option is\nautomatically applied (see section below). therefore images for passing\ntests will also be shown.\n\n+---------------+---------------+---------------+\n| |html all|    | |html filter| | |html result| |\n+---------------+---------------+---------------+\n\nas well as ``html``, ``basic-html`` can be specified for an alternative html\nsummary which does not rely on javascript or external resources. a ``json``\nsummary can also be saved. multiple options can be specified comma-separated.\n\noptions\n-------\n\ntolerance\n^^^^^^^^^\n\nthe rms tolerance for the image comparison (which defaults to 2) can be\nspecified in the ``mpl_image_compare`` decorator with the ``tolerance``\nargument:\n\n.. code:: python\n\n    @pytest.mark.mpl_image_compare(tolerance=20)\n    def test_image():\n        ...\n\nsavefig options\n^^^^^^^^^^^^^^^\n\nyou can pass keyword arguments to ``savefig`` by using\n``savefig_kwargs`` in the ``mpl_image_compare`` decorator:\n\n.. code:: python\n\n    @pytest.mark.mpl_image_compare(savefig_kwargs={'dpi':300})\n    def test_image():\n        ...\n\nbaseline images\n^^^^^^^^^^^^^^^\n\nthe baseline directory (which defaults to ``baseline`` ) and the\nfilename of the plot (which defaults to the name of the test with a\n``.png`` suffix) can be customized with the ``baseline_dir`` and\n``filename`` arguments in the ``mpl_image_compare`` decorator:\n\n.. code:: python\n\n    @pytest.mark.mpl_image_compare(baseline_dir='baseline_images',\n                                   filename='other_name.png')\n    def test_image():\n        ...\n\nthe baseline directory in the decorator above will be interpreted as\nbeing relative to the test file. note that the baseline directory can\nalso be a url (which should start with ``http://`` or ``https://`` and\nend in a slash). if you want to specify mirrors, set ``baseline_dir`` to\na comma-separated list of urls (real commas in the url should be encoded\nas ``%2c``).\n\nfinally, you can also set a custom baseline directory globally when\nrunning tests by running ``pytest`` with::\n\n    pytest --mpl --mpl-baseline-path=baseline_images\n\nthis directory will be interpreted as being relative to where pytest\nis run. however, if the ``--mpl-baseline-relative`` option is also\nincluded, this directory will be interpreted as being relative to\nthe current test directory.\nin addition, if both this option and the ``baseline_dir``\noption in the ``mpl_image_compare`` decorator are used, the one in the\ndecorator takes precedence.\n\nresults always\n^^^^^^^^^^^^^^\n\nby default, result images are only saved for tests that fail.\npassing ``--mpl-results-always`` to pytest will force result images\nto be saved for all tests, even for tests that pass.\n\nwhen in **hybrid mode**, even if a test passes hash comparison,\na comparison to the baseline image will also be carried out,\nwith the baseline image and diff image (if image comparison fails)\nsaved for all tests. this secondary comparison will not affect\nthe success status of the test.\n\nthis option is useful for always *comparing* the result images against\nthe baseline images, while only *assessing* the tests against the\nhash library.\nif you only update your baseline images after merging a pr, this\noption means that the generated summary will always show how the\npr affects the baseline images, with the success status of each\ntest (based on the hash library) also shown in the generated\nsummary. this option is applied automatically when generating\na html summary.\n\nwhen the ``--mpl-results-always`` option is active, and some hash\ncomparison tests are performed, a hash library containing all the\nresult hashes will also be saved to the root of the results directory.\nthe filename will be extracted from ``--mpl-generate-hash-library``,\n``--mpl-hash-library`` or ``hash_library=`` in that order.\n\nbase style\n^^^^^^^^^^\n\nby default, tests will be run using the matplotlib 'classic' style\n(ignoring any locally defined rc parameters). this can be overridden by\nusing the ``style`` argument:\n\n.. code:: python\n\n    @pytest.mark.mpl_image_compare(style='fivethirtyeight')\n    def test_image():\n        ...\n\npackage version dependencies\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ndifferent versions of matplotlib and freetype may result in slightly\ndifferent images. when testing on multiple platforms or as part of a\npipeline, it is important to ensure that the versions of these\npackages match the versions used to generate the images used for\ncomparison. it can be useful to pin versions of matplotlib and freetype\nso as to avoid automatic updates that fail tests.\n\nremoving text\n^^^^^^^^^^^^^\n\nif you are running a test for which you are not interested in comparing\nthe text labels, you can use the ``remove_text`` argument to the\ndecorator:\n\n.. code:: python\n\n    @pytest.mark.mpl_image_compare(remove_text=true)\n    def test_image():\n        ...\n\nthis will make the test insensitive to changes in e.g. the freetype\nlibrary.\n\ntest failure example\n--------------------\n\nif the images produced by the tests are correct, then the test will\npass, but if they are not, the test will fail with a message similar to\nthe following::\n\n    e               exception: error: image files did not match.\n    e                 rms value: 142.2287807767823\n    e                 expected:\n    e                   /var/folders/zy/t1l3sx310d3d6p0kyxqzlrnr0000gr/t/tmp4h4oxr7y/baseline-coords_overlay_auto_coord_meta.png\n    e                 actual:\n    e                   /var/folders/zy/t1l3sx310d3d6p0kyxqzlrnr0000gr/t/tmp4h4oxr7y/coords_overlay_auto_coord_meta.png\n    e                 difference:\n    e                   /var/folders/zy/t1l3sx310d3d6p0kyxqzlrnr0000gr/t/tmp4h4oxr7y/coords_overlay_auto_coord_meta-failed-diff.png\n    e                 tolerance:\n    e                   10\n\nthe image paths included in the exception are then available for\ninspection:\n\n+----------------+----------------+-------------+\n| expected       | actual         | difference  |\n+================+================+=============+\n| |expected|     | |actual|       | |diff|      |\n+----------------+----------------+-------------+\n\nin this case, the differences are very clear, while in some cases it may\nbe necessary to use the difference image, or blink the expected and\nactual images, in order to see what changed.\n\nthe default tolerance is 2, which is very strict. in some cases, you may\nwant to relax this to account for differences in fonts across different\nsystems.\n\nby default, the expected, actual and difference files are written to a\ntemporary directory with a non-deterministic path. if you want to instead\nwrite them to a specific directory, you can use::\n\n    pytest --mpl --mpl-results-path=results\n\nthe ``results`` directory will then contain one sub-directory per test, and each\nsub-directory will contain the three files mentioned above. if you are using a\ncontinuous integration service, you can then use the option to upload artifacts\nto upload these results to somewhere where you can view them. for more\ninformation, see:\n\n* `uploading artifacts on travis-ci <https://docs.travis-ci.com/user/uploading-artifacts/>`_\n* `build artifacts (circleci) <https://circleci.com/docs/1.0/build-artifacts/>`_\n* `packaging artifacts (appveyor) <https://www.appveyor.com/docs/packaging-artifacts/>`_\n\nrunning the tests for pytest-mpl\n--------------------------------\n\nif you are contributing some changes and want to run the tests, first\ninstall the latest version of the plugin then do::\n\n    cd tests\n    pytest --mpl\n\nthe reason for having to install the plugin first is to ensure that the\nplugin is correctly loaded as part of the test suite.\n\n.. |html all| image:: images/html_all.png\n.. |html filter| image:: images/html_filter.png\n.. |html result| image:: images/html_result.png\n.. |expected| image:: images/baseline-coords_overlay_auto_coord_meta.png\n.. |actual| image:: images/coords_overlay_auto_coord_meta.png\n.. |diff| image:: images/coords_overlay_auto_coord_meta-failed-diff.png\n",
  "docs_url": null,
  "keywords": "",
  "license": "bsd",
  "name": "pytest-mpl",
  "package_url": "https://pypi.org/project/pytest-mpl/",
  "project_url": "https://pypi.org/project/pytest-mpl/",
  "project_urls": {
    "Homepage": "https://github.com/matplotlib/pytest-mpl"
  },
  "release_url": "https://pypi.org/project/pytest-mpl/0.16.1/",
  "requires_dist": [
    "pytest",
    "matplotlib",
    "packaging",
    "Jinja2",
    "Pillow",
    "importlib-resources ; python_version < \"3.8\"",
    "pytest-cov ; extra == 'test'"
  ],
  "requires_python": ">=3.6",
  "summary": "pytest plugin to help with testing figures output from matplotlib",
  "version": "0.16.1",
  "releases": [],
  "developers": [
    "thomas.robitaille@gmail.com",
    "thomas_robitaille"
  ],
  "kwds": "matplotlib test_image mpl_image_compare pytest pyplot",
  "license_kwds": "bsd",
  "libtype": "pypi",
  "id": "pypi_pytest_mpl",
  "homepage": "https://github.com/matplotlib/pytest-mpl",
  "release_count": 21,
  "dependency_ids": [
    "pypi_importlib_resources",
    "pypi_jinja2",
    "pypi_matplotlib",
    "pypi_packaging",
    "pypi_pillow",
    "pypi_pytest",
    "pypi_pytest_cov"
  ]
}