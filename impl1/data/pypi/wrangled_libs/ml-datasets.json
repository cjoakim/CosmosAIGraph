{
  "classifiers": [],
  "description": "<a href=\"https://explosion.ai\"><img src=\"https://explosion.ai/assets/img/logo.svg\" width=\"125\" height=\"125\" align=\"right\" /></a>\n\n# machine learning dataset loaders for testing and examples\n\nloaders for various machine learning datasets for testing and example scripts.\npreviously in `thinc.extra.datasets`.\n\n[![pypi version](https://img.shields.io/pypi/v/ml-datasets.svg?style=flat-square&logo=pypi&logocolor=white)](https://pypi.python.org/pypi/ml-datasets)\n\n## setup and installation\n\nthe package can be installed via pip:\n\n```bash\npip install ml-datasets\n```\n\n## loaders\n\nloaders can be imported directly or used via their string name (which is useful if they're set via command line arguments). some loaders may take arguments \u2013 see the source for details.\n\n```python\n# import directly\nfrom ml_datasets import imdb\ntrain_data, dev_data = imdb()\n```\n\n```python\n# load via registry\nfrom ml_datasets import loaders\nimdb_loader = loaders.get(\"imdb\")\ntrain_data, dev_data = imdb_loader()\n```\n\n### available loaders\n\n#### nlp datasets\n\n| id / function        | description                                  | nlp task                                  | from url |\n| -------------------- | -------------------------------------------- | ----------------------------------------- | :------: |\n| `imdb`               | imdb sentiment dataset                       | binary classification: sentiment analysis |    \u2713     |\n| `dbpedia`            | dbpedia ontology dataset                     | multi-class single-label classification   |    \u2713     |\n| `cmu`                | cmu movie genres dataset                     | multi-class, multi-label classification   |    \u2713     |\n| `quora_questions`    | duplicate quora questions dataset            | detecting duplicate questions             |    \u2713     |\n| `reuters`            | reuters dataset (texts not included)         | multi-class multi-label classification    |    \u2713     |\n| `snli`               | stanford natural language inference corpus   | recognizing textual entailment            |    \u2713     |\n| `stack_exchange`     | stack exchange dataset                       | question answering                        |          |\n| `ud_ancora_pos_tags` | universal dependencies spanish ancora corpus | pos tagging                               |    \u2713     |\n| `ud_ewtb_pos_tags`   | universal dependencies english ewt corpus    | pos tagging                               |    \u2713     |\n| `wikiner`            | wikiner data                                 | named entity recognition                  |          |\n\n#### other ml datasets\n\n| id / function | description | ml task           | from url |\n| ------------- | ----------- | ----------------- | :------: |\n| `mnist`       | mnist data  | image recognition |    \u2713     |\n\n### dataset details\n\n#### imdb\n\neach instance contains the text of a movie review, and a sentiment expressed as `0` or `1`.\n\n```python\ntrain_data, dev_data = ml_datasets.imdb()\nfor text, annot in train_data[0:5]:\n    print(f\"review: {text}\")\n    print(f\"sentiment: {annot}\")\n```\n\n- download url: [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)\n- citation: [andrew l. maas et al., 2011](https://www.aclweb.org/anthology/p11-1015/)\n\n| property            | training         | dev              |\n| ------------------- | ---------------- | ---------------- |\n| # instances         | 25000            | 25000            |\n| label values        | {`0`, `1`}       | {`0`, `1`}       |\n| labels per instance | single           | single           |\n| label distribution  | balanced (50/50) | balanced (50/50) |\n\n#### dbpedia\n\neach instance contains an ontological description, and a classification into one of the 14 distinct labels.\n\n```python\ntrain_data, dev_data = ml_datasets.dbpedia()\nfor text, annot in train_data[0:5]:\n    print(f\"text: {text}\")\n    print(f\"category: {annot}\")\n```\n\n- download url: [via fast.ai](https://course.fast.ai/datasets)\n- original citation: [xiang zhang et al., 2015](https://arxiv.org/abs/1509.01626)\n\n| property            | training | dev      |\n| ------------------- | -------- | -------- |\n| # instances         | 560000   | 70000    |\n| label values        | `1`-`14` | `1`-`14` |\n| labels per instance | single   | single   |\n| label distribution  | balanced | balanced |\n\n#### cmu\n\neach instance contains a movie description, and a classification into a list of appropriate genres.\n\n```python\ntrain_data, dev_data = ml_datasets.cmu()\nfor text, annot in train_data[0:5]:\n    print(f\"text: {text}\")\n    print(f\"genres: {annot}\")\n```\n\n- download url: [http://www.cs.cmu.edu/~ark/personas/](http://www.cs.cmu.edu/~ark/personas/)\n- original citation: [david bamman et al., 2013](https://www.aclweb.org/anthology/p13-1035/)\n\n| property            | training                                                                                      | dev |\n| ------------------- | --------------------------------------------------------------------------------------------- | --- |\n| # instances         | 41793                                                                                         | 0   |\n| label values        | 363 different genres                                                                          | -   |\n| labels per instance | multiple                                                                                      | -   |\n| label distribution  | imbalanced: 147 labels with less than 20 examples, while `drama` occurs more than 19000 times | -   |\n\n#### quora\n\n```python\ntrain_data, dev_data = ml_datasets.quora_questions()\nfor questions, annot in train_data[0:50]:\n    q1, q2 = questions\n    print(f\"question 1: {q1}\")\n    print(f\"question 2: {q2}\")\n    print(f\"similarity: {annot}\")\n```\n\neach instance contains two quora questions, and a label indicating whether or not they are duplicates (`0`: no, `1`: yes).\nthe ground-truth labels contain some amount of noise: they are not guaranteed to be perfect.\n\n- download url: [http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv](http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv)\n- original citation: [korn\u00e9l csernai et al., 2017](https://www.quora.com/q/quoradata/first-quora-dataset-release-question-pairs)\n\n| property            | training                  | dev                       |\n| ------------------- | ------------------------- | ------------------------- |\n| # instances         | 363859                    | 40429                     |\n| label values        | {`0`, `1`}                | {`0`, `1`}                |\n| labels per instance | single                    | single                    |\n| label distribution  | imbalanced: 63% label `0` | imbalanced: 63% label `0` |\n\n### registering loaders\n\nloaders can be registered externally using the `loaders` registry as a decorator. for example:\n\n```python\n@ml_datasets.loaders(\"my_custom_loader\")\ndef my_custom_loader():\n    return load_some_data()\n\nassert \"my_custom_loader\" in ml_datasets.loaders\n```\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "ml-datasets",
  "package_url": "https://pypi.org/project/ml-datasets/",
  "project_url": "https://pypi.org/project/ml-datasets/",
  "project_urls": {
    "Homepage": "https://github.com/explosion/ml-datasets"
  },
  "release_url": "https://pypi.org/project/ml-datasets/0.2.0/",
  "requires_dist": [
    "numpy (>=1.7.0)",
    "tqdm (<5.0.0,>=4.10.0)",
    "srsly (<3.0.0,>=1.0.1)",
    "catalogue (<3.0.0,>=0.2.0)"
  ],
  "requires_python": ">=3.6",
  "summary": "machine learning dataset loaders",
  "version": "0.2.0",
  "releases": [],
  "developers": [
    "contact@explosion.ai",
    "explosion"
  ],
  "kwds": "pypi ml_datasets dataset datasets loaders",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_ml_datasets",
  "homepage": "https://github.com/explosion/ml-datasets",
  "release_count": 12,
  "dependency_ids": [
    "pypi_catalogue",
    "pypi_numpy",
    "pypi_srsly",
    "pypi_tqdm"
  ]
}