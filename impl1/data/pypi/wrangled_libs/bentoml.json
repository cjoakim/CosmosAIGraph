{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "operating system :: os independent",
    "programming language :: python :: 3",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "programming language :: python :: implementation :: cpython",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: software development :: libraries"
  ],
  "description": "<div align=\"center\">\n  <img src=\"https://github.com/bentoml/bentoml/assets/489344/398274c1-a572-477b-b115-52497a085496\" width=\"180px\" alt=\"bentoml\" />\n  <h1 align=\"center\">bentoml: the unified ai application framework</h1>\n  <a href=\"https://pypi.org/project/bentoml\"><img src=\"https://img.shields.io/pypi/v/bentoml.svg\" alt=\"pypi_status\" /></a>\n  <a href=\"https://github.com/bentoml/bentoml/actions/workflows/ci.yml?query=branch%3amain\"><img src=\"https://github.com/bentoml/bentoml/workflows/ci/badge.svg?branch=main\" alt=\"ci\" /></a>\n  <a href=\"https://twitter.com/bentomlai\"><img src=\"https://badgen.net/badge/icon/@bentomlai/1da1f2?icon=twitter&label=follow%20us\" alt=\"twitter\" /></a>\n  <a href=\"https://join.slack.bentoml.org\"><img src=\"https://badgen.net/badge/join/community/cyan?icon=slack\" alt=\"community\" /></a>\n  <p>bentoml is a framework for building <b>reliable, scalable, and cost-efficient ai\napplications</b>. it comes with everything you need for model serving, application\npackaging, and production deployment.</p>\n  <i><a href=\"https://l.bentoml.com/join-slack\">\ud83d\udc49 join our slack community!</a></i>\n</div>\n\n# highlights\n\n### \ud83c\udf71 bento is the container for ai apps\n\n- open standard and sdk for ai apps, pack your code, inference pipelines, model\n  files, dependencies, and runtime configurations in a\n  [bento](https://docs.bentoml.com/en/latest/concepts/bento.html).\n- auto-generate api servers, supporting rest api, grpc, and long-running\n  inference jobs.\n- auto-generate docker container images.\n\n### \ud83c\udfc4 freedom to build with any ai models\n\n- import from any model hub or bring your own models built with frameworks like\n  pytorch, tensorflow, keras, scikit-learn, xgboost and\n  [many more](https://docs.bentoml.com/en/latest/frameworks/index.html).\n- native support for\n  [llm inference](https://github.com/bentoml/openllm/#bentoml),\n  [generative ai](https://github.com/bentoml/stable-diffusion-bentoml),\n  [embedding creation](https://github.com/bentoml/clip-api-service), and\n  [multi-modal ai apps](https://github.com/bentoml/distributed-visual-chatgpt).\n- run and debug your bentoml apps locally on mac, windows, or linux.\n\n### \ud83c\udf6d simplify modern ai application architecture\n\n- python-first! effortlessly scale complex ai workloads.\n- enable gpu inference\n  [without the headache](https://docs.bentoml.com/en/latest/guides/gpu.html).\n- [compose multiple models](https://docs.bentoml.com/en/latest/guides/graph.html)\n  to run concurrently or sequentially, over\n  [multiple gpus](https://docs.bentoml.com/en/latest/guides/scheduling.html) or\n  [on a kubernetes cluster](https://github.com/bentoml/yatai).\n- natively integrates with\n  [mlflow](https://docs.bentoml.com/en/latest/integrations/mlflow.html),\n  [langchain](https://github.com/ssheng/bentochain),\n  [kubeflow](https://www.kubeflow.org/docs/external-add-ons/serving/bentoml/),\n  [triton](https://docs.bentoml.com/en/latest/integrations/triton.html),\n  [spark](https://docs.bentoml.com/en/latest/integrations/spark.html),\n  [ray](https://docs.bentoml.com/en/latest/integrations/ray.html), and many more\n  to complete your production ai stack.\n\n### \ud83d\ude80 deploy anywhere\n\n- one-click deployment to [\u2601\ufe0f bentocloud](https://bentoml.com/cloud), the\n  serverless platform made for hosting and operating ai apps.\n- scalable bentoml deployment with [\ud83e\udd84\ufe0f yatai](https://github.com/bentoml/yatai)\n  on kubernetes.\n- deploy auto-generated container images anywhere docker runs.\n\n# documentation\n\n- installation: `pip install bentoml`\n- full documentation: [docs.bentoml.com](https://docs.bentoml.com/en/latest/)\n- tutorial: [intro to bentoml](https://docs.bentoml.com/en/latest/tutorial.html)\n\n### \ud83d\udee0\ufe0f what you can build with bentoml\n\n- [openllm](https://github.com/bentoml/openllm) - an open platform for operating\n  large language models (llms) in production.\n- [stablediffusion](https://github.com/bentoml/stable-diffusion-bentoml) -\n  create your own text-to-image service with any diffusion models.\n- [clip-api-service](https://github.com/bentoml/clip-api-service) - embed images\n  and sentences, object recognition, visual reasoning, image classification, and\n  reverse image search.\n- [transformer nlp service](https://github.com/bentoml/transformers-nlp-service) -\n  online inference api for transformer nlp models.\n- [distributed taskmatrix(visual chatgpt)](https://github.com/bentoml/distributed-visual-chatgpt) -\n  scalable deployment for taskmatrix from microsoft.\n- [fraud detection](https://github.com/bentoml/fraud-detection-model-serving) -\n  online model serving with custom xgboost model.\n- [ocr as a service](https://github.com/bentoml/ocr-as-a-service) - turn any ocr\n  models into online inference api endpoints.\n- [replace anything](https://github.com/yuqwu/replace-anything) - combine the\n  power of segment anything and stable diffusion.\n- [deepfloyd if multi-gpu serving](https://github.com/bentoml/if-multi-gpus-demo) -\n  serve if models easily across multiple gpus.\n- [sentence embedding as a service](https://github.com/bentoml/sentence-embedding-bento) -\n  start a high-performance rest api server for generating text embeddings with one command.\n- check out more examples\n  [here](https://github.com/bentoml/bentoml/tree/main/examples).\n\n# getting started\n\nsave or import models in bentoml local model store:\n\n```python\nimport bentoml\nimport transformers\n\npipe = transformers.pipeline(\"text-classification\")\n\nbentoml.transformers.save_model(\n  \"text-classification-pipe\",\n  pipe,\n  signatures={\n    \"__call__\": {\"batchable\": true}  # enable dynamic batching for model\n  }\n)\n```\n\nview all models saved locally:\n\n```bash\n$ bentoml models list\n\ntag                                     module                size        creation time\ntext-classification-pipe:kn6mr3aubcuf\u2026  bentoml.transformers  256.35 mib  2023-05-17 14:36:25\n```\n\ndefine how your model runs in a `service.py` file:\n\n```python\nimport bentoml\n\nmodel_runner = bentoml.models.get(\"text-classification-pipe\").to_runner()\n\nsvc = bentoml.service(\"text-classification-service\", runners=[model_runner])\n\n@svc.api(input=bentoml.io.text(), output=bentoml.io.json())\nasync def classify(text: str) -> str:\n    results = await model_runner.async_run([text])\n    return results[0]\n```\n\nnow, run the api service locally:\n\n```bash\nbentoml serve service.py:svc\n```\n\nsent a prediction request:\n\n```bash\n$ curl -x post -h \"content-type: text/plain\" --data \"bentoml is awesome\" http://localhost:3000/classify\n\n{\"label\":\"positive\",\"score\":0.9129443168640137}%\n```\n\ndefine how a [bento](https://docs.bentoml.com/en/latest/concepts/bento.html) can\nbe built for deployment, with `bentofile.yaml`:\n\n```yaml\nservice: 'service.py:svc'\nname: text-classification-svc\ninclude:\n  - 'service.py'\npython:\n  packages:\n  - torch>=2.0\n  - transformers\n```\n\nbuild a bento and generate a docker image:\n\n```bash\n$ bentoml build\n...\nsuccessfully built bento(tag=\"text-classification-svc:mc322vaubkuapuqj\").\n```\n\n```bash\n$ bentoml containerize text-classification-svc\nbuilding oci-compliant image for text-classification-svc:mc322vaubkuapuqj with docker\n...\nsuccessfully built bento container for \"text-classification-svc\" with tag(s) \"text-classification-svc:mc322vaubkuapuqj\"\n```\n\n```bash\n$ docker run -p 3000:3000 text-classification-svc:mc322vaubkuapuqj\n```\n\nfor a more detailed user guide, check out the\n[bentoml tutorial](https://docs.bentoml.com/en/latest/tutorial.html).\n\n---\n\n## community\n\nbentoml supports billions of model runs per day and is used by thousands of\norganizations around the globe.\n\njoin our [community slack \ud83d\udcac](https://l.bentoml.com/join-slack), where thousands\nof ai application developers contribute to the project and help each other.\n\nto report a bug or suggest a feature request, use\n[github issues](https://github.com/bentoml/bentoml/issues/new/choose).\n\n## contributing\n\nthere are many ways to contribute to the project:\n\n- report bugs and \"thumbs up\" on issues that are relevant to you.\n- investigate issues and review other developers' pull requests.\n- contribute code or documentation to the project by submitting a github pull\n  request.\n- check out the\n  [contributing guide](https://github.com/bentoml/bentoml/blob/main/contributing.md)\n  and\n  [development guide](https://github.com/bentoml/bentoml/blob/main/development.md)\n  to learn more\n- share your feedback and discuss roadmap plans in the `#bentoml-contributors`\n  channel [here](https://l.bentoml.com/join-slack).\n\nthanks to all of our amazing contributors!\n\n<a href=\"https://github.com/bentoml/bentoml/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=bentoml/bentoml\" />\n</a>\n\n---\n\n### usage reporting\n\nbentoml collects usage data that helps our team to improve the product. only\nbentoml's internal api calls are being reported. we strip out as much\npotentially sensitive information as possible, and we will never collect user\ncode, model data, model names, or stack traces. here's the\n[code](./src/bentoml/_internal/utils/analytics/usage_stats.py) for usage\ntracking. you can opt-out of usage tracking by the `--do-not-track` cli option:\n\n```bash\nbentoml [command] --do-not-track\n```\n\nor by setting environment variable `bentoml_do_not_track=true`:\n\n```bash\nexport bentoml_do_not_track=true\n```\n\n---\n\n### license\n\n[apache license 2.0](https://github.com/bentoml/bentoml/blob/main/license)\n\n[![fossa status](https://app.fossa.com/api/projects/git%2bgithub.com%2fbentoml%2fbentoml.svg?type=small)](https://app.fossa.com/projects/git%2bgithub.com%2fbentoml%2fbentoml?ref=badge_small)\n\n### citation\n\nif you use bentoml in your research, please cite using the following\n[citation](./citation.cff:\n\n```bibtex\n@software{yang_bentoml_the_framework,\nauthor = {yang, chaoyu and sheng, sean and pham, aaron and  zhao, shenyang and lee, sauyon and jiang, bo and dong, fog and guan, xipeng and ming, frost},\nlicense = {apache-2.0},\ntitle = {{bentoml: the framework for building reliable, scalable and cost-efficient ai application}},\nurl = {https://github.com/bentoml/bentoml}\n}\n```\n",
  "docs_url": null,
  "keywords": "ai,bentoml,mlops,model deployment,model serving",
  "license": "apache-2.0",
  "name": "bentoml",
  "package_url": "https://pypi.org/project/bentoml/",
  "project_url": "https://pypi.org/project/bentoml/",
  "project_urls": {
    "Blog": "https://modelserving.com",
    "Documentation": "https://docs.bentoml.com",
    "GitHub": "https://github.com/bentoml/bentoml",
    "Homepage": "https://bentoml.com",
    "Slack": "https://l.bentoml.com/join-slack",
    "Tracker": "https://github.com/bentoml/BentoML/issues",
    "Twitter": "https://twitter.com/bentomlai"
  },
  "release_url": "https://pypi.org/project/bentoml/1.1.10/",
  "requires_dist": [
    "aiohttp",
    "attrs>=21.1.0",
    "cattrs<23.2.0,>=22.1.0",
    "circus!=0.17.2,>=0.17.0",
    "click-option-group",
    "click>=7.0",
    "cloudpickle>=2.0.0",
    "deepmerge",
    "fs",
    "httpx",
    "inflection",
    "jinja2>=3.0.1",
    "numpy",
    "nvidia-ml-py<12",
    "opentelemetry-api==1.20.0",
    "opentelemetry-instrumentation-aiohttp-client==0.41b0",
    "opentelemetry-instrumentation-asgi==0.41b0",
    "opentelemetry-instrumentation==0.41b0",
    "opentelemetry-sdk==1.20.0",
    "opentelemetry-semantic-conventions==0.41b0",
    "opentelemetry-util-http==0.41b0",
    "packaging>=22.0",
    "pathspec",
    "pip-requirements-parser>=31.2.0",
    "pip-tools>=6.6.2",
    "prometheus-client>=0.10.0",
    "psutil",
    "python-dateutil",
    "python-json-logger",
    "python-multipart",
    "pyyaml>=5.0",
    "requests",
    "rich>=11.2.0",
    "schema",
    "simple-di>=0.1.4",
    "starlette>=0.13.5",
    "uvicorn",
    "watchfiles>=0.15.0",
    "bentoml[aws,grpc,grpc-channelz,grpc-reflection,io,monitor-otlp,tracing]; extra == 'all'",
    "fs-s3fs; extra == 'aws'",
    "grpcio; extra == 'grpc'",
    "grpcio-health-checking; extra == 'grpc'",
    "opentelemetry-instrumentation-grpc==0.41b0; extra == 'grpc'",
    "protobuf; extra == 'grpc'",
    "bentoml[grpc]; extra == 'grpc-channelz'",
    "grpcio-channelz; extra == 'grpc-channelz'",
    "bentoml[grpc]; extra == 'grpc-reflection'",
    "grpcio-reflection; extra == 'grpc-reflection'",
    "bentoml[io-file,io-image,io-json,io-pandas]; extra == 'io'",
    "filetype; extra == 'io-file'",
    "bentoml[io-file]; extra == 'io-image'",
    "pillow; extra == 'io-image'",
    "pydantic<2; extra == 'io-json'",
    "pandas>=1; extra == 'io-pandas'",
    "pyarrow; extra == 'io-pandas'",
    "opentelemetry-exporter-otlp-proto-http==1.20.0; extra == 'monitor-otlp'",
    "bentoml[tracing-jaeger,tracing-otlp,tracing-zipkin]; extra == 'tracing'",
    "opentelemetry-exporter-jaeger==1.20.0; extra == 'tracing-jaeger'",
    "opentelemetry-exporter-otlp==1.20.0; extra == 'tracing-otlp'",
    "opentelemetry-exporter-zipkin==1.20.0; extra == 'tracing-zipkin'",
    "tritonclient[all]>=2.29.0; extra == 'triton'"
  ],
  "requires_python": ">=3.8",
  "summary": "bentoml: build production-grade ai applications",
  "version": "1.1.10",
  "releases": [],
  "developers": [
    "contact@bentoml.com"
  ],
  "kwds": "bentoml bento yang_bentoml_the_framework bentoml_do_not_track branch",
  "license_kwds": "apache-2.0",
  "libtype": "pypi",
  "id": "pypi_bentoml",
  "homepage": "",
  "release_count": 119,
  "dependency_ids": [
    "pypi_aiohttp",
    "pypi_attrs",
    "pypi_bentoml",
    "pypi_cattrs",
    "pypi_circus!",
    "pypi_click",
    "pypi_click_option_group",
    "pypi_cloudpickle",
    "pypi_deepmerge",
    "pypi_filetype",
    "pypi_fs",
    "pypi_fs_s3fs",
    "pypi_grpcio",
    "pypi_grpcio_channelz",
    "pypi_grpcio_health_checking",
    "pypi_grpcio_reflection",
    "pypi_httpx",
    "pypi_inflection",
    "pypi_jinja2",
    "pypi_numpy",
    "pypi_nvidia_ml_py",
    "pypi_opentelemetry_api",
    "pypi_opentelemetry_exporter_jaeger",
    "pypi_opentelemetry_exporter_otlp",
    "pypi_opentelemetry_exporter_otlp_proto_http",
    "pypi_opentelemetry_exporter_zipkin",
    "pypi_opentelemetry_instrumentation",
    "pypi_opentelemetry_instrumentation_aiohttp_client",
    "pypi_opentelemetry_instrumentation_asgi",
    "pypi_opentelemetry_instrumentation_grpc",
    "pypi_opentelemetry_sdk",
    "pypi_opentelemetry_semantic_conventions",
    "pypi_opentelemetry_util_http",
    "pypi_packaging",
    "pypi_pandas",
    "pypi_pathspec",
    "pypi_pillow",
    "pypi_pip_requirements_parser",
    "pypi_pip_tools",
    "pypi_prometheus_client",
    "pypi_protobuf",
    "pypi_psutil",
    "pypi_pyarrow",
    "pypi_pydantic",
    "pypi_python_dateutil",
    "pypi_python_json_logger",
    "pypi_python_multipart",
    "pypi_pyyaml",
    "pypi_requests",
    "pypi_rich",
    "pypi_schema",
    "pypi_simple_di",
    "pypi_starlette",
    "pypi_tritonclient",
    "pypi_uvicorn",
    "pypi_watchfiles"
  ]
}