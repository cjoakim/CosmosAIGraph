{
  "classifiers": [
    "development status :: 4 - beta",
    "intended audience :: developers",
    "license :: osi approved :: bsd license",
    "operating system :: os independent",
    "programming language :: python",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "programming language :: python :: implementation :: cpython",
    "programming language :: python :: implementation :: pypy"
  ],
  "description": "=======\nprotego\n=======\n\n.. image:: https://img.shields.io/pypi/pyversions/protego.svg\n   :target: https://pypi.python.org/pypi/protego\n   :alt: supported python versions\n\n.. image:: https://github.com/scrapy/protego/workflows/ci/badge.svg\n   :target: https://github.com/scrapy/protego/actions?query=workflow%3aci\n   :alt: ci\n\nprotego is a pure-python ``robots.txt`` parser with support for modern\nconventions.\n\n\ninstall\n=======\n\nto install protego, simply use pip:\n\n.. code-block:: none\n\n    pip install protego\n\n\nusage\n=====\n\n>>> from protego import protego\n>>> robotstxt = \"\"\"\n... user-agent: *\n... disallow: /\n... allow: /about\n... allow: /account\n... disallow: /account/contact$\n... disallow: /account/*/profile\n... crawl-delay: 4\n... request-rate: 10/1m                 # 10 requests every 1 minute\n... \n... sitemap: http://example.com/sitemap-index.xml\n... host: http://example.co.in\n... \"\"\"\n>>> rp = protego.parse(robotstxt)\n>>> rp.can_fetch(\"http://example.com/profiles\", \"mybot\")\nfalse\n>>> rp.can_fetch(\"http://example.com/about\", \"mybot\")\ntrue\n>>> rp.can_fetch(\"http://example.com/account\", \"mybot\")\ntrue\n>>> rp.can_fetch(\"http://example.com/account/myuser/profile\", \"mybot\")\nfalse\n>>> rp.can_fetch(\"http://example.com/account/contact\", \"mybot\")\nfalse\n>>> rp.crawl_delay(\"mybot\")\n4.0\n>>> rp.request_rate(\"mybot\")\nrequestrate(requests=10, seconds=60, start_time=none, end_time=none)\n>>> list(rp.sitemaps)\n['http://example.com/sitemap-index.xml']\n>>> rp.preferred_host\n'http://example.co.in'\n\nusing protego with requests_:\n\n>>> from protego import protego\n>>> import requests\n>>> r = requests.get(\"https://google.com/robots.txt\")\n>>> rp = protego.parse(r.text)\n>>> rp.can_fetch(\"https://google.com/search\", \"mybot\")\nfalse\n>>> rp.can_fetch(\"https://google.com/search/about\", \"mybot\")\ntrue\n>>> list(rp.sitemaps)\n['https://www.google.com/sitemap.xml']\n\n.. _requests: https://3.python-requests.org/\n\n\ncomparison\n==========\n\nthe following table compares protego to the most popular ``robots.txt`` parsers\nimplemented in python or featuring python bindings:\n\n+----------------------------+---------+-----------------+--------+---------------------------+\n|                            | protego | robotfileparser | reppy  | robotexclusionrulesparser |\n+============================+=========+=================+========+===========================+\n| implementation language    | python  | python          | c++    | python                    |\n+----------------------------+---------+-----------------+--------+---------------------------+\n| reference specification    | google_ | `martijn koster\u2019s 1996 draft`_                       |\n+----------------------------+---------+-----------------+--------+---------------------------+\n| `wildcard support`_        | \u2713       |                 | \u2713      | \u2713                         |\n+----------------------------+---------+-----------------+--------+---------------------------+\n| `length-based precedence`_ | \u2713       |                 | \u2713      |                           |\n+----------------------------+---------+-----------------+--------+---------------------------+\n| performance_               |         | +40%            | +1300% | -25%                      |\n+----------------------------+---------+-----------------+--------+---------------------------+\n\n.. _google: https://developers.google.com/search/reference/robots_txt\n.. _length-based precedence: https://developers.google.com/search/reference/robots_txt#order-of-precedence-for-group-member-lines\n.. _martijn koster\u2019s 1996 draft: https://www.robotstxt.org/norobots-rfc.txt\n.. _performance: https://anubhavp28.github.io/gsoc-weekly-checkin-12/\n.. _wildcard support: https://developers.google.com/search/reference/robots_txt#url-matching-based-on-path-values\n\n\napi reference\n=============\n\nclass ``protego.protego``:\n\nproperties\n----------\n\n*   ``sitemaps`` {``list_iterator``} a list of sitemaps specified in\n    ``robots.txt``.\n\n*   ``preferred_host`` {string} preferred host specified in ``robots.txt``.\n\n\nmethods\n-------\n\n*   ``parse(robotstxt_body)`` parse ``robots.txt`` and return a new instance of\n    ``protego.protego``.\n\n*   ``can_fetch(url, user_agent)`` return true if the user agent can fetch the\n    url, otherwise return ``false``.\n\n*   ``crawl_delay(user_agent)`` return the crawl delay specified for the user\n    agent as a float. if nothing is specified, return ``none``.\n\n*   ``request_rate(user_agent)`` return the request rate specified for the user\n    agent as a named tuple ``requestrate(requests, seconds, start_time,\n    end_time)``. if nothing is specified, return ``none``.\n\n*   ``visit_time(user_agent)`` return the visit time specified for the user \n    agent as a named tuple ``visittime(start_time, end_time)``. \n    if nothing is specified, return ``none``.\n",
  "docs_url": null,
  "keywords": "robots.txt,parser,robots,rep",
  "license": "bsd",
  "name": "protego",
  "package_url": "https://pypi.org/project/Protego/",
  "project_url": "https://pypi.org/project/Protego/",
  "project_urls": {
    "Homepage": "https://github.com/scrapy/protego"
  },
  "release_url": "https://pypi.org/project/Protego/0.3.0/",
  "requires_dist": [],
  "requires_python": ">=3.7",
  "summary": "pure-python robots.txt parser with support for modern conventions",
  "version": "0.3.0",
  "releases": [],
  "developers": [
    "anubhav_patel",
    "anubhavp28@gmail.com"
  ],
  "kwds": "robots_txt robotfileparser robotstxt_body pip scrapy",
  "license_kwds": "bsd",
  "libtype": "pypi",
  "id": "pypi_protego",
  "homepage": "https://github.com/scrapy/protego",
  "release_count": 9,
  "dependency_ids": []
}