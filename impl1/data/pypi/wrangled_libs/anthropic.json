{
  "classifiers": [
    "intended audience :: developers",
    "license :: osi approved :: mit license",
    "operating system :: macos",
    "operating system :: microsoft :: windows",
    "operating system :: os independent",
    "operating system :: posix",
    "operating system :: posix :: linux",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.12",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: software development :: libraries :: python modules",
    "typing :: typed"
  ],
  "description": "# anthropic python api library\n\n[![pypi version](https://img.shields.io/pypi/v/anthropic.svg)](https://pypi.org/project/anthropic/)\n\nthe anthropic python library provides convenient access to the anthropic rest api from any python 3.7+\napplication. it includes type definitions for all request params and response fields,\nand offers both synchronous and asynchronous clients powered by [httpx](https://github.com/encode/httpx).\n\nfor the aws bedrock api, see [`anthropic-bedrock`](https://github.com/anthropics/anthropic-bedrock-python).\n\n## documentation\n\nthe api documentation can be found [here](https://docs.anthropic.com/claude/reference/).\n\n## installation\n\n```sh\npip install anthropic\n```\n\n## usage\n\nthe full api of this library can be found in [api.md](https://www.github.com/anthropics/anthropic-sdk-python/blob/main/api.md).\n\n```python\nfrom anthropic import anthropic, human_prompt, ai_prompt\n\nanthropic = anthropic(\n    # defaults to os.environ.get(\"anthropic_api_key\")\n    api_key=\"my api key\",\n)\n\ncompletion = anthropic.completions.create(\n    model=\"claude-2.1\",\n    max_tokens_to_sample=300,\n    prompt=f\"{human_prompt} how does a court case get to the supreme court?{ai_prompt}\",\n)\nprint(completion.completion)\n```\n\nwhile you can provide an `api_key` keyword argument,\nwe recommend using [python-dotenv](https://pypi.org/project/python-dotenv/)\nto add `anthropic_api_key=\"my-anthropic-api-key\"` to your `.env` file\nso that your api key is not stored in source control.\n\n## async usage\n\nsimply import `asyncanthropic` instead of `anthropic` and use `await` with each api call:\n\n```python\nfrom anthropic import asyncanthropic, human_prompt, ai_prompt\n\nanthropic = asyncanthropic(\n    # defaults to os.environ.get(\"anthropic_api_key\")\n    api_key=\"my api key\",\n)\n\n\nasync def main():\n    completion = await anthropic.completions.create(\n        model=\"claude-2.1\",\n        max_tokens_to_sample=300,\n        prompt=f\"{human_prompt} how does a court case get to the supreme court?{ai_prompt}\",\n    )\n    print(completion.completion)\n\n\nasyncio.run(main())\n```\n\nfunctionality between the synchronous and asynchronous clients is otherwise identical.\n\n## streaming responses\n\nwe provide support for streaming responses using server side events (sse).\n\n```python\nfrom anthropic import anthropic, human_prompt, ai_prompt\n\nanthropic = anthropic()\n\nstream = anthropic.completions.create(\n    prompt=f\"{human_prompt} your prompt here{ai_prompt}\",\n    max_tokens_to_sample=300,\n    model=\"claude-2.1\",\n    stream=true,\n)\nfor completion in stream:\n    print(completion.completion, end=\"\", flush=true)\n```\n\nthe async client uses the exact same interface.\n\n```python\nfrom anthropic import asyncanthropic, human_prompt, ai_prompt\n\nanthropic = asyncanthropic()\n\nstream = await anthropic.completions.create(\n    prompt=f\"{human_prompt} your prompt here{ai_prompt}\",\n    max_tokens_to_sample=300,\n    model=\"claude-2.1\",\n    stream=true,\n)\nasync for completion in stream:\n    print(completion.completion, end=\"\", flush=true)\n```\n\n### streaming helpers\n\nthis library provides several conveniences for streaming messages, for example:\n\n```py\nimport asyncio\nfrom anthropic import asyncanthropic\n\nclient = asyncanthropic()\n\nasync def main() -> none:\n    async with client.beta.messages.stream(\n        max_tokens=1024,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"say hello there!\",\n            }\n        ],\n        model=\"claude-2.1\",\n    ) as stream:\n        async for text in stream.text_stream:\n            print(text, end=\"\", flush=true)\n        print()\n\n    message = await stream.get_final_message()\n    print(message.model_dump_json(indent=2))\n\nasyncio.run(main())\n```\n\nstreaming with `client.beta.messages.stream(...)` exposes [various helpers for your convenience](helpers.md) including event handlers and accumulation.\n\nalternatively, you can use `client.beta.messages.create(..., stream=true)` which only returns an async iterable of the events in the stream and thus uses less memory (it does not build up a final message object for you).\n\n## token counting\n\nyou can estimate billing for a given request with the `client.count_tokens()` method, eg:\n\n```py\nclient = anthropic()\nclient.count_tokens('hello world!')  # 3\n```\n\n## using types\n\nnested request parameters are [typeddicts](https://docs.python.org/3/library/typing.html#typing.typeddict). responses are [pydantic models](https://docs.pydantic.dev), which provide helper methods for things like:\n\n- serializing back into json, `model.model_dump_json(indent=2, exclude_unset=true)`\n- converting to a dictionary, `model.model_dump(exclude_unset=true)`\n\ntyped requests and responses provide autocomplete and documentation within your editor. if you would like to see type errors in vs code to help catch bugs earlier, set `python.analysis.typecheckingmode` to `basic`.\n\n## handling errors\n\nwhen the library is unable to connect to the api (for example, due to network connection problems or a timeout), a subclass of `anthropic.apiconnectionerror` is raised.\n\nwhen the api returns a non-success status code (that is, 4xx or 5xx\nresponse), a subclass of `anthropic.apistatuserror` is raised, containing `status_code` and `response` properties.\n\nall errors inherit from `anthropic.apierror`.\n\n```python\nimport anthropic\n\nclient = anthropic.anthropic()\n\ntry:\n    client.completions.create(\n        prompt=f\"{anthropic.human_prompt} your prompt here{anthropic.ai_prompt}\",\n        max_tokens_to_sample=300,\n        model=\"claude-2.1\",\n    )\nexcept anthropic.apiconnectionerror as e:\n    print(\"the server could not be reached\")\n    print(e.__cause__)  # an underlying exception, likely raised within httpx.\nexcept anthropic.ratelimiterror as e:\n    print(\"a 429 status code was received; we should back off a bit.\")\nexcept anthropic.apistatuserror as e:\n    print(\"another non-200-range status code was received\")\n    print(e.status_code)\n    print(e.response)\n```\n\nerror codes are as followed:\n\n| status code | error type                 |\n| ----------- | -------------------------- |\n| 400         | `badrequesterror`          |\n| 401         | `authenticationerror`      |\n| 403         | `permissiondeniederror`    |\n| 404         | `notfounderror`            |\n| 422         | `unprocessableentityerror` |\n| 429         | `ratelimiterror`           |\n| >=500       | `internalservererror`      |\n| n/a         | `apiconnectionerror`       |\n\n### retries\n\ncertain errors are automatically retried 2 times by default, with a short exponential backoff.\nconnection errors (for example, due to a network connectivity problem), 408 request timeout, 409 conflict,\n429 rate limit, and >=500 internal errors are all retried by default.\n\nyou can use the `max_retries` option to configure or disable retry settings:\n\n```python\nfrom anthropic import anthropic, human_prompt, ai_prompt\n\n# configure the default for all requests:\nanthropic = anthropic(\n    # default is 2\n    max_retries=0,\n)\n\n# or, configure per-request:\nanthropic.with_options(max_retries=5).completions.create(\n    prompt=f\"{human_prompt} can you help me effectively ask for a raise at work?{ai_prompt}\",\n    max_tokens_to_sample=300,\n    model=\"claude-2.1\",\n)\n```\n\n### timeouts\n\nby default requests time out after 10 minutes. you can configure this with a `timeout` option,\nwhich accepts a float or an [`httpx.timeout`](https://www.python-httpx.org/advanced/#fine-tuning-the-configuration) object:\n\n```python\nfrom anthropic import anthropic, human_prompt, ai_prompt\n\n# configure the default for all requests:\nanthropic = anthropic(\n    # default is 10 minutes\n    timeout=20.0,\n)\n\n# more granular control:\nanthropic = anthropic(\n    timeout=httpx.timeout(60.0, read=5.0, write=10.0, connect=2.0),\n)\n\n# override per-request:\nanthropic.with_options(timeout=5 * 1000).completions.create(\n    prompt=f\"{human_prompt} where can i get a good coffee in my neighbourhood?{ai_prompt}\",\n    max_tokens_to_sample=300,\n    model=\"claude-2.1\",\n)\n```\n\non timeout, an `apitimeouterror` is thrown.\n\nnote that requests that time out are [retried twice by default](#retries).\n\n## default headers\n\nwe automatically send the `anthropic-version` header set to `2023-06-01`.\n\nif you need to, you can override it by setting default headers per-request or on the client object.\n\nbe aware that doing so may result in incorrect types and other unexpected or undefined behavior in the sdk.\n\n```python\nfrom anthropic import anthropic\n\nclient = anthropic(\n    default_headers={\"anthropic-version\": \"my-custom-value\"},\n)\n```\n\n## advanced\n\n### logging\n\nwe use the standard library [`logging`](https://docs.python.org/3/library/logging.html) module.\n\nyou can enable logging by setting the environment variable `anthropic_log` to `debug`.\n\n```shell\n$ export anthropic_log=debug\n```\n\n### how to tell whether `none` means `null` or missing\n\nin an api response, a field may be explicitly `null`, or missing entirely; in either case, its value is `none` in this library. you can differentiate the two cases with `.model_fields_set`:\n\n```py\nif response.my_field is none:\n  if 'my_field' not in response.model_fields_set:\n    print('got json like {}, without a \"my_field\" key present at all.')\n  else:\n    print('got json like {\"my_field\": null}.')\n```\n\n### accessing raw response data (e.g. headers)\n\nthe \"raw\" response object can be accessed by prefixing `.with_raw_response.` to any http method call.\n\n```py\nfrom anthropic import anthropic, human_prompt, ai_prompt\n\nanthropic = anthropic()\n\nresponse = anthropic.completions.with_raw_response.create(\n    model=\"claude-2.1\",\n    max_tokens_to_sample=300,\n    prompt=f\"{human_prompt} how does a court case get to the supreme court?{ai_prompt}\",\n)\nprint(response.headers.get('x-my-header'))\n\ncompletion = response.parse()  # get the object that `completions.create()` would have returned\nprint(completion.completion)\n```\n\nthese methods return an [`apiresponse`](https://github.com/anthropics/anthropic-sdk-python/tree/main/src/anthropic/_response.py) object.\n\n### configuring the http client\n\nyou can directly override the [httpx client](https://www.python-httpx.org/api/#client) to customize it for your use case, including:\n\n- support for proxies\n- custom transports\n- additional [advanced](https://www.python-httpx.org/advanced/#client-instances) functionality\n\n```python\nimport httpx\nfrom anthropic import anthropic\n\nclient = anthropic(\n    # or use the `anthropic_base_url` env var\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=httpx.client(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.httptransport(local_address=\"0.0.0.0\"),\n    ),\n)\n```\n\n### managing http resources\n\nby default the library closes underlying http connections whenever the client is [garbage collected](https://docs.python.org/3/reference/datamodel.html#object.__del__). you can manually close the client using the `.close()` method if desired, or with a context manager that closes when exiting.\n\n## versioning\n\nthis package generally follows [semver](https://semver.org/spec/v2.0.0.html) conventions, though certain backwards-incompatible changes may be released as minor versions:\n\n1. changes that only affect static types, without breaking runtime behavior.\n2. changes to library internals which are technically public but not intended or documented for external use. _(please open a github issue to let us know if you are relying on such internals)_.\n3. changes that we do not expect to impact the vast majority of users in practice.\n\nwe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nwe are keen for your feedback; please open an [issue](https://www.github.com/anthropics/anthropic-sdk-python/issues) with questions, bugs, or suggestions.\n\n## requirements\n\npython 3.7 or higher.\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "anthropic",
  "package_url": "https://pypi.org/project/anthropic/",
  "project_url": "https://pypi.org/project/anthropic/",
  "project_urls": {
    "Homepage": "https://github.com/anthropics/anthropic-sdk-python",
    "Repository": "https://github.com/anthropics/anthropic-sdk-python"
  },
  "release_url": "https://pypi.org/project/anthropic/0.8.1/",
  "requires_dist": [
    "anyio<5,>=3.5.0",
    "distro<2,>=1.7.0",
    "httpx<1,>=0.23.0",
    "pydantic<3,>=1.9.0",
    "sniffio",
    "tokenizers>=0.13.0",
    "typing-extensions<5,>=4.7"
  ],
  "requires_python": ">=3.7",
  "summary": "the official python library for the anthropic api",
  "version": "0.8.1",
  "releases": [],
  "developers": [
    "support@anthropic.com"
  ],
  "kwds": "anthropic_api_key api apiresponse bedrock pip",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_anthropic",
  "homepage": "",
  "release_count": 35,
  "dependency_ids": [
    "pypi_anyio",
    "pypi_distro",
    "pypi_httpx",
    "pypi_pydantic",
    "pypi_sniffio",
    "pypi_tokenizers",
    "pypi_typing_extensions"
  ]
}