{
  "classifiers": [
    "development status :: 4 - beta",
    "environment :: console",
    "intended audience :: developers",
    "license :: osi approved :: mit license",
    "operating system :: macos :: macos x",
    "operating system :: microsoft :: windows",
    "operating system :: posix :: linux",
    "programming language :: c++",
    "programming language :: python",
    "programming language :: python :: implementation :: cpython"
  ],
  "description": "# onnxruntime-extensions\n\n[![build status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status%2fmicrosoft.onnxruntime-extensions?branchname=main)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionid=209&branchname=main)\n\n## what's onnxruntime-extensions\n\nintroduction: onnxruntime-extensions is a library that extends the capability of the onnx models and inference with onnx runtime, via onnx runtime custom operator abis. it includes a set of [onnx runtime custom operator](https://onnxruntime.ai/docs/reference/operators/add-custom-op.html) to support the common pre- and post-processing operators for vision, text, and nlp models. and it supports multiple languages and platforms, like python on windows/linux/macos, some mobile platforms like android and ios, and web-assembly etc. the basic workflow is to enhance a onnx model firstly and then do the model inference with onnx runtime and onnxruntime-extensions package.\n\n\n## quickstart\n\n### **python installation**\n```bash\npip install onnxruntime-extensions\n````\n\n\n### **nightly build**\n\n#### <strong>on windows</strong>\n```cmd\npip install --index-url https://aiinfra.pkgs.visualstudio.com/publicpackages/_packaging/ort-nightly/pypi/simple/ onnxruntime-extensions\n```\nplease ensure that you have met the prerequisites of onnxruntime-extensions (e.g., onnx and onnxruntime) in your python environment.\n#### <strong>on linux/macos</strong>\nplease make sure the compiler toolkit like gcc(later than g++ 8.0) or clang are installed before the following command\n```bash\npython -m pip install git+https://github.com/microsoft/onnxruntime-extensions.git\n```\n\n\n## usage\n\n## 1. generate the pre-/post- processing onnx model\nwith onnxruntime-extensions python package, you can easily get the onnx processing graph by converting them from huggingface transformer data processing classes, check the following api for details.\n```python\nhelp(onnxruntime_extensions.gen_processing_models)\n```\n### note: these data processing model can be merged into other model [onnx.compose](https://onnx.ai/onnx/api/compose.html) if needed.\n## 2. using extensions for onnx runtime inference\n\n### python\nthere are individual packages for the following languages, please install it for the build.\n```python\nimport onnxruntime as _ort\nfrom onnxruntime_extensions import get_library_path as _lib_path\n\nso = _ort.sessionoptions()\nso.register_custom_ops_library(_lib_path())\n\n# run the onnxruntime session, as onnxruntime docs suggested.\n# sess = _ort.inferencesession(model, so)\n# sess.run (...)\n```\n### c++\n\n```c++\n  // the line loads the customop library into onnxruntime engine to load the onnx model with the custom op\n  ort::throwonerror(ort::getapi().registercustomopslibrary((ortsessionoptions*)session_options, custom_op_library_filename, &handle));\n\n  // the regular onnxruntime invoking to run the model.\n  ort::session session(env, model_uri, session_options);\n  runsession(session, inputs, outputs);\n```\n### java\n```java\nvar env = ortenvironment.getenvironment();\nvar sess_opt = new ortsession.sessionoptions();\n\n/* register the custom ops from onnxruntime-extensions */\nsess_opt.registercustomoplibrary(ortxpackage.getlibrarypath());\n```\n\n### c#\n```c#\nsessionoptions options = new sessionoptions()\noptions.registerortextensions()\nsession = new inferencesession(model, options)\n```\n\n\n#\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit license",
  "name": "onnxruntime-extensions",
  "package_url": "https://pypi.org/project/onnxruntime-extensions/",
  "project_url": "https://pypi.org/project/onnxruntime-extensions/",
  "project_urls": {
    "Homepage": "https://github.com/microsoft/onnxruntime-extensions"
  },
  "release_url": "https://pypi.org/project/onnxruntime-extensions/0.9.0/",
  "requires_dist": [
    "onnx >=1.9.0"
  ],
  "requires_python": null,
  "summary": "onnxruntime extensions",
  "version": "0.9.0",
  "releases": [],
  "developers": [
    "microsoft_corporation",
    "onnxruntime@microsoft.com"
  ],
  "kwds": "onnxruntime_extensions onnxruntime onnx azure runtime",
  "license_kwds": "mit license",
  "libtype": "pypi",
  "id": "pypi_onnxruntime_extensions",
  "homepage": "https://github.com/microsoft/onnxruntime-extensions",
  "release_count": 7,
  "dependency_ids": [
    "pypi_onnx"
  ]
}