{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: developers",
    "intended audience :: education",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "operating system :: os independent",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/evaluate/media/resolve/main/evaluate-banner.png\" width=\"400\"/>\n    <br>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://github.com/huggingface/evaluate/actions/workflows/ci.yml?query=branch%3amain\">\n        <img alt=\"build\" src=\"https://github.com/huggingface/evaluate/actions/workflows/ci.yml/badge.svg?branch=main\">\n    </a>\n    <a href=\"https://github.com/huggingface/evaluate/blob/master/license\">\n        <img alt=\"github\" src=\"https://img.shields.io/github/license/huggingface/evaluate.svg?color=blue\">\n    </a>\n    <a href=\"https://huggingface.co/docs/evaluate/index\">\n        <img alt=\"documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/evaluate/index.svg?down_color=red&down_message=offline&up_message=online\">\n    </a>\n    <a href=\"https://github.com/huggingface/evaluate/releases\">\n        <img alt=\"github release\" src=\"https://img.shields.io/github/release/huggingface/evaluate.svg\">\n    </a>\n    <a href=\"code_of_conduct.md\">\n        <img alt=\"contributor covenant\" src=\"https://img.shields.io/badge/contributor%20covenant-2.0-4baaaa.svg\">\n    </a>\n</p>\n\n\ud83e\udd17 evaluate is a library that makes evaluating and comparing models and reporting their performance easier and more standardized. \n\nit currently contains:\n\n- **implementations of dozens of popular metrics**: the existing metrics cover a variety of tasks spanning from nlp to computer vision, and include dataset-specific metrics for datasets. with a simple command like `accuracy = load(\"accuracy\")`, get any of these metrics ready to use for evaluating a ml model in any framework (numpy/pandas/pytorch/tensorflow/jax).\n- **comparisons and measurements**: comparisons are used to measure the difference between models and measurements are tools to evaluate datasets.\n- **an easy way of adding new evaluation modules to the \ud83e\udd17 hub**: you can create new evaluation modules and push them to a dedicated space in the \ud83e\udd17 hub with `evaluate-cli create [metric name]`, which allows you to see easily compare different metrics and their outputs for the same sets of references and predictions.\n\n[\ud83c\udf93 **documentation**](https://huggingface.co/docs/evaluate/)\n\n\ud83d\udd0e **find a [metric](https://huggingface.co/evaluate-metric), [comparison](https://huggingface.co/evaluate-comparison), [measurement](https://huggingface.co/evaluate-measurement) on the hub**\n\n[\ud83c\udf1f **add a new evaluation module**](https://huggingface.co/docs/evaluate/)\n\n\ud83e\udd17 evaluate also has lots of useful features like:\n\n- **type checking**: the input types are checked to make sure that you are using the right input formats for each metric\n- **metric cards**: each metrics comes with a card that describes the values, limitations and their ranges, as well as providing examples of their usage and usefulness.\n- **community metrics:** metrics live on the hugging face hub and you can easily add your own metrics for your project or to collaborate with others.\n\n\n# installation\n\n## with pip\n\n\ud83e\udd17 evaluate can be installed from pypi and has to be installed in a virtual environment (venv or conda for instance)\n\n```bash\npip install evaluate\n```\n\n# usage\n\n\ud83e\udd17 evaluate's main methods are:\n\n- `evaluate.list_evaluation_modules()` to list the available metrics, comparisons and measurements\n- `evaluate.load(module_name, **kwargs)` to instantiate an evaluation module\n- `results = module.compute(*kwargs)` to compute the result of an evaluation module\n\n# adding a new evaluation module\n\nfirst install the necessary dependencies to create a new metric with the following command:\n```bash\npip install evaluate[template]\n```\nthen you can get started with the following command which will create a new folder for your metric and display the necessary steps:\n```bash\nevaluate-cli create \"awesome metric\"\n```\nsee this [step-by-step guide](https://huggingface.co/docs/evaluate/creating_and_sharing) in the documentation for detailed instructions.\n\n## credits\n\nthanks to [@marella](https://github.com/marella) for letting us use the `evaluate` namespace on pypi previously used by his [library](https://github.com/marella/evaluate).\n\n\n",
  "docs_url": null,
  "keywords": "metrics machine learning evaluate evaluation",
  "license": "apache 2.0",
  "name": "evaluate",
  "package_url": "https://pypi.org/project/evaluate/",
  "project_url": "https://pypi.org/project/evaluate/",
  "project_urls": {
    "Download": "https://github.com/huggingface/evaluate/tags",
    "Homepage": "https://github.com/huggingface/evaluate"
  },
  "release_url": "https://pypi.org/project/evaluate/0.4.1/",
  "requires_dist": [
    "datasets >=2.0.0",
    "numpy >=1.17",
    "dill",
    "pandas",
    "requests >=2.19.0",
    "tqdm >=4.62.1",
    "xxhash",
    "multiprocess",
    "fsspec[http] >=2021.05.0",
    "huggingface-hub >=0.7.0",
    "packaging",
    "responses <0.19",
    "importlib-metadata ; python_version < \"3.8\"",
    "absl-py ; extra == 'dev'",
    "charcut >=1.1.1 ; extra == 'dev'",
    "cer >=1.2.0 ; extra == 'dev'",
    "nltk ; extra == 'dev'",
    "pytest ; extra == 'dev'",
    "pytest-datadir ; extra == 'dev'",
    "pytest-xdist ; extra == 'dev'",
    "tensorflow !=2.6.0,!=2.6.1,<=2.10,>=2.3 ; extra == 'dev'",
    "torch ; extra == 'dev'",
    "accelerate ; extra == 'dev'",
    "bert-score >=0.3.6 ; extra == 'dev'",
    "rouge-score >=0.1.2 ; extra == 'dev'",
    "sacrebleu ; extra == 'dev'",
    "sacremoses ; extra == 'dev'",
    "scipy ; extra == 'dev'",
    "seqeval ; extra == 'dev'",
    "scikit-learn ; extra == 'dev'",
    "jiwer ; extra == 'dev'",
    "sentencepiece ; extra == 'dev'",
    "transformers ; extra == 'dev'",
    "mauve-text ; extra == 'dev'",
    "trectools ; extra == 'dev'",
    "toml >=0.10.1 ; extra == 'dev'",
    "requests-file >=1.5.1 ; extra == 'dev'",
    "tldextract >=3.1.0 ; extra == 'dev'",
    "texttable >=1.6.3 ; extra == 'dev'",
    "unidecode >=1.3.4 ; extra == 'dev'",
    "Werkzeug >=1.0.1 ; extra == 'dev'",
    "six ~=1.15.0 ; extra == 'dev'",
    "black ~=22.0 ; extra == 'dev'",
    "flake8 >=3.8.3 ; extra == 'dev'",
    "isort >=5.0.0 ; extra == 'dev'",
    "pyyaml >=5.3.1 ; extra == 'dev'",
    "s3fs ; extra == 'docs'",
    "transformers ; extra == 'evaluator'",
    "scipy >=1.7.1 ; extra == 'evaluator'",
    "black ~=22.0 ; extra == 'quality'",
    "flake8 >=3.8.3 ; extra == 'quality'",
    "isort >=5.0.0 ; extra == 'quality'",
    "pyyaml >=5.3.1 ; extra == 'quality'",
    "cookiecutter ; extra == 'template'",
    "gradio >=3.0.0 ; extra == 'template'",
    "tensorflow !=2.6.0,!=2.6.1,>=2.2.0 ; extra == 'tensorflow'",
    "tensorflow-gpu !=2.6.0,!=2.6.1,>=2.2.0 ; extra == 'tensorflow_gpu'",
    "absl-py ; extra == 'tests'",
    "charcut >=1.1.1 ; extra == 'tests'",
    "cer >=1.2.0 ; extra == 'tests'",
    "nltk ; extra == 'tests'",
    "pytest ; extra == 'tests'",
    "pytest-datadir ; extra == 'tests'",
    "pytest-xdist ; extra == 'tests'",
    "tensorflow !=2.6.0,!=2.6.1,<=2.10,>=2.3 ; extra == 'tests'",
    "torch ; extra == 'tests'",
    "accelerate ; extra == 'tests'",
    "bert-score >=0.3.6 ; extra == 'tests'",
    "rouge-score >=0.1.2 ; extra == 'tests'",
    "sacrebleu ; extra == 'tests'",
    "sacremoses ; extra == 'tests'",
    "scipy ; extra == 'tests'",
    "seqeval ; extra == 'tests'",
    "scikit-learn ; extra == 'tests'",
    "jiwer ; extra == 'tests'",
    "sentencepiece ; extra == 'tests'",
    "transformers ; extra == 'tests'",
    "mauve-text ; extra == 'tests'",
    "trectools ; extra == 'tests'",
    "toml >=0.10.1 ; extra == 'tests'",
    "requests-file >=1.5.1 ; extra == 'tests'",
    "tldextract >=3.1.0 ; extra == 'tests'",
    "texttable >=1.6.3 ; extra == 'tests'",
    "unidecode >=1.3.4 ; extra == 'tests'",
    "Werkzeug >=1.0.1 ; extra == 'tests'",
    "six ~=1.15.0 ; extra == 'tests'",
    "torch ; extra == 'torch'"
  ],
  "requires_python": ">=3.7.0",
  "summary": "huggingface community-driven open-source library of evaluation",
  "version": "0.4.1",
  "releases": [],
  "developers": [
    "huggingface_inc",
    "leandro@huggingface.co"
  ],
  "kwds": "badge evaluation evaluating evaluate metrics",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_evaluate",
  "homepage": "https://github.com/huggingface/evaluate",
  "release_count": 9,
  "dependency_ids": [
    "pypi_absl_py",
    "pypi_accelerate",
    "pypi_bert_score",
    "pypi_black",
    "pypi_cer",
    "pypi_charcut",
    "pypi_cookiecutter",
    "pypi_datasets",
    "pypi_dill",
    "pypi_flake8",
    "pypi_fsspec",
    "pypi_gradio",
    "pypi_huggingface_hub",
    "pypi_importlib_metadata",
    "pypi_isort",
    "pypi_jiwer",
    "pypi_mauve_text",
    "pypi_multiprocess",
    "pypi_nltk",
    "pypi_numpy",
    "pypi_packaging",
    "pypi_pandas",
    "pypi_pytest",
    "pypi_pytest_datadir",
    "pypi_pytest_xdist",
    "pypi_pyyaml",
    "pypi_requests",
    "pypi_requests_file",
    "pypi_responses",
    "pypi_rouge_score",
    "pypi_s3fs",
    "pypi_sacrebleu",
    "pypi_sacremoses",
    "pypi_scikit_learn",
    "pypi_scipy",
    "pypi_sentencepiece",
    "pypi_seqeval",
    "pypi_six",
    "pypi_tensorflow",
    "pypi_tensorflow_gpu",
    "pypi_texttable",
    "pypi_tldextract",
    "pypi_toml",
    "pypi_torch",
    "pypi_tqdm",
    "pypi_transformers",
    "pypi_trectools",
    "pypi_unidecode",
    "pypi_werkzeug",
    "pypi_xxhash"
  ]
}