{
  "classifiers": [
    "development status :: 5 - production/stable",
    "license :: osi approved :: apache software license",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "# apache flink\n\napache flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.\n\nlearn more about flink at [https://flink.apache.org/](https://flink.apache.org/)\n\n## python packaging\n\npyflink is a python api for apache flink that allows you to build scalable batch and streaming workloads,\nsuch as real-time data processing pipelines, large-scale exploratory data analysis, machine learning (ml)\npipelines and etl processes. if you\u2019re already familiar with python and libraries such as pandas,\nthen pyflink makes it simpler to leverage the full capabilities of the flink ecosystem.\ndepending on the level of abstraction you need, there are two different apis that can be used in pyflink: pyflink table api and pyflink datastream api.\n\nthe pyflink table api allows you to write powerful relational queries in a way that is similar to\nusing sql or working with tabular data in python. you can find more information about it via the tutorial\n[https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/table_api_tutorial/](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/table_api_tutorial/)\n\nthe pyflink datastream api gives you lower-level control over the core building blocks of flink,\nstate and time, to build more complex stream processing use cases.\ntutorial can be found at [https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/datastream_tutorial/](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/datastream_tutorial/)\n\nyou can find more information via the documentation at [https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/overview/](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/overview/)\n\nthe auto-generated python docs can be found at [https://nightlies.apache.org/flink/flink-docs-stable/api/python/](https://nightlies.apache.org/flink/flink-docs-stable/api/python/)\n\n## python requirements\n\napache flink python api depends on py4j (currently version 0.10.9.7), cloudpickle (currently version 2.2.0), python-dateutil (currently version >=2.8.0,<3), apache beam (currently version >=2.43.0,<2.49.0).\n\n## development notices\n\n### protobuf code generation\n\nprotocol buffer is used in file `flink_fn_execution_pb2.py` and the file is generated from `flink-fn-execution.proto`. whenever `flink-fn-execution.proto` is updated, please re-generate `flink_fn_execution_pb2.py` by executing:\n\n```\npython pyflink/gen_protos.py\n```\n\npyflink depends on the following libraries to execute the above script:\n1. grpcio-tools (>=1.29.0,<=1.48.2)\n2. setuptools (>=37.0.0)\n3. pip (>=20.3)\n\n### running test cases \n\ncurrently, we use conda and tox to verify the compatibility of the flink python api for multiple versions of python and will integrate some useful plugins with tox, such as flake8.\nwe can enter the directory where this readme.md file is located and run test cases by executing\n\n```\n./dev/lint-python.sh\n```\n\nto use your system conda environment, you can set `flink_conda_home` variable:\n\n```shell\nexport flink_conda_home=$(dirname $(dirname $conda_exe))\n```\n\ncreate a virtual environment:\n```shell\nconda create -n pyflink_38 python=3.8\n```\n\nthen you can activate your environment and run tests, for example:\n\n```shell\nconda activate pyflink_38\npip install -r ./dev/dev-requirements.txt\n./dev/lint-python.sh\n```\n",
  "docs_url": null,
  "keywords": "",
  "license": "https://www.apache.org/licenses/license-2.0",
  "name": "apache-flink",
  "package_url": "https://pypi.org/project/apache-flink/",
  "project_url": "https://pypi.org/project/apache-flink/",
  "project_urls": {
    "Homepage": "https://flink.apache.org"
  },
  "release_url": "https://pypi.org/project/apache-flink/1.18.0/",
  "requires_dist": [
    "py4j ==0.10.9.7",
    "python-dateutil <3,>=2.8.0",
    "apache-beam <2.49.0,>=2.43.0",
    "cloudpickle >=2.2.0",
    "avro-python3 !=1.9.2,>=1.8.1",
    "pytz >=2018.3",
    "fastavro !=1.8.0,>=1.1.0",
    "requests >=2.26.0",
    "protobuf >=3.19.0",
    "numpy >=1.21.4",
    "pandas >=1.3.0",
    "pyarrow >=5.0.0",
    "httplib2 >=0.19.0",
    "apache-flink-libraries <1.18.1,>=1.18.0",
    "pemja ==0.3.0 ; platform_system != \"Windows\""
  ],
  "requires_python": ">=3.7",
  "summary": "apache flink python api",
  "version": "1.18.0",
  "releases": [],
  "developers": [
    "apache_software_foundation",
    "dev@flink.apache.org"
  ],
  "kwds": "flink pyflink python streams streaming",
  "license_kwds": "https://www.apache.org/licenses/license-2.0",
  "libtype": "pypi",
  "id": "pypi_apache_flink",
  "homepage": "https://flink.apache.org",
  "release_count": 42,
  "dependency_ids": [
    "pypi_apache_beam",
    "pypi_apache_flink_libraries",
    "pypi_avro_python3",
    "pypi_cloudpickle",
    "pypi_fastavro",
    "pypi_httplib2",
    "pypi_numpy",
    "pypi_pandas",
    "pypi_pemja",
    "pypi_protobuf",
    "pypi_py4j",
    "pypi_pyarrow",
    "pypi_python_dateutil",
    "pypi_pytz",
    "pypi_requests"
  ]
}