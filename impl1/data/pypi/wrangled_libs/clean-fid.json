{
  "classifiers": [
    "license :: osi approved :: bsd license",
    "operating system :: os independent",
    "programming language :: python :: 3"
  ],
  "description": "# clean-fid for evaluating generative models\n\n<br>\n\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/gaparmar/clean-fid/main/docs/images/cleanfid_demo_folders.gif\" />\n</p>\n\n[![downloads](https://pepy.tech/badge/clean-fid)](https://pepy.tech/project/clean-fid) [![downloads](https://pepy.tech/badge/clean-fid/month)](https://pepy.tech/project/clean-fid)\n\n[**project**](https://www.cs.cmu.edu/~clean-fid/) | [**paper**](https://arxiv.org/abs/2104.11222) | \n[**colab-fid**](https://colab.research.google.com/drive/1elgahvlwtilif_3d3cw1boircekfsawi?usp=sharing) |\n[**colab-resize**](https://colab.research.google.com/drive/1q-n94s2mnlsflput7wwy6d5wxgvwlgpg?usp=sharing) |\n[**leaderboard tables**](#cleanfid-leaderboard-for-common-tasks) <br>\n**quick start:** [**calculate fid**](#computing-fid) | [**calculate kid**](#computing-kid)\n\n**[new]** computing the fid using clip features [[kynk\u00e4\u00e4nniemi et al, 2022]](https://arxiv.org/pdf/2203.06026.pdf) is now supported. see [here](#computing-clip-fid) for more details. \n\n\nthe fid calculation involves many steps that can produce inconsistencies in the final metric. as shown below, different implementations use different low-level image quantization and resizing functions, the latter of which are often implemented incorrectly.\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/gaparmar/clean-fid/main/docs/images/resize_circle.png\"  width=\"800\" />\n</p>\n\n\nwe provide an easy-to-use library to address the above issues and make the fid scores comparable across different methods, papers, and groups.\n\n![fid steps](https://raw.githubusercontent.com/gaparmar/clean-fid/main/docs/images/fid_steps.jpg)\n\n\n---\n\n### corresponding manuscript\n\n[on aliased resizing and surprising subtleties in gan evaluation](https://www.cs.cmu.edu/~clean-fid/) <br>\n [gaurav parmar](https://gauravparmar.com/), [richard zhang](https://richzhang.github.io/), [jun-yan zhu](https://www.cs.cmu.edu/~junyanz/)<br>\ncvpr, 2022 <br>\ncmu and adobe\n\n\nif you find this repository useful for your research, please cite the following work.\n```\n@inproceedings{parmar2021cleanfid,\n  title={on aliased resizing and surprising subtleties in gan evaluation},\n  author={parmar, gaurav and zhang, richard and zhu, jun-yan},\n  booktitle={cvpr},\n  year={2022}\n}\n```\n\n---\n\n<br>\n\n**aliased resizing operations** <br>\n\n\n  the definitions of resizing functions are mathematical and <em>should never be a function of the library being used</em>. unfortunately, implementations differ across commonly-used libraries.  they are often implemented incorrectly by popular libraries. try out the different resizing implementations in the google colab notebook [here](https://colab.research.google.com/drive/1q-n94s2mnlsflput7wwy6d5wxgvwlgpg?usp=sharing).\n\n  <img src=\"https://raw.githubusercontent.com/gaparmar/clean-fid/main/docs/images/resize_circle_extended.png\"  width=\"800\" />\n<br>\n\nthe inconsistencies among implementations can have a drastic effect of the evaluations metrics. the table below shows that ffhq dataset images resized with  bicubic implementation from other libraries (opencv, pytorch, tensorflow, opencv) have a large fid score (\u2265 6) when compared to the same images resized with the correctly implemented pil-bicubic filter. other correctly implemented filters from pil (lanczos, bilinear, box) all result in relatively smaller fid score (\u2264 0.75). note that since tf 2.0, the new flag `antialias` (default: `false`) can produce results close to pil. however, it was not used in the existing tf-fid repo and set as `false` by default.\n\n <p align=\"center\"><img src=\"https://raw.githubusercontent.com/gaparmar/clean-fid/main/docs/images/table_resize_sc.png\"  width=\"500\" /></p>\n\n**jpeg image compression**\n\n  image compression can have a surprisingly large effect on fid.  images are perceptually indistinguishable from each other but have a large fid score. the fid scores under the images are calculated between all ffhq images saved using the corresponding jpeg format and the png format.\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/gaparmar/clean-fid/main/docs/images/jpeg_effects.png\"  width=\"800\" />\n</p>\n\nbelow, we study the effect of jpeg compression for stylegan2 models trained on the ffhq dataset (left) and lsun outdoor church dataset (right). note that lsun dataset images were collected with jpeg compression (quality 75), whereas ffhq images were collected as png. interestingly, for lsun dataset, the best fid score (3.48) is obtained when the generated images are compressed with jpeg quality 87.\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/gaparmar/clean-fid/main/docs/images/jpeg_plots.png\"  width=\"800\" />\n</p>\n\n---\n\n## quick start\n\n- install the library\n    ```\n    pip install clean-fid\n    ```\n### computing fid\n- compute fid between two image folders\n    ```\n    from cleanfid import fid\n    score = fid.compute_fid(fdir1, fdir2)\n    ```\n- compute fid between one folder of images and pre-computed datasets statistics (e.g., `ffhq`)\n    ```\n    from cleanfid import fid\n    score = fid.compute_fid(fdir1, dataset_name=\"ffhq\", dataset_res=1024, dataset_split=\"trainval70k\")\n    ```\n- compute fid using a generative model and pre-computed dataset statistics:\n    ```\n    from cleanfid import fid\n    # function that accepts a latent and returns an image in range[0,255]\n    gen = lambda z: gan(latent=z, ... , <other_flags>)\n    score = fid.compute_fid(gen=gen, dataset_name=\"ffhq\",\n            dataset_res=256, num_gen=50_000, dataset_split=\"trainval70k\")\n    ```\n\n### computing clip-fid\nto use the clip features when computing the fid [[kynk\u00e4\u00e4nniemi et al, 2022]](https://arxiv.org/pdf/2203.06026.pdf), specify the flag `model_name=\"clip_vit_b_32\"`\n- e.g. to compute the clip-fid between two folders of images use the following commands. \n    ```\n    from cleanfid import fid\n    score = fid.compute_fid(fdir1, fdir2, mode=\"clean\", model_name=\"clip_vit_b_32\")\n    ```\n\n### computing kid\nthe kid score can be computed using a similar interface as fid. \nthe dataset statistics for kid are only precomputed for smaller datasets `afhq`, `brecahad`, and `metfaces`.\n\n- compute kid between two image folders\n    ```\n    from cleanfid import fid\n    score = fid.compute_kid(fdir1, fdir2)\n    ```\n- compute kid between one folder of images and pre-computed datasets statistics\n    ```\n    from cleanfid import fid\n    score = fid.compute_kid(fdir1, dataset_name=\"brecahad\", dataset_res=512, dataset_split=\"train\")\n    ```\n- compute kid using a generative model and pre-computed dataset statistics:\n    ```\n    from cleanfid import fid\n    # function that accepts a latent and returns an image in range[0,255]\n    gen = lambda z: gan(latent=z, ... , <other_flags>)\n    score = fid.compute_kid(gen=gen, dataset_name=\"brecahad\", dataset_res=512, num_gen=50_000, dataset_split=\"train\")\n    ```\n\n---\n### supported precomputed datasets\n\nwe provide precompute statistics for the following commonly used configurations. please contact us if you want to add statistics for your new datasets. \n\n| task             | dataset   | resolution | reference split          | # reference images | mode |\n| :-:              | :---:     | :-:        | :-:            |  :-:          | :-: |\n| image generation | [`cifar10`](https://www.cs.toronto.edu/~kriz/cifar.html)     | 32         | `train`        |  50,000       |`clean`, `legacy_tensorflow`, `legacy_pytorch`|\n| image generation | [`cifar10`](https://www.cs.toronto.edu/~kriz/cifar.html)     | 32         | `test`         |  10,000       |`clean`, `legacy_tensorflow`, `legacy_pytorch`|\n| image generation | [`ffhq`](https://github.com/nvlabs/ffhq-dataset)        | 1024, 256  | `trainval`     |  50,000       |`clean`, `legacy_tensorflow`, `legacy_pytorch`|\n| image generation | [`ffhq`](https://github.com/nvlabs/ffhq-dataset)        | 1024, 256  | `trainval70k`  |  70,000       |`clean`, `legacy_tensorflow`, `legacy_pytorch`|\n| image generation | [`lsun_church`](https://www.yf.io/p/lsun/) | 256        | `train`        |  50,000       |`clean`, `legacy_tensorflow`, `legacy_pytorch`|\n| image generation | [`lsun_church`](https://www.yf.io/p/lsun/) | 256        | `trainfull`    |  126,227       |`clean`|\n| image generation | [`lsun_horse`](https://www.yf.io/p/lsun/)  | 256        | `train`        |  50,000       |`clean`, `legacy_tensorflow`, `legacy_pytorch`|\n| image generation | [`lsun_horse`](https://www.yf.io/p/lsun/)  | 256        | `trainfull`    |  2,000,340       |`clean`|\n| image generation | [`lsun_cat`](https://www.yf.io/p/lsun/)    | 256        | `train`        |  50,000       |`clean`, `legacy_tensorflow`, `legacy_pytorch`|\n| image generation | [`lsun_cat`](https://www.yf.io/p/lsun/)    | 256        | `trainfull`    |  1,657,264    |`clean`, `legacy_tensorflow`, `legacy_pytorch`|\n| few shot generation | [`afhq_cat`](https://github.com/clovaai/stargan-v2/)  | 512        | `train`       |  5153         |`clean`, `legacy_tensorflow`, `legacy_pytorch`|\n| few shot generation | [`afhq_dog`](https://github.com/clovaai/stargan-v2/)  | 512        | `train`       |  4739         |`clean`, `legacy_tensorflow`, `legacy_pytorch`|\n| few shot generation | [`afhq_wild`](https://github.com/clovaai/stargan-v2/) | 512        | `train`       |  4738         |`clean`, `legacy_tensorflow`, `legacy_pytorch`|\n| few shot generation | [`brecahad`](https://figshare.com/articles/dataset/brecahad_a_dataset_for_breast_cancer_histopathological_annotation_and_diagnosis/7379186)  | 512        | `train`       |  1944         |`clean`, `legacy_tensorflow`, `legacy_pytorch`|\n| few shot generation | [`metfaces`](https://github.com/nvlabs/metfaces-dataset)  | 1024       | `train`       |  1336         |`clean`, `legacy_tensorflow`, `legacy_pytorch`|\n| image to image   | `horse2zebra`  | 256        | `test`        |  140          |`clean`, `legacy_tensorflow`, `legacy_pytorch`|\n| image to image   | `cat2dog`      | 256        | `test`        |  500          |`clean`, `legacy_tensorflow`, `legacy_pytorch`|\n\n\n\n**using precomputed statistics**\nin order to compute the fid score with the precomputed dataset statistics, use the corresponding options. for instance, to compute the clean-fid score on generated 256x256 ffhq images use the command:\n  ```\n  fid_score = fid.compute_fid(fdir1, dataset_name=\"ffhq\", dataset_res=256,  mode=\"clean\", dataset_split=\"trainval70k\")\n  ```\n\n---\n\n### create custom dataset statistics\n- *dataset_path*: folder where the dataset images are stored\n- *custom_name*: name to be used for the statistics\n- generating custom statistics (saved to local cache)\n  ```\n  from cleanfid import fid\n  fid.make_custom_stats(custom_name, dataset_path, mode=\"clean\")\n  ```\n\n- using the generated custom statistics\n  ```\n  from cleanfid import fid\n  score = fid.compute_fid(\"folder_fake\", dataset_name=custom_name,\n            mode=\"clean\", dataset_split=\"custom\")\n  ```\n\n- removing the custom stats\n  ```\n  from cleanfid import fid\n  fid.remove_custom_stats(custom_name, mode=\"clean\")\n  ```\n\n- check if a custom statistic already exists\n  ```\n  from cleanfid import fid\n  fid.test_stats_exists(custom_name, mode)\n  ```\n\n---\n\n## backwards compatibility\n\nwe provide two flags to reproduce the legacy fid score.\n\n- `mode=\"legacy_pytorch\"` <br>\n    this flag is equivalent to using the popular pytorch fid implementation provided [here](https://github.com/mseitzer/pytorch-fid/)\n    <br>\n    the difference between using clean-fid with this option and [code](https://github.com/mseitzer/pytorch-fid/) is **~2e-06**\n    <br>\n    see [doc](https://github.com/gaparmar/clean-fid/blob/main/docs/pytorch_fid.md) for how the methods are compared\n\n\n- `mode=\"legacy_tensorflow\"` <br>\n    this flag is equivalent to using the official [implementation of fid](https://github.com/bioinf-jku/ttur) released by the authors.\n    <br>\n    the difference between using clean-fid with this option and [code](https://github.com/bioinf-jku/ttur) is **~2e-05**\n    <br>\n  see [doc](https://github.com/gaparmar/clean-fid/blob/main/docs/tensorflow_fid.md) for detailed steps for how the methods are compared\n\n---\n\n## building clean-fid locally from source\n   ```\n   python setup.py bdist_wheel\n   pip install dist/*\n   ```\n\n---\n\n## cleanfid leaderboard for common tasks\n\nwe compute the fid scores using the corresponding methods used in the original papers and using the clean-fid proposed here. \nall values are computed using 10 evaluation runs. we provide an [api](#cleanfid-leaderboard-api) to query the results shown in the tables below directly from the pip package.\n\nif you would like to add new numbers and models to our leaderboard, feel free to contact us. \n\n### cifar-10 (few shot)\n\nthe `test` set is used as the reference distribution and compared to 10k generated images.\n\n**100% data (unconditional)**\n| model\t| legacy-fid<br>(reported)\t| legacy-fid<br>(reproduced)\t| clean-fid\t|\n| :--- | :---: | :---: | :---: |\n| stylegan2 (+ada + tuning) [[karras et al, 2020]](https://arxiv.org/abs/2006.06676) | - \u2020 | - \u2020 | 8.20 \u00b1 0.10\n| stylegan2 (+ada) [[karras et al, 2020]](https://arxiv.org/abs/2006.06676) | - \u2020 | - \u2020 | 9.26 \u00b1 0.06\n| stylegan2 (diff-augment) [[zhao et al, 2020]](https://arxiv.org/abs/2006.10738) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/diffaugment-stylegan2-cifar10.pkl)\t| 9.89\t| 9.90 \u00b1 0.09\t| 10.85 \u00b1 0.10\t|\n| stylegan2 (mirror-flips) \t[[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/stylegan2-cifar10.pkl)\t| 11.07\t| 11.07 \u00b1 0.10\t| 12.96 \u00b1 0.07\t|\n| stylegan2 (without-flips) [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) | - \u2020 | - \u2020 | 14.53 \u00b1 0.13 |\n| autogan (config a) [[gong et al, 2019]](https://arxiv.org/abs/1908.03835) | - \u2020 | - \u2020 | 21.18 \u00b1 0.12 |\n| autogan (config b) [[gong et al, 2019]](https://arxiv.org/abs/1908.03835) | - \u2020 | - \u2020 | 22.46 \u00b1 0.15 |\n| autogan (config c) [[gong et al, 2019]](https://arxiv.org/abs/1908.03835) | - \u2020 | - \u2020 | 23.62 \u00b1 0.30 |\n\n\u2020 these methods use the training set as the reference distribution and compare to 50k generated images \n\n**20% data**\n| model\t| legacy-fid<br>(reported)\t| legacy-fid<br>(reproduced)\t| clean-fid\t|\n| :---: | :---: | :---: | :---: |\n| stylegan2-diff-augment [[zhao et al, 2020]](https://arxiv.org/pdf/2006.10738.pdf) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/diffaugment-stylegan2-cifar10-0.2.pkl)\t| 12.15\t| 12.12 \u00b1 0.15\t| 14.18 \u00b1 0.13\t|\n| stylegan2-mirror-flips [[karras et al, 2020]](https://arxiv.org/abs/1912.04958)\t[[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/stylegan2-cifar10-0.2.pkl)\t| 23.08\t| 23.01 \u00b1 0.19\t| 29.49 \u00b1 0.17\t|\n\n**10% data**\n| model\t| legacy-fid<br>(reported)\t| legacy-fid<br>(reproduced)\t| clean-fid\t|\n| :---: | :---: | :---: | :---: |\n| stylegan2-diff-augment [[zhao et al, 2020]](https://arxiv.org/pdf/2006.10738.pdf)\t[[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/diffaugment-stylegan2-cifar10-0.1.pkl)\t| 14.50\t| 14.53 \u00b1 0.12\t| 16.98 \u00b1 0.18\t|\n| stylegan2-mirror-flips [[karras et al, 2020]](https://arxiv.org/abs/1912.04958)\t[[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/stylegan2-cifar10-0.1.pkl)\t| 36.02\t| 35.94 \u00b1 0.17\t| 43.60 \u00b1 0.17\t|\n\n<br>\n\n### cifar-100 (few shot)\n\nthe `test` set is used as the reference distribution and compared to 10k generated images.\n\n**100% data**\n| model\t| legacy-fid<br>(reported)\t| legacy-fid<br>(reproduced)\t| clean-fid\t|\n| :---: | :---: | :---: | :---: |\n| stylegan2-mirror-flips\t[[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/stylegan2-cifar100.pkl)\t| 16.54\t| 16.44 \u00b1 0.19\t| 18.44 \u00b1 0.24\t|\n| stylegan2-diff-augment\t[[zhao et al, 2020]](https://arxiv.org/pdf/2006.10738.pdf) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/diffaugment-stylegan2-cifar100.pkl)\t| 15.22\t| 15.15 \u00b1 0.13\t| 16.80 \u00b1 0.13\t|\n\n**20% data**\n| model\t| legacy-fid<br>(reported)\t| legacy-fid<br>(reproduced)\t| clean-fid\t|\n| :---: | :---: | :---: | :---: |\n| stylegan2-mirror-flips [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/stylegan2-cifar100-0.2.pkl)\t| 32.30\t| 32.26 \u00b1 0.19\t| 34.88 \u00b1 0.14\t|\n| stylegan2-diff-augment [[zhao et al, 2020]](https://arxiv.org/pdf/2006.10738.pdf) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/diffaugment-stylegan2-cifar100-0.2.pkl)\t| 16.65\t| 16.74 \u00b1 0.10\t| 18.49 \u00b1 0.08\t|\n\n**10% data**\n| model\t| legacy-fid<br>(reported)\t| legacy-fid<br>(reproduced)\t| clean-fid\t|\n| :---: | :---: | :---: | :---: |\n| stylegan2-mirror-flips [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/stylegan2-cifar100-0.1.pkl)\t| 45.87\t| 45.97 \u00b1 0.20\t| 46.77 \u00b1 0.19\t|\n| stylegan2-diff-augment [[zhao et al, 2020]](https://arxiv.org/pdf/2006.10738.pdf) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/diffaugment-stylegan2-cifar100-0.1.pkl)\t| 20.75\t| 20.69 \u00b1 0.12\t| 23.40 \u00b1 0.09\t|\n\n<br>\n\n### ffhq\n\n**all images @ 1024x1024**<br>\nvalues are computed using 50k generated images\n| model     | legacy-fid<br>(reported) | legacy-fid<br>(reproduced)    | clean-fid  | reference split |\n| :---:     | :-:          | :-:          | :-:         | :-: |\n | stylegan1 (config a) [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) | 4.4  | 4.39 \u00b1 0.03 | 4.77 \u00b1 0.03 | `trainval` |\n | stylegan2 (config b) [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) | 4.39 | 4.43 \u00b1 0.03 | 4.89 \u00b1 0.03 | `trainval` |\n | stylegan2 (config c) [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) | 4.38 | 4.40 \u00b1 0.02 | 4.79 \u00b1 0.02 | `trainval` |\n | stylegan2 (config d) [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) | 4.34 | 4.34 \u00b1 0.02 | 4.78 \u00b1 0.03 | `trainval` |\n | stylegan2 (config e) [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) | 3.31 | 3.33 \u00b1 0.02 | 3.79 \u00b1 0.02 | `trainval` |\n | stylegan2 (config f) [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl) | 2.84 | 2.83 +- 0.03 | 3.06 +- 0.02 | `trainval` |\n | stylegan2 [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl) | n/a | 2.76 \u00b1 0.03 | 2.98 \u00b1 0.03 | `trainval70k` |\n \n<br>\n\n**140k - images @ 256x256 (entire training set with horizontal flips)**\n the 70k images from `trainval70k` set is used as the reference images and compared to 50k generated images.\n| model     | legacy-fid<br>(reported) | legacy-fid<br>(reproduced)    | clean-fid  |\n| :---      | :-:          | :-:          | :-:         |\n| zcr [[zhao et al, 2020]](https://arxiv.org/abs/2002.04724) \u2020                  | 3.45 \u00b1 0.19 | 3.29 \u00b1 0.01 | 3.40 \u00b1 0.01 |\n| stylegan2 [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) \u2020          | 3.66 \u00b1 0.10 | 3.57 \u00b1 0.03 | 3.73 \u00b1 0.03 |\n| pa-gan [[zhang and khoreva et al, 2019]](https://arxiv.org/abs/1901.10422) \u2020  | 3.78 \u00b1 0.06 | 3.67 \u00b1 0.03 | 3.81 \u00b1 0.03 |\n| stylegan2-ada [[karras et al, 2020]](https://arxiv.org/abs/2006.06676) \u2020      | 3.88 \u00b1 0.13 | 3.84 \u00b1 0.02 | 3.93 \u00b1 0.02 |\n| auxiliary rotation [[chen et al, 2019]](https://arxiv.org/abs/1811.11212) \u2020   | 4.16 \u00b1 0.05 | 4.10 \u00b1 0.02 | 4.29 \u00b1 0.03 |\n| adaptive dropout [[karras et al, 2020]](https://arxiv.org/abs/2006.06676) \u2020   | 4.16 \u00b1 0.05 | 4.09 \u00b1 0.02 | 4.20 \u00b1 0.02 |\n| spectral norm [[miyato et al, 2018]](https://arxiv.org/abs/1802.05957) \u2020      | 4.60 \u00b1 0.19 | 4.43 \u00b1 0.02 | 4.65 \u00b1 0.02 |\n| wgan-gp [[gulrajani et al, 2017]](https://arxiv.org/abs/1704.00028) \u2020         | 6.54 \u00b1 0.37 | 6.19 \u00b1 0.03 | 6.62 \u00b1 0.03 |\n\n\u2020 reported by [[karras et al, 2020]](https://arxiv.org/abs/2006.06676)\n<br>\n\n**30k - images @ 256x256 (few shot generation)**<br>\n the 70k images from `trainval70k` set is used as the reference images and compared to 50k generated images.\n| model     | legacy-fid<br>(reported) | legacy-fid<br>(reproduced)    | clean-fid  |\n| :---:     | :-:          | :-:          | :-:         |\n| stylegan2 [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/stylegan2-ffhq-30k.pkl) | 6.16 | 6.14 \u00b1 0.064 | 6.49 \u00b1 0.068 |\n| diffaugment-stylegan2 [[zhao et al, 2020]](https://arxiv.org/pdf/2006.10738.pdf) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/diffaugment-stylegan2-ffhq-30k.pkl) | 5.05 | 5.07 \u00b1 0.030 | 5.18 \u00b1 0.032 |\n\n**10k - images @ 256x256 (few shot generation)**<br>\nthe 70k images from `trainval70k` set is used as the reference images and compared to 50k generated images.\n| model     | legacy-fid<br>(reported) | legacy-fid<br>(reproduced)    | clean-fid  |\n| :---:     | :-:          | :-:          | :-:         |\n| stylegan2 [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/stylegan2-ffhq-10k.pkl) | 14.75 | 14.88 \u00b1 0.070 | 16.04 \u00b1 0.078 |\n| diffaugment-stylegan2 [[zhao et al, 2020]](https://arxiv.org/pdf/2006.10738.pdf) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/diffaugment-stylegan2-ffhq-10k.pkl) | 7.86 | 7.82 \u00b1 0.045 | 8.12 \u00b1 0.044 |\n\n\n**5k - images @ 256x256 (few shot generation)**<br>\nthe 70k images from `trainval70k` set is used as the reference images and compared to 50k generated images.\n| model     | legacy-fid<br>(reported) | legacy-fid<br>(reproduced)    | clean-fid  |\n| :---:     | :-:          | :-:          | :-:         |\n| stylegan2 [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/stylegan2-ffhq-5k.pkl) | 26.60 | 26.64 \u00b1 0.086 | 28.17 \u00b1 0.090 |\n| diffaugment-stylegan2 [[zhao et al, 2020]](https://arxiv.org/pdf/2006.10738.pdf) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/diffaugment-stylegan2-ffhq-5k.pkl) | 10.45 | 10.45 \u00b1 0.047 | 10.99 \u00b1 0.050 |\n\n**1k - images @ 256x256 (few shot generation)** <br>\nthe 70k images from `trainval70k` set is used as the reference images and compared to 50k generated images.\n| model     | legacy-fid<br>(reported) | legacy-fid<br>(reproduced)    | clean-fid  |\n| :---:     | :-:          | :-:          | :-:         |\n| stylegan2 [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/stylegan2-ffhq-1k.pkl) | 62.16 | 62.14 \u00b1 0.108 | 64.17 \u00b1 0.113 |\n| diffaugment-stylegan2 [[zhao et al, 2020]](https://arxiv.org/pdf/2006.10738.pdf) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/diffaugment-stylegan2-ffhq-1k.pkl) | 25.66 | 25.60 \u00b1 0.071 | 27.26 \u00b1 0.077 |\n\n <br>\n\n ### lsun categories\n \n **100% data**<br>\n the 50k images from `train` set is used as the reference images and compared to 50k generated images.\n| category | model     | legacy-fid<br>(reported)  | legacy-fid<br>(reproduced)    | clean-fid  |\n|:-: | :---:           | :-:                    | :-:          | :-:         |\noutdoor churches | stylegan2 [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-church-config-f.pkl) | 3.86 | 3.87 \u00b1 0.029 | 4.08 \u00b1 0.028 |\nhorses           | stylegan2 [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-horse-config-f.pkl)  | 3.43 | 3.41 \u00b1 0.021 | 3.62 \u00b1 0.023 |\ncat              | stylegan2 [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-cat-config-f.pkl)    | 6.93 | 7.02 \u00b1 0.039 | 7.47 \u00b1 0.035 |\n\n<br>\n\n**lsun cat - 30k images (few shot generation)**<br>\nall 1,657,264 images from `trainfull` split are used as the reference images and compared to 50k generated images.\n| model     | legacy-fid<br>(reported) | legacy-fid<br>(reproduced)    | clean-fid  |\n| :---:     | :-:          | :-:          | :-:         |\n| stylegan2-mirror-flips [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/stylegan2-lsun-cat-30k.pkl)\t| 10.12\t| 10.15 \u00b1 0.04\t| 10.87 \u00b1 0.04\t|\n| stylegan2-diff-augment [[zhao et al, 2020]](https://arxiv.org/pdf/2006.10738.pdf) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/diffaugment-stylegan2-lsun-cat-30k.pkl)\t| 9.68\t| 9.70 \u00b1 0.07\t| 10.25 \u00b1 0.07\t|\n\n**lsun cat - 10k images (few shot generation)**<br>\nall 1,657,264 images from `trainfull` split are used as the reference images and compared to 50k generated images.\n| model     | legacy-fid<br>(reported) | legacy-fid<br>(reproduced)    | clean-fid  |\n| :---:     | :-:          | :-:          | :-:         |\n| stylegan2-mirror-flips [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/stylegan2-lsun-cat-10k.pkl)\t| 17.93\t| 17.98 \u00b1 0.09\t| 18.71 \u00b1 0.09\t|\n| stylegan2-diff-augment [[zhao et al, 2020]](https://arxiv.org/pdf/2006.10738.pdf) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/diffaugment-stylegan2-lsun-cat-10k.pkl)\t| 12.07\t| 12.04 \u00b1 0.08\t| 12.53 \u00b1 0.08\t|\n\n**lsun cat - 5k images (few shot generation)**<br>\nall 1,657,264 images from `trainfull` split are used as the reference images and compared to 50k generated images.\n| model     | legacy-fid<br>(reported) | legacy-fid<br>(reproduced)    | clean-fid  |\n| :---:     | :-:          | :-:          | :-:         |\n| stylegan2-mirror-flips [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/stylegan2-lsun-cat-5k.pkl)\t| 34.69\t| 34.66 \u00b1 0.12\t| 35.85 \u00b1 0.12\t|\n| stylegan2-diff-augment [[zhao et al, 2020]](https://arxiv.org/pdf/2006.10738.pdf) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/diffaugment-stylegan2-lsun-cat-5k.pkl)\t| 16.11\t| 16.11 \u00b1 0.09\t| 16.79 \u00b1 0.09\t|\n\n**lsun cat - 1k images (few shot generation)**<br>\nall 1,657,264 images from `trainfull` split are used as the reference images and compared to 50k generated images.\n| model     | legacy-fid<br>(reported) | legacy-fid<br>(reproduced)    | clean-fid  |\n| :---:     | :-:          | :-:          | :-:         |\n| stylegan2-mirror-flips [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/stylegan2-lsun-cat-1k.pkl)\t| 182.85\t| 182.80 \u00b1 0.21\t| 185.86 \u00b1 0.21\t|\n| stylegan2-diff-augment [[zhao et al, 2020]](https://arxiv.org/pdf/2006.10738.pdf) [[ckpt]](https://hanlab.mit.edu/projects/data-efficient-gans/models/diffaugment-stylegan2-lsun-cat-1k.pkl)\t| 42.26\t| 42.07 \u00b1 0.16\t| 43.12 \u00b1 0.16\t|\n\n<br>\n\n### afhq (few shot generation)\n**afhq dog**<br>\nall 4739 images from `train` split are used as the reference images and compared to 50k generated images.\n| model     | legacy-fid<br>(reported) | legacy-fid<br>(reproduced)    | clean-fid  |\n| :---:     | :-:          | :-:          | :-:         |\n| stylegan2 [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/paper-fig11a-small-datasets/afhqdog-mirror-stylegan2-noaug.pkl)\t| 19.37\t| 19.34 \u00b1 0.08\t| 20.10 \u00b1 0.08\t| 9.62\t| 9.56 \u00b1 0.12\t| 10.21 \u00b1 0.11\t|\n| stylegan2-ada [[karras et al, 2020]](https://arxiv.org/abs/2006.06676) [[ckpt]](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/paper-fig11a-small-datasets/afhqdog-mirror-paper512-ada.pkl)\t| 7.40\t| 7.41 \u00b1 0.02\t| 7.61 \u00b1 0.02\t| 1.16\t| 1.17 \u00b1 0.03\t| 1.28 \u00b1 0.03\t|\n\n**afhq wild**<br>\nall 4738 images from `train` split are used as the reference images and compared to 50k generated images.\n| model     | legacy-fid<br>(reported) | legacy-fid<br>(reproduced)    | clean-fid  |\n| :---:     | :-:          | :-:          | :-:         |\n| stylegan2 [[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/paper-fig11a-small-datasets/afhqwild-mirror-stylegan2-noaug.pkl)\t| 3.48\t| 3.55 \u00b1 0.03\t| 3.66 \u00b1 0.02\t| 0.77\t| 0.78 \u00b1 0.02\t| 0.83 \u00b1 0.01\t|\n| stylegan2-ada [[karras et al, 2020]](https://arxiv.org/abs/2006.06676) [[ckpt]](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/paper-fig11a-small-datasets/afhqwild-mirror-paper512-ada.pkl)\t| 3.05\t| 3.01 \u00b1 0.02\t| 3.03 \u00b1 0.02\t| 0.45\t| 0.45 \u00b1 0.01\t| 0.45 \u00b1 0.01\t|\n\n<br>\n\n### brecahad (few shot generation)\nall 1944 images from `train` split are used as the reference images and compared to 50k generated images.\n| model     | legacy<br>fid<br>(reported) | legacy<br>fid<br>(reproduced)    | clean-fid  | legacy<br>kid<br>(reported)<br>10^3 | legacy<br>kid<br>(reproduced)<br>10^3    | clean<br>kid<br>10^3  |\n| :---:     | :-:          | :-: | :-: | :-: | :-: | :-: |\n| stylegan2\t[[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/paper-fig11a-small-datasets/brecahad-mirror-stylegan2-noaug.pkl)\t| 97.72\t| 97.46 \u00b1 0.17\t| 98.35 \u00b1 0.17\t| 89.76\t| 89.90 \u00b1 0.31\t| 92.51 \u00b1 0.32\t|\n| stylegan2-ada\t[[karras et al, 2020]](https://arxiv.org/abs/2006.06676) [[ckpt]](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/paper-fig11a-small-datasets/brecahad-mirror-paper512-ada.pkl)\t| 15.71\t| 15.70 \u00b1 0.06\t| 15.63 \u00b1 0.06\t| 2.88\t| 2.93 \u00b1 0.08\t| 3.08 \u00b1 0.08\t|\n\n<br>\n\n### metfaces (few shot generation)\nall 1336 images from `train` split are used as the reference images and compared to 50k generated images.\n| model     | legacy<br>fid<br>(reported) | legacy<br>fid<br>(reproduced)    | clean-fid  | legacy<br>kid<br>(reported)<br>10^3 | legacy<br>kid<br>(reproduced)<br>10^3    | clean<br>kid<br>10^3  |\n| :---:     | :-:          | :-: | :-: | :-: | :-: | :-: |\n| stylegan2\t[[karras et al, 2020]](https://arxiv.org/abs/1912.04958) [[ckpt]](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/paper-fig11a-small-datasets/metfaces-mirror-stylegan2-noaug.pkl)\t| 57.26\t| 57.36 \u00b1 0.10\t| 65.74 \u00b1 0.11\t| 35.66\t| 35.69 \u00b1 0.16\t| 40.90 \u00b1 0.14\t|\n| stylegan2-ada\t[[karras et al, 2020]](https://arxiv.org/abs/2006.06676) [[ckpt]](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/paper-fig11a-small-datasets/metfaces-mirror-paper1024-ada.pkl)\t| 18.22\t| 18.18 \u00b1 0.03\t| 19.60 \u00b1 0.03\t| 2.41\t| 2.38 \u00b1 0.05\t| 2.86 \u00b1 0.04\t|\n\n\n<br>\n\n### horse2zebra (image to image translation)\nall 140 images from `test` split are used as the reference images and compared to 120 translated images.\n\n| model     | legacy-fid<br>(reported) | legacy-fid<br>(reproduced)    | clean-fid  |\n| :---      | :-:          | :-:          | :-:         |\n| cut     [[park et al, 2020]](https://arxiv.org/abs/2007.15651)| 45.5 | 45.51 | 43.71 |\n| distance     [[benaim and wolf et al, 2017]](https://arxiv.org/pdf/1706.00826.pdf) reported by [[park et al, 2020]](https://arxiv.org/abs/2007.15651)   | 72.0 | 71.97 | 71.01 |\n| fastcut [[park et al, 2020]](https://arxiv.org/abs/2007.15651) | 73.4 | 73.38 | 72.53 |\n| cyclegan [[zhu et al, 2017]](https://arxiv.org/pdf/1703.10593.pdf) reported by [[park et al, 2020]](https://arxiv.org/abs/2007.15651)       | 77.2  | 77.20  | 75.17 |\n| selfdistance [[benaim and wolf et al, 2017]](https://arxiv.org/pdf/1706.00826.pdf) reported by [[park et al, 2020]](https://arxiv.org/abs/2007.15651)   | 80.8 | 80.78 | 79.28 |\n| gcgan        [[fu et al, 2019]](https://arxiv.org/pdf/1809.05852.pdf) reported by [[park et al, 2020]](https://arxiv.org/abs/2007.15651)                | 86.7 | 85.86 | 83.65 |\n| munit  [[huang et al, 2018]](https://arxiv.org/pdf/1804.04732.pdf) reported by [[park et al, 2020]](https://arxiv.org/abs/2007.15651)    | 133.8 | - \u2020 |120.48 |\n| drit   [[lee et al, 2017]](https://arxiv.org/pdf/1808.00948.pdf) reported by [[park et al, 2020]](https://arxiv.org/abs/2007.15651)      | 140.0 | - \u2020 | 99.56 |\n\n\u2020 the translated images for these methods were intitially compared by [[park et al, 2020]](https://arxiv.org/abs/2007.15651) using .jpeg compression. \nwe retrain these two methods using the same protocal and generate the images as .png for a fair comparision. \n\n<br>\n\n### cat2dog (image to image translation)\nall 500 images from `test` split are used as the reference images and compared to 500 translated images.\n| model     | legacy-fid<br>(reported) | legacy-fid<br>(reproduced)    | clean-fid  |\n| :---      | :-:          | :-:          | :-:         |\n| cut     [[park et al, 2020]](https://arxiv.org/abs/2007.15651) | 76.2 | 76.21 | 77.58 |\n| fastcut [[park et al, 2020]](https://arxiv.org/abs/2007.15651) | 94.0 | 93.95 | 95.37 |\n| gcgan   [[fu et al, 2019]](https://arxiv.org/pdf/1809.05852.pdf) reported by [[park et al, 2020]](https://arxiv.org/abs/2007.15651) | 96.6 | 96.61 | 96.49 |\n| munit  [[huang et al, 2018]](https://arxiv.org/pdf/1804.04732.pdf) reported by [[park et al, 2020]](https://arxiv.org/abs/2007.15651) | 104.4 | - \u2020 | 123.73 |\n| drit [[lee et al, 2017]](https://arxiv.org/pdf/1808.00948.pdf) reported by [[park et al, 2020]](https://arxiv.org/abs/2007.15651) | 123.4 | - \u2020 | 127.21 |\n| selfdistance [[benaim and wolf et al, 2017]](https://arxiv.org/pdf/1706.00826.pdf) reported by [[park et al, 2020]](https://arxiv.org/abs/2007.15651) | 144.4 | 144.42 | 147.23 |\n| distance     [[benaim and wolf et al, 2017]](https://arxiv.org/pdf/1706.00826.pdf) reported by [[park et al, 2020]](https://arxiv.org/abs/2007.15651) | 155.3 | 155.34 | 158.39 |\n\n\u2020 the translated images for these methods were intitially compared by [[park et al, 2020]](https://arxiv.org/abs/2007.15651) using .jpeg compression. \nwe retrain these two methods using the same protocal and generate the images as .png for a fair comparision. \n\n---\n\n### related projects\n[torch-fidelity](https://github.com/toshas/torch-fidelity): high-fidelity performance metrics for generative models in pytorch. <br>\n[ttur](https://github.com/bioinf-jku/ttur): two time-scale update rule for training gans. <br>\n[lpips](https://github.com/richzhang/perceptualsimilarity): perceptual similarity metric and dataset. <br>\n\n\n--- \n### licenses\nall material in this repository is made available under the [mit license](https://github.com/gaparmar/clean-fid/blob/main/license). \n\n[inception_pytorch.py](https://github.com/gaparmar/clean-fid/blob/main/cleanfid/inception_pytorch.py) is derived from the pytorch implementation of fid provided by maximilian seitzer. these files were originally shared under the [apache 2.0 license.](https://github.com/mseitzer/pytorch-fid/blob/master/license)  \n\n[inception-2015-12-05.pt](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/inception-2015-12-05.pt) is a torchscript model of the pre-trained inception-v3 network by christian szegedy, vincent vanhoucke, sergey ioffe, jonathon shlens, and zbigniew wojna. the network was originally shared under apache 2.0 license on the tensorflow models repository. the torchscript wrapper is provided by tero karras and miika aittala and janne hellsten and samuli laine and jaakko lehtinen and timo aila which is released under the [nvidia source code license.](https://nvlabs.github.io/stylegan2-ada-pytorch/license.html) \n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "clean-fid",
  "package_url": "https://pypi.org/project/clean-fid/",
  "project_url": "https://pypi.org/project/clean-fid/",
  "project_urls": {
    "Homepage": "https://github.com/GaParmar/clean-fid"
  },
  "release_url": "https://pypi.org/project/clean-fid/0.1.35/",
  "requires_dist": [
    "torch",
    "torchvision",
    "numpy (>=1.14.3)",
    "scipy (>=1.0.1)",
    "tqdm (>=4.28.1)",
    "pillow (>=8.1)",
    "requests"
  ],
  "requires_python": "",
  "summary": "fid calculation in pytorch with proper image resizing and quantization steps",
  "version": "0.1.35",
  "releases": [],
  "developers": [
    "gaurav_parmar",
    "gparmar@andrew.cmu.edu"
  ],
  "kwds": "cleanfid fid_steps fid compute_fid cleanfid_demo_folders",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_clean_fid",
  "homepage": "https://github.com/gaparmar/clean-fid",
  "release_count": 35,
  "dependency_ids": [
    "pypi_numpy",
    "pypi_pillow",
    "pypi_requests",
    "pypi_scipy",
    "pypi_torch",
    "pypi_torchvision",
    "pypi_tqdm"
  ]
}