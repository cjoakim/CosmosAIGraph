{
  "classifiers": [
    "license :: osi approved :: apache software license",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-dark.png\">\n    <img alt=\"vllm\" src=\"https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png\" width=55%>\n  </picture>\n</p>\n\n<h3 align=\"center\">\neasy, fast, and cheap llm serving for everyone\n</h3>\n\n<p align=\"center\">\n| <a href=\"https://docs.vllm.ai\"><b>documentation</b></a> | <a href=\"https://vllm.ai\"><b>blog</b></a> | <a href=\"https://arxiv.org/abs/2309.06180\"><b>paper</b></a> | <a href=\"https://discord.gg/jz7wjkhh6g\"><b>discord</b></a> |\n\n</p>\n\n---\n\n*latest news* \ud83d\udd25\n- [2023/12] added rocm support to vllm.\n- [2023/10] we hosted [the first vllm meetup](https://lu.ma/first-vllm-meetup) in sf! please find the meetup slides [here](https://docs.google.com/presentation/d/1ql-xpfxifpdbh86dbeegfxbxfxjix4v032ghshbkf3s/edit?usp=sharing).\n- [2023/09] we created our [discord server](https://discord.gg/jz7wjkhh6g)! join us to discuss vllm and llm serving! we will also post the latest announcements and updates there.\n- [2023/09] we released our [pagedattention paper](https://arxiv.org/abs/2309.06180) on arxiv!\n- [2023/08] we would like to express our sincere gratitude to [andreessen horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vllm.\n- [2023/07] added support for llama-2! you can run and serve 7b/13b/70b llama-2s on vllm with a single command!\n- [2023/06] serving vllm on any cloud with skypilot. check out a 1-click [example](https://github.com/skypilot-org/skypilot/blob/master/llm/vllm) to start the vllm demo, and the [blog post](https://blog.skypilot.co/serving-llm-24x-faster-on-the-cloud-with-vllm-and-skypilot/) for the story behind vllm development on the clouds.\n- [2023/06] we officially released vllm! fastchat-vllm integration has powered [lmsys vicuna and chatbot arena](https://chat.lmsys.org) since mid-april. check out our [blog post](https://vllm.ai).\n\n---\n\nvllm is a fast and easy-to-use library for llm inference and serving.\n\nvllm is fast with:\n\n- state-of-the-art serving throughput\n- efficient management of attention key and value memory with **pagedattention**\n- continuous batching of incoming requests\n- fast model execution with cuda/hip graph\n- quantization: [gptq](https://arxiv.org/abs/2210.17323), [awq](https://arxiv.org/abs/2306.00978), [squeezellm](https://arxiv.org/abs/2306.07629)\n- optimized cuda kernels\n\nvllm is flexible and easy to use with:\n\n- seamless integration with popular hugging face models\n- high-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more\n- tensor parallelism support for distributed inference\n- streaming outputs\n- openai-compatible api server\n- support nvidia gpus and amd gpus\n\nvllm seamlessly supports many hugging face models, including the following architectures:\n\n- aquila & aquila2 (`baai/aquilachat2-7b`, `baai/aquilachat2-34b`, `baai/aquila-7b`, `baai/aquilachat-7b`, etc.)\n- baichuan & baichuan2 (`baichuan-inc/baichuan2-13b-chat`, `baichuan-inc/baichuan-7b`, etc.)\n- bloom (`bigscience/bloom`, `bigscience/bloomz`, etc.)\n- chatglm (`thudm/chatglm2-6b`, `thudm/chatglm3-6b`, etc.)\n- falcon (`tiiuae/falcon-7b`, `tiiuae/falcon-40b`, `tiiuae/falcon-rw-7b`, etc.)\n- gpt-2 (`gpt2`, `gpt2-xl`, etc.)\n- gpt bigcode (`bigcode/starcoder`, `bigcode/gpt_bigcode-santacoder`, etc.)\n- gpt-j (`eleutherai/gpt-j-6b`, `nomic-ai/gpt4all-j`, etc.)\n- gpt-neox (`eleutherai/gpt-neox-20b`, `databricks/dolly-v2-12b`, `stabilityai/stablelm-tuned-alpha-7b`, etc.)\n- internlm (`internlm/internlm-7b`, `internlm/internlm-chat-7b`, etc.)\n- llama & llama-2 (`meta-llama/llama-2-70b-hf`, `lmsys/vicuna-13b-v1.3`, `young-geng/koala`, `openlm-research/open_llama_13b`, etc.)\n- mistral (`mistralai/mistral-7b-v0.1`, `mistralai/mistral-7b-instruct-v0.1`, etc.)\n- mixtral (`mistralai/mixtral-8x7b-v0.1`, `mistralai/mixtral-8x7b-instruct-v0.1`, etc.)\n- mpt (`mosaicml/mpt-7b`, `mosaicml/mpt-30b`, etc.)\n- opt (`facebook/opt-66b`, `facebook/opt-iml-max-30b`, etc.)\n- phi (`microsoft/phi-1_5`, `microsoft/phi-2`, etc.)\n- qwen (`qwen/qwen-7b`, `qwen/qwen-7b-chat`, etc.)\n- yi (`01-ai/yi-6b`, `01-ai/yi-34b`, etc.)\n\ninstall vllm with pip or [from source](https://vllm.readthedocs.io/en/latest/getting_started/installation.html#build-from-source):\n\n```bash\npip install vllm\n```\n\n## getting started\n\nvisit our [documentation](https://vllm.readthedocs.io/en/latest/) to get started.\n- [installation](https://vllm.readthedocs.io/en/latest/getting_started/installation.html)\n- [quickstart](https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html)\n- [supported models](https://vllm.readthedocs.io/en/latest/models/supported_models.html)\n\n## contributing\n\nwe welcome and value any contributions and collaborations.\nplease check out [contributing.md](./contributing.md) for how to get involved.\n\n## citation\n\nif you use vllm for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):\n```bibtex\n@inproceedings{kwon2023efficient,\n  title={efficient memory management for large language model serving with pagedattention},\n  author={woosuk kwon and zhuohan li and siyuan zhuang and ying sheng and lianmin zheng and cody hao yu and joseph e. gonzalez and hao zhang and ion stoica},\n  booktitle={proceedings of the acm sigops 29th symposium on operating systems principles},\n  year={2023}\n}\n```\n",
  "docs_url": null,
  "keywords": "",
  "license": "apache 2.0",
  "name": "vllm",
  "package_url": "https://pypi.org/project/vllm/",
  "project_url": "https://pypi.org/project/vllm/",
  "project_urls": {
    "Documentation": "https://vllm.readthedocs.io/en/latest/",
    "Homepage": "https://github.com/vllm-project/vllm"
  },
  "release_url": "https://pypi.org/project/vllm/0.2.6/",
  "requires_dist": [
    "ninja",
    "psutil",
    "ray >=2.5.1",
    "pandas",
    "pyarrow",
    "sentencepiece",
    "numpy",
    "torch ==2.1.2",
    "transformers >=4.36.0",
    "xformers ==0.0.23.post1",
    "fastapi",
    "uvicorn[standard]",
    "pydantic ==1.10.13",
    "aioprometheus[starlette]"
  ],
  "requires_python": ">=3.8",
  "summary": "a high-throughput and memory-efficient inference and serving engine for llms",
  "version": "0.2.6",
  "releases": [],
  "developers": [
    "vllm_team"
  ],
  "kwds": "vllm logos logo html href",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_vllm",
  "homepage": "https://github.com/vllm-project/vllm",
  "release_count": 17,
  "dependency_ids": [
    "pypi_aioprometheus",
    "pypi_fastapi",
    "pypi_ninja",
    "pypi_numpy",
    "pypi_pandas",
    "pypi_psutil",
    "pypi_pyarrow",
    "pypi_pydantic",
    "pypi_ray",
    "pypi_sentencepiece",
    "pypi_torch",
    "pypi_transformers",
    "pypi_uvicorn",
    "pypi_xformers"
  ]
}