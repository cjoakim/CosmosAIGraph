{
  "classifiers": [
    "development status :: 4 - beta",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: mit license",
    "programming language :: python :: 3",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "[![ci](https://github.com/guillaumekln/faster-whisper/workflows/ci/badge.svg)](https://github.com/guillaumekln/faster-whisper/actions?query=workflow%3aci) [![pypi version](https://badge.fury.io/py/faster-whisper.svg)](https://badge.fury.io/py/faster-whisper)\n\n# faster whisper transcription with ctranslate2\n\n**faster-whisper** is a reimplementation of openai's whisper model using [ctranslate2](https://github.com/opennmt/ctranslate2/), which is a fast inference engine for transformer models.\n\nthis implementation is up to 4 times faster than [openai/whisper](https://github.com/openai/whisper) for the same accuracy while using less memory. the efficiency can be further improved with 8-bit quantization on both cpu and gpu.\n\n## benchmark\n\nfor reference, here's the time and memory usage that are required to transcribe [**13 minutes**](https://www.youtube.com/watch?v=0u7ttptbo9i) of audio using different implementations:\n\n* [openai/whisper](https://github.com/openai/whisper)@[6dea21fd](https://github.com/openai/whisper/commit/6dea21fd7f7253bfe450f1e2512a0fe47ee2d258)\n* [whisper.cpp](https://github.com/ggerganov/whisper.cpp)@[3b010f9](https://github.com/ggerganov/whisper.cpp/commit/3b010f9bed9a6068609e9faf52383aea792b0362)\n* [faster-whisper](https://github.com/guillaumekln/faster-whisper)@[cce6b53e](https://github.com/guillaumekln/faster-whisper/commit/cce6b53e4554f71172dad188c45f10fb100f6e3e)\n\n### large-v2 model on gpu\n\n| implementation | precision | beam size | time | max. gpu memory | max. cpu memory |\n| --- | --- | --- | --- | --- | --- |\n| openai/whisper | fp16 | 5 | 4m30s | 11325mb | 9439mb |\n| faster-whisper | fp16 | 5 | 54s | 4755mb | 3244mb |\n| faster-whisper | int8 | 5 | 59s | 3091mb | 3117mb |\n\n*executed with cuda 11.7.1 on a nvidia tesla v100s.*\n\n### small model on cpu\n\n| implementation | precision | beam size | time | max. memory |\n| --- | --- | --- | --- | --- |\n| openai/whisper | fp32 | 5 | 10m31s | 3101mb |\n| whisper.cpp | fp32 | 5 | 17m42s | 1581mb |\n| whisper.cpp | fp16 | 5 | 12m39s | 873mb |\n| faster-whisper | fp32 | 5 | 2m44s | 1675mb |\n| faster-whisper | int8 | 5 | 2m04s | 995mb |\n\n*executed with 8 threads on a intel(r) xeon(r) gold 6226r.*\n\n## requirements\n\n* python 3.8 or greater\n\nunlike openai-whisper, ffmpeg does **not** need to be installed on the system. the audio is decoded with the python library [pyav](https://github.com/pyav-org/pyav) which bundles the ffmpeg libraries in its package.\n\n### gpu\n\ngpu execution requires the following nvidia libraries to be installed:\n\n* [cublas for cuda 11](https://developer.nvidia.com/cublas)\n* [cudnn 8 for cuda 11](https://developer.nvidia.com/cudnn)\n\nthere are multiple ways to install these libraries. the recommended way is described in the official nvidia documentation, but we also suggest other installation methods below.\n\n<details>\n<summary>other installation methods (click to expand)</summary>\n\n#### use docker\n\nthe libraries are installed in this official nvidia docker image: `nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04`.\n\n#### install with `pip` (linux only)\n\non linux these libraries can be installed with `pip`. note that `ld_library_path` must be set before launching python.\n\n```bash\npip install nvidia-cublas-cu11 nvidia-cudnn-cu11\n\nexport ld_library_path=`python3 -c 'import os; import nvidia.cublas.lib; import nvidia.cudnn.lib; print(os.path.dirname(nvidia.cublas.lib.__file__) + \":\" + os.path.dirname(nvidia.cudnn.lib.__file__))'`\n```\n\n#### download the libraries from purfview's repository (windows only)\n\npurfview's [whisper-standalone-win](https://github.com/purfview/whisper-standalone-win) provides the required nvidia libraries for windows in a [single archive](https://github.com/purfview/whisper-standalone-win/releases/tag/libs). decompress the archive and place the libraries in a directory included in the `path`.\n\n</details>\n\n## installation\n\nthe module can be installed from [pypi](https://pypi.org/project/faster-whisper/):\n\n```bash\npip install faster-whisper\n```\n\n<details>\n<summary>other installation methods (click to expand)</summary>\n\n### install the master branch\n\n```bash\npip install --force-reinstall \"faster-whisper @ https://github.com/guillaumekln/faster-whisper/archive/refs/heads/master.tar.gz\"\n```\n\n### install a specific commit\n\n```bash\npip install --force-reinstall \"faster-whisper @ https://github.com/guillaumekln/faster-whisper/archive/a4f1cc8f11433e454c3934442b5e1a4ed5e865c3.tar.gz\"\n```\n\n</details>\n\n## usage\n\n```python\nfrom faster_whisper import whispermodel\n\nmodel_size = \"large-v3\"\n\n# run on gpu with fp16\nmodel = whispermodel(model_size, device=\"cuda\", compute_type=\"float16\")\n\n# or run on gpu with int8\n# model = whispermodel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n# or run on cpu with int8\n# model = whispermodel(model_size, device=\"cpu\", compute_type=\"int8\")\n\nsegments, info = model.transcribe(\"audio.mp3\", beam_size=5)\n\nprint(\"detected language '%s' with probability %f\" % (info.language, info.language_probability))\n\nfor segment in segments:\n    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n```\n\n**warning:** `segments` is a *generator* so the transcription only starts when you iterate over it. the transcription can be run to completion by gathering the segments in a list or a `for` loop:\n\n```python\nsegments, _ = model.transcribe(\"audio.mp3\")\nsegments = list(segments)  # the transcription will actually run here.\n```\n\n### word-level timestamps\n\n```python\nsegments, _ = model.transcribe(\"audio.mp3\", word_timestamps=true)\n\nfor segment in segments:\n    for word in segment.words:\n        print(\"[%.2fs -> %.2fs] %s\" % (word.start, word.end, word.word))\n```\n\n### vad filter\n\nthe library integrates the [silero vad](https://github.com/snakers4/silero-vad) model to filter out parts of the audio without speech:\n\n```python\nsegments, _ = model.transcribe(\"audio.mp3\", vad_filter=true)\n```\n\nthe default behavior is conservative and only removes silence longer than 2 seconds. see the available vad parameters and default values in the [source code](https://github.com/guillaumekln/faster-whisper/blob/master/faster_whisper/vad.py). they can be customized with the dictionary argument `vad_parameters`:\n\n```python\nsegments, _ = model.transcribe(\n    \"audio.mp3\",\n    vad_filter=true,\n    vad_parameters=dict(min_silence_duration_ms=500),\n)\n```\n\n### logging\n\nthe library logging level can be configured like this:\n\n```python\nimport logging\n\nlogging.basicconfig()\nlogging.getlogger(\"faster_whisper\").setlevel(logging.debug)\n```\n\n### going further\n\nsee more model and transcription options in the [`whispermodel`](https://github.com/guillaumekln/faster-whisper/blob/master/faster_whisper/transcribe.py) class implementation.\n\n## community integrations\n\nhere is a non exhaustive list of open-source projects using faster-whisper. feel free to add your project to the list!\n\n* [whisper-ctranslate2](https://github.com/softcatala/whisper-ctranslate2) is a command line client based on faster-whisper and compatible with the original client from openai/whisper.\n* [whisper-diarize](https://github.com/mahmoudashraf97/whisper-diarization) is a speaker diarization tool that is based on faster-whisper and nvidia nemo.\n* [whisper-standalone-win](https://github.com/purfview/whisper-standalone-win) contains the portable ready to run binaries of faster-whisper for windows.\n* [asr-sd-pipeline](https://github.com/hedrergudene/asr-sd-pipeline) provides a scalable, modular, end to end multi-speaker speech to text solution implemented using azureml pipelines.\n* [open-lyrics](https://github.com/zh-plus/open-lyrics) is a python library that transcribes voice files using faster-whisper, and translates/polishes the resulting text into `.lrc` files in the desired language using openai-gpt.\n* [wscribe](https://github.com/geekodour/wscribe) is a flexible transcript generation tool supporting faster-whisper, it can export word level transcript and the exported transcript then can be edited with [wscribe-editor](https://github.com/geekodour/wscribe-editor)\n\n## model conversion\n\nwhen loading a model from its size such as `whispermodel(\"large-v3\")`, the correspondig ctranslate2 model is automatically downloaded from the [hugging face hub](https://huggingface.co/systran).\n\nwe also provide a script to convert any whisper models compatible with the transformers library. they could be the original openai models or user fine-tuned models.\n\nfor example the command below converts the [original \"large-v3\" whisper model](https://huggingface.co/openai/whisper-large-v3) and saves the weights in fp16:\n\n```bash\npip install transformers[torch]>=4.23\n\nct2-transformers-converter --model openai/whisper-large-v3 --output_dir whisper-large-v3-ct2\n--copy_files tokenizer.json preprocessor_config.json --quantization float16\n```\n\n* the option `--model` accepts a model name on the hub or a path to a model directory.\n* if the option `--copy_files tokenizer.json` is not used, the tokenizer configuration is automatically downloaded when the model is loaded later.\n\nmodels can also be converted from the code. see the [conversion api](https://opennmt.net/ctranslate2/python/ctranslate2.converters.transformersconverter.html).\n\n### load a converted model\n\n1. directly load the model from a local directory:\n```python\nmodel = faster_whisper.whispermodel(\"whisper-large-v3-ct2\")\n```\n\n2. [upload your model to the hugging face hub](https://huggingface.co/docs/transformers/model_sharing#upload-with-the-web-interface) and load it from its name:\n```python\nmodel = faster_whisper.whispermodel(\"username/whisper-large-v3-ct2\")\n```\n\n## comparing performance against other implementations\n\nif you are comparing the performance against other whisper implementations, you should make sure to run the comparison with similar settings. in particular:\n\n* verify that the same transcription options are used, especially the same beam size. for example in openai/whisper, `model.transcribe` uses a default beam size of 1 but here we use a default beam size of 5.\n* when running on cpu, make sure to set the same number of threads. many frameworks will read the environment variable `omp_num_threads`, which can be set when running your script:\n\n```bash\nomp_num_threads=4 python3 my_script.py\n```",
  "docs_url": null,
  "keywords": "openai whisper speech ctranslate2 inference quantization transformer",
  "license": "mit",
  "name": "faster-whisper",
  "package_url": "https://pypi.org/project/faster-whisper/",
  "project_url": "https://pypi.org/project/faster-whisper/",
  "project_urls": {
    "Homepage": "https://github.com/guillaumekln/faster-whisper"
  },
  "release_url": "https://pypi.org/project/faster-whisper/0.10.0/",
  "requires_dist": [],
  "requires_python": ">=3.8",
  "summary": "faster whisper transcription with ctranslate2",
  "version": "0.10.0",
  "releases": [],
  "developers": [
    "guillaume_klein"
  ],
  "kwds": "faster_whisper whisper ctranslate2 whispermodel quantization",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_faster_whisper",
  "homepage": "https://github.com/guillaumekln/faster-whisper",
  "release_count": 12,
  "dependency_ids": []
}