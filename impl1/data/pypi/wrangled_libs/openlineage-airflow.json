{
  "classifiers": [],
  "description": "# openlineage airflow integration\n\na library that integrates [airflow `dags`]() with [openlineage](https://openlineage.io) for automatic metadata collection.\n\n## native integration with airflow\n\nstarting from airflow version 2.7.0 openlineage integration is included in airflow repository as a provider.\nthe `apache-airflow-providers-openlineage` \n[package](https://airflow.apache.org/docs/apache-airflow-providers-openlineage) \nsignificantly ease lineage tracking in airflow, \nensuring stability by embedding the functionality directly into each provider and \nsimplifying the process for users to select and manage lineage collection consumers.\n\nas a result, **starting from airflow 2.7.0** one should use the native airflow openlineage provider \n[package](https://airflow.apache.org/docs/apache-airflow-providers-openlineage).  \n\nthe ongoing development and enhancements will be focused on the `apache-airflow-providers-openlineage` package, \nwhile the `openlineage-airflow` will primarily be updated for bug fixes.\n\n## features\n\n**metadata**\n\n* task lifecycle\n* task parameters\n* task runs linked to **versioned** code\n* task inputs / outputs\n\n**lineage**\n\n* track inter-dag dependencies\n\n**built-in**\n\n* sql parser\n* link to code builder (ex: **github**)\n* metadata extractors\n\n## requirements\n\n- [python 3.8](https://www.python.org/downloads)\n- [airflow >= 2.1,<2.8](https://pypi.org/project/apache-airflow)\n\n## installation\n\n```bash\n$ pip3 install openlineage-airflow\n```\n\n> **note:** you can also add `openlineage-airflow` to your `requirements.txt` for airflow.\n\nto install from source, run:\n\n```bash\n$ python3 setup.py install\n```\n\n## setup\n\n### airflow 2.7+\n\nthis package **should not** be used starting with airflow 2.7.0 and **must not** be used with airflow 2.8+. \nit was designed as airflow's external integration that works mainly for airflow versions <2.7.\nfor airflow 2.7+ use the native airflow openlineage provider \n[package](https://airflow.apache.org/docs/apache-airflow-providers-openlineage) `apache-airflow-providers-openlineage`.\n\n### airflow 2.3 - 2.6\n\nthe integration automatically registers itself starting from airflow 2.3 if it's installed on the airflow worker's python.\nthis means you don't have to do anything besides configuring it, which is described in the configuration section.\n\n### airflow 2.1 - 2.2\n\nthis method has limited support: it does not support tracking failed jobs, and job starts are registered only when a job ends.\n\nset your lineagebackend in your [airflow.cfg](https://airflow.apache.org/docs/apache-airflow/stable/howto/set-config.html) or via environmental variable `airflow__lineage__backend` to \n```\nopenlineage.lineage_backend.openlineagebackend\n```\n\nin contrast to integration via subclassing a `dag`, a `lineagebackend`-based approach collects all metadata \nfor a task on each task's completion.\n\nthe openlineagebackend does not take into account manually configured inlets and outlets. \n\nwhen enabled, the library will:\n\n1. on dag **start**, collect metadata for each task using an `extractor` if it exists for a given operator.\n2. collect task input / output metadata (`source`, `schema`, etc.)\n3. collect task run-level metadata (execution time, state, parameters, etc.)\n4. on dag **complete**, also mark the task as _complete_ in openlineage\n\n## configuration\n\n### `http` backend environment variables\n\n`openlineage-airflow` uses the openlineage client to push data to openlineage backend.\n\nthe openlineage client depends on environment variables:\n\n* `openlineage_url` - point to the service that will consume openlineage events.\n* `openlineage_api_key` - set if the consumer of openlineage events requires a `bearer` authentication key.\n* `openlineage_namespace` - set if you are using something other than the `default` namespace for the job namespace.\n* `openlineage_airflow_disable_source_code` - set to `false` if you want the source code of callables provided in the pythonoperator to be sent in openlineage events.\n\nfor backwards compatibility, `openlineage-airflow` also supports configuration via\n`marquez_url`, `marquez_namespace` and `marquez_api_key` variables.\n\n```\nmarquez_url=http://my_hosted_marquez.example.com:5000\nmarquez_namespace=my_special_ns\n```\n\n### extractors : sending the correct data from your dags\n\nif you do nothing, the openlineage backend will receive the `job` and the `run` from your dags, but,\nunless you use one of the few operators for which this integration provides an extractor, input and output metadata will not be sent.\n\n`openlineage-airflow` allows you to do more than that by building \"extractors.\" an extractor is an object\nsuited to extract metadata from a particular operator (or operators). \n\n1. name : the name of the task\n2. inputs : a list of input datasets\n3. outputs : a list of output datasets\n4. context : the airflow context for the task\n\n#### bundled extractors\n\n`openlineage-airflow` provides extractors for:\n\n* `postgresoperator`\n* `mysqloperator`\n* `athenaoperator`\n* `bigqueryoperator`\n* `snowflakeoperator`\n* `trinooperator`\n* `greatexpectationsoperator`\n* `sftpoperator`\n* `ftpfiletransmitoperator`\n* `pythonoperator`\n* `redshiftdataoperator`, `redshiftsqloperator`\n* `sagemakerprocessingoperator`, `sagemakerprocessingoperatorasync`\n* `sagemakertrainingoperator`, `sagemakertrainingoperatorasync`\n* `sagemakertransformoperator`, `sagemakertransformoperatorasync`\n* `s3copyobjectextractor`, `s3filetransformextractor`\n* `gcstogcsoperator`\n* `dbtcloudrunjoboperator`\n\nsql operators utilize the sql parser. there is an experimental sql parser activated if you install [openlineage-sql](https://pypi.org/project/openlineage-sql) on your airflow worker.\n\n\n#### custom extractors\n\nif your dags contain additional operators from which you want to extract lineage data, fear not - you can always\nprovide custom extractors. they should derive from `baseextractor`. \n\nthere are two ways to register them for use in `openlineage-airflow`. \n\none way is to add them to the `openlineage_extractors` environment variable, separated by a semi-colon `(;)`. \n```\nopenlineage_extractors=full.path.to.extractorclass;full.path.to.anotherextractorclass\n```\n\nto ensure openlineage logging propagation to custom extractors you should use `self.log` instead of creating a logger yourself.\n\n#### default extractor\n\nwhen you own operators' code this is not necessary to provide custom extractors. you can also use default extractor's capability.\n\nin order to do that you should define at least one of two methods in operator:\n\n* `get_openlineage_facets_on_start()`\n\nextracts metadata on start of task.\n\n* `get_openlineage_facets_on_complete(task_instance: taskinstance)`\n\nextracts metadata on complete of task. this should accept `task_instance` argument, similar to `extract_on_complete` method in base extractors.\n\nif you don't define `get_openlineage_facets_on_complete` method it would fall back to `get_openlineage_facets_on_start`.\n\n\n#### great expectations\n\nthe great expectations integration works by providing an openlineagevalidationaction. you need to include it into your `action_list` in `great_expectations.yml`.\n\nthe following example illustrates a way to change the default configuration: \n```diff\nvalidation_operators:\n  action_list_operator:\n    # to learn how to configure sending slack notifications during evaluation\n    # (and other customizations), read: https://docs.greatexpectations.io/en/latest/autoapi/great_expectations/validation_operators/index.html#great_expectations.validation_operators.actionlistvalidationoperator\n    class_name: actionlistvalidationoperator\n    action_list:\n      - name: store_validation_result\n        action:\n          class_name: storevalidationresultaction\n      - name: store_evaluation_params\n        action:\n          class_name: storeevaluationparametersaction\n      - name: update_data_docs\n        action:\n          class_name: updatedatadocsaction\n+     - name: openlineage\n+       action:\n+         class_name: openlineagevalidationaction\n+         module_name: openlineage.common.provider.great_expectations.action\n      # - name: send_slack_notification_on_validation_result\n      #   action:\n      #     class_name: slacknotificationaction\n      #     # put the actual webhook url in the uncommitted/config_variables.yml file\n      #     slack_webhook: ${validation_notification_slack_webhook}\n      #     notify_on: all # possible values: \"all\", \"failure\", \"success\"\n      #     renderer:\n      #       module_name: great_expectations.render.renderer.slack_renderer\n      #       class_name: slackrenderer\n```\n\nif you're using `greatexpectationsoperator`, you need to set `validation_operator_name` to an operator that includes openlineagevalidationaction.\nsetting it in `great_expectations.yml` files isn't enough - the operator overrides it with the default name if a different one is not provided.\n\nto see an example of a working configuration, see [dag](https://github.com/openlineage/openlineage/blob/main/integration/airflow/tests/integration/airflow/dags/greatexpectations_dag.py) and [great expectations configuration](https://github.com/openlineage/openlineage/tree/main/integration/airflow/tests/integration/data/great_expectations) in the integration tests.\n\n### logging\nin addition to conventional logging approaches, the `openlineage-airflow` package provides an alternative way of configuring its logging behavior. by setting the `openlineage_airflow_logging` environment variable, you can establish the logging level for the `openlineage.airflow` and its child modules.\n\n## triggering child jobs\ncommonly, airflow dags will trigger processes on remote systems, such as an apache spark or apache\nbeam job. those systems may have their own openlineage integrations and report their own\njob runs and dataset inputs/outputs. to propagate the job hierarchy, tasks must send their own run\nids so that the downstream process can report the [parentrunfacet](https://github.com/openlineage/openlineage/blob/main/spec/openlineage.json#/definitions/parentrunfacet)\nwith the proper run id.\n\nthe `lineage_run_id` and `lineage_parent_id` macros exists to inject the run id or whole parent run information\nof a given task into the arguments sent to a remote processing job's airflow operator. the macro requires the \ndag `run_id` and the task to access the generated `run_id` for that task. for example, a spark job can be triggered\nusing the `dataprocpysparkoperator` with the correct parent `run_id` using the following configuration:\n\n```python\nt1 = dataprocpysparkoperator(\n    task_id=job_name,\n    #required pyspark configuration,\n    job_name=job_name,\n    dataproc_pyspark_properties={\n        'spark.driver.extrajavaoptions':\n            f\"-javaagent:{jar}={os.environ.get('openlineage_url')}/api/v1/namespaces/{os.getenv('openlineage_namespace', 'default')}/jobs/{job_name}/runs/{{{{macros.openlineageplugin.lineage_run_id(task, task_instance)}}}}?api_key={os.environ.get('openlineage_api_key')}\"\n        dag=dag)\n```\n\n## secrets redaction\nthe integration uses airflow secretsmasker to hide secrets from produced metadata events. as not all fields in the metadata should be redacted, `redactmixin` is used to pass information about which fields should be ignored by the process. \n\ntypically, you should subclass `redactmixin` and use the `_skip_redact` attribute as a list of names of fields to be skipped.\n\nhowever, all facets inheriting from `basefacet` should use the `_additional_skip_redact` attribute as an addition to the regular list (`['_producer', '_schemaurl']`).\n\n## development\n\nto install all dependencies for _local_ development:\n\nthe airflow integration depends on `openlineage.sql`, `openlineage.common` and `openlineage.client.python`. you should install them first independently or try to install them with following command:\n\n```bash\n$ pip install -r dev-requirements.txt\n```\n\nthere is also a bash script that can run an arbitrary airflow image with an openlineage integration built from the current branch. additionally, it mounts openlineage python packages as docker volumes. this enables you to change your code without the need to constantly rebuild docker images to run tests.\nrun it as:\n```bash\n$ airflow_image=<airflow_image_with_tag> ./scripts/run-dev-airflow.sh [--help]\n```\n### unit tests\nto run the entire unit test suite, use the below command:\n```bash\n$ tox\n```\nor choose one of the environments, e.g.:\n```bash\n$ tox -e py-airflow214\n```\nyou can also skip using `tox` and run `pytest` on your own dev environment.\n### integration tests\nthe integration tests require the use of _docker compose_. there are scripts prepared to make build images and run tests easier.\n\n```bash\n$ airflow_image=<name-of-airflow-image> ./tests/integration/docker/up.sh\n```\n\n```bash\n$ airflow_image=apache/airflow:2.3.1-python3.8 ./tests/integration/docker/up.sh\n```\n\nwhen using `run-dev-airflow.sh`, you can add the `-i` flag or `--attach-integration` flag to run integration tests in a dev environment.\nthis can be helpful when you need to run arbitrary integration tests during development. for example, the following command run in the integration container...\n```bash\npython -m pytest test_integration.py::test_integration[great_expectations_validation-requests/great_expectations.json]\n```\n...runs a single test which you can repeat after changes in code.\n\n----\nspdx-license-identifier: apache-2.0\\\ncopyright 2018-2023 contributors to the openlineage project\n",
  "docs_url": null,
  "keywords": "openlineage",
  "license": "",
  "name": "openlineage-airflow",
  "package_url": "https://pypi.org/project/openlineage-airflow/",
  "project_url": "https://pypi.org/project/openlineage-airflow/",
  "project_urls": null,
  "release_url": "https://pypi.org/project/openlineage-airflow/1.7.0/",
  "requires_dist": [
    "attrs >=20.0",
    "openlineage-integration-common[sql] ==1.7.0",
    "openlineage-python ==1.7.0",
    "apache-airflow-providers-postgres >=2.0.0 ; extra == 'airflow'",
    "apache-airflow-providers-mysql >=2.0.0 ; extra == 'airflow'",
    "apache-airflow-providers-trino >=3.1.0 ; extra == 'airflow'",
    "apache-airflow-providers-snowflake >=2.1.0 ; extra == 'airflow'",
    "apache-airflow-providers-google >=5.0.0 ; extra == 'airflow'",
    "apache-airflow-providers-amazon >=3.1.1 ; extra == 'airflow'",
    "apache-airflow-providers-sftp >=2.1.1 ; extra == 'airflow'",
    "apache-airflow-providers-ssh >=2.1.0 ; extra == 'airflow'",
    "apache-airflow-providers-ftp >=3.3.0 ; extra == 'airflow'",
    "apache-airflow-providers-dbt-cloud <3.2.0 ; extra == 'airflow'",
    "airflow-provider-great-expectations ==0.1.5 ; extra == 'airflow'",
    "great-expectations <=0.15.23 ; extra == 'airflow'",
    "protobuf <4.23,>=3.20 ; extra == 'airflow'",
    "aiohttp ; extra == 'dev'",
    "pytest ; extra == 'dev'",
    "pytest-cov ; extra == 'dev'",
    "pytest-mock ; extra == 'dev'",
    "mock ; extra == 'dev'",
    "SQLAlchemy ; extra == 'dev'",
    "Flask-SQLAlchemy ; extra == 'dev'",
    "pandas-gbq ==0.14.1 ; extra == 'dev'",
    "snowflake-connector-python ; extra == 'dev'",
    "aiohttp ; extra == 'tests'",
    "pytest ; extra == 'tests'",
    "pytest-cov ; extra == 'tests'",
    "pytest-mock ; extra == 'tests'",
    "mock ; extra == 'tests'",
    "SQLAlchemy ; extra == 'tests'",
    "Flask-SQLAlchemy ; extra == 'tests'",
    "pandas-gbq ==0.14.1 ; extra == 'tests'",
    "snowflake-connector-python ; extra == 'tests'"
  ],
  "requires_python": ">=3.8",
  "summary": "openlineage integration with airflow",
  "version": "1.7.0",
  "releases": [],
  "developers": [
    "openlineage"
  ],
  "kwds": "airflow__lineage__backend openlineage_airflow_logging openlineage_airflow_disable_source_code airflow214 airflow",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_openlineage_airflow",
  "homepage": "",
  "release_count": 60,
  "dependency_ids": [
    "pypi_aiohttp",
    "pypi_airflow_provider_great_expectations",
    "pypi_apache_airflow_providers_amazon",
    "pypi_apache_airflow_providers_dbt_cloud",
    "pypi_apache_airflow_providers_ftp",
    "pypi_apache_airflow_providers_google",
    "pypi_apache_airflow_providers_mysql",
    "pypi_apache_airflow_providers_postgres",
    "pypi_apache_airflow_providers_sftp",
    "pypi_apache_airflow_providers_snowflake",
    "pypi_apache_airflow_providers_ssh",
    "pypi_apache_airflow_providers_trino",
    "pypi_attrs",
    "pypi_flask_sqlalchemy",
    "pypi_great_expectations",
    "pypi_mock",
    "pypi_openlineage_integration_common",
    "pypi_openlineage_python",
    "pypi_pandas_gbq",
    "pypi_protobuf",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_pytest_mock",
    "pypi_snowflake_connector_python",
    "pypi_sqlalchemy"
  ]
}