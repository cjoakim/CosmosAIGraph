{
  "classifiers": [],
  "description": "# whisper\n\n[[blog]](https://openai.com/blog/whisper)\n[[paper]](https://arxiv.org/abs/2212.04356)\n[[model card]](https://github.com/openai/whisper/blob/main/model-card.md)\n[[colab example]](https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/librispeech.ipynb)\n\nwhisper is a general-purpose speech recognition model. it is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.\n\n\n## approach\n\n![approach](https://raw.githubusercontent.com/openai/whisper/main/approach.png)\n\na transformer sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. these tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing a single model to replace many stages of a traditional speech-processing pipeline. the multitask training format uses a set of special tokens that serve as task specifiers or classification targets.\n\n\n## setup\n\nwe used python 3.9.9 and [pytorch](https://pytorch.org/) 1.10.1 to train and test our models, but the codebase is expected to be compatible with python 3.8-3.11 and recent pytorch versions. the codebase also depends on a few python packages, most notably [openai's tiktoken](https://github.com/openai/tiktoken) for their fast tokenizer implementation. you can download and install (or update to) the latest release of whisper with the following command:\n\n    pip install -u openai-whisper\n\nalternatively, the following command will pull and install the latest commit from this repository, along with its python dependencies:\n\n    pip install git+https://github.com/openai/whisper.git \n\nto update the package to the latest version of this repository, please run:\n\n    pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\n\nit also requires the command-line tool [`ffmpeg`](https://ffmpeg.org/) to be installed on your system, which is available from most package managers:\n\n```bash\n# on ubuntu or debian\nsudo apt update && sudo apt install ffmpeg\n\n# on arch linux\nsudo pacman -s ffmpeg\n\n# on macos using homebrew (https://brew.sh/)\nbrew install ffmpeg\n\n# on windows using chocolatey (https://chocolatey.org/)\nchoco install ffmpeg\n\n# on windows using scoop (https://scoop.sh/)\nscoop install ffmpeg\n```\n\nyou may need [`rust`](http://rust-lang.org) installed as well, in case [tiktoken](https://github.com/openai/tiktoken) does not provide a pre-built wheel for your platform. if you see installation errors during the `pip install` command above, please follow the [getting started page](https://www.rust-lang.org/learn/get-started) to install rust development environment. additionally, you may need to configure the `path` environment variable, e.g. `export path=\"$home/.cargo/bin:$path\"`. if the installation fails with `no module named 'setuptools_rust'`, you need to install `setuptools_rust`, e.g. by running:\n\n```bash\npip install setuptools-rust\n```\n\n\n## available models and languages\n\nthere are five model sizes, four with english-only versions, offering speed and accuracy tradeoffs. below are the names of the available models and their approximate memory requirements and inference speed relative to the large model; actual speed may vary depending on many factors including the available hardware.\n\n|  size  | parameters | english-only model | multilingual model | required vram | relative speed |\n|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n|  tiny  |    39 m    |     `tiny.en`      |       `tiny`       |     ~1 gb     |      ~32x      |\n|  base  |    74 m    |     `base.en`      |       `base`       |     ~1 gb     |      ~16x      |\n| small  |   244 m    |     `small.en`     |      `small`       |     ~2 gb     |      ~6x       |\n| medium |   769 m    |    `medium.en`     |      `medium`      |     ~5 gb     |      ~2x       |\n| large  |   1550 m   |        n/a         |      `large`       |    ~10 gb     |       1x       |\n\nthe `.en` models for english-only applications tend to perform better, especially for the `tiny.en` and `base.en` models. we observed that the difference becomes less significant for the `small.en` and `medium.en` models.\n\nwhisper's performance varies widely depending on the language. the figure below shows a performance breakdown of `large-v3` and `large-v2` models by language, using wers (word error rates) or cer (character error rates, shown in *italic*) evaluated on the common voice 15 and fleurs datasets. additional wer/cer metrics corresponding to the other models and datasets can be found in appendix d.1, d.2, and d.4 of [the paper](https://arxiv.org/abs/2212.04356), as well as the bleu (bilingual evaluation understudy) scores for translation in appendix d.3.\n\n![wer breakdown by language](https://github.com/openai/whisper/assets/266841/f4619d66-1058-4005-8f67-a9d811b77c62)\n\n\n\n## command-line usage\n\nthe following command will transcribe speech in audio files, using the `medium` model:\n\n    whisper audio.flac audio.mp3 audio.wav --model medium\n\nthe default setting (which selects the `small` model) works well for transcribing english. to transcribe an audio file containing non-english speech, you can specify the language using the `--language` option:\n\n    whisper japanese.wav --language japanese\n\nadding `--task translate` will translate the speech into english:\n\n    whisper japanese.wav --language japanese --task translate\n\nrun the following to view all available options:\n\n    whisper --help\n\nsee [tokenizer.py](https://github.com/openai/whisper/blob/main/whisper/tokenizer.py) for the list of all available languages.\n\n\n## python usage\n\ntranscription can also be performed within python: \n\n```python\nimport whisper\n\nmodel = whisper.load_model(\"base\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])\n```\n\ninternally, the `transcribe()` method reads the entire file and processes the audio with a sliding 30-second window, performing autoregressive sequence-to-sequence predictions on each window.\n\nbelow is an example usage of `whisper.detect_language()` and `whisper.decode()` which provide lower-level access to the model.\n\n```python\nimport whisper\n\nmodel = whisper.load_model(\"base\")\n\n# load audio and pad/trim it to fit 30 seconds\naudio = whisper.load_audio(\"audio.mp3\")\naudio = whisper.pad_or_trim(audio)\n\n# make log-mel spectrogram and move to the same device as the model\nmel = whisper.log_mel_spectrogram(audio).to(model.device)\n\n# detect the spoken language\n_, probs = model.detect_language(mel)\nprint(f\"detected language: {max(probs, key=probs.get)}\")\n\n# decode the audio\noptions = whisper.decodingoptions()\nresult = whisper.decode(model, mel, options)\n\n# print the recognized text\nprint(result.text)\n```\n\n## more examples\n\nplease use the [\ud83d\ude4c show and tell](https://github.com/openai/whisper/discussions/categories/show-and-tell) category in discussions for sharing more example usages of whisper and third-party extensions such as web demos, integrations with other tools, ports for different platforms, etc.\n\n\n## license\n\nwhisper's code and model weights are released under the mit license. see [license](https://github.com/openai/whisper/blob/main/license) for further details.",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "openai-whisper",
  "package_url": "https://pypi.org/project/openai-whisper/",
  "project_url": "https://pypi.org/project/openai-whisper/",
  "project_urls": {
    "Homepage": "https://github.com/openai/whisper"
  },
  "release_url": "https://pypi.org/project/openai-whisper/20231117/",
  "requires_dist": [],
  "requires_python": ">=3.8",
  "summary": "robust speech recognition via large-scale weak supervision",
  "version": "20231117",
  "releases": [],
  "developers": [
    "openai"
  ],
  "kwds": "voice decoder speech whisper transcription",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_openai_whisper",
  "homepage": "https://github.com/openai/whisper",
  "release_count": 10,
  "dependency_ids": []
}