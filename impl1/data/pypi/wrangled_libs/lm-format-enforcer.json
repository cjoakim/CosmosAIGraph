{
  "classifiers": [
    "development status :: 3 - alpha",
    "intended audience :: developers",
    "license :: osi approved :: mit license",
    "operating system :: os independent",
    "programming language :: python",
    "programming language :: python :: 3",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.12",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "# lm-format-enforcer\n\n![lmfe logo](https://raw.githubusercontent.com/noamgat/lm-format-enforcer/main/docs/logo.png)\n\n**enforce the output format (json schema, regex etc) of a language model**\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/noamgat/lm-format-enforcer/blob/main/samples/colab_llama2_enforcer.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"open in colab\"/>\n</a>\n\n[![code coverage](https://codecov.io/gh/noamgat/lm-format-enforcer/graph/badge.svg?token=63u3s58vws)](https://codecov.io/gh/noamgat/lm-format-enforcer)\n![tests](https://github.com/noamgat/lm-format-enforcer/actions/workflows/run_tests.yml/badge.svg)\n\n\n![solution at a glance](https://raw.githubusercontent.com/noamgat/lm-format-enforcer/main/docs/intro.webp)\n\n\nlanguage models are able to generate text, but when requiring a precise output format, they do not always perform as instructed.\nvarious prompt engineering techniques have been introduced to improve the robustness of the generated text, but they are not always sufficient.\nthis project solves the issues by filtering the tokens that the language model is allowed to generate at every timestep, thus ensuring that the output format is respected, while minimizing the limitations on the language model.\n\n## installation\n```pip install lm-format-enforcer```\n\n## basic tutorial\n```python\n# requirements if running from google colab with a t4 gpu. \n!pip install transformers torch lm-format-enforcer huggingface_hub optimum\n!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ \n\nfrom pydantic import basemodel\nfrom lmformatenforcer import jsonschemaparser\nfrom lmformatenforcer.integrations.transformers import build_transformers_prefix_allowed_tokens_fn\nfrom transformers import pipeline\n\nclass answerformat(basemodel):\n    first_name: str\n    last_name: str\n    year_of_birth: int\n    num_seasons_in_nba: int\n\n# create a transformers pipeline\nhf_pipeline = pipeline('text-generation', model='thebloke/llama-2-7b-chat-gptq', device_map='auto')\nprompt = f'here is information about michael jordan in the following json schema: {answerformat.schema_json()} :\\n'\n\n# create a character level parser and build a transformers prefix function from it\nparser = jsonschemaparser(answerformat.schema())\nprefix_function = build_transformers_prefix_allowed_tokens_fn(hf_pipeline.tokenizer, parser)\n\n# call the pipeline with the prefix function\noutput_dict = hf_pipeline(prompt, prefix_allowed_tokens_fn=prefix_function)\n\n# extract the results\nresult = output_dict[0]['generated_text'][len(prompt):]\nprint(result)\n# {'first_name': 'michael', 'last_name': 'jordan', 'year_of_birth': 1963, 'num_seasons_in_nba': 15}\n```\n\n## capabilities / advantages\n\n- works with any python language model and tokenizer. already supports [transformers](https://github.com/huggingface/transformers), [langchain](https://python.langchain.com/docs/integrations/llms/lmformatenforcer_experimental), [llamaindex](https://docs.llamaindex.ai/en/latest/community/integrations/lmformatenforcer.html), [llama.cpp](https://github.com/noamgat/lm-format-enforcer/blob/main/samples/colab_llamacpppython_integration.ipynb), [vllm](https://github.com/noamgat/lm-format-enforcer/blob/main/samples/colab_vllm_integration.ipynb), [haystack](https://haystack.deepset.ai/integrations/lmformatenforcer) and [exllamav2](https://github.com/noamgat/lm-format-enforcer/blob/main/samples/colab_exllamav2_integration.ipynb). can be adapted to others.\n- supports batched generation and beam searches - each input / beam can have different tokens filtered at every timestep\n- supports json schema, json mode (schemaless) and regular expression formats\n- supports both required and optional fields in json schemas\n- supports nested fields, arrays and dictionaries in json schemas\n- gives the language model freedom to control whitespacing and field ordering in json schemas, reducing hallucinations.\n- does not modify the high level loop of transformers api, so can be used in any scenario.\n\n\n## comparison to other libraries\n\ncapability | lm format enforcer | [guidance](https://github.com/guidance-ai/guidance) | [jsonformer](https://github.com/1rgs/jsonformer) | [outlines](https://github.com/outlines-dev/outlines)\n:------------ | :-------------| :-------------| :------------- | :----\nregular expressions | \u2705 |  \u2705 | \u274c | \u2705\njson schema | \u2705 |  \ud83d\udfe1 ([partial conversion is possible](https://github.com/guidance-ai/guidance/blob/main/notebooks/applications/jsonformer.ipynb)) | \u2705 | \u2705\nbatched generation | \u2705 |  \u274c | \u274c | \u274c\nbeam search | \u2705 |  \u274c | \u274c | \u274c\nintegrates into existing pipelines | \u2705 | \u274c | \u274c | \u274c\noptional json fields | \u2705 |  \u274c | \u274c | \u274c\nllm controls json field ordering and whitespace | \u2705 | \u274c | \u274c | \u274c\n\nspotted a mistake? library updated with new capabilities? [open an issue!](https://github.com/noamgat/lm-format-enforcer/issues)\n\n## detailed example\n\nwe created a google colab notebook which contains a full example of how to use this library to enforce the output format of llama2, including interpreting the intermediate results. the notebook can run on a free gpu-backed runtime in colab.\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/noamgat/lm-format-enforcer/blob/main/samples/colab_llama2_enforcer.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"open in colab\"/>\n</a>\n\nyou can also [view the notebook in github](https://github.com/noamgat/lm-format-enforcer/blob/main/samples/colab_llama2_enforcer.ipynb).\n\nfor the different ways to integrate with huggingface transformers, see the [unit tests](https://github.com/noamgat/lm-format-enforcer/blob/main/tests/test_transformerenforcer.py).\n\n## how does it work?\n\nthe library works by combining a character level parser and a tokenizer prefix tree into a smart token filtering mechanism.\n\n![an example of the character level parser and tokenizer prefix tree in a certain timestep](https://raw.githubusercontent.com/noamgat/lm-format-enforcer/main/docs/trees.drawio.svg?sanitize=true)\n\n### character level parser\n\nparsing a string into any kind of formatter can be looked at as an implicit tree structure - at any moment in the parsing process, there is a set of allowed next characters, and if any of them are selected, there is a new set of allowed next characters, and so on.\n\n```characterlevelparser``` is an interface for parsing according to this implicit structure. ```add_character()``` and ```get_allowed_characters()``` can be seen as tree traversal methods.\n\nthere are several implementations of this interface:\n- ```jsonschemaparser``` - parses according to a json schema (or pure json output - `jsonschemaparser(none) will result in any json object allowed`). \n- ```stringparser``` - forces an exact string (used mainly for diagnostics)\n- ```regexparser``` - parses according to a regular expression. note that this cannot use the built in python regex and uses a manually implemented one (via the [interegular](https://pypi.org/project/interegular/) library), so it doesn't cover 100% of the regex standard.\n### tokenizer prefix tree\n\ngiven a tokenizer used by a certain language model, we can build a prefix tree of all the tokens that the language model can generate. this is done by generating all possible sequences of tokens, and adding them to the tree.\nsee ```tokenizerprefixtree```\n\n### combining the two\n\ngiven a character level parser and a tokenizer prefix tree, we can elegantly and efficiently filter the tokens that the language model is allowed to generate at the next timestep:\nwe only traverse the characters that are in both the character level parsing node and the tokenizer prefix tree node. this allows us to find all of the tokens (including complex subword tokens such as ```\",\"``` which are critical in json parsing).\nwe do this recursively on both trees and return all of the allowed tokens. when the language model generates a token, we advance the character level parser according to the new characters, ready to filter the next timestep.\n\n### how is this approach different? why is it good?\n\nthis is not the first library to enforce the output format of a language model. however, other similar libraries (such as guidance, jsonformer and outlines) enforce an exact output format. this means that the language model is not allowed to control whitespacing, field optionality and field ordering (in the json usecase). while this seems inconsequencial to humans, it means that the language model may not be generating the json formats that it \"wants to\" generate, and could put its internal states in a suboptimal value, reducing the quality of the output in later timesteps.\n\nthis forces language model users to know the details of the language model they are using (for example - were jsons minified before pretraining?) and modify the libraries to generate the precise format.\n\nwe avoid this problem by scanning potential next tokens and allowing any token sequence that will be parsed into the output format. this means that the language model can control all of these aspects, and output the token sequence that matches its' style in the most natural way, without requiring the developer to know the details.\n\n\n## diagnostics - will i always get good results?\n\nusing this library guarantees that the output will match the format, but it does not guarantee that the output will be semantically correct. forcing the language model to conform to a certain output may lead to increased hallucinations. guiding the model via prompt engineering is still likely to improve results.\n\nin order to help you understand the aggressiveness caused by the format enforcement, if you pass ```output_scores=true``` and ```return_dict_in_generate=true``` in the ```kwargs``` to ```generate_enforced()``` (these are existing optional parameters in the ```transformers``` library), you will also get a token-by-token dataframe showing which token was selected, its score, and what was the token that would have been chosen if the format enforcement was not applied. if you see that the format enforcer forced the language model to select tokens with very low weights, it is a likely contributor to the poor results. try modifying the prompt to guide the language model to not force the format enforcer to be so aggressive.\n\nexample using the regular expression format ``` michael jordan was born in (\\d)+.```\n\nidx | generated_token | generated_token_idx | generated_score | leading_token | leading_token_idx | leading_score\n:------------ | :-------------| :-------------| :------------- | :------------ | :-------------| :-------------\n0 | \u2581 | 29871 | 1.000000 | \u2581 | 29871 | 1.000000\n1 | michael | 24083 | 0.000027 | \u2581sure | 18585 | 0.959473\n2 | \u2581jordan | 18284 | 1.000000 | \u2581jordan | 18284 | 1.000000\n3 | \u2581was | 471 | 1.000000 | \u2581was | 471 | 1.000000\n4 | \u2581born | 19298 | 0.000008 | \u2581born | 6345 | 1.000000\n5 | \u2581in | 297 | 0.994629 | \u2581in | 297 | 0.994629\n6 | \u2581 | 29871 | 0.982422 | \u2581 | 29871 | 0.982422\n7 | 1 | 29896 | 1.000000 | 1 | 29896 | 1.000000\n8 | 9 | 29929 | 1.000000 | 9 | 29929 | 1.000000\n9 | 6 | 29953 | 1.000000 | 6 | 29953 | 1.000000\n10 | 3 | 29941 | 1.000000 | 3 | 29941 | 1.000000\n11 | . | 29889 | 0.999512 | . | 29889 | 0.999512\n12 | ```</s>``` | 2 | 0.981445 | ```</s>``` | 2 | 0.981445\n\n\nyou can see that the model \"wanted\" to start the answer using ```sure```, but the format enforcer forced it to use ```michael``` - there was a big gap in token 1. afterwards, almost all of the leading scores are all within the allowed token set, meaning the model likely did not hallucinate due to the token forcing. the only exception was timestep 4 - \" born\" was forced while the llm wanted to choose \"born\". this is a hint for the prompt engineer, to change the prompt to use a lowercase b instead.\n\n\n## known issues and limitations\n\n- lm format enforcer requires a python api to process the output logits of the language model. this means that until the apis are extended, it can not be used with openai chatgpt and similar api based solutions.\n- regular expression syntax is not 100% supported. see [interegular](https://pypi.org/project/interegular/) for more details.\n- lm format enforcer regex parser can only generate characters that exist in the tokenizer vocabulary. this may be solved in a later version, see [the issue on github](https://github.com/noamgat/lm-format-enforcer/issues/13).\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "lm-format-enforcer",
  "package_url": "https://pypi.org/project/lm-format-enforcer/",
  "project_url": "https://pypi.org/project/lm-format-enforcer/",
  "project_urls": {
    "Bug Tracker": "https://github.com/noamgat/lm-format-enforcer/issues",
    "Documentation": "https://github.com/noamgat/lm-format-enforcer",
    "Homepage": "https://github.com/noamgat/lm-format-enforcer",
    "Repository": "https://github.com/noamgat/lm-format-enforcer"
  },
  "release_url": "https://pypi.org/project/lm-format-enforcer/0.8.0/",
  "requires_dist": [
    "pydantic (>=1.10.8)",
    "interegular (>=0.3.2)"
  ],
  "requires_python": ">=3.8,<4.0",
  "summary": "enforce the output format (json schema, regex etc) of a language model",
  "version": "0.8.0",
  "releases": [],
  "developers": [
    "noam_gat",
    "noamgat@gmail.com"
  ],
  "kwds": "colab_llama2_enforcer tokenizer colab_llamacpppython_integration tokenizerprefixtree code",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_lm_format_enforcer",
  "homepage": "https://github.com/noamgat/lm-format-enforcer",
  "release_count": 49,
  "dependency_ids": [
    "pypi_interegular",
    "pypi_pydantic"
  ]
}