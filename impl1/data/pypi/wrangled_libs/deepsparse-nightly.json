{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: developers",
    "intended audience :: education",
    "intended audience :: information technology",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "license :: other/proprietary license",
    "operating system :: posix :: linux",
    "programming language :: python :: 3",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: scientific/engineering :: mathematics",
    "topic :: software development",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "<!--\ncopyright (c) 2021 - present / neuralmagic, inc. all rights reserved.\n\nlicensed under the apache license, version 2.0 (the \"license\");\nyou may not use this file except in compliance with the license.\nyou may obtain a copy of the license at\n\n   http://www.apache.org/licenses/license-2.0\n\nunless required by applicable law or agreed to in writing,\nsoftware distributed under the license is distributed on an \"as is\" basis,\nwithout warranties or conditions of any kind, either express or implied.\nsee the license for the specific language governing permissions and\nlimitations under the license.\n-->\n\n\n<div style=\"display: flex; flex-direction: column; align-items: center;\">\n  <h1>\n    <img alt=\"tool icon\" src=\"https://raw.githubusercontent.com/neuralmagic/deepsparse/main/docs/old/source/icon-deepsparse.png\" />\n    &nbsp;&nbsp;deepsparse\n  </h1>\n  <h4>sparsity-aware deep learning inference runtime for cpus</h4>\n  <div align=\"center\">\n  <a href=\"https://docs.neuralmagic.com/deepsparse/\">\n    <img alt=\"documentation\" src=\"https://img.shields.io/badge/documentation-darkred?&style=for-the-badge&logo=read-the-docs\" height=\"20\" />\n  </a>\n  <a href=\"https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-yboicsiw3l1dmqpjbedurq/\">\n    <img alt=\"slack\" src=\"https://img.shields.io/badge/slack-purple?style=for-the-badge&logo=slack\" height=\"20\" />\n  </a>\n  <a href=\"https://github.com/neuralmagic/deepsparse/issues/\">\n    <img alt=\"support\" src=\"https://img.shields.io/badge/support%20forums-navy?style=for-the-badge&logo=github\" height=\"20\" />\n  </a>\n  <a href=\"https://www.youtube.com/channel/uco8do_wmgybwcrnj_dxr4ea\">\n    <img alt=\"youtube\" src=\"https://img.shields.io/badge/-youtube-red?&style=for-the-badge&logo=youtube&logocolor=white\" height=\"20\" />\n  </a>\n  <a href=\"https://twitter.com/neuralmagic\">\n    <img alt=\"twitter\" src=\"https://img.shields.io/twitter/follow/neuralmagic?color=darkgreen&label=follow&style=social\" height=\"20\" />\n  </a>\n</div>\n\n[deepsparse](https://github.com/neuralmagic/deepsparse) is a cpu inference runtime that takes advantage of sparsity to accelerate neural network inference. coupled with [sparseml](https://github.com/neuralmagic/sparseml), our optimization library for pruning and quantizing your models, deepsparse delivers exceptional inference performance on cpu hardware.\n\n<p align=\"center\">\n   <img alt=\"nm flow\" src=\"https://github.com/neuralmagic/deepsparse/assets/3195154/51e62fe7-9d9a-4fa5-a774-877158da1e29\" width=\"60%\" />\n</p>\n\n## \u2728new\u2728 deepsparse llms\n\nneural magic is excited to announce initial support for performant llm inference in deepsparse with:\n- sparse kernels for speedups and memory savings from unstructured sparse weights.\n- 8-bit weight and activation quantization support.\n- efficient usage of cached attention keys and values for minimal memory movement.\n\n![mpt-chat-comparison](https://github.com/neuralmagic/deepsparse/assets/3195154/ccf39323-4603-4489-8462-7b103872aeb3)\n\n### try it now\n\ninstall (requires linux):\n```bash\npip install -u deepsparse-nightly[llm]\n```\n\nrun inference:\n```python\nfrom deepsparse import textgeneration\npipeline = textgeneration(model=\"zoo:mpt-7b-dolly_mpt_pretrain-pruned50_quantized\")\n\nprompt=\"\"\"\nbelow is an instruction that describes a task. write a response that appropriately completes the request. ### instruction: what is sparsity? ### response:\n\"\"\"\nprint(pipeline(prompt, max_new_tokens=75).generations[0].text)\n\n# sparsity is the property of a matrix or other data structure in which a large number of elements are zero and a smaller number of elements are non-zero. in the context of machine learning, sparsity can be used to improve the efficiency of training and prediction.\n```\n\n> [check out the `textgeneration` documentation for usage details.](https://github.com/neuralmagic/deepsparse/blob/main/docs/llms/text-generation-pipeline.md)\n\n### sparsity :handshake: performance\n\ndeveloped in collaboration with ist austria, [our recent paper](https://arxiv.org/abs/2310.06927) details a new technique called **sparse fine-tuning**, which allows us to prune mpt-7b to 60% sparsity during fine-tuning without drop in accuracy. with our new support for llms, deepsparse accelerates the sparse-quantized model 7x over the dense baseline:\n\n<div align=\"center\">\n    <img src=\"https://github.com/neuralmagic/deepsparse/assets/3195154/8687401c-f479-4999-ba6b-e01c747dace9\" width=\"60%\"/>\n</div>\n\n> [learn more about our sparse fine-tuning research.](https://github.com/neuralmagic/deepsparse/blob/main/research/mpt#sparse-finetuned-llms-with-deepsparse)\n\n> [check out the model running live on hugging face.](https://huggingface.co/spaces/neuralmagic/sparse-mpt-7b-gsm8k)\n\n### llm roadmap\n\nfollowing this initial launch, we are rapidly expanding our support for llms, including:\n\n1. productizing sparse fine-tuning: enable external users to apply sparse fine-tuning to their datasets via sparseml.\n2. expanding model support: apply our sparse fine-tuning results to llama 2 and mistral models.\n3. pushing for higher sparsity: improving our pruning algorithms to reach even higher sparsity.\n\n## computer vision and nlp models\n\nin addition to llms, deepsparse supports many variants of cnns and transformer models, such as bert, vit, resnet, efficientnet, yolov5/8, and many more! take a look at the [computer vision](https://sparsezoo.neuralmagic.com/?modelset=computer_vision) and [natural language processing](https://sparsezoo.neuralmagic.com/?modelset=natural_language_processing) domains of [sparsezoo](https://sparsezoo.neuralmagic.com/), our home for optimized models.\n\n### installation\n\ninstall via [pypi](https://pypi.org/project/deepsparse/) ([optional dependencies detailed here](https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/installation.md)):\n\n```bash\npip install deepsparse \n```\n\nto experiment with the latest features, there is a nightly build available using `pip install deepsparse-nightly` or you can clone and install from source using `pip install -e path/to/deepsparse`.\n\n#### system requirements\n- hardware: [x86 avx2, avx-512, avx-512 vnni and arm v8.2+](https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/hardware-support.md)\n- operating system: linux\n- python: 3.8-3.11\n- onnx versions 1.5.0-1.15.0, onnx opset version 11 or higher\n\nfor those using mac or windows, we recommend using linux containers with docker.\n\n## deployment apis\n\ndeepsparse includes three deployment apis:\n\n- **engine** is the lowest-level api. with engine, you compile an onnx model, pass tensors as input, and receive the raw outputs.\n- **pipeline** wraps the engine with pre- and post-processing. with pipeline, you pass raw data and receive the prediction.\n- **server** wraps pipelines with a rest api using fastapi. with server, you send raw data over http and receive the prediction.\n\n### engine\n\nthe example below downloads a 90% pruned-quantized bert model for sentiment analysis in onnx format from sparsezoo, compiles the model, and runs inference on randomly generated input. users can provide their own onnx models, whether dense or sparse.\n\n```python\nfrom deepsparse import engine\n\n# download onnx, compile\nzoo_stub = \"zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none\"\ncompiled_model = engine(model=zoo_stub, batch_size=1)\n\n# run inference (input is raw numpy tensors, output is raw scores)\ninputs = compiled_model.generate_random_inputs()\noutput = compiled_model(inputs)\nprint(output)\n\n# > [array([[-0.3380675 ,  0.09602544]], dtype=float32)] << raw scores\n```\n\n### pipeline\n\npipelines wrap engine with pre- and post-processing, enabling you to pass raw data and receive the post-processed prediction. the example below downloads a 90% pruned-quantized bert model for sentiment analysis in onnx format from sparsezoo, sets up a pipeline, and runs inference on sample data.\n\n```python\nfrom deepsparse import pipeline\n\n# download onnx, set up pipeline\nzoo_stub = \"zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none\"  \nsentiment_analysis_pipeline = pipeline.create(\n  task=\"sentiment-analysis\",    # name of the task\n  model_path=zoo_stub,          # zoo stub or path to local onnx file\n)\n\n# run inference (input is a sentence, output is the prediction)\nprediction = sentiment_analysis_pipeline(\"i love using deepsparse pipelines\")\nprint(prediction)\n# > labels=['positive'] scores=[0.9954759478569031]\n```\n\n### server\n\nserver wraps pipelines with rest apis, enabling you to set up a model-serving endpoint running deepsparse. this enables you to send raw data to deepsparse over http and receive the post-processed predictions. deepsparse server is launched from the command line and configured via arguments or a server configuration file. the following downloads a 90% pruned-quantized bert model for sentiment analysis in onnx format from sparsezoo and launches a sentiment analysis endpoint:\n\n```bash\ndeepsparse.server \\\n  --task sentiment-analysis \\\n  --model_path zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none\n```\n\nsending a request:\n\n```python\nimport requests\n\nurl = \"http://localhost:5543/v2/models/sentiment_analysis/infer\" # server's port default to 5543\nobj = {\"sequences\": \"snorlax loves my tesla!\"}\n\nresponse = requests.post(url, json=obj)\nprint(response.text)\n# {\"labels\":[\"positive\"],\"scores\":[0.9965094327926636]}\n```\n\n### additional resources\n- [use cases page](https://github.com/neuralmagic/deepsparse/tree/main/docs/use-cases) for more details on supported tasks\n- [pipelines user guide](https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/deepsparse-pipelines.md) for pipeline documentation\n- [server user guide](https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/deepsparse-server.md) for server documentation\n- [benchmarking user guide](https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/deepsparse-benchmarking.md) for benchmarking documentation\n- [cloud deployments and demos](https://github.com/neuralmagic/deepsparse/tree/main/examples/)\n- [user guide](https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide) for more detailed documentation\n\n\n## product usage analytics\n\ndeepsparse gathers basic usage telemetry, including, but not limited to, invocations, package, version, and ip address, for product usage analytics purposes. review neural magic's [products privacy policy](https://neuralmagic.com/legal/) for further details on how we process this data. \n\nto disable product usage analytics, run:\n```bash\nexport nm_disable_analytics=true\n```\n\nconfirm that telemetry is shut off through info logs streamed with engine invocation by looking for the phrase \"skipping neural magic's latest package version check.\"\n\n## community\n\n### get in touch\n\n- [contribution guide](https://github.com/neuralmagic/deepsparse/blob/main/contributing.md)\n- [community slack](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-yboicsiw3l1dmqpjbedurq)\n- [github issue queue](https://github.com/neuralmagic/deepsparse/issues) \n- [subscribe to our newsletter](https://neuralmagic.com/subscribe/)\n- [blog](https://www.neuralmagic.com/blog/) \n\nfor more general questions about neural magic, [complete this form.](http://neuralmagic.com/contact/)\n\n### license\n\n- **deepsparse community** is free to use and is licensed under the [neural magic deepsparse community license.](https://github.com/neuralmagic/deepsparse/blob/main/license-neuralmagic)\nsome source code, example files, and scripts included in the deepsparse github repository or directory are licensed under the [apache license version 2.0](https://github.com/neuralmagic/deepsparse/blob/main/license) as noted.\n\n- **deepsparse enterprise** requires a trial license or [can be fully licensed](https://neuralmagic.com/legal/master-software-license-and-service-agreement/) for production, commercial applications.\n\n### cite\n\nfind this project useful in your research or other communications? please consider citing:\n\n```bibtex\n@misc{kurtic2023sparse,\n      title={sparse fine-tuning for inference acceleration of large language models}, \n      author={eldar kurtic and denis kuznedelev and elias frantar and michael goin and dan alistarh},\n      year={2023},\n      url={https://arxiv.org/abs/2310.06927},\n      eprint={2310.06927},\n      archiveprefix={arxiv},\n      primaryclass={cs.cl}\n}\n\n@misc{kurtic2022optimal,\n      title={the optimal bert surgeon: scalable and accurate second-order pruning for large language models}, \n      author={eldar kurtic and daniel campos and tuan nguyen and elias frantar and mark kurtz and benjamin fineran and michael goin and dan alistarh},\n      year={2022},\n      url={https://arxiv.org/abs/2203.07259},\n      eprint={2203.07259},\n      archiveprefix={arxiv},\n      primaryclass={cs.cl}\n}\n\n@inproceedings{\n    pmlr-v119-kurtz20a, \n    title = {inducing and exploiting activation sparsity for fast inference on deep neural networks}, \n    author = {kurtz, mark and kopinsky, justin and gelashvili, rati and matveev, alexander and carr, john and goin, michael and leiserson, william and moore, sage and nell, bill and shavit, nir and alistarh, dan}, \n    booktitle = {proceedings of the 37th international conference on machine learning}, \n    pages = {5533--5543}, \n    year = {2020}, \n    editor = {hal daum\u00e9 iii and aarti singh}, \n    volume = {119}, \n    series = {proceedings of machine learning research}, \n    address = {virtual}, \n    month = {13--18 jul}, \n    publisher = {pmlr}, \n    pdf = {http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf},\n    url = {http://proceedings.mlr.press/v119/kurtz20a.html}\n}\n\n@article{dblp:journals/corr/abs-2111-13445,\n  author    = {eugenia iofinova and alexandra peste and mark kurtz and dan alistarh},\n  title     = {how well do sparse imagenet models transfer?},\n  journal   = {corr},\n  volume    = {abs/2111.13445},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2111.13445},\n  eprinttype = {arxiv},\n  eprint    = {2111.13445},\n  timestamp = {wed, 01 dec 2021 15:16:43 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-13445.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n# all thanks to our contributors\n\n<a href=\"https://github.com/neuralmagic/deepsparse/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=neuralmagic/deepsparse\" />\n</a>\n",
  "docs_url": null,
  "keywords": "inference,machine learning,x86,x86_64,avx2,avx512,neural network,sparse,inference engine,cpu,runtime,deepsparse,computer vision,object detection,sparsity",
  "license": "neural magic deepsparse community license, apache",
  "name": "deepsparse-nightly",
  "package_url": "https://pypi.org/project/deepsparse-nightly/",
  "project_url": "https://pypi.org/project/deepsparse-nightly/",
  "project_urls": {
    "Homepage": "https://github.com/neuralmagic/deepsparse"
  },
  "release_url": "https://pypi.org/project/deepsparse-nightly/1.7.0.20231210/",
  "requires_dist": [
    "sparsezoo-nightly ~=1.7.0",
    "numpy >=1.16.3",
    "onnx <1.15.0,>=1.5.0",
    "pydantic <2.0.0,>=1.8.2",
    "requests >=2.0.0",
    "tqdm >=4.0.0",
    "protobuf >=3.12.2",
    "click !=8.0.0,>=7.1.2",
    "open-clip-torch ==2.20.0 ; extra == 'clip'",
    "scipy <1.10,>=1.8 ; extra == 'clip'",
    "transformers <4.35 ; extra == 'clip'",
    "beautifulsoup4 >=4.9.3 ; extra == 'dev'",
    "black ==22.12.0 ; extra == 'dev'",
    "flake8 >=3.8.3 ; extra == 'dev'",
    "isort >=5.7.0 ; extra == 'dev'",
    "flaky ~=3.7.0 ; extra == 'dev'",
    "ndjson >=0.3.1 ; extra == 'dev'",
    "wheel >=0.36.2 ; extra == 'dev'",
    "pytest >=6.0.0 ; extra == 'dev'",
    "onnxruntime >=1.7.0 ; extra == 'dev'",
    "flask >=1.0.0 ; extra == 'dev'",
    "flask-cors >=3.0.0 ; extra == 'dev'",
    "Pillow >=8.3.2 ; extra == 'dev'",
    "m2r2 ~=0.2.7 ; extra == 'docs'",
    "mistune ==0.8.4 ; extra == 'docs'",
    "myst-parser ~=0.14.0 ; extra == 'docs'",
    "rinohtype >=0.4.2 ; extra == 'docs'",
    "sphinx >=3.4.0 ; extra == 'docs'",
    "sphinx-copybutton >=0.3.0 ; extra == 'docs'",
    "sphinx-markdown-tables >=0.0.15 ; extra == 'docs'",
    "sphinx-multiversion ==0.2.4 ; extra == 'docs'",
    "sphinx-rtd-theme ; extra == 'docs'",
    "importlib-metadata ; extra == 'haystack'",
    "torch >=1.12.1 ; extra == 'haystack'",
    "requests ; extra == 'haystack'",
    "pydantic ; extra == 'haystack'",
    "nltk ; extra == 'haystack'",
    "pandas ; extra == 'haystack'",
    "dill ; extra == 'haystack'",
    "tqdm ; extra == 'haystack'",
    "networkx ; extra == 'haystack'",
    "mmh3 ; extra == 'haystack'",
    "quantulum3 ; extra == 'haystack'",
    "posthog ; extra == 'haystack'",
    "azure-ai-formrecognizer >=3.2.0b2 ; extra == 'haystack'",
    "azure-core <1.23 ; extra == 'haystack'",
    "more-itertools ; extra == 'haystack'",
    "python-docx ; extra == 'haystack'",
    "langdetect ; extra == 'haystack'",
    "tika ; extra == 'haystack'",
    "sentence-transformers >=2.2.0 ; extra == 'haystack'",
    "scipy >=1.3.2 ; extra == 'haystack'",
    "scikit-learn >=1.0.0 ; extra == 'haystack'",
    "seqeval ; extra == 'haystack'",
    "mlflow ; extra == 'haystack'",
    "elasticsearch <=7.10,>=7.7 ; extra == 'haystack'",
    "elastic-apm ; extra == 'haystack'",
    "rapidfuzz ; extra == 'haystack'",
    "jsonschema ; extra == 'haystack'",
    "sqlalchemy <2,>=1.4.2 ; extra == 'haystack'",
    "sqlalchemy-utils ; extra == 'haystack'",
    "psycopg2-binary ; extra == 'haystack'",
    "faiss-cpu ==1.7.2 ; extra == 'haystack'",
    "pymilvus <2.0.0 ; extra == 'haystack'",
    "weaviate-client ==3.3.3 ; extra == 'haystack'",
    "pinecone-client ; extra == 'haystack'",
    "SPARQLWrapper ; extra == 'haystack'",
    "selenium ; extra == 'haystack'",
    "webdriver-manager ; extra == 'haystack'",
    "beautifulsoup4 ; extra == 'haystack'",
    "markdown ; extra == 'haystack'",
    "python-magic ; extra == 'haystack'",
    "pytesseract ==0.3.7 ; extra == 'haystack'",
    "pillow ; extra == 'haystack'",
    "pdf2image ==1.14.0 ; extra == 'haystack'",
    "onnxruntime ; extra == 'haystack'",
    "onnxruntime-tools ; extra == 'haystack'",
    "ray ; extra == 'haystack'",
    "aiorwlock <2,>=1.3.0 ; extra == 'haystack'",
    "grpcio ==1.43.0 ; extra == 'haystack'",
    "beir ; extra == 'haystack'",
    "mypy ; extra == 'haystack'",
    "typing-extensions ; extra == 'haystack'",
    "pytest ; extra == 'haystack'",
    "responses ; extra == 'haystack'",
    "tox ; extra == 'haystack'",
    "coverage ; extra == 'haystack'",
    "python-multipart ; extra == 'haystack'",
    "psutil ; extra == 'haystack'",
    "pylint ; extra == 'haystack'",
    "black[jupyter] ; extra == 'haystack'",
    "mkdocs ; extra == 'haystack'",
    "jupytercontrib ; extra == 'haystack'",
    "watchdog ; extra == 'haystack'",
    "requests-cache ; extra == 'haystack'",
    "torchvision <0.17,>=0.3.0 ; extra == 'image_classification'",
    "opencv-python <=4.6.0.66 ; extra == 'image_classification'",
    "transformers <4.35 ; extra == 'llm'",
    "datasets <=2.14.6 ; extra == 'llm'",
    "scikit-learn ; extra == 'llm'",
    "seqeval ; extra == 'llm'",
    "onnxruntime >=1.7.0 ; extra == 'onnxruntime'",
    "openpifpaf ==0.13.11 ; extra == 'openpifpaf'",
    "opencv-python <=4.6.0.66 ; extra == 'openpifpaf'",
    "pycocotools >=2.0.6 ; extra == 'openpifpaf'",
    "scipy ==1.10.1 ; extra == 'openpifpaf'",
    "optimum-deepsparse ; extra == 'sentence_transformers'",
    "torch <2.2,>=1.7.0 ; extra == 'sentence_transformers'",
    "uvicorn >=0.15.0 ; extra == 'server'",
    "fastapi <0.87.0,>=0.70.0 ; extra == 'server'",
    "requests >=2.26.0 ; extra == 'server'",
    "python-multipart >=0.0.5 ; extra == 'server'",
    "prometheus-client >=0.14.1 ; extra == 'server'",
    "psutil >=5.9.4 ; extra == 'server'",
    "anyio <4.0.0 ; extra == 'server'",
    "torch <2.2,>=1.7.0 ; extra == 'torch'",
    "transformers <4.35 ; extra == 'transformers'",
    "datasets <=2.14.6 ; extra == 'transformers'",
    "scikit-learn ; extra == 'transformers'",
    "seqeval ; extra == 'transformers'",
    "torchvision <0.17,>=0.3.0 ; extra == 'yolo'",
    "opencv-python <=4.6.0.66 ; extra == 'yolo'",
    "torchvision <0.17,>=0.3.0 ; extra == 'yolov5'",
    "opencv-python <=4.6.0.66 ; extra == 'yolov5'",
    "torchvision <0.17,>=0.3.0 ; extra == 'yolov8'",
    "opencv-python <=4.6.0.66 ; extra == 'yolov8'",
    "ultralytics ==8.0.124 ; extra == 'yolov8'"
  ],
  "requires_python": ">=3.8, <3.12",
  "summary": "an inference runtime offering gpu-class performance on cpus and apis to integrate ml into your application",
  "version": "1.7.0.20231210",
  "releases": [],
  "developers": [
    "neuralmagic",
    "support@neuralmagic.com"
  ],
  "kwds": "imagenet neural computer_vision licenses license",
  "license_kwds": "neural magic deepsparse community license, apache",
  "libtype": "pypi",
  "id": "pypi_deepsparse_nightly",
  "homepage": "https://github.com/neuralmagic/deepsparse",
  "release_count": 21,
  "dependency_ids": [
    "pypi_aiorwlock",
    "pypi_anyio",
    "pypi_azure_ai_formrecognizer",
    "pypi_azure_core",
    "pypi_beautifulsoup4",
    "pypi_beir",
    "pypi_black",
    "pypi_click",
    "pypi_coverage",
    "pypi_datasets",
    "pypi_dill",
    "pypi_elastic_apm",
    "pypi_elasticsearch",
    "pypi_faiss_cpu",
    "pypi_fastapi",
    "pypi_flake8",
    "pypi_flaky",
    "pypi_flask",
    "pypi_flask_cors",
    "pypi_grpcio",
    "pypi_importlib_metadata",
    "pypi_isort",
    "pypi_jsonschema",
    "pypi_jupytercontrib",
    "pypi_langdetect",
    "pypi_m2r2",
    "pypi_markdown",
    "pypi_mistune",
    "pypi_mkdocs",
    "pypi_mlflow",
    "pypi_mmh3",
    "pypi_more_itertools",
    "pypi_mypy",
    "pypi_myst_parser",
    "pypi_ndjson",
    "pypi_networkx",
    "pypi_nltk",
    "pypi_numpy",
    "pypi_onnx",
    "pypi_onnxruntime",
    "pypi_onnxruntime_tools",
    "pypi_open_clip_torch",
    "pypi_opencv_python",
    "pypi_openpifpaf",
    "pypi_optimum_deepsparse",
    "pypi_pandas",
    "pypi_pdf2image",
    "pypi_pillow",
    "pypi_pinecone_client",
    "pypi_posthog",
    "pypi_prometheus_client",
    "pypi_protobuf",
    "pypi_psutil",
    "pypi_psycopg2_binary",
    "pypi_pycocotools",
    "pypi_pydantic",
    "pypi_pylint",
    "pypi_pymilvus",
    "pypi_pytesseract",
    "pypi_pytest",
    "pypi_python_docx",
    "pypi_python_magic",
    "pypi_python_multipart",
    "pypi_quantulum3",
    "pypi_rapidfuzz",
    "pypi_ray",
    "pypi_requests",
    "pypi_requests_cache",
    "pypi_responses",
    "pypi_rinohtype",
    "pypi_scikit_learn",
    "pypi_scipy",
    "pypi_selenium",
    "pypi_sentence_transformers",
    "pypi_seqeval",
    "pypi_sparqlwrapper",
    "pypi_sparsezoo_nightly",
    "pypi_sphinx",
    "pypi_sphinx_copybutton",
    "pypi_sphinx_markdown_tables",
    "pypi_sphinx_multiversion",
    "pypi_sphinx_rtd_theme",
    "pypi_sqlalchemy",
    "pypi_sqlalchemy_utils",
    "pypi_tika",
    "pypi_torch",
    "pypi_torchvision",
    "pypi_tox",
    "pypi_tqdm",
    "pypi_transformers",
    "pypi_typing_extensions",
    "pypi_ultralytics",
    "pypi_uvicorn",
    "pypi_watchdog",
    "pypi_weaviate_client",
    "pypi_webdriver_manager",
    "pypi_wheel"
  ]
}