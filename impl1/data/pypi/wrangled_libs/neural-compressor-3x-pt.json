{
  "classifiers": [
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "programming language :: python :: 3",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "<div align=\"center\">\n\nintel\u00ae neural compressor\n===========================\n<h3> an open-source python library supporting popular model compression techniques on all mainstream deep learning frameworks (tensorflow, pytorch, onnx runtime, and mxnet)</h3>\n\n[![python](https://img.shields.io/badge/python-3.8%2b-blue)](https://github.com/intel/neural-compressor)\n[![version](https://img.shields.io/badge/release-2.4-green)](https://github.com/intel/neural-compressor/releases)\n[![license](https://img.shields.io/badge/license-apache%202-blue)](https://github.com/intel/neural-compressor/blob/master/license)\n[![coverage](https://img.shields.io/badge/coverage-85%25-green)](https://github.com/intel/neural-compressor)\n[![downloads](https://static.pepy.tech/personalized-badge/neural-compressor?period=total&units=international_system&left_color=grey&right_color=green&left_text=downloads)](https://pepy.tech/project/neural-compressor)\n\n[architecture](./docs/source/design.md#architecture)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[workflow](./docs/source/design.md#workflow)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[results](./docs/source/validated_model_list.md)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[examples](./examples/readme.md)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[documentations](https://intel.github.io/neural-compressor)\n\n---\n<div align=\"left\">\n\nintel\u00ae neural compressor aims to provide popular model compression techniques such as quantization, pruning (sparsity), distillation, and neural architecture search on mainstream frameworks such as [tensorflow](https://www.tensorflow.org/), [pytorch](https://pytorch.org/), [onnx runtime](https://onnxruntime.ai/), and [mxnet](https://mxnet.apache.org/),\nas well as intel extensions such as [intel extension for tensorflow](https://github.com/intel/intel-extension-for-tensorflow) and [intel extension for pytorch](https://github.com/intel/intel-extension-for-pytorch).\nin particular, the tool provides the key features, typical examples, and open collaborations as below:\n\n* support a wide range of intel hardware such as [intel xeon scalable processors](https://www.intel.com/content/www/us/en/products/details/processors/xeon/scalable.html), [intel xeon cpu max series](https://www.intel.com/content/www/us/en/products/details/processors/xeon/max-series.html), [intel data center gpu flex series](https://www.intel.com/content/www/us/en/products/details/discrete-gpus/data-center-gpu/flex-series.html), and [intel data center gpu max series](https://www.intel.com/content/www/us/en/products/details/discrete-gpus/data-center-gpu/max-series.html) with extensive testing; support amd cpu, arm cpu, and nvidia gpu through onnx runtime with limited testing\n\n* validate popular llms such as [llama2](/examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm), [falcon](/examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm), [gpt-j](/examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm), [bloom](/examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm), [opt](/examples/pytorch/nlp/huggingface_models/language-modeling/quantization/llm), and more than 10,000 broad models such as [stable diffusion](/examples/pytorch/nlp/huggingface_models/text-to-image/quantization), [bert-large](/examples/pytorch/nlp/huggingface_models/text-classification/quantization/ptq_static/fx), and [resnet50](/examples/pytorch/image_recognition/torchvision_models/quantization/ptq/cpu/fx) from popular model hubs such as [hugging face](https://huggingface.co/), [torch vision](https://pytorch.org/vision/stable/index.html), and [onnx model zoo](https://github.com/onnx/models#models), by leveraging zero-code optimization solution [neural coder](/neural_coder#what-do-we-offer) and automatic [accuracy-driven](/docs/source/design.md#workflow) quantization strategies\n\n* collaborate with cloud marketplaces such as [google cloud platform](https://console.cloud.google.com/marketplace/product/bitnami-launchpad/inc-tensorflow-intel?project=verdant-sensor-286207), [amazon web services](https://aws.amazon.com/marketplace/pp/prodview-yjyh2xmggbmga#pdp-support), and [azure](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/bitnami.inc-tensorflow-intel), software platforms such as [alibaba cloud](https://www.intel.com/content/www/us/en/developer/articles/technical/quantize-ai-by-oneapi-analytics-on-alibaba-cloud.html), [tencent taco](https://new.qq.com/rain/a/20221202a00b9s00) and [microsoft olive](https://github.com/microsoft/olive), and open ai ecosystem such as [hugging face](https://huggingface.co/blog/intel), [pytorch](https://pytorch.org/tutorials/recipes/intel_neural_compressor_for_pytorch.html), [onnx](https://github.com/onnx/models#models), [onnx runtime](https://github.com/microsoft/onnxruntime), and [lightning ai](https://github.com/lightning-ai/lightning/blob/master/docs/source-pytorch/advanced/post_training_quantization.rst)\n\n## installation\n\n### install from pypi\n```shell\npip install neural-compressor\n```\n> **note**: \n> more installation methods can be found at [installation guide](https://github.com/intel/neural-compressor/blob/master/docs/source/installation_guide.md). please check out our [faq](https://github.com/intel/neural-compressor/blob/master/docs/source/faq.md) for more details.\n\n## getting started\n### quantization with python api\n\n```shell\n# install intel neural compressor and tensorflow\npip install neural-compressor\npip install tensorflow\n# prepare fp32 model\nwget https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_6/mobilenet_v1_1.0_224_frozen.pb\n```\n```python\nfrom neural_compressor.data import dataloader, datasets\nfrom neural_compressor.config import posttrainingquantconfig\n\ndataset = datasets(\"tensorflow\")[\"dummy\"](shape=(1, 224, 224, 3))\ndataloader = dataloader(framework=\"tensorflow\", dataset=dataset)\n\nfrom neural_compressor.quantization import fit\n\nq_model = fit(\n    model=\"./mobilenet_v1_1.0_224_frozen.pb\",\n    conf=posttrainingquantconfig(),\n    calib_dataloader=dataloader,\n)\n```\n\n## documentation\n\n<table class=\"docutils\">\n  <thead>\n  <tr>\n    <th colspan=\"8\">overview</th>\n  </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td colspan=\"2\" align=\"center\"><a href=\"./docs/source/design.md#architecture\">architecture</a></td>\n      <td colspan=\"2\" align=\"center\"><a href=\"./docs/source/design.md#workflow\">workflow</a></td>\n      <td colspan=\"2\" align=\"center\"><a href=\"examples/readme.md\">examples</a></td>\n      <td colspan=\"2\" align=\"center\"><a href=\"https://intel.github.io/neural-compressor/latest/docs/source/api-doc/apis.html\">apis</a></td>\n    </tr>\n  </tbody>\n  <thead>\n    <tr>\n      <th colspan=\"8\">python-based apis</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n        <td colspan=\"2\" align=\"center\"><a href=\"./docs/source/quantization.md\">quantization</a></td>\n        <td colspan=\"2\" align=\"center\"><a href=\"./docs/source/mixed_precision.md\">advanced mixed precision</a></td>\n        <td colspan=\"2\" align=\"center\"><a href=\"./docs/source/pruning.md\">pruning (sparsity)</a></td>\n        <td colspan=\"2\" align=\"center\"><a href=\"./docs/source/distillation.md\">distillation</a></td>\n    </tr>\n    <tr>\n        <td colspan=\"2\" align=\"center\"><a href=\"./docs/source/orchestration.md\">orchestration</a></td>\n        <td colspan=\"2\" align=\"center\"><a href=\"./docs/source/benchmark.md\">benchmarking</a></td>\n        <td colspan=\"2\" align=\"center\"><a href=\"./docs/source/distributed.md\">distributed compression</a></td>\n        <td colspan=\"2\" align=\"center\"><a href=\"./docs/source/export.md\">model export</a></td>\n    </tr>\n  </tbody>\n  <thead>\n    <tr>\n      <th colspan=\"8\">neural coder (zero-code optimization)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n        <td colspan=\"2\" align=\"center\"><a href=\"./neural_coder/docs/pythonlauncher.md\">launcher</a></td>\n        <td colspan=\"2\" align=\"center\"><a href=\"./neural_coder/extensions/neural_compressor_ext_lab/readme.md\">jupyterlab extension</a></td>\n        <td colspan=\"2\" align=\"center\"><a href=\"./neural_coder/extensions/neural_compressor_ext_vscode/readme.md\">visual studio code extension</a></td>\n        <td colspan=\"2\" align=\"center\"><a href=\"./neural_coder/docs/supportmatrix.md\">supported matrix</a></td>\n    </tr>\n  </tbody>\n  <thead>\n      <tr>\n        <th colspan=\"8\">advanced topics</th>\n      </tr>\n  </thead>\n  <tbody>\n      <tr>\n          <td colspan=\"2\" align=\"center\"><a href=\"./docs/source/adaptor.md\">adaptor</a></td>\n          <td colspan=\"2\" align=\"center\"><a href=\"./docs/source/tuning_strategies.md\">strategy</a></td>\n          <td colspan=\"2\" align=\"center\"><a href=\"./docs/source/distillation_quantization.md\">distillation for quantization</a></td>\n          <td colspan=\"2\" align=\"center\"><a href=\"./docs/source/smooth_quant.md\">smoothquant</td>\n      </tr>\n      <tr>\n          <td colspan=\"4\" align=\"center\"><a href=\"./docs/source/quantization_weight_only.md\">weight-only quantization (int8/int4/fp4/nf4) </td>\n          <td colspan=\"2\" align=\"center\"><a href=\"https://github.com/intel/neural-compressor/blob/fp8_adaptor/docs/source/fp8.md\">fp8 quantization </td>\n          <td colspan=\"2\" align=\"center\"><a href=\"./docs/source/quantization_layer_wise.md\">layer-wise quantization </td>\n      </tr>\n  </tbody>\n  <thead>\n      <tr>\n        <th colspan=\"8\">innovations for productivity</th>\n      </tr>\n  </thead>\n  <tbody>\n      <tr>\n          <td colspan=\"4\" align=\"center\"><a href=\"./neural_insights/readme.md\">neural insights</a></td>\n          <td colspan=\"4\" align=\"center\"><a href=\"./neural_solution/readme.md\">neural solution</a></td>\n      </tr>\n  </tbody>\n</table>\n\n> **note**: \n> more documentations can be found at [user guide](https://github.com/intel/neural-compressor/blob/master/docs/source/user_guide.md).\n\n## selected publications/events\n* blog by intel: [effective weight-only quantization for large language models with intel\u00ae neural compressor](https://community.intel.com/t5/blogs/tech-innovation/artificial-intelligence-ai/effective-weight-only-quantization-for-large-language-models/post/1529552) (oct 2023)\n* emnlp'2023 (under review): [teq: trainable equivalent transformation for quantization of llms](https://openreview.net/forum?id=iai8xeinaf&referrer=%5bauthor%20console%5d) (sep 2023)\n* arxiv: [efficient post-training quantization with fp8 formats](https://arxiv.org/abs/2309.14592) (sep 2023)\n* arxiv: [optimize weight rounding via signed gradient descent for the quantization of llms](https://arxiv.org/abs/2309.05516) (sep 2023)\n* neurips'2022: [fast distilbert on cpus](https://arxiv.org/abs/2211.07715) (oct 2022)\n* neurips'2022: [quala-minilm: a quantized length adaptive minilm](https://arxiv.org/abs/2210.17114) (oct 2022)\n\n> **note**: \n> view [full publication list](https://github.com/intel/neural-compressor/blob/master/docs/source/publication_list.md).\n\n## additional content\n\n* [release information](./docs/source/releases_info.md)\n* [contribution guidelines](./docs/source/contributing.md)\n* [legal information](./docs/source/legal_information.md)\n* [security policy](security.md)\n\n## communication \n- [github issues](https://github.com/intel/neural-compressor/issues): mainly for bug reports, new feature requests, question asking, etc.\n- [email](mailto:inc.maintainers@intel.com): welcome to raise any interesting research ideas on model compression techniques by email for collaborations.  \n- [discord channel](https://discord.com/invite/wxk3j3zjku): join the discord channel for more flexible technical discussion.\n- [wechat group](/docs/source/imgs/wechat_group.jpg): scan the qa code to join the technical discussion.\n",
  "docs_url": null,
  "keywords": "quantization,auto-tuning,post-training static quantization,post-training dynamic quantization,quantization-aware training",
  "license": "apache 2.0",
  "name": "neural-compressor-3x-pt",
  "package_url": "https://pypi.org/project/neural-compressor-3x-pt/",
  "project_url": "https://pypi.org/project/neural-compressor-3x-pt/",
  "project_urls": {
    "Homepage": "https://github.com/intel/neural-compressor"
  },
  "release_url": "https://pypi.org/project/neural-compressor-3x-pt/2.4/",
  "requires_dist": [
    "torch"
  ],
  "requires_python": ">=3.7.0",
  "summary": "repository of intel\u00ae neural compressor",
  "version": "2.4",
  "releases": [],
  "developers": [
    "feng.tian@intel.com",
    "haihao.shen@intel.com",
    "intel_aia_team",
    "suyue.chen@intel.com"
  ],
  "kwds": "intel_neural_compressor_for_pytorch compression neural_compressor post_training_quantization quantization",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_neural_compressor_3x_pt",
  "homepage": "https://github.com/intel/neural-compressor",
  "release_count": 1,
  "dependency_ids": [
    "pypi_torch"
  ]
}