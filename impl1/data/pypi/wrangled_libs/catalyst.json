{
  "classifiers": [
    "development status :: 4 - beta",
    "environment :: console",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "natural language :: english",
    "operating system :: os independent",
    "programming language :: python",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "programming language :: python :: implementation :: cpython",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: scientific/engineering :: image recognition",
    "topic :: scientific/engineering :: information analysis"
  ],
  "description": "catalyst. accelerated deep learning r&d with pytorch.\n\n<div align=\"center\">\n\n[![catalyst logo](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst_logo.png)](https://github.com/catalyst-team/catalyst)\n\n**accelerated deep learning r&d**\n\n[![codefactor](https://www.codefactor.io/repository/github/catalyst-team/catalyst/badge)](https://www.codefactor.io/repository/github/catalyst-team/catalyst)\n[![pipi version](https://img.shields.io/pypi/v/catalyst.svg)](https://pypi.org/project/catalyst/)\n[![docs](https://img.shields.io/badge/dynamic/json.svg?label=docs&url=https%3a%2f%2fpypi.org%2fpypi%2fcatalyst%2fjson&query=%24.info.version&colorb=brightgreen&prefix=v)](https://catalyst-team.github.io/catalyst/index.html)\n[![docker](https://img.shields.io/badge/docker-hub-blue)](https://hub.docker.com/r/catalystteam/catalyst/tags)\n[![pypi status](https://pepy.tech/badge/catalyst)](https://pepy.tech/project/catalyst)\n\n[![twitter](https://img.shields.io/badge/news-twitter-499feb)](https://twitter.com/catalystteam)\n[![telegram](https://img.shields.io/badge/channel-telegram-blue)](https://t.me/catalyst_team)\n[![slack](https://img.shields.io/badge/catalyst-slack-success)](https://join.slack.com/t/catalyst-team-devs/shared_invite/zt-d9miirnn-z86okdzfmklmg4fgfdzafw)\n[![github contributors](https://img.shields.io/github/contributors/catalyst-team/catalyst.svg?logo=github&logocolor=white)](https://github.com/catalyst-team/catalyst/graphs/contributors)\n\n![codestyle](https://github.com/catalyst-team/catalyst/workflows/codestyle/badge.svg?branch=master&event=push)\n![docs](https://github.com/catalyst-team/catalyst/workflows/docs/badge.svg?branch=master&event=push)\n![catalyst](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n![integrations](https://github.com/catalyst-team/catalyst/workflows/integrations/badge.svg?branch=master&event=push)\n\n[![python](https://img.shields.io/badge/python_3.6-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n[![python](https://img.shields.io/badge/python_3.7-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n[![python](https://img.shields.io/badge/python_3.8-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n\n[![os](https://img.shields.io/badge/linux-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n[![os](https://img.shields.io/badge/osx-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n[![os](https://img.shields.io/badge/wsl-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n</div>\n\ncatalyst is a pytorch framework for deep learning research and development.\nit focuses on reproducibility, rapid experimentation, and codebase reuse\nso you can create something new rather than write yet another train loop.\n<br/> break the cycle \u2013 use the catalyst!\n\n- [project manifest](https://github.com/catalyst-team/catalyst/blob/master/manifest.md)\n- [framework architecture](https://miro.com/app/board/o9j_lxbo-2k=/)\n- [catalyst at ai landscape](https://landscape.lfai.foundation/selected=catalyst)\n- part of the [pytorch ecosystem](https://pytorch.org/ecosystem/)\n\n<details>\n<summary>catalyst at pytorch ecosystem day 2021</summary>\n<p>\n\n[![catalyst poster](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst-pted21.png)](https://github.com/catalyst-team/catalyst)\n\n</p>\n</details>\n\n<details>\n<summary>catalyst at pytorch developer day 2021</summary>\n<p>\n\n[![catalyst poster](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst-ptdd21.png)](https://github.com/catalyst-team/catalyst)\n\n</p>\n</details>\n\n----\n\n## getting started\n\n```bash\npip install -u catalyst\n```\n\n```python\nimport os\nfrom torch import nn, optim\nfrom torch.utils.data import dataloader\nfrom catalyst import dl, utils\nfrom catalyst.contrib.datasets import mnist\n\nmodel = nn.sequential(nn.flatten(), nn.linear(28 * 28, 10))\ncriterion = nn.crossentropyloss()\noptimizer = optim.adam(model.parameters(), lr=0.02)\nloaders = {\n    \"train\": dataloader(mnist(os.getcwd(), train=true), batch_size=32),\n    \"valid\": dataloader(mnist(os.getcwd(), train=false), batch_size=32),\n}\n\nrunner = dl.supervisedrunner(\n    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n)\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    num_epochs=1,\n    callbacks=[\n        dl.accuracycallback(input_key=\"logits\", target_key=\"targets\", topk=(1, 3, 5)),\n        dl.precisionrecallf1supportcallback(input_key=\"logits\", target_key=\"targets\"),\n    ],\n    logdir=\"./logs\",\n    valid_loader=\"valid\",\n    valid_metric=\"loss\",\n    minimize_valid_metric=true,\n    verbose=true,\n)\n\n# model evaluation\nmetrics = runner.evaluate_loader(\n    loader=loaders[\"valid\"],\n    callbacks=[dl.accuracycallback(input_key=\"logits\", target_key=\"targets\", topk=(1, 3, 5))],\n)\n\n# model inference\nfor prediction in runner.predict_loader(loader=loaders[\"valid\"]):\n    assert prediction[\"logits\"].detach().cpu().numpy().shape[-1] == 10\n\n# model post-processing\nmodel = runner.model.cpu()\nbatch = next(iter(loaders[\"valid\"]))[0]\nutils.trace_model(model=model, batch=batch)\nutils.quantize_model(model=model)\nutils.prune_model(model=model, pruning_fn=\"l1_unstructured\", amount=0.8)\nutils.onnx_export(model=model, batch=batch, file=\"./logs/mnist.onnx\", verbose=true)\n```\n\n### step-by-step guide\n1. start with [catalyst \u2014 a pytorch framework for accelerated deep learning r&d](https://medium.com/pytorch/catalyst-a-pytorch-framework-for-accelerated-deep-learning-r-d-ad9621e4ca88?source=friends_link&sk=885b4409aecab505db0a63b06f19dcef) introduction.\n1. try [notebook tutorials](#minimal-examples) or check [minimal examples](#minimal-examples) for first deep dive.\n1. read [blog posts](https://catalyst-team.com/post/) with use-cases and guides.\n1. learn machine learning with our [\"deep learning with catalyst\" course](https://catalyst-team.com/#course).\n1. and finally, [join our slack](https://join.slack.com/t/catalyst-team-core/shared_invite/zt-d9miirnn-z86okdzfmklmg4fgfdzafw) if you want to chat with the team and contributors.\n\n\n## table of contents\n- [getting started](#getting-started)\n  - [step-by-step guide](#step-by-step-guide)\n- [table of contents](#table-of-contents)\n- [overview](#overview)\n  - [installation](#installation)\n  - [documentation](#documentation)\n  - [minimal examples](#minimal-examples)\n  - [tests](#tests)\n  - [blog posts](#blog-posts)\n  - [talks](#talks)\n- [community](#community)\n  - [contribution guide](#contribution-guide)\n  - [user feedback](#user-feedback)\n  - [acknowledgments](#acknowledgments)\n  - [trusted by](#trusted-by)\n  - [citation](#citation)\n\n\n## overview\ncatalyst helps you implement compact\nbut full-featured deep learning pipelines with just a few lines of code.\nyou get a training loop with metrics, early-stopping, model checkpointing,\nand other features without the boilerplate.\n\n\n### installation\n\ngeneric installation:\n```bash\npip install -u catalyst\n```\n\n<details>\n<summary>specialized versions, extra requirements might apply</summary>\n<p>\n\n```bash\npip install catalyst[ml]         # installs ml-based catalyst\npip install catalyst[cv]         # installs cv-based catalyst\n# master version installation\npip install git+https://github.com/catalyst-team/catalyst@master --upgrade\n# all available extensions are listed here:\n# https://github.com/catalyst-team/catalyst/blob/master/setup.py\n```\n</p>\n</details>\n\ncatalyst is compatible with: python 3.7+. pytorch 1.4+. <br/>\ntested on ubuntu 16.04/18.04/20.04, macos 10.15, windows 10, and windows subsystem for linux.\n\n### documentation\n- [master](https://catalyst-team.github.io/catalyst/)\n- [22.02](https://catalyst-team.github.io/catalyst/v22.02/index.html)\n\n- <details>\n  <summary>2021 edition</summary>\n  <p>\n\n    - [21.12](https://catalyst-team.github.io/catalyst/v21.12/index.html)\n    - [21.11](https://catalyst-team.github.io/catalyst/v21.11/index.html)\n    - [21.10](https://catalyst-team.github.io/catalyst/v21.10/index.html)\n    - [21.09](https://catalyst-team.github.io/catalyst/v21.09/index.html)\n    - [21.08](https://catalyst-team.github.io/catalyst/v21.08/index.html)\n    - [21.07](https://catalyst-team.github.io/catalyst/v21.07/index.html)\n    - [21.06](https://catalyst-team.github.io/catalyst/v21.06/index.html)\n    - [21.05](https://catalyst-team.github.io/catalyst/v21.05/index.html) ([catalyst \u2014 a pytorch framework for accelerated deep learning r&d](https://medium.com/pytorch/catalyst-a-pytorch-framework-for-accelerated-deep-learning-r-d-ad9621e4ca88?source=friends_link&sk=885b4409aecab505db0a63b06f19dcef))\n    - [21.04/21.04.1](https://catalyst-team.github.io/catalyst/v21.04/index.html), [21.04.2](https://catalyst-team.github.io/catalyst/v21.04.2/index.html)\n    - [21.03](https://catalyst-team.github.io/catalyst/v21.03/index.html), [21.03.1/21.03.2](https://catalyst-team.github.io/catalyst/v21.03.1/index.html)\n\n  </p>\n  </details>\n- <details>\n  <summary>2020 edition</summary>\n  <p>\n\n    - [20.12](https://catalyst-team.github.io/catalyst/v20.12/index.html)\n    - [20.11](https://catalyst-team.github.io/catalyst/v20.11/index.html)\n    - [20.10](https://catalyst-team.github.io/catalyst/v20.10/index.html)\n    - [20.09](https://catalyst-team.github.io/catalyst/v20.09/index.html)\n    - [20.08.2](https://catalyst-team.github.io/catalyst/v20.08.2/index.html)\n    - [20.07](https://catalyst-team.github.io/catalyst/v20.07/index.html) ([dev blog: 20.07 release](https://medium.com/pytorch/catalyst-dev-blog-20-07-release-fb489cd23e14?source=friends_link&sk=7ab92169658fe9a9e1c44068f28cc36c))\n    - [20.06](https://catalyst-team.github.io/catalyst/v20.06/index.html)\n    - [20.05](https://catalyst-team.github.io/catalyst/v20.05/index.html), [20.05.1](https://catalyst-team.github.io/catalyst/v20.05.1/index.html)\n    - [20.04](https://catalyst-team.github.io/catalyst/v20.04/index.html), [20.04.1](https://catalyst-team.github.io/catalyst/v20.04.1/index.html), [20.04.2](https://catalyst-team.github.io/catalyst/v20.04.2/index.html)\n\n  </p>\n  </details>\n\n\n### minimal examples\n\n- [![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/customizing_what_happens_in_train.ipynb) introduction tutorial \"[customizing what happens in `train`](./examples/notebooks/customizing_what_happens_in_train.ipynb)\"\n- [![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/customization_tutorial.ipynb) demo with [customization examples](./examples/notebooks/customization_tutorial.ipynb)\n- [![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/reinforcement_learning.ipynb) [reinforcement learning with catalyst](./examples/notebooks/reinforcement_learning.ipynb)\n- [and more](./examples/)\n\n<details>\n<summary>customrunner \u2013 pytorch for-loop decomposition</summary>\n<p>\n\n```python\nimport os\nfrom torch import nn, optim\nfrom torch.nn import functional as f\nfrom torch.utils.data import dataloader\nfrom catalyst import dl, metrics\nfrom catalyst.contrib.datasets import mnist\n\nmodel = nn.sequential(nn.flatten(), nn.linear(28 * 28, 10))\noptimizer = optim.adam(model.parameters(), lr=0.02)\n\ntrain_data = mnist(os.getcwd(), train=true)\nvalid_data = mnist(os.getcwd(), train=false)\nloaders = {\n    \"train\": dataloader(train_data, batch_size=32),\n    \"valid\": dataloader(valid_data, batch_size=32),\n}\n\nclass customrunner(dl.runner):\n    def predict_batch(self, batch):\n        # model inference step\n        return self.model(batch[0].to(self.engine.device))\n\n    def on_loader_start(self, runner):\n        super().on_loader_start(runner)\n        self.meters = {\n            key: metrics.additivemetric(compute_on_call=false)\n            for key in [\"loss\", \"accuracy01\", \"accuracy03\"]\n        }\n\n    def handle_batch(self, batch):\n        # model train/valid step\n        # unpack the batch\n        x, y = batch\n        # run model forward pass\n        logits = self.model(x)\n        # compute the loss\n        loss = f.cross_entropy(logits, y)\n        # compute the metrics\n        accuracy01, accuracy03 = metrics.accuracy(logits, y, topk=(1, 3))\n        # log metrics\n        self.batch_metrics.update(\n            {\"loss\": loss, \"accuracy01\": accuracy01, \"accuracy03\": accuracy03}\n        )\n        for key in [\"loss\", \"accuracy01\", \"accuracy03\"]:\n            self.meters[key].update(self.batch_metrics[key].item(), self.batch_size)\n        # run model backward pass\n        if self.is_train_loader:\n            self.engine.backward(loss)\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n    def on_loader_end(self, runner):\n        for key in [\"loss\", \"accuracy01\", \"accuracy03\"]:\n            self.loader_metrics[key] = self.meters[key].compute()[0]\n        super().on_loader_end(runner)\n\nrunner = customrunner()\n# model training\nrunner.train(\n    model=model,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=\"./logs\",\n    num_epochs=5,\n    verbose=true,\n    valid_loader=\"valid\",\n    valid_metric=\"loss\",\n    minimize_valid_metric=true,\n)\n# model inference\nfor logits in runner.predict_loader(loader=loaders[\"valid\"]):\n    assert logits.detach().cpu().numpy().shape[-1] == 10\n```\n</p>\n</details>\n\n<details>\n<summary>ml - linear regression</summary>\n<p>\n\n```python\nimport torch\nfrom torch.utils.data import dataloader, tensordataset\nfrom catalyst import dl\n\n# data\nnum_samples, num_features = int(1e4), int(1e1)\nx, y = torch.rand(num_samples, num_features), torch.rand(num_samples)\ndataset = tensordataset(x, y)\nloader = dataloader(dataset, batch_size=32, num_workers=1)\nloaders = {\"train\": loader, \"valid\": loader}\n\n# model, criterion, optimizer, scheduler\nmodel = torch.nn.linear(num_features, 1)\ncriterion = torch.nn.mseloss()\noptimizer = torch.optim.adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.multisteplr(optimizer, [3, 6])\n\n# model training\nrunner = dl.supervisedrunner()\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    logdir=\"./logdir\",\n    valid_loader=\"valid\",\n    valid_metric=\"loss\",\n    minimize_valid_metric=true,\n    num_epochs=8,\n    verbose=true,\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>ml - multiclass classification</summary>\n<p>\n\n```python\nimport torch\nfrom torch.utils.data import dataloader, tensordataset\nfrom catalyst import dl\n\n# sample data\nnum_samples, num_features, num_classes = int(1e4), int(1e1), 4\nx = torch.rand(num_samples, num_features)\ny = (torch.rand(num_samples,) * num_classes).to(torch.int64)\n\n# pytorch loaders\ndataset = tensordataset(x, y)\nloader = dataloader(dataset, batch_size=32, num_workers=1)\nloaders = {\"train\": loader, \"valid\": loader}\n\n# model, criterion, optimizer, scheduler\nmodel = torch.nn.linear(num_features, num_classes)\ncriterion = torch.nn.crossentropyloss()\noptimizer = torch.optim.adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.multisteplr(optimizer, [2])\n\n# model training\nrunner = dl.supervisedrunner(\n    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n)\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    logdir=\"./logdir\",\n    num_epochs=3,\n    valid_loader=\"valid\",\n    valid_metric=\"accuracy03\",\n    minimize_valid_metric=false,\n    verbose=true,\n    callbacks=[\n        dl.accuracycallback(input_key=\"logits\", target_key=\"targets\", num_classes=num_classes),\n        # uncomment for extra metrics:\n        # dl.precisionrecallf1supportcallback(\n        #     input_key=\"logits\", target_key=\"targets\", num_classes=num_classes\n        # ),\n        # dl.auccallback(input_key=\"logits\", target_key=\"targets\"),\n        # catalyst[ml] required ``pip install catalyst[ml]``\n        # dl.confusionmatrixcallback(\n        #     input_key=\"logits\", target_key=\"targets\", num_classes=num_classes\n        # ),\n    ],\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>ml - multilabel classification</summary>\n<p>\n\n```python\nimport torch\nfrom torch.utils.data import dataloader, tensordataset\nfrom catalyst import dl\n\n# sample data\nnum_samples, num_features, num_classes = int(1e4), int(1e1), 4\nx = torch.rand(num_samples, num_features)\ny = (torch.rand(num_samples, num_classes) > 0.5).to(torch.float32)\n\n# pytorch loaders\ndataset = tensordataset(x, y)\nloader = dataloader(dataset, batch_size=32, num_workers=1)\nloaders = {\"train\": loader, \"valid\": loader}\n\n# model, criterion, optimizer, scheduler\nmodel = torch.nn.linear(num_features, num_classes)\ncriterion = torch.nn.bcewithlogitsloss()\noptimizer = torch.optim.adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.multisteplr(optimizer, [2])\n\n# model training\nrunner = dl.supervisedrunner(\n    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n)\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    logdir=\"./logdir\",\n    num_epochs=3,\n    valid_loader=\"valid\",\n    valid_metric=\"accuracy01\",\n    minimize_valid_metric=false,\n    verbose=true,\n    callbacks=[\n        dl.batchtransformcallback(\n            transform=torch.sigmoid,\n            scope=\"on_batch_end\",\n            input_key=\"logits\",\n            output_key=\"scores\"\n        ),\n        dl.auccallback(input_key=\"scores\", target_key=\"targets\"),\n        # uncomment for extra metrics:\n        # dl.multilabelaccuracycallback(input_key=\"scores\", target_key=\"targets\", threshold=0.5),\n        # dl.multilabelprecisionrecallf1supportcallback(\n        #     input_key=\"scores\", target_key=\"targets\", threshold=0.5\n        # ),\n    ]\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>ml - multihead classification</summary>\n<p>\n\n```python\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import dataloader, tensordataset\nfrom catalyst import dl\n\n# sample data\nnum_samples, num_features, num_classes1, num_classes2 = int(1e4), int(1e1), 4, 10\nx = torch.rand(num_samples, num_features)\ny1 = (torch.rand(num_samples,) * num_classes1).to(torch.int64)\ny2 = (torch.rand(num_samples,) * num_classes2).to(torch.int64)\n\n# pytorch loaders\ndataset = tensordataset(x, y1, y2)\nloader = dataloader(dataset, batch_size=32, num_workers=1)\nloaders = {\"train\": loader, \"valid\": loader}\n\nclass custommodule(nn.module):\n    def __init__(self, in_features: int, out_features1: int, out_features2: int):\n        super().__init__()\n        self.shared = nn.linear(in_features, 128)\n        self.head1 = nn.linear(128, out_features1)\n        self.head2 = nn.linear(128, out_features2)\n\n    def forward(self, x):\n        x = self.shared(x)\n        y1 = self.head1(x)\n        y2 = self.head2(x)\n        return y1, y2\n\n# model, criterion, optimizer, scheduler\nmodel = custommodule(num_features, num_classes1, num_classes2)\ncriterion = nn.crossentropyloss()\noptimizer = optim.adam(model.parameters())\nscheduler = optim.lr_scheduler.multisteplr(optimizer, [2])\n\nclass customrunner(dl.runner):\n    def handle_batch(self, batch):\n        x, y1, y2 = batch\n        y1_hat, y2_hat = self.model(x)\n        self.batch = {\n            \"features\": x,\n            \"logits1\": y1_hat,\n            \"logits2\": y2_hat,\n            \"targets1\": y1,\n            \"targets2\": y2,\n        }\n\n# model training\nrunner = customrunner()\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    num_epochs=3,\n    verbose=true,\n    callbacks=[\n        dl.criterioncallback(metric_key=\"loss1\", input_key=\"logits1\", target_key=\"targets1\"),\n        dl.criterioncallback(metric_key=\"loss2\", input_key=\"logits2\", target_key=\"targets2\"),\n        dl.metricaggregationcallback(metric_key=\"loss\", metrics=[\"loss1\", \"loss2\"], mode=\"mean\"),\n        dl.backwardcallback(metric_key=\"loss\"),\n        dl.optimizercallback(metric_key=\"loss\"),\n        dl.schedulercallback(),\n        dl.accuracycallback(\n            input_key=\"logits1\", target_key=\"targets1\", num_classes=num_classes1, prefix=\"one_\"\n        ),\n        dl.accuracycallback(\n            input_key=\"logits2\", target_key=\"targets2\", num_classes=num_classes2, prefix=\"two_\"\n        ),\n        # catalyst[ml] required ``pip install catalyst[ml]``\n        # dl.confusionmatrixcallback(\n        #     input_key=\"logits1\", target_key=\"targets1\", num_classes=num_classes1, prefix=\"one_cm\"\n        # ),\n        # dl.confusionmatrixcallback(\n        #     input_key=\"logits2\", target_key=\"targets2\", num_classes=num_classes2, prefix=\"two_cm\"\n        # ),\n        dl.checkpointcallback(\n            logdir=\"./logs/one\",\n            loader_key=\"valid\", metric_key=\"one_accuracy01\", minimize=false, topk=1\n        ),\n        dl.checkpointcallback(\n            logdir=\"./logs/two\",\n            loader_key=\"valid\", metric_key=\"two_accuracy03\", minimize=false, topk=3\n        ),\n    ],\n    loggers={\"console\": dl.consolelogger(), \"tb\": dl.tensorboardlogger(\"./logs/tb\")},\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>ml \u2013 recsys</summary>\n<p>\n\n```python\nimport torch\nfrom torch.utils.data import dataloader, tensordataset\nfrom catalyst import dl\n\n# sample data\nnum_users, num_features, num_items = int(1e4), int(1e1), 10\nx = torch.rand(num_users, num_features)\ny = (torch.rand(num_users, num_items) > 0.5).to(torch.float32)\n\n# pytorch loaders\ndataset = tensordataset(x, y)\nloader = dataloader(dataset, batch_size=32, num_workers=1)\nloaders = {\"train\": loader, \"valid\": loader}\n\n# model, criterion, optimizer, scheduler\nmodel = torch.nn.linear(num_features, num_items)\ncriterion = torch.nn.bcewithlogitsloss()\noptimizer = torch.optim.adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.multisteplr(optimizer, [2])\n\n# model training\nrunner = dl.supervisedrunner(\n    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n)\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    num_epochs=3,\n    verbose=true,\n    callbacks=[\n        dl.batchtransformcallback(\n            transform=torch.sigmoid,\n            scope=\"on_batch_end\",\n            input_key=\"logits\",\n            output_key=\"scores\"\n        ),\n        dl.criterioncallback(input_key=\"logits\", target_key=\"targets\", metric_key=\"loss\"),\n        # uncomment for extra metrics:\n        # dl.auccallback(input_key=\"scores\", target_key=\"targets\"),\n        # dl.hitratecallback(input_key=\"scores\", target_key=\"targets\", topk=(1, 3, 5)),\n        # dl.mrrcallback(input_key=\"scores\", target_key=\"targets\", topk=(1, 3, 5)),\n        # dl.mapcallback(input_key=\"scores\", target_key=\"targets\", topk=(1, 3, 5)),\n        # dl.ndcgcallback(input_key=\"scores\", target_key=\"targets\", topk=(1, 3, 5)),\n        dl.backwardcallback(metric_key=\"loss\"),\n        dl.optimizercallback(metric_key=\"loss\"),\n        dl.schedulercallback(),\n        dl.checkpointcallback(\n            logdir=\"./logs\", loader_key=\"valid\", metric_key=\"loss\", minimize=true\n        ),\n    ]\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>cv - mnist classification</summary>\n<p>\n\n```python\nimport os\nfrom torch import nn, optim\nfrom torch.utils.data import dataloader\nfrom catalyst import dl\nfrom catalyst.contrib.datasets import mnist\n\nmodel = nn.sequential(nn.flatten(), nn.linear(28 * 28, 10))\ncriterion = nn.crossentropyloss()\noptimizer = optim.adam(model.parameters(), lr=0.02)\n\ntrain_data = mnist(os.getcwd(), train=true)\nvalid_data = mnist(os.getcwd(), train=false)\nloaders = {\n    \"train\": dataloader(train_data, batch_size=32),\n    \"valid\": dataloader(valid_data, batch_size=32),\n}\n\nrunner = dl.supervisedrunner()\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    num_epochs=1,\n    logdir=\"./logs\",\n    valid_loader=\"valid\",\n    valid_metric=\"loss\",\n    minimize_valid_metric=true,\n    verbose=true,\n# uncomment for extra metrics:\n#     callbacks=[\n#         dl.accuracycallback(input_key=\"logits\", target_key=\"targets\", num_classes=10),\n#         dl.precisionrecallf1supportcallback(\n#             input_key=\"logits\", target_key=\"targets\", num_classes=10\n#         ),\n#         dl.auccallback(input_key=\"logits\", target_key=\"targets\"),\n#         # catalyst[ml] required ``pip install catalyst[ml]``\n#         dl.confusionmatrixcallback(\n#             input_key=\"logits\", target_key=\"targets\", num_classes=num_classes\n#         ),\n#     ]\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>cv - mnist segmentation</summary>\n<p>\n\n```python\nimport os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import dataloader\nfrom catalyst import dl\nfrom catalyst.contrib.datasets import mnist\nfrom catalyst.contrib.losses import iouloss\n\n\nmodel = nn.sequential(\n    nn.conv2d(1, 1, 3, 1, 1), nn.relu(),\n    nn.conv2d(1, 1, 3, 1, 1), nn.sigmoid(),\n)\ncriterion = iouloss()\noptimizer = torch.optim.adam(model.parameters(), lr=0.02)\n\ntrain_data = mnist(os.getcwd(), train=true)\nvalid_data = mnist(os.getcwd(), train=false)\nloaders = {\n    \"train\": dataloader(train_data, batch_size=32),\n    \"valid\": dataloader(valid_data, batch_size=32),\n}\n\nclass customrunner(dl.supervisedrunner):\n    def handle_batch(self, batch):\n        x = batch[self._input_key]\n        x_noise = (x + torch.rand_like(x)).clamp_(0, 1)\n        x_ = self.model(x_noise)\n        self.batch = {self._input_key: x, self._output_key: x_, self._target_key: x}\n\nrunner = customrunner(\n    input_key=\"features\", output_key=\"scores\", target_key=\"targets\", loss_key=\"loss\"\n)\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    num_epochs=1,\n    callbacks=[\n        dl.ioucallback(input_key=\"scores\", target_key=\"targets\"),\n        dl.dicecallback(input_key=\"scores\", target_key=\"targets\"),\n        dl.trevskycallback(input_key=\"scores\", target_key=\"targets\", alpha=0.2),\n    ],\n    logdir=\"./logdir\",\n    valid_loader=\"valid\",\n    valid_metric=\"loss\",\n    minimize_valid_metric=true,\n    verbose=true,\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>cv - mnist metric learning</summary>\n<p>\n\n```python\nimport os\nfrom torch.optim import adam\nfrom torch.utils.data import dataloader\nfrom catalyst import dl\nfrom catalyst.contrib.data import hardtripletssampler\nfrom catalyst.contrib.datasets import mnistmldataset, mnistqgdataset\nfrom catalyst.contrib.losses import tripletmarginlosswithsampler\nfrom catalyst.contrib.models import mnistsimplenet\nfrom catalyst.data.sampler import batchbalanceclasssampler\n\n\n# 1. train and valid loaders\ntrain_dataset = mnistmldataset(root=os.getcwd())\nsampler = batchbalanceclasssampler(\n    labels=train_dataset.get_labels(), num_classes=5, num_samples=10, num_batches=10\n)\ntrain_loader = dataloader(dataset=train_dataset, batch_sampler=sampler)\n\nvalid_dataset = mnistqgdataset(root=os.getcwd(), gallery_fraq=0.2)\nvalid_loader = dataloader(dataset=valid_dataset, batch_size=1024)\n\n# 2. model and optimizer\nmodel = mnistsimplenet(out_features=16)\noptimizer = adam(model.parameters(), lr=0.001)\n\n# 3. criterion with triplets sampling\nsampler_inbatch = hardtripletssampler(norm_required=false)\ncriterion = tripletmarginlosswithsampler(margin=0.5, sampler_inbatch=sampler_inbatch)\n\n# 4. training with catalyst runner\nclass customrunner(dl.supervisedrunner):\n    def handle_batch(self, batch) -> none:\n        if self.is_train_loader:\n            images, targets = batch[\"features\"].float(), batch[\"targets\"].long()\n            features = self.model(images)\n            self.batch = {\"embeddings\": features, \"targets\": targets,}\n        else:\n            images, targets, is_query = \\\n                batch[\"features\"].float(), batch[\"targets\"].long(), batch[\"is_query\"].bool()\n            features = self.model(images)\n            self.batch = {\"embeddings\": features, \"targets\": targets, \"is_query\": is_query}\n\ncallbacks = [\n    dl.controlflowcallbackwrapper(\n        dl.criterioncallback(input_key=\"embeddings\", target_key=\"targets\", metric_key=\"loss\"),\n        loaders=\"train\",\n    ),\n    dl.controlflowcallbackwrapper(\n        dl.cmcscorecallback(\n            embeddings_key=\"embeddings\",\n            labels_key=\"targets\",\n            is_query_key=\"is_query\",\n            topk=[1],\n        ),\n        loaders=\"valid\",\n    ),\n    dl.periodicloadercallback(\n        valid_loader_key=\"valid\", valid_metric_key=\"cmc01\", minimize=false, valid=2\n    ),\n]\n\nrunner = customrunner(input_key=\"features\", output_key=\"embeddings\")\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    callbacks=callbacks,\n    loaders={\"train\": train_loader, \"valid\": valid_loader},\n    verbose=false,\n    logdir=\"./logs\",\n    valid_loader=\"valid\",\n    valid_metric=\"cmc01\",\n    minimize_valid_metric=false,\n    num_epochs=10,\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>cv - mnist gan</summary>\n<p>\n\n```python\nimport os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import dataloader\nfrom catalyst import dl\nfrom catalyst.contrib.datasets import mnist\nfrom catalyst.contrib.layers import globalmaxpool2d, lambda\n\nlatent_dim = 128\ngenerator = nn.sequential(\n    # we want to generate 128 coefficients to reshape into a 7x7x128 map\n    nn.linear(128, 128 * 7 * 7),\n    nn.leakyrelu(0.2, inplace=true),\n    lambda(lambda x: x.view(x.size(0), 128, 7, 7)),\n    nn.convtranspose2d(128, 128, (4, 4), stride=(2, 2), padding=1),\n    nn.leakyrelu(0.2, inplace=true),\n    nn.convtranspose2d(128, 128, (4, 4), stride=(2, 2), padding=1),\n    nn.leakyrelu(0.2, inplace=true),\n    nn.conv2d(128, 1, (7, 7), padding=3),\n    nn.sigmoid(),\n)\ndiscriminator = nn.sequential(\n    nn.conv2d(1, 64, (3, 3), stride=(2, 2), padding=1),\n    nn.leakyrelu(0.2, inplace=true),\n    nn.conv2d(64, 128, (3, 3), stride=(2, 2), padding=1),\n    nn.leakyrelu(0.2, inplace=true),\n    globalmaxpool2d(),\n    nn.flatten(),\n    nn.linear(128, 1),\n)\n\nmodel = nn.moduledict({\"generator\": generator, \"discriminator\": discriminator})\ncriterion = {\"generator\": nn.bcewithlogitsloss(), \"discriminator\": nn.bcewithlogitsloss()}\noptimizer = {\n    \"generator\": torch.optim.adam(generator.parameters(), lr=0.0003, betas=(0.5, 0.999)),\n    \"discriminator\": torch.optim.adam(discriminator.parameters(), lr=0.0003, betas=(0.5, 0.999)),\n}\ntrain_data = mnist(os.getcwd(), train=false)\nloaders = {\"train\": dataloader(train_data, batch_size=32)}\n\nclass customrunner(dl.runner):\n    def predict_batch(self, batch):\n        batch_size = 1\n        # sample random points in the latent space\n        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.engine.device)\n        # decode them to fake images\n        generated_images = self.model[\"generator\"](random_latent_vectors).detach()\n        return generated_images\n\n    def handle_batch(self, batch):\n        real_images, _ = batch\n        batch_size = real_images.shape[0]\n\n        # sample random points in the latent space\n        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.engine.device)\n\n        # decode them to fake images\n        generated_images = self.model[\"generator\"](random_latent_vectors).detach()\n        # combine them with real images\n        combined_images = torch.cat([generated_images, real_images])\n\n        # assemble labels discriminating real from fake images\n        labels = \\\n            torch.cat([torch.ones((batch_size, 1)), torch.zeros((batch_size, 1))]).to(self.engine.device)\n        # add random noise to the labels - important trick!\n        labels += 0.05 * torch.rand(labels.shape).to(self.engine.device)\n\n        # discriminator forward\n        combined_predictions = self.model[\"discriminator\"](combined_images)\n\n        # sample random points in the latent space\n        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.engine.device)\n        # assemble labels that say \"all real images\"\n        misleading_labels = torch.zeros((batch_size, 1)).to(self.engine.device)\n\n        # generator forward\n        generated_images = self.model[\"generator\"](random_latent_vectors)\n        generated_predictions = self.model[\"discriminator\"](generated_images)\n\n        self.batch = {\n            \"combined_predictions\": combined_predictions,\n            \"labels\": labels,\n            \"generated_predictions\": generated_predictions,\n            \"misleading_labels\": misleading_labels,\n        }\n\n\nrunner = customrunner()\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    callbacks=[\n        dl.criterioncallback(\n            input_key=\"combined_predictions\",\n            target_key=\"labels\",\n            metric_key=\"loss_discriminator\",\n            criterion_key=\"discriminator\",\n        ),\n        dl.backwardcallback(metric_key=\"loss_discriminator\"),\n        dl.optimizercallback(\n            optimizer_key=\"discriminator\",\n            metric_key=\"loss_discriminator\",\n        ),\n        dl.criterioncallback(\n            input_key=\"generated_predictions\",\n            target_key=\"misleading_labels\",\n            metric_key=\"loss_generator\",\n            criterion_key=\"generator\",\n        ),\n        dl.backwardcallback(metric_key=\"loss_generator\"),\n        dl.optimizercallback(\n            optimizer_key=\"generator\",\n            metric_key=\"loss_generator\",\n        ),\n    ],\n    valid_loader=\"train\",\n    valid_metric=\"loss_generator\",\n    minimize_valid_metric=true,\n    num_epochs=20,\n    verbose=true,\n    logdir=\"./logs_gan\",\n)\n\n# visualization (matplotlib required):\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n# plt.imshow(runner.predict_batch(none)[0, 0].cpu().numpy())\n```\n</p>\n</details>\n\n\n<details>\n<summary>cv - mnist vae</summary>\n<p>\n\n```python\nimport os\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as f\nfrom torch.utils.data import dataloader\nfrom catalyst import dl, metrics\nfrom catalyst.contrib.datasets import mnist\n\nlog_scale_max = 2\nlog_scale_min = -10\n\ndef normal_sample(loc, log_scale):\n    scale = torch.exp(0.5 * log_scale)\n    return loc + scale * torch.randn_like(scale)\n\nclass vae(nn.module):\n    def __init__(self, in_features, hid_features):\n        super().__init__()\n        self.hid_features = hid_features\n        self.encoder = nn.linear(in_features, hid_features * 2)\n        self.decoder = nn.sequential(nn.linear(hid_features, in_features), nn.sigmoid())\n\n    def forward(self, x, deterministic=false):\n        z = self.encoder(x)\n        bs, z_dim = z.shape\n\n        loc, log_scale = z[:, : z_dim // 2], z[:, z_dim // 2 :]\n        log_scale = torch.clamp(log_scale, log_scale_min, log_scale_max)\n\n        z_ = loc if deterministic else normal_sample(loc, log_scale)\n        z_ = z_.view(bs, -1)\n        x_ = self.decoder(z_)\n\n        return x_, loc, log_scale\n\nclass customrunner(dl.irunner):\n    def __init__(self, hid_features, logdir, engine):\n        super().__init__()\n        self.hid_features = hid_features\n        self._logdir = logdir\n        self._engine = engine\n\n    def get_engine(self):\n        return self._engine\n\n    def get_loggers(self):\n        return {\n            \"console\": dl.consolelogger(),\n            \"csv\": dl.csvlogger(logdir=self._logdir),\n            \"tensorboard\": dl.tensorboardlogger(logdir=self._logdir),\n        }\n\n    @property\n    def num_epochs(self) -> int:\n        return 1\n\n    def get_loaders(self):\n        loaders = {\n            \"train\": dataloader(mnist(os.getcwd(), train=false), batch_size=32),\n            \"valid\": dataloader(mnist(os.getcwd(), train=false), batch_size=32),\n        }\n        return loaders\n\n    def get_model(self):\n        model = self.model if self.model is not none else vae(28 * 28, self.hid_features)\n        return model\n\n    def get_optimizer(self, model):\n        return optim.adam(model.parameters(), lr=0.02)\n\n    def get_callbacks(self):\n        return {\n            \"backward\": dl.backwardcallback(metric_key=\"loss\"),\n            \"optimizer\": dl.optimizercallback(metric_key=\"loss\"),\n            \"checkpoint\": dl.checkpointcallback(\n                self._logdir,\n                loader_key=\"valid\",\n                metric_key=\"loss\",\n                minimize=true,\n                topk=3,\n            ),\n        }\n\n    def on_loader_start(self, runner):\n        super().on_loader_start(runner)\n        self.meters = {\n            key: metrics.additivemetric(compute_on_call=false)\n            for key in [\"loss_ae\", \"loss_kld\", \"loss\"]\n        }\n\n    def handle_batch(self, batch):\n        x, _ = batch\n        x = x.view(x.size(0), -1)\n        x_, loc, log_scale = self.model(x, deterministic=not self.is_train_loader)\n\n        loss_ae = f.mse_loss(x_, x)\n        loss_kld = (\n            -0.5 * torch.sum(1 + log_scale - loc.pow(2) - log_scale.exp(), dim=1)\n        ).mean()\n        loss = loss_ae + loss_kld * 0.01\n\n        self.batch_metrics = {\"loss_ae\": loss_ae, \"loss_kld\": loss_kld, \"loss\": loss}\n        for key in [\"loss_ae\", \"loss_kld\", \"loss\"]:\n            self.meters[key].update(self.batch_metrics[key].item(), self.batch_size)\n\n    def on_loader_end(self, runner):\n        for key in [\"loss_ae\", \"loss_kld\", \"loss\"]:\n            self.loader_metrics[key] = self.meters[key].compute()[0]\n        super().on_loader_end(runner)\n\n    def predict_batch(self, batch):\n        random_latent_vectors = torch.randn(1, self.hid_features).to(self.engine.device)\n        generated_images = self.model.decoder(random_latent_vectors).detach()\n        return generated_images\n\nrunner = customrunner(128, \"./logs\", dl.cpuengine())\nrunner.run()\n# visualization (matplotlib required):\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n# plt.imshow(runner.predict_batch(none)[0].cpu().numpy().reshape(28, 28))\n```\n</p>\n</details>\n\n\n<details>\n<summary>automl - hyperparameters optimization with optuna</summary>\n<p>\n\n```python\nimport os\nimport optuna\nimport torch\nfrom torch import nn\nfrom torch.utils.data import dataloader\nfrom catalyst import dl\nfrom catalyst.contrib.datasets import mnist\n\n\ndef objective(trial):\n    lr = trial.suggest_loguniform(\"lr\", 1e-3, 1e-1)\n    num_hidden = int(trial.suggest_loguniform(\"num_hidden\", 32, 128))\n\n    train_data = mnist(os.getcwd(), train=true)\n    valid_data = mnist(os.getcwd(), train=false)\n    loaders = {\n        \"train\": dataloader(train_data, batch_size=32),\n        \"valid\": dataloader(valid_data, batch_size=32),\n    }\n    model = nn.sequential(\n        nn.flatten(), nn.linear(784, num_hidden), nn.relu(), nn.linear(num_hidden, 10)\n    )\n    optimizer = torch.optim.adam(model.parameters(), lr=lr)\n    criterion = nn.crossentropyloss()\n\n    runner = dl.supervisedrunner(input_key=\"features\", output_key=\"logits\", target_key=\"targets\")\n    runner.train(\n        model=model,\n        criterion=criterion,\n        optimizer=optimizer,\n        loaders=loaders,\n        callbacks={\n            \"accuracy\": dl.accuracycallback(\n                input_key=\"logits\", target_key=\"targets\", num_classes=10\n            ),\n            # catalyst[optuna] required ``pip install catalyst[optuna]``\n            \"optuna\": dl.optunapruningcallback(\n                loader_key=\"valid\", metric_key=\"accuracy01\", minimize=false, trial=trial\n            ),\n        },\n        num_epochs=3,\n    )\n    score = trial.best_score\n    return score\n\nstudy = optuna.create_study(\n    direction=\"maximize\",\n    pruner=optuna.pruners.medianpruner(\n        n_startup_trials=1, n_warmup_steps=0, interval_steps=1\n    ),\n)\nstudy.optimize(objective, n_trials=3, timeout=300)\nprint(study.best_value, study.best_params)\n```\n</p>\n</details>\n\n<details>\n<summary>config api - minimal example</summary>\n<p>\n\n```yaml title=\"example.yaml\"\nrunner:\n  _target_: catalyst.runners.supervisedrunner\n  model:\n    _var_: model\n    _target_: torch.nn.sequential\n    args:\n      - _target_: torch.nn.flatten\n      - _target_: torch.nn.linear\n        in_features: 784  # 28 * 28\n        out_features: 10\n  input_key: features\n  output_key: &output_key logits\n  target_key: &target_key targets\n  loss_key: &loss_key loss\n\nrun:\n  # \u2248 stage 1\n  - _call_: train  # runner.train(...)\n\n    criterion:\n      _target_: torch.nn.crossentropyloss\n\n    optimizer:\n      _target_: torch.optim.adam\n      params:  # model.parameters()\n        _var_: model.parameters\n      lr: 0.02\n\n    loaders:\n      train:\n        _target_: torch.utils.data.dataloader\n        dataset:\n          _target_: catalyst.contrib.datasets.mnist\n          root: data\n          train: y\n        batch_size: 32\n\n      &valid_loader_key valid:\n        &valid_loader\n        _target_: torch.utils.data.dataloader\n        dataset:\n          _target_: catalyst.contrib.datasets.mnist\n          root: data\n          train: n\n        batch_size: 32\n\n    callbacks:\n      - &accuracy_metric\n        _target_: catalyst.callbacks.accuracycallback\n        input_key: *output_key\n        target_key: *target_key\n        topk: [1,3,5]\n      - _target_: catalyst.callbacks.precisionrecallf1supportcallback\n        input_key: *output_key\n        target_key: *target_key\n\n    num_epochs: 1\n    logdir: logs\n    valid_loader: *valid_loader_key\n    valid_metric: *loss_key\n    minimize_valid_metric: y\n    verbose: y\n\n  # \u2248 stage 2\n  - _call_: evaluate_loader  # runner.evaluate_loader(...)\n    loader: *valid_loader\n    callbacks:\n      - *accuracy_metric\n\n```\n\n```sh\ncatalyst-run --config example.yaml\n```\n</p>\n</details>\n\n### tests\nall catalyst code, features, and pipelines [are fully tested](./tests).\nwe also have our own [catalyst-codestyle](https://github.com/catalyst-team/codestyle) and a corresponding pre-commit hook.\nduring testing, we train a variety of different models: image classification,\nimage segmentation, text classification, gans, and much more.\nwe then compare their convergence metrics in order to verify\nthe correctness of the training procedure and its reproducibility.\nas a result, catalyst provides fully tested and reproducible\nbest practices for your deep learning research and development.\n\n### [blog posts](https://catalyst-team.com/post/)\n\n### [talks](https://catalyst-team.com/talk/)\n\n\n## community\n\n### accelerated with catalyst\n\n<details>\n<summary>research papers</summary>\n<p>\n\n- [hierarchical attention for sentiment classification with visualization](https://github.com/neuromation/ml-recipe-hier-attention)\n- [pediatric bone age assessment](https://github.com/neuromation/ml-recipe-bone-age)\n- [implementation of the paper \"tell me where to look: guided attention inference network\"](https://github.com/ngxbac/gain)\n- [implementation of the paper \"filter response normalization layer: eliminating batch dependence in the training of deep neural networks\"](https://github.com/yukkyo/pytorch-filterresponsenormalizationlayer)\n- [implementation of the paper \"utterance-level aggregation for speaker recognition in the wild\"](https://github.com/ptjexio/speaker-recognition)\n- [implementation of the paper \"looking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation\"](https://github.com/vitrioil/speech-separation)\n- [implementation of the paper \"esrgan: enhanced super-resolution generative adversarial networks\"](https://github.com/leverxgroup/esrgan)\n\n</p>\n</details>\n\n<details>\n<summary>blog posts</summary>\n<p>\n\n- [solving the cocktail party problem using pytorch](https://medium.com/pytorch/addressing-the-cocktail-party-problem-using-pytorch-305fb74560ea)\n- [beyond fashion: deep learning with catalyst (config api)](https://evilmartians.com/chronicles/beyond-fashion-deep-learning-with-catalyst)\n- [tutorial from notebook api to config api (ru)](https://github.com/bekovmi/segmentation_tutorial)\n\n</p>\n</details>\n\n<details>\n<summary>competitions</summary>\n<p>\n\n- [kaggle quick, draw! doodle recognition challenge](https://github.com/ngxbac/kaggle-quickdraw) - 11th place\n- [catalyst.rl - neurips 2018: ai for prosthetics challenge](https://github.com/scitator/neurips-18-prosthetics-challenge) \u2013 3rd place\n- [kaggle google landmark 2019](https://github.com/ngxbac/kaggle-google-landmark-2019) - 30th place\n- [imet collection 2019 - fgvc6](https://github.com/ngxbac/kaggle-imet) - 24th place\n- [id r&d anti-spoofing challenge](https://github.com/bagxi/idrnd-anti-spoofing-challenge-solution) - 14th place\n- [neurips 2019: recursion cellular image classification](https://github.com/ngxbac/kaggle-recursion-cellular) - 4th place\n- [miccai 2019: automatic structure segmentation for radiotherapy planning challenge 2019](https://github.com/ngxbac/structseg2019)\n  * 3rd place solution for `task 3: organ-at-risk segmentation from chest ct scans`\n  * and 4th place solution for `task 4: gross target volume segmentation of lung cancer`\n- [kaggle seversteal steel detection](https://github.com/bamps53/kaggle-severstal) - 5th place\n- [rsna intracranial hemorrhage detection](https://github.com/ngxbac/kaggle-rsna) - 5th place\n- [aptos 2019 blindness detection](https://github.com/bloodaxe/kaggle-2019-blindness-detection) \u2013 7th place\n- [catalyst.rl - neurips 2019: learn to move - walk around](https://github.com/scitator/run-skeleton-run-in-3d) \u2013 2nd place\n- [xview2 damage assessment challenge](https://github.com/bloodaxe/xview2-solution) - 3rd place\n\n\n</p>\n</details>\n\n<details>\n<summary>toolkits</summary>\n<p>\n\n- [catalyst.rl](https://github.com/scitator/catalyst-rl-framework) \u2013 a distributed framework for reproducible rl research by [scitator](https://github.com/scitator)\n- [catalyst.classification](https://github.com/catalyst-team/classification) - comprehensive classification pipeline with pseudo-labeling by [bagxi](https://github.com/bagxi) and [pdanilov](https://github.com/pdanilov)\n- [catalyst.segmentation](https://github.com/catalyst-team/segmentation) - segmentation pipelines - binary, semantic and instance, by [bagxi](https://github.com/bagxi)\n- [catalyst.detection](https://github.com/catalyst-team/detection) - anchor-free detection pipeline by [avi2011class](https://github.com/avi2011class) and [tezromach](https://github.com/tezromach)\n- [catalyst.gan](https://github.com/catalyst-team/gan) - reproducible gans pipelines by [asmekal](https://github.com/asmekal)\n- [catalyst.neuro](https://github.com/catalyst-team/neuro) - brain image analysis project, in collaboration with [trends center](https://trendscenter.org)\n- [mlcomp](https://github.com/catalyst-team/mlcomp) \u2013 distributed dag framework for machine learning with ui by [lightforever](https://github.com/lightforever)\n- [pytorch toolbelt](https://github.com/bloodaxe/pytorch-toolbelt) - pytorch extensions for fast r&d prototyping and kaggle farming by [bloodaxe](https://github.com/bloodaxe)\n- [helper functions](https://github.com/ternaus/iglovikov_helper_functions) - an assorted collection of helper functions by [ternaus](https://github.com/ternaus)\n- [bert distillation with catalyst](https://github.com/elephantmipt/bert-distillation) by [elephantmipt](https://github.com/elephantmipt)\n\n</p>\n</details>\n\n\n<details>\n<summary>other</summary>\n<p>\n\n- [camvid segmentation example](https://github.com/bloodaxe/catalyst-camvid-segmentation-example) - example of semantic segmentation for camvid dataset\n- [notebook api tutorial for segmentation in understanding clouds from satellite images competition](https://www.kaggle.com/artgor/segmentation-in-pytorch-using-convenient-tools/)\n- [catalyst.rl - neurips 2019: learn to move - walk around](https://github.com/scitator/learning-to-move-starter-kit) \u2013 starter kit\n- [catalyst.rl - neurips 2019: animal-ai olympics](https://github.com/scitator/animal-olympics-starter-kit) - starter kit\n- [inria segmentation example](https://github.com/bloodaxe/catalyst-inria-segmentation-example) - an example of training segmentation model for inria sattelite segmentation challenge\n- [iglovikov_segmentation](https://github.com/ternaus/iglovikov_segmentation) - semantic segmentation pipeline using catalyst\n- [logging catalyst runs to comet](https://colab.research.google.com/drive/1tag27hcmh2jyrkbgsqrxligufshvycq6?usp=sharing) - an example of how to log metrics, hyperparameters and more from catalyst runs to [comet](https://www.comet.ml/site/data-scientists/)\n\n</p>\n</details>\n\n\nsee other projects at [the github dependency graph](https://github.com/catalyst-team/catalyst/network/dependents).\n\nif your project implements a paper,\na notable use-case/tutorial, or a kaggle competition solution, or\nif your code simply presents interesting results and uses catalyst,\nwe would be happy to add your project to the list above!\ndo not hesitate to send us a pr with a brief description of the project similar to the above.\n\n### contribution guide\n\nwe appreciate all contributions.\nif you are planning to contribute back bug-fixes, there is no need to run that by us; just send a pr.\nif you plan to contribute new features, new utility functions, or extensions,\nplease open an issue first and discuss it with us.\n\n- please see the [contribution guide](contributing.md) for more information.\n- by participating in this project, you agree to abide by its [code of conduct](code_of_conduct.md).\n\n\n### user feedback\n\nwe've created `feedback@catalyst-team.com` as an additional channel for user feedback.\n\n- if you like the project and want to thank us, this is the right place.\n- if you would like to start a collaboration between your team and catalyst team to improve deep learning r&d, you are always welcome.\n- if you don't like github issues and prefer email, feel free to email us.\n- finally, if you do not like something, please, share it with us, and we can see how to improve it.\n\nwe appreciate any type of feedback. thank you!\n\n\n### acknowledgments\n\nsince the beginning of the \u0441atalyst development, a lot of people have influenced it in a lot of different ways.\n\n#### catalyst.team\n- [dmytro doroshenko](https://www.linkedin.com/in/dmytro-doroshenko-05671112a/) ([ditwoo](https://github.com/ditwoo))\n- [eugene kachan](https://www.linkedin.com/in/yauheni-kachan/) ([bagxi](https://github.com/bagxi))\n- [nikita balagansky](https://www.linkedin.com/in/nikita-balagansky-50414a19a/) ([elephantmipt](https://github.com/elephantmipt))\n- [sergey kolesnikov](https://www.scitator.com/) ([scitator](https://github.com/scitator))\n\n#### catalyst.contributors\n- [aleksey grinchuk](https://www.facebook.com/grinchuk.alexey) ([alexgrinch](https://github.com/alexgrinch))\n- [aleksey shabanov](https://linkedin.com/in/aleksey-shabanov-96b351189) ([alekseysh](https://github.com/alekseysh))\n- [alex gaziev](https://www.linkedin.com/in/alexgaziev/) ([gazay](https://github.com/gazay))\n- [andrey zharkov](https://www.linkedin.com/in/andrey-zharkov-8554a1153/) ([asmekal](https://github.com/asmekal))\n- [artem zolkin](https://www.linkedin.com/in/artem-zolkin-b5155571/) ([arquestro](https://github.com/arquestro))\n- [david kuryakin](https://www.linkedin.com/in/dkuryakin/) ([dkuryakin](https://github.com/dkuryakin))\n- [evgeny semyonov](https://www.linkedin.com/in/ewan-semyonov/) ([lightforever](https://github.com/lightforever))\n- [eugene khvedchenya](https://www.linkedin.com/in/cvtalks/) ([bloodaxe](https://github.com/bloodaxe))\n- [ivan stepanenko](https://www.facebook.com/istepanenko)\n- [julia shenshina](https://github.com/julia-shenshina) ([julia-shenshina](https://github.com/julia-shenshina))\n- [nguyen xuan bac](https://www.linkedin.com/in/bac-nguyen-xuan-70340b66/) ([ngxbac](https://github.com/ngxbac))\n- [roman tezikov](http://linkedin.com/in/roman-tezikov/) ([tezromach](https://github.com/tezromach))\n- [valentin khrulkov](https://www.linkedin.com/in/vkhrulkov/) ([khrulkovv](https://github.com/khrulkovv))\n- [vladimir iglovikov](https://www.linkedin.com/in/iglovikov/) ([ternaus](https://github.com/ternaus))\n- [vsevolod poletaev](https://linkedin.com/in/vsevolod-poletaev-468071165) ([hexfaker](https://github.com/hexfaker))\n- [yury kashnitsky](https://www.linkedin.com/in/kashnitskiy/) ([yorko](https://github.com/yorko))\n\n\n### trusted by\n- [awecom](https://www.awecom.com)\n- researchers at the [center for translational research in neuroimaging and data science (trends)](https://trendscenter.org)\n- [deep learning school](https://en.dlschool.org)\n- researchers at [emory university](https://www.emory.edu)\n- [evil martians](https://evilmartians.com)\n- researchers at the [georgia institute of technology](https://www.gatech.edu)\n- researchers at [georgia state university](https://www.gsu.edu)\n- [helios](http://helios.to)\n- [hpcd lab](https://www.hpcdlab.com)\n- [ifarm](https://ifarmproject.com)\n- [kinoplan](http://kinoplan.io/)\n- researchers at the [moscow institute of physics and technology](https://mipt.ru/english/)\n- [neuromation](https://neuromation.io)\n- [poteha labs](https://potehalabs.com/en/)\n- [provectus](https://provectus.com)\n- researchers at the [skolkovo institute of science and technology](https://www.skoltech.ru/en)\n- [softconstruct](https://www.softconstruct.io/)\n- researchers at [tinkoff](https://www.tinkoff.ru/eng/)\n- researchers at [yandex.research](https://research.yandex.com)\n\n\n### citation\n\nplease use this bibtex if you want to cite this repository in your publications:\n\n    @misc{catalyst,\n        author = {kolesnikov, sergey},\n        title = {catalyst - accelerated deep learning r&d},\n        year = {2018},\n        publisher = {github},\n        journal = {github repository},\n        howpublished = {\\url{https://github.com/catalyst-team/catalyst}},\n    }\n\n\n",
  "docs_url": null,
  "keywords": "machine learning,distributed computing,deep learning,reinforcement learning,computer vision,natural language processing,recommendation systems,information retrieval,pytorch",
  "license": "apache license 2.0",
  "name": "catalyst",
  "package_url": "https://pypi.org/project/catalyst/",
  "project_url": "https://pypi.org/project/catalyst/",
  "project_urls": {
    "Bug Tracker": "https://github.com/catalyst-team/catalyst/issues",
    "Documentation": "https://catalyst-team.github.io/catalyst",
    "Download": "https://github.com/catalyst-team/catalyst",
    "Homepage": "https://github.com/catalyst-team/catalyst",
    "Source Code": "https://github.com/catalyst-team/catalyst"
  },
  "release_url": "https://pypi.org/project/catalyst/22.4/",
  "requires_dist": [
    "numpy (>=1.18)",
    "torch (>=1.4.0)",
    "accelerate (>=0.5.1)",
    "hydra-slayer (>=0.4.0)",
    "tqdm (>=4.33.0)",
    "tensorboardX (>=2.1.0)",
    "imageio (>=2.5.0) ; extra == 'all'",
    "opencv-python-headless (>=4.2.0.32) ; extra == 'all'",
    "scikit-image (<0.19.0>=0.16.1) ; extra == 'all'",
    "torchvision (>=0.5.0) ; extra == 'all'",
    "Pillow (>=6.1) ; extra == 'all'",
    "requests ; extra == 'all'",
    "scipy (>=1.4.1) ; extra == 'all'",
    "matplotlib (>=3.1.0) ; extra == 'all'",
    "pandas (>=1.0.0) ; extra == 'all'",
    "scikit-learn (>=1.0) ; extra == 'all'",
    "optuna (>=2.0.0) ; extra == 'all'",
    "comet-ml ; extra == 'comet'",
    "imageio (>=2.5.0) ; extra == 'cv'",
    "opencv-python-headless (>=4.2.0.32) ; extra == 'cv'",
    "scikit-image (<0.19.0>=0.16.1) ; extra == 'cv'",
    "torchvision (>=0.5.0) ; extra == 'cv'",
    "Pillow (>=6.1) ; extra == 'cv'",
    "requests ; extra == 'cv'",
    "deepspeed (>=0.4.0) ; extra == 'deepspeed'",
    "pytest ; extra == 'dev'",
    "sphinx (==2.2.1) ; extra == 'dev'",
    "Jinja2 (<=3.0.3) ; extra == 'dev'",
    "docutils (==0.17.1) ; extra == 'dev'",
    "mock (==3.0.5) ; extra == 'dev'",
    "catalyst-codestyle (==21.09.2) ; extra == 'dev'",
    "black (==21.8b0) ; extra == 'dev'",
    "click (<=8.0.4) ; extra == 'dev'",
    "catalyst-sphinx-theme (==1.2.0) ; extra == 'dev'",
    "tomlkit (==0.7.2) ; extra == 'dev'",
    "pre-commit (==2.13.0) ; extra == 'dev'",
    "path ; extra == 'dev'",
    "scipy (>=1.4.1) ; extra == 'ml'",
    "matplotlib (>=3.1.0) ; extra == 'ml'",
    "pandas (>=1.0.0) ; extra == 'ml'",
    "scikit-learn (>=1.0) ; extra == 'ml'",
    "mlflow ; extra == 'mlflow'",
    "neptune-client (>=0.9.8) ; extra == 'neptune'",
    "onnx ; extra == 'onnx'",
    "onnxruntime ; extra == 'onnx'",
    "onnx ; extra == 'onnx-gpu'",
    "onnxruntime-gpu ; extra == 'onnx-gpu'",
    "optuna (>=2.0.0) ; extra == 'optuna'",
    "torch-tb-profiler ; extra == 'profiler'",
    "wandb ; extra == 'wandb'"
  ],
  "requires_python": ">=3.7.0",
  "summary": "catalyst. accelerated deep learning r&d with pytorch.",
  "version": "22.4",
  "releases": [],
  "developers": [
    "scitator@gmail.com",
    "sergey_kolesnikov"
  ],
  "kwds": "catalyst catalyst_logo catalyst_team catalystteam supervisedrunner",
  "license_kwds": "apache license 2.0",
  "libtype": "pypi",
  "id": "pypi_catalyst",
  "homepage": "https://github.com/catalyst-team/catalyst",
  "release_count": 104,
  "dependency_ids": [
    "pypi_accelerate",
    "pypi_black",
    "pypi_catalyst_codestyle",
    "pypi_catalyst_sphinx_theme",
    "pypi_click",
    "pypi_comet_ml",
    "pypi_deepspeed",
    "pypi_docutils",
    "pypi_hydra_slayer",
    "pypi_imageio",
    "pypi_jinja2",
    "pypi_matplotlib",
    "pypi_mlflow",
    "pypi_mock",
    "pypi_neptune_client",
    "pypi_numpy",
    "pypi_onnx",
    "pypi_onnxruntime",
    "pypi_onnxruntime_gpu",
    "pypi_opencv_python_headless",
    "pypi_optuna",
    "pypi_pandas",
    "pypi_path",
    "pypi_pillow",
    "pypi_pre_commit",
    "pypi_pytest",
    "pypi_requests",
    "pypi_scikit_image",
    "pypi_scikit_learn",
    "pypi_scipy",
    "pypi_sphinx",
    "pypi_tensorboardx",
    "pypi_tomlkit",
    "pypi_torch",
    "pypi_torch_tb_profiler",
    "pypi_torchvision",
    "pypi_tqdm",
    "pypi_wandb"
  ],
  "documentation_summary": "The GitHub repository \"catalyst-team/catalyst\" is focused on accelerating deep learning research and development. It provides a PyTorch framework designed for reproducibility, rapid experimentation, and codebase reuse, aiming to streamline the creation of new deep learning models by eliminating the need to write repetitive training loops. The project emphasizes ease of use and efficiency, offering a variety of features such as metrics, early-stopping, model checkpointing, and more, with just a few lines of code. It supports Python 3.7+ and PyTorch 1.4+, and is tested across multiple platforms. The repository includes comprehensive documentation, examples, and a guide for getting started, making it a valuable tool for deep learning practitioners looking to enhance their R&D workflows.",
  "embedding": [
    -0.022896479815244675,
    -0.005692455917596817,
    0.00010098348138853908,
    -0.025908062234520912,
    0.011940788477659225,
    0.026302101090550423,
    0.004911414347589016,
    -0.017900627106428146,
    -0.007240466307848692,
    -0.022052109241485596,
    0.026653921231627464,
    0.01899830810725689,
    -0.015451956540346146,
    0.009273989126086235,
    0.000959590426646173,
    -0.005382854025810957,
    0.004556075669825077,
    -0.04624328762292862,
    -0.010807925835251808,
    0.01074459869414568,
    0.004534966312348843,
    1.159564703812066e-06,
    -0.01819615624845028,
    -0.011406021192669868,
    -0.018674632534384727,
    0.00423943717032671,
    0.0070328922010958195,
    -0.02642875723540783,
    0.01601487025618553,
    -0.0006640612264163792,
    0.028244150802493095,
    -0.010955690406262875,
    -0.007166584022343159,
    -0.03087576851248741,
    0.004992333240807056,
    -0.01645112782716751,
    0.008816622197628021,
    0.0029535330832004547,
    0.011518603190779686,
    0.012841449119150639,
    0.036139003932476044,
    0.017816191539168358,
    -0.006336287595331669,
    -0.021714361384510994,
    -0.013376215472817421,
    -0.005949284881353378,
    -0.015170500613749027,
    -0.035266488790512085,
    -0.001949085621163249,
    0.03188901022076607,
    -0.002103886567056179,
    0.020715191960334778,
    -0.01463573332875967,
    -0.017084404826164246,
    0.0011398984352126718,
    -0.006058349274098873,
    -0.004876232240349054,
    0.04618699848651886,
    0.012644428759813309,
    0.003287762636318803,
    -0.008218526840209961,
    0.02811749465763569,
    -0.03298668935894966,
    0.012637392617762089,
    0.0026351355481892824,
    -0.017970992252230644,
    -0.0034742276184260845,
    0.047200240194797516,
    0.01008317619562149,
    0.020771482959389687,
    0.03535092622041702,
    0.01656370982527733,
    0.025654751807451248,
    -0.011771914549171925,
    0.012229280546307564,
    -0.02852560766041279,
    -0.01602894254028797,
    0.009639882482588291,
    0.0022358193527907133,
    0.0010633774800226092,
    0.013052540831267834,
    -0.011391947977244854,
    0.010723489336669445,
    0.02310757152736187,
    -0.005703010596334934,
    0.01604301482439041,
    -0.02479630894958973,
    -0.01342547032982111,
    -0.00786670669913292,
    -0.0033387767616659403,
    0.02481038309633732,
    0.016605928540229797,
    0.03599827364087105,
    0.036589331924915314,
    -0.002179528120905161,
    0.005133061204105616,
    -0.015930432826280594,
    0.018702778965234756,
    -0.009907265193760395,
    -0.046074412763118744,
    0.0014917189255356789,
    0.008218526840209961,
    -0.015606758184731007,
    -0.018069501966238022,
    -0.03248006850481033,
    -0.025429585948586464,
    -0.025654751807451248,
    0.006255368702113628,
    0.021362541243433952,
    0.03141053393483162,
    -0.014044675044715405,
    0.04469527676701546,
    -0.001333399792201817,
    -0.03141053393483162,
    0.0013474725419655442,
    -0.0024240433704108,
    0.02725905366241932,
    -0.00047891566646285355,
    0.014762388542294502,
    -0.009970593266189098,
    0.02474001795053482,
    0.020912211388349533,
    0.016000796109437943,
    -0.012468518689274788,
    0.007979289628565311,
    -0.004925487097352743,
    -0.004401274491101503,
    0.008795512840151787,
    0.0010396295692771673,
    -0.020124133676290512,
    -0.0030502837616950274,
    -0.0007687277975492179,
    0.0040424177423119545,
    0.018674632534384727,
    -0.014677952043712139,
    0.014051711186766624,
    -0.0035340371541678905,
    -0.027484217658638954,
    -0.013531017117202282,
    0.0007265093736350536,
    -0.01074459869414568,
    0.006631816737353802,
    -0.034900594502687454,
    0.0002942098944913596,
    -0.03726482763886452,
    0.026259882375597954,
    -0.006930863950401545,
    0.008239636197686195,
    0.02065890096127987,
    -0.011349729262292385,
    0.009105115197598934,
    -0.010955690406262875,
    -0.006093531381338835,
    0.01176487747579813,
    0.008211490698158741,
    0.0009024196187965572,
    0.014832752756774426,
    -0.0007608118467032909,
    -0.02595028094947338,
    0.013833582401275635,
    0.028990009799599648,
    0.020321153104305267,
    -0.013742108829319477,
    -0.03098835051059723,
    0.00866182055324316,
    0.030200272798538208,
    0.0338873527944088,
    -0.01342547032982111,
    -0.02559846080839634,
    -0.0029201102443039417,
    0.0013712204527109861,
    0.013671745546162128,
    -0.0028321552090346813,
    0.004278137348592281,
    0.004443493206053972,
    -0.03706780821084976,
    0.02058853581547737,
    0.013080686330795288,
    -0.019504928961396217,
    0.022840186953544617,
    0.013573234900832176,
    -0.019504928961396217,
    0.010047993622720242,
    0.02475409209728241,
    0.0016007833182811737,
    -0.020349297672510147,
    0.012433337047696114,
    -0.001104716444388032,
    0.008521093055605888,
    0.017942845821380615,
    0.012461482547223568,
    0.04874825105071068,
    -0.0010624979622662067,
    -0.005270271096378565,
    -0.6119987964630127,
    -0.021249959245324135,
    -0.014917189255356789,
    -0.030341001227498055,
    -0.016605928540229797,
    -0.013038468547165394,
    -0.00032433454180136323,
    0.003166384529322386,
    -0.012531846761703491,
    0.022544657811522484,
    -0.021770652383565903,
    0.005861329846084118,
    -0.026288028806447983,
    -0.0125881377607584,
    -0.02268538624048233,
    -0.025809552520513535,
    -0.026682067662477493,
    -0.019955258816480637,
    -0.0268087238073349,
    0.013312888331711292,
    -0.03425324335694313,
    0.011680440977215767,
    -0.02274167723953724,
    0.006160377059131861,
    -0.007796342484652996,
    0.034450262784957886,
    0.028216004371643066,
    0.012046334333717823,
    0.019617510959506035,
    -0.0047636497765779495,
    -0.025809552520513535,
    0.019519001245498657,
    -0.0007770835654810071,
    -0.01280626654624939,
    0.038221780210733414,
    0.0010501842480152845,
    0.008605529554188251,
    0.03582940250635147,
    0.008725148625671864,
    0.057107504457235336,
    -0.006276478059589863,
    0.01032241340726614,
    0.020546317100524902,
    0.01739400625228882,
    -0.017436224967241287,
    0.005928175523877144,
    0.013376215472817421,
    0.002519034780561924,
    -0.019293837249279022,
    -0.0065157157368958,
    -0.006237777881324291,
    -0.0027864184230566025,
    -0.009013641625642776,
    -0.02633024752140045,
    -0.00011895930947503075,
    -0.023248299956321716,
    0.029355904087424278,
    -0.030706893652677536,
    0.0007366241770796478,
    0.0028866874054074287,
    -0.020475953817367554,
    0.02022264339029789,
    -0.007268611807376146,
    -0.031551264226436615,
    -0.013995420187711716,
    0.00178021180909127,
    -0.018660560250282288,
    0.010378705337643623,
    -0.012552956119179726,
    0.006371469236910343,
    0.027526436373591423,
    -0.002872614422813058,
    0.004130372777581215,
    -0.0074656312353909016,
    0.03301483765244484,
    0.029046300798654556,
    0.025260712951421738,
    0.010209831409156322,
    0.0018980716122314334,
    0.029384048655629158,
    0.009949483908712864,
    -0.00022043753415346146,
    0.005833183880895376,
    -0.027385707944631577,
    0.04261250048875809,
    0.021221812814474106,
    -0.0032754489220678806,
    -0.0038770621176809072,
    0.020729264244437218,
    -0.0006732965121045709,
    -0.0003296118520665914,
    0.010843108408153057,
    -0.021179594099521637,
    -0.04005124792456627,
    0.006895681843161583,
    0.03011583536863327,
    -0.0054250722751021385,
    -0.005206943489611149,
    0.007937070913612843,
    -0.011560821905732155,
    -0.017422150820493698,
    -0.01649334467947483,
    0.022403929382562637,
    0.007578213699162006,
    0.019730094820261,
    0.013960237614810467,
    -0.044751569628715515,
    -0.014509078115224838,
    0.02354382909834385,
    -0.03886913135647774,
    -0.04241548106074333,
    -0.025682898238301277,
    -0.0036870790645480156,
    -0.0011495734797790647,
    0.008704039268195629,
    -0.03096020594239235,
    -0.0028655780479311943,
    -0.0044153472408652306,
    0.006691626273095608,
    -0.036927081644535065,
    -0.0017221614252775908,
    -0.005537654738873243,
    0.001344833872281015,
    0.007965216413140297,
    0.01649334467947483,
    0.02056039124727249,
    0.006139267701655626,
    -0.024528926238417625,
    -0.010259086266160011,
    0.019096817821264267,
    0.03560423478484154,
    0.005783929023891687,
    0.015817850828170776,
    -0.015212719328701496,
    0.03588569164276123,
    0.020715191960334778,
    0.031241660937666893,
    -0.019476784393191338,
    -0.001841780380345881,
    -0.024866674095392227,
    -0.012158917263150215,
    -0.0177598986774683,
    0.021123303100466728,
    0.0040142722427845,
    -0.014290949329733849,
    0.011546749621629715,
    -0.026611704379320145,
    0.009034750983119011,
    -0.005474327132105827,
    0.006367951165884733,
    -0.012848485261201859,
    0.013108831830322742,
    -0.01944863796234131,
    0.021742507815361023,
    0.017605097964406013,
    -0.023318663239479065,
    -0.033634040504693985,
    -0.026499120518565178,
    -0.015536393970251083,
    -0.031523119658231735,
    -0.010329450480639935,
    0.023262372240424156,
    -0.003393308725208044,
    0.011659331619739532,
    -0.039769791066646576,
    0.012461482547223568,
    -0.04328799620270729,
    0.02597842738032341,
    -0.016620000824332237,
    -0.053870756179094315,
    0.001150453113950789,
    -0.031241660937666893,
    -0.012827375903725624,
    0.012348899617791176,
    0.018421322107315063,
    0.005931694060564041,
    -0.0180976465344429,
    -0.007521922700107098,
    -0.010125393979251385,
    0.0010466660605743527,
    -0.013573234900832176,
    0.017492515966296196,
    -0.01359434425830841,
    -0.01175080519169569,
    0.02724497951567173,
    0.02932775765657425,
    0.011349729262292385,
    0.008612565696239471,
    -0.017225131392478943,
    0.02431783452630043,
    -0.025795480236411095,
    0.02716054394841194,
    0.003993162885308266,
    0.02633024752140045,
    -0.01215188018977642,
    0.0029552923515439034,
    0.011518603190779686,
    -0.008950313553214073,
    -0.01446685940027237,
    0.03751813992857933,
    0.013432507403194904,
    0.025274785235524178,
    0.0242896880954504,
    -0.026991669088602066,
    -0.024416344240307808,
    -0.004566630348563194,
    0.016183743253350258,
    0.00172480009496212,
    0.005164725240319967,
    -0.016788875684142113,
    0.02063075453042984,
    -0.005952802952378988,
    -0.02977808751165867,
    -0.015339374542236328,
    -0.010969763621687889,
    0.010294267907738686,
    0.00825370941311121,
    0.01943456567823887,
    -0.02597842738032341,
    -0.004478675313293934,
    0.006420724559575319,
    0.011011982336640358,
    0.011441202834248543,
    -0.014354276470839977,
    0.007894852198660374,
    0.01768953539431095,
    -0.00273012719117105,
    0.01446685940027237,
    0.009210661053657532,
    -0.012433337047696114,
    0.004081117920577526,
    0.030735040083527565,
    0.0101394671946764,
    -0.00868996698409319,
    0.01609930582344532,
    -0.005924657452851534,
    0.011912642978131771,
    -0.029890671372413635,
    0.02597842738032341,
    0.0027741047088056803,
    0.007240466307848692,
    -0.020095987245440483,
    0.013242524117231369,
    0.011504530906677246,
    0.001436307211406529,
    0.02765309251844883,
    0.04373832419514656,
    0.015508248470723629,
    -0.009858010336756706,
    0.003481263993307948,
    -0.0028673370834439993,
    0.022544657811522484,
    0.0021584187634289265,
    0.011476385407149792,
    0.011497494764626026,
    0.006170931737869978,
    0.0048234593123197556,
    0.02188323624432087,
    0.01612745225429535,
    0.011223074048757553,
    0.009752464480698109,
    0.0107516348361969,
    0.02345939166843891,
    0.01237000897526741,
    0.013326960615813732,
    -0.0017274387646466494,
    0.006719771772623062,
    -0.027343491092324257,
    0.0017397524788975716,
    -0.015508248470723629,
    -0.018773142248392105,
    -0.0021091639064252377,
    0.008908095769584179,
    -0.04610256105661392,
    0.011631186120212078,
    0.01359434425830841,
    0.001350990729406476,
    0.036082711070775986,
    0.009759500622749329,
    0.026217663660645485,
    -0.0022164692636579275,
    -0.03087576851248741,
    0.0251762755215168,
    0.02929961308836937,
    0.0009419994312338531,
    -0.002195359906181693,
    -0.038643963634967804,
    0.015775632113218307,
    -0.015775632113218307,
    0.025035547092556953,
    0.016422981396317482,
    0.004974741954356432,
    -0.020461881533265114,
    0.011159746907651424,
    -0.017422150820493698,
    -0.015902286395430565,
    0.0276953112334013,
    -0.028342660516500473,
    -0.009358425624668598,
    -0.025711042806506157,
    0.010772744193673134,
    -0.014804607257246971,
    0.006726808380335569,
    0.0012093830155208707,
    0.051787979900836945,
    -0.003855952760204673,
    -0.005090842954814434,
    -0.011201964691281319,
    -0.013819510117173195,
    -0.025837698951363564,
    0.0027248498518019915,
    0.006002057809382677,
    -0.0007383832708001137,
    -0.03667376935482025,
    -0.01014650333672762,
    -0.01894201710820198,
    0.013348069973289967,
    0.008528129197657108,
    0.029412195086479187,
    -0.0007185934227891266,
    -0.0014081615954637527,
    -0.007303793914616108,
    -0.0007313468959182501,
    -0.005699492059648037,
    0.03847509250044823,
    0.04964891076087952,
    -0.013882837258279324,
    -0.0005536775570362806,
    -0.021995818242430687,
    0.012285572476685047,
    -0.027019815519452095,
    -0.024219324812293053,
    0.009231770411133766,
    -0.024641508236527443,
    -0.007944107055664062,
    0.001049304730258882,
    -0.004063527099788189,
    0.006220186594873667,
    -0.0026967041194438934,
    0.010765708051621914,
    0.00454200292006135,
    0.02309349924325943,
    -0.01815393753349781,
    -0.016254108399152756,
    0.0242896880954504,
    -0.02521849423646927,
    0.02400823123753071,
    -0.024669654667377472,
    0.001980749424546957,
    0.0017740549519658089,
    0.028567826375365257,
    0.0017485478892922401,
    0.0005778651684522629,
    -0.009977629408240318,
    -0.022530585527420044,
    0.022150618955492973,
    -0.010575724765658379,
    0.003085465868934989,
    -0.02267131395637989,
    0.019223472103476524,
    0.012876630760729313,
    0.023726776242256165,
    -0.009597663767635822,
    0.008725148625671864,
    0.009998738765716553,
    0.006241295952349901,
    0.005572836846113205,
    0.009196587838232517,
    0.0015330578899011016,
    -0.007451558485627174,
    0.012679611332714558,
    0.020110059529542923,
    -0.027864184230566025,
    -0.024191178381443024,
    0.011286402121186256,
    0.03464728221297264,
    -0.03681449964642525,
    -0.007039928343147039,
    0.011209001764655113,
    0.010547579266130924,
    0.025725115090608597,
    -0.014523150399327278,
    0.03349331393837929,
    -0.03495688736438751,
    -0.013868764974176884,
    -0.028328588232398033,
    0.0037328156176954508,
    -0.0039227986708283424,
    0.00904882326722145,
    0.028694480657577515,
    0.00211795954965055,
    -0.01384765561670065,
    -0.03149497136473656,
    -0.0072826845571398735,
    -0.011251219548285007,
    -0.03754628449678421,
    -0.009499154053628445,
    0.015114209614694118,
    0.017928773537278175,
    0.004404793027788401,
    0.0017714162822812796,
    -0.004098708741366863,
    -0.02474001795053482,
    0.006547379773110151,
    -0.022023964673280716,
    0.009013641625642776,
    -0.008619602769613266,
    -0.04874825105071068,
    0.0107516348361969,
    -0.028272297233343124,
    0.0006222825031727552,
    -0.03419695422053337,
    0.0021848054602742195,
    0.03622344136238098,
    0.0005844618426635861,
    -0.01813986524939537,
    0.006575525272637606,
    0.0030362110119313,
    -0.0028884464409202337,
    0.01362952683120966,
    0.020461881533265114,
    0.0013923296937718987,
    0.0003524801868479699,
    -0.012264463119208813,
    0.008978459052741528,
    -0.022826114669442177,
    0.0005787447444163263,
    -0.00032433454180136323,
    -0.00868996698409319,
    0.018477613106369972,
    0.0013677022652700543,
    0.022812042385339737,
    -0.0045807030983269215,
    -0.0023519201204180717,
    0.03473171964287758,
    -0.009414716623723507,
    0.010280195623636246,
    0.0026175444945693016,
    0.021672142669558525,
    -0.01096272747963667,
    0.0035832920111715794,
    0.028638189658522606,
    -2.2854590497445315e-05,
    -0.00720528420060873,
    -0.002285074209794402,
    -0.016591856256127357,
    0.023614192381501198,
    0.026569485664367676,
    -0.0165355633944273,
    0.04984593018889427,
    -0.006990673486143351,
    -0.030003253370523453,
    -0.026667995378375053,
    -0.005294898524880409,
    0.0009156129090115428,
    0.025091838091611862,
    -0.022544657811522484,
    -0.030256563797593117,
    -0.015198646113276482,
    -0.011173819191753864,
    -0.015423811040818691,
    -0.0030608384404331446,
    -0.0038735438138246536,
    -0.028722627088427544,
    -0.01196189783513546,
    0.012243353761732578,
    -0.027850111946463585,
    0.012426300905644894,
    0.008767367340624332,
    -0.03090391308069229,
    -0.012728866189718246,
    -0.001511948648840189,
    0.0136435991153121,
    0.02261502295732498,
    -0.0004965066909790039,
    0.01692960225045681,
    -0.01771768182516098,
    -0.018322812393307686,
    -0.028666336089372635,
    0.013291778974235058,
    0.024866674095392227,
    -0.003729297313839197,
    0.03549165278673172,
    0.019195327535271645,
    0.03186086565256119,
    -0.009175478480756283,
    0.010716453194618225,
    0.002789936726912856,
    -0.00074893789133057,
    -0.01940641924738884,
    -0.008668857626616955,
    0.029834378510713577,
    -0.012890703976154327,
    -0.022910552099347115,
    0.011497494764626026,
    -0.00371522456407547,
    -0.0007062796503305435,
    -0.027568655088543892,
    -0.006846426986157894,
    0.018336884677410126,
    0.0006930864183232188,
    -0.0036307875998318195,
    -0.0017195227555930614,
    -0.04083932563662529,
    -0.04078303277492523,
    0.017956918105483055,
    -0.0016843406483530998,
    0.00992133840918541,
    -0.04573666676878929,
    -0.005523581989109516,
    0.03630787506699562,
    0.0022622058168053627,
    0.02634431980550289,
    0.0074023036286234856,
    -5.678602974512614e-05,
    0.017661388963460922,
    -0.013742108829319477,
    0.009949483908712864,
    0.02227727510035038,
    -0.01902645267546177,
    -0.010449069552123547,
    -0.02599249966442585,
    -0.03380291536450386,
    0.005006405990570784,
    0.014959407970309258,
    0.028272297233343124,
    0.023220153525471687,
    0.0034320091363042593,
    -0.0031118523329496384,
    0.032367486506700516,
    0.018421322107315063,
    0.00319101195782423,
    0.014129111543297768,
    -0.013319924473762512,
    0.017267350107431412,
    -0.010216867551207542,
    -0.043485015630722046,
    -0.0054285903461277485,
    -0.01093458104878664,
    0.00462995795533061,
    -0.018843507394194603,
    0.012215208262205124,
    -0.034422118216753006,
    -0.0037504066713154316,
    0.03301483765244484,
    0.01235593669116497,
    0.025049621239304543,
    -0.00037183030508458614,
    0.01092754490673542,
    0.03461913764476776,
    0.008668857626616955,
    0.0070364102721214294,
    -0.01297514047473669,
    -0.00515065249055624,
    -0.0011891532922163606,
    0.045117463916540146,
    0.013059576973319054,
    -0.019519001245498657,
    -0.025767333805561066,
    0.0016825816128402948,
    -0.004193700384348631,
    -0.012749975547194481,
    -0.019575294107198715,
    0.0048234593123197556,
    0.002563012531027198,
    0.017576953396201134,
    -0.031100932508707047,
    -0.015916360542178154,
    -0.016380762681365013,
    -0.005892993416637182,
    -0.0036694880109280348,
    0.001949085621163249,
    0.0032824852969497442,
    -0.021686216816306114,
    -0.014790534041821957,
    0.030763184651732445,
    0.0036413422785699368,
    0.039460189640522,
    0.0072756484150886536,
    -0.008120017126202583,
    -0.004207773134112358,
    -0.003287762636318803,
    -0.025654751807451248,
    -0.005889475345611572,
    -0.009189551696181297,
    0.05533432960510254,
    -0.0215032696723938,
    0.002133791334927082,
    0.015944505110383034,
    0.017942845821380615,
    -0.015972651541233063,
    -0.01815393753349781,
    0.0015902287559583783,
    0.03988237306475639,
    -0.0136435991153121,
    -0.020757410675287247,
    -0.00515417056158185,
    0.009794683195650578,
    -0.00721232034265995,
    -0.04497673362493515,
    -0.01773175410926342,
    -0.014987553469836712,
    -0.014762388542294502,
    0.008999568410217762,
    0.00299575156532228,
    -0.02352975495159626,
    -0.02018042467534542,
    0.02770938351750374,
    0.012081515975296497,
    0.00904882326722145,
    -0.02018042467534542,
    -0.012897740118205547,
    0.016211889684200287,
    -0.03259265050292015,
    -0.017140695825219154,
    -0.004520893562585115,
    -0.004534966312348843,
    -0.008014471270143986,
    -0.0009173720027320087,
    0.008204454556107521,
    -0.016380762681365013,
    0.033268146216869354,
    -0.02966550551354885,
    0.01567712239921093,
    0.01729549653828144,
    0.006420724559575319,
    -0.018871651962399483,
    0.036054566502571106,
    -0.03673006221652031,
    0.01258110161870718,
    0.039797935634851456,
    -0.009245842695236206,
    -0.04421680048108101,
    -0.03684264421463013,
    0.020954430103302002,
    0.04261250048875809,
    0.010259086266160011,
    -0.020869992673397064,
    0.023192008957266808,
    0.01157489512115717,
    0.006695144344121218,
    0.016802947968244553,
    -0.02016635239124298,
    0.01859019510447979,
    -0.015522320754826069,
    0.0181680116802454,
    0.014361313544213772,
    -0.013777291402220726,
    0.016197815537452698,
    -0.022023964673280716,
    -0.013509907759726048,
    0.004974741954356432,
    -0.041796278208494186,
    -0.014818679541349411,
    -0.005541173275560141,
    0.005326562561094761,
    -0.007578213699162006,
    0.020152278244495392,
    -0.019575294107198715,
    -0.03906615078449249,
    -0.023698629811406136,
    0.04458269476890564,
    0.0022692421916872263,
    -0.015944505110383034,
    0.01342547032982111,
    -0.012320754118263721,
    -0.00847887434065342,
    -5.284179223963292e-06,
    -0.001887517049908638,
    -0.00782448798418045,
    -0.017056258395314217,
    0.00115660997107625,
    -0.024894818663597107,
    -0.015845995396375656,
    -0.019223472103476524,
    0.03391549736261368,
    0.02765309251844883,
    -0.026175446808338165,
    -0.02101072110235691,
    -0.011645259335637093,
    -0.00288141006603837,
    -0.01237000897526741,
    -0.013158086687326431,
    0.028990009799599648,
    -0.001227853586897254,
    -0.0053687808103859425,
    0.005819111131131649,
    0.014009492471814156,
    -0.01657778210937977,
    0.04624328762292862,
    -0.006628298666328192,
    0.00036589332739822567,
    0.005857811309397221,
    -0.0012304922565817833,
    -0.007803379092365503,
    -0.011004945263266563,
    -0.04112078249454498,
    0.0006851704674772918,
    0.008141126483678818,
    0.011265292763710022,
    -0.01940641924738884,
    0.02689315937459469,
    -0.007022337522357702,
    0.0015128281665965915,
    -0.0013061336940154433,
    -0.001410800265148282,
    -0.014333168044686317,
    0.005970394238829613,
    0.04337243363261223,
    -0.02644282951951027,
    -0.009105115197598934,
    0.001015002140775323,
    0.0048480867408216,
    0.007046964950859547,
    -0.005618573632091284,
    0.000868117087520659,
    -0.015437884256243706,
    0.016225961968302727,
    -0.02029300667345524,
    -0.013538053259253502,
    -0.006695144344121218,
    0.017520662397146225,
    -0.0142557667568326,
    0.0071560293436050415,
    0.017408078536391258,
    0.036955226212739944,
    0.0037996615283191204,
    -0.0039861262775957584,
    -0.008950313553214073,
    0.0017731754342094064,
    -0.024430416524410248,
    -0.002872614422813058,
    0.006061867345124483,
    -0.0056748646311461926,
    -0.006385542452335358,
    -0.006536825094372034,
    0.031241660937666893,
    -0.011166783049702644,
    0.0026439311914145947,
    0.004313319455832243,
    -0.010237976908683777,
    -0.0028004911728203297,
    0.004805868025869131,
    -0.01774582639336586,
    -0.003620233153924346,
    -0.012630356475710869,
    -0.002687908709049225,
    0.004897341597825289,
    -0.002580603351816535,
    -0.022094327956438065,
    0.015972651541233063,
    -0.004028344992548227,
    0.012510737404227257,
    -0.017816191539168358,
    -0.010786816477775574,
    -0.018421322107315063,
    0.0032807262614369392,
    0.004834013991057873,
    -0.022150618955492973,
    -0.01524086482822895,
    0.1837347447872162,
    0.0015823127469047904,
    0.01196189783513546,
    0.015874141827225685,
    -0.0018065982731059194,
    0.015536393970251083,
    0.021601779386401176,
    -0.02345939166843891,
    0.0048269773833453655,
    0.007979289628565311,
    0.01899830810725689,
    0.01441056840121746,
    -0.0015137078007683158,
    -0.005748747382313013,
    -0.009576554410159588,
    0.01567712239921093,
    -0.022755751386284828,
    -0.01819615624845028,
    -0.02687908709049225,
    -0.0040494538843631744,
    0.0026755949947983027,
    0.002466261852532625,
    -0.010632015764713287,
    -0.02634431980550289,
    0.023332735523581505,
    -0.005653755739331245,
    -0.013573234900832176,
    -0.019969332963228226,
    0.030228419229388237,
    0.0027090178336948156,
    -0.01811172068119049,
    0.013453616760671139,
    -0.022192837670445442,
    0.0029869561549276114,
    -0.008570347912609577,
    -0.019237546250224113,
    -0.001013243105262518,
    -0.0012727107387036085,
    0.020447807386517525,
    0.00948508083820343,
    -0.03867211192846298,
    -0.0018241893267259002,
    0.011596004478633404,
    -0.013333997689187527,
    0.002056390978395939,
    -0.011997079476714134,
    -0.024205250665545464,
    0.0005532377399504185,
    0.010357595980167389,
    0.02770938351750374,
    -0.017844336107373238,
    -0.0074023036286234856,
    0.018815360963344574,
    0.019068671390414238,
    -0.027329416945576668,
    -0.00925287976861,
    0.012672575190663338,
    0.04047343134880066,
    0.056769758462905884,
    0.02344531938433647,
    -0.027132397517561913,
    0.0288492813706398,
    -0.013066614046692848,
    0.017070330679416656,
    -0.013622489757835865,
    -0.0026632812805473804,
    -0.014593514613807201,
    0.012243353761732578,
    0.00019855870050378144,
    -0.0029482559766620398,
    -0.014663878828287125,
    -0.052941951900720596,
    0.002596435369923711,
    -0.004665140062570572,
    -0.014987553469836712,
    -0.028201932087540627,
    0.02143290638923645,
    0.00842961948364973,
    0.012714792974293232,
    0.049283016473054886,
    0.0010176408104598522,
    0.007690796162933111,
    -0.009062896482646465,
    0.0029517740476876497,
    0.006234259344637394,
    -0.010765708051621914,
    0.0018083574250340462,
    -0.005878920666873455,
    -0.03926317021250725,
    -0.013573234900832176,
    0.004411829169839621,
    -0.015409738756716251,
    4.706973868451314e-06,
    -0.016662219539284706,
    0.02644282951951027,
    0.004147964064031839,
    0.0040494538843631744,
    0.0072263930924236774,
    0.0011671645333990455,
    0.009562481194734573,
    -0.028638189658522606,
    0.05730452388525009,
    0.015367520041763783,
    0.029609214514493942,
    -0.005942248273640871,
    -0.0019402900943532586,
    -0.01074459869414568,
    0.012911812402307987,
    0.016183743253350258,
    0.005625609774142504,
    0.00848591048270464,
    -0.017239205539226532,
    0.002781141083687544,
    0.0011574894888326526,
    -0.011427130550146103,
    0.01198300626128912,
    -0.005604500882327557,
    -0.025317003950476646,
    0.017126621678471565,
    -0.008598493412137032,
    -0.005118988454341888,
    -0.026513194665312767,
    -0.007025855593383312,
    0.012869594618678093,
    -0.013242524117231369,
    -0.01821023039519787,
    -0.0018083574250340462,
    -0.0387846939265728,
    -0.025359222665429115,
    -0.006723289843648672,
    0.021573632955551147,
    -0.01697182096540928,
    0.01525493711233139,
    -0.0021232368890196085,
    0.004992333240807056,
    -0.027779746800661087,
    -0.01380543690174818,
    -0.01401652954518795,
    -0.004746058490127325,
    0.016310399398207664,
    -0.021981745958328247,
    0.0019913041032850742,
    0.030284710228443146,
    -0.0017098477110266685,
    0.008394437842071056,
    -0.0007718062261119485,
    -0.014832752756774426,
    -0.0035463508684188128,
    -0.005023996811360121,
    0.012243353761732578,
    -0.00026870291912928224,
    -0.00987911969423294,
    0.003898171242326498,
    -0.002993992529809475,
    0.023698629811406136,
    -0.022389857098460197,
    -0.008000398054718971,
    -0.019983405247330666,
    0.005133061204105616,
    -0.008718112483620644,
    -0.04050157591700554,
    -0.026738358661532402,
    0.0074656312353909016,
    0.019716020673513412,
    -0.012524810619652271,
    -0.025809552520513535,
    -0.17945660650730133,
    0.013122905045747757,
    0.015184572897851467,
    -0.043485015630722046,
    0.02882113680243492,
    0.009161406196653843,
    0.015578612685203552,
    -0.004591257777065039,
    -0.012855521403253078,
    -0.005646719131618738,
    0.02772345580160618,
    -0.009421753697097301,
    -0.027470145374536514,
    -0.013650636188685894,
    0.004362574312835932,
    0.004957151133567095,
    -0.013728036545217037,
    0.01093458104878664,
    0.0379684679210186,
    0.012475555762648582,
    0.028173785656690598,
    -0.03757442906498909,
    0.02064482681453228,
    0.014192439615726471,
    0.0014600551221519709,
    -0.025893989950418472,
    0.004003717564046383,
    0.007768196985125542,
    0.018773142248392105,
    -0.022009890526533127,
    -0.013608417473733425,
    -0.008211490698158741,
    0.0347035750746727,
    -0.0013677022652700543,
    0.019167181104421616,
    0.013615453615784645,
    -0.005695973988622427,
    -0.010892363265156746,
    -0.013939128257334232,
    0.009653954766690731,
    0.0396009162068367,
    0.025077765807509422,
    -0.0006011733203195035,
    -0.006445351988077164,
    -0.015269010327756405,
    0.027301272377371788,
    -0.0024521888699382544,
    -0.023318663239479065,
    0.025302931666374207,
    -0.02801898494362831,
    0.03546350821852684,
    -0.026076937094330788,
    0.008338145911693573,
    -0.017098477110266685,
    0.012461482547223568,
    -0.0043520196340978146,
    0.022474294528365135,
    0.010843108408153057,
    -0.01318623311817646,
    -0.020307080820202827,
    -0.00047319859731942415,
    -0.009027713909745216,
    0.03352145850658417,
    -0.002332570031285286,
    -0.004253509920090437,
    -0.026611704379320145,
    -0.011026054620742798,
    0.014108002185821533,
    -0.03881283849477768,
    0.00023637940466869622,
    0.020461881533265114,
    0.030003253370523453,
    -0.012243353761732578,
    0.017478443682193756,
    0.01030130498111248,
    0.003057320136576891,
    0.0015946264611557126,
    0.001496116747148335,
    0.032367486506700516,
    -0.0177598986774683,
    2.093057264573872e-05,
    0.0255140233784914,
    0.003166384529322386,
    -0.0027705866377800703,
    0.010181685909628868,
    -0.011469348333775997,
    0.005857811309397221,
    0.02056039124727249,
    -0.025795480236411095,
    -0.01130751147866249,
    0.005287862382829189,
    0.0009710246231406927,
    -0.005382854025810957,
    -0.0010528229176998138,
    -0.014959407970309258,
    0.0002948695619124919,
    -0.008528129197657108,
    0.013685817830264568,
    0.013221414759755135,
    -0.005259716417640448,
    0.022164693102240562,
    0.006856981664896011,
    -0.016662219539284706,
    -0.013685817830264568,
    0.05643200874328613,
    0.02058853581547737,
    -0.03222675994038582,
    0.03748999536037445,
    0.026752430945634842,
    -0.003407381707802415,
    -0.008767367340624332,
    0.02347346395254135,
    0.025007402524352074,
    0.006396097131073475,
    -0.006040757987648249,
    0.033577751368284225,
    -0.01774582639336586,
    -0.001710727228783071,
    0.001248083310201764,
    -0.006367951165884733,
    0.030453583225607872,
    0.020771482959389687,
    -0.0017494275234639645,
    0.008295928128063679,
    -0.04447011277079582,
    -0.023023134097456932,
    -0.10644681751728058,
    -0.03391549736261368,
    0.0181680116802454,
    -0.0042253644205629826,
    -0.012559992261230946,
    -0.005038069561123848,
    -0.011237147264182568,
    0.0227276049554348,
    -0.015100136399269104,
    0.0027864184230566025,
    -0.007993361912667751,
    -0.005031033419072628,
    0.006772544700652361,
    -0.005333599168807268,
    -0.0015743967378512025,
    -0.011884496547281742,
    0.0026193037629127502,
    0.0028339142445474863,
    -0.01131454762071371,
    -0.0014301503542810678,
    0.008915131911635399,
    -0.0028972418513149023,
    0.016662219539284706,
    0.011075309477746487,
    -0.008591457270085812,
    0.003915762063115835,
    -0.02182694524526596,
    0.005087324418127537,
    0.009682100266218185,
    0.010434996336698532,
    0.004306282848119736,
    -0.01647927239537239,
    -0.012299644760787487,
    -0.034450262784957886,
    -0.004788277205079794,
    -0.01650741882622242,
    -0.014649806544184685,
    -0.014495004899799824,
    0.028258223086595535,
    -0.02557031437754631,
    0.007965216413140297,
    -0.010399814695119858,
    0.02810342237353325,
    0.012102625332772732,
    -0.02969365194439888,
    -0.002930664923042059,
    0.013559162616729736,
    0.02101072110235691,
    0.011124564334750175,
    -0.017970992252230644,
    -0.02595028094947338,
    0.0018294666660949588,
    -0.012222244404256344,
    -0.006357396487146616,
    0.02064482681453228,
    -0.02224912866950035,
    0.02063075453042984,
    -0.004475156776607037,
    -0.025063693523406982,
    -0.032367486506700516,
    -0.02016635239124298,
    -0.006878091022372246,
    0.0019051081035286188,
    0.007353048771619797,
    0.005780410952866077,
    -0.01115270983427763,
    0.005432108882814646,
    -0.026597630232572556,
    0.01895608939230442,
    -0.03678635135293007,
    0.0019438082817941904,
    0.0011398984352126718,
    -0.021953599527478218,
    0.014677952043712139,
    -0.030397292226552963,
    0.003954462707042694,
    -0.02264316752552986,
    -0.012341863475739956,
    0.011398984119296074,
    -0.009984666481614113,
    -0.02018042467534542,
    0.004837532062083483,
    0.003592087421566248,
    -0.017914701253175735,
    -0.015437884256243706,
    0.006047794595360756,
    -0.0041057453490793705,
    -0.013728036545217037,
    0.025668824091553688,
    -0.02804713137447834,
    0.006174449808895588,
    -0.002483852906152606,
    0.029412195086479187,
    -0.014692024327814579,
    0.0020528726745396852,
    -0.00386650743894279,
    0.006533307023346424,
    0.005481363739818335,
    -0.018435394391417503,
    0.009590626694262028,
    -0.043879054486751556,
    -0.011828205548226833,
    -0.08224156498908997,
    0.02144697867333889,
    -0.01130751147866249,
    -0.026949450373649597,
    0.01819615624845028,
    -0.007367121521383524,
    0.002058150013908744,
    -0.015156427398324013,
    0.008746257983148098,
    0.01935012824833393,
    -0.02151734195649624,
    -0.008570347912609577,
    0.001365063595585525,
    0.011673404835164547,
    0.009576554410159588,
    -0.014108002185821533,
    0.024430416524410248,
    -0.01736585982143879,
    0.034478411078453064,
    0.009351389482617378,
    -0.01013243105262518,
    -0.0021425869781523943,
    0.024191178381443024,
    0.020447807386517525,
    -0.03642046079039574,
    -0.005787447560578585,
    -0.004753095097839832,
    0.022164693102240562,
    0.016591856256127357,
    -0.029130738228559494,
    0.016605928540229797,
    -0.008464801125228405,
    0.033268146216869354,
    0.028638189658522606,
    0.017056258395314217,
    0.006279996130615473,
    0.026189519092440605,
    0.0002568289637565613,
    0.004401274491101503,
    0.039009857922792435,
    -0.0009754223865456879,
    -0.022136546671390533,
    0.019476784393191338,
    -0.02396601252257824,
    -0.012510737404227257,
    -0.012834412045776844,
    -0.005819111131131649,
    -0.008000398054718971,
    0.030312854796648026,
    0.036026421934366226,
    0.02305128052830696,
    0.005921139381825924,
    -0.020504100248217583,
    -0.014213548973202705,
    0.005871884524822235,
    -0.01158896740525961,
    0.01937827467918396,
    -0.006276478059589863,
    -0.029862524941563606,
    -0.0033827542793005705,
    0.009815792553126812,
    0.0006345962756313384,
    0.024191178381443024,
    0.010111321695148945,
    0.024163031950592995,
    -0.011061237193644047,
    -0.03301483765244484,
    -0.01851983182132244,
    -0.003238507779315114,
    -0.013214378617703915,
    -0.03329629451036453,
    -0.017408078536391258,
    0.023698629811406136,
    0.015367520041763783,
    0.017070330679416656,
    -0.020110059529542923,
    -0.014157257042825222,
    -0.005221016239374876,
    -0.008162235841155052,
    0.007810415234416723,
    0.012264463119208813,
    -0.00690271845087409,
    -0.013552126474678516,
    0.015100136399269104,
    0.020377444103360176,
    0.007775233127176762,
    -0.01657778210937977,
    -0.015944505110383034,
    -0.00947100855410099,
    -0.004257027991116047,
    0.010786816477775574,
    0.02316386252641678,
    0.019490856677293777,
    0.020757410675287247,
    0.003039729315787554,
    -0.012609247118234634,
    -0.012025224976241589,
    -0.013495834544301033,
    0.02313571609556675,
    0.020405590534210205,
    -0.002346642781049013,
    0.008605529554188251,
    0.01054054219275713,
    -0.019279764965176582,
    -0.03262079879641533,
    3.564931103028357e-05,
    0.004471638705581427,
    0.015409738756716251,
    -0.015874141827225685,
    0.013460652902722359,
    0.036898937076330185,
    -0.020405590534210205,
    -0.001452139113098383,
    0.0036131965462118387,
    -0.01650741882622242,
    0.004907896276563406,
    0.006645889487117529,
    -0.01730956882238388,
    0.0031382390297949314,
    0.007922997698187828,
    0.007641541305929422,
    0.0047671678476035595,
    0.034562848508358,
    -0.00826074555516243,
    0.01614152453839779,
    0.007852633483707905,
    0.01649334467947483,
    -0.007810415234416723,
    0.01856205053627491,
    0.019068671390414238,
    -0.02801898494362831,
    0.008310000412166119,
    -0.006550897844135761,
    -0.01546602975577116,
    -0.025359222665429115,
    -0.01341843418776989,
    -0.028328588232398033,
    0.006832354236394167,
    0.026724286377429962,
    0.0670992061495781,
    0.017042186111211777,
    -0.02977808751165867,
    -0.00787374284118414,
    0.003694115439429879,
    -0.006009094417095184,
    0.01649334467947483,
    0.015156427398324013,
    -0.02314979024231434,
    -0.014818679541349411,
    -0.018407249823212624,
    -0.025485876947641373,
    -0.007915961556136608,
    0.004211291670799255,
    -0.02023671567440033,
    -0.01215188018977642,
    -0.009583590552210808,
    0.015775632113218307,
    0.013925055973231792,
    -0.016310399398207664,
    0.022488366812467575,
    0.010364632122218609,
    -0.0012630356941372156,
    0.009105115197598934,
    -0.03836250677704811,
    -0.010688306763768196,
    0.03138238936662674,
    0.006213150452822447,
    0.00992133840918541,
    -0.038953568786382675,
    0.01821023039519787,
    0.020391516387462616,
    -0.027329416945576668,
    -0.03371847793459892,
    0.00783152412623167,
    0.02434597909450531,
    0.012447409331798553,
    0.01739400625228882,
    0.020912211388349533,
    0.008725148625671864,
    0.005907066166400909,
    -0.021643998101353645,
    -0.008127054199576378,
    -0.03504132479429245,
    0.023037206381559372,
    -0.0007001227932050824,
    0.003512927796691656,
    -0.033577751368284225,
    -0.02396601252257824
  ]
}