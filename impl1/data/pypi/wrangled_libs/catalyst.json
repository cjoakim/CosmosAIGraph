{
  "classifiers": [
    "development status :: 4 - beta",
    "environment :: console",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "natural language :: english",
    "operating system :: os independent",
    "programming language :: python",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "programming language :: python :: implementation :: cpython",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: scientific/engineering :: image recognition",
    "topic :: scientific/engineering :: information analysis"
  ],
  "description": "\n<div align=\"center\">\n\n[![catalyst logo](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst_logo.png)](https://github.com/catalyst-team/catalyst)\n\n**accelerated deep learning r&d**\n\n[![codefactor](https://www.codefactor.io/repository/github/catalyst-team/catalyst/badge)](https://www.codefactor.io/repository/github/catalyst-team/catalyst)\n[![pipi version](https://img.shields.io/pypi/v/catalyst.svg)](https://pypi.org/project/catalyst/)\n[![docs](https://img.shields.io/badge/dynamic/json.svg?label=docs&url=https%3a%2f%2fpypi.org%2fpypi%2fcatalyst%2fjson&query=%24.info.version&colorb=brightgreen&prefix=v)](https://catalyst-team.github.io/catalyst/index.html)\n[![docker](https://img.shields.io/badge/docker-hub-blue)](https://hub.docker.com/r/catalystteam/catalyst/tags)\n[![pypi status](https://pepy.tech/badge/catalyst)](https://pepy.tech/project/catalyst)\n\n[![twitter](https://img.shields.io/badge/news-twitter-499feb)](https://twitter.com/catalystteam)\n[![telegram](https://img.shields.io/badge/channel-telegram-blue)](https://t.me/catalyst_team)\n[![slack](https://img.shields.io/badge/catalyst-slack-success)](https://join.slack.com/t/catalyst-team-devs/shared_invite/zt-d9miirnn-z86okdzfmklmg4fgfdzafw)\n[![github contributors](https://img.shields.io/github/contributors/catalyst-team/catalyst.svg?logo=github&logocolor=white)](https://github.com/catalyst-team/catalyst/graphs/contributors)\n\n![codestyle](https://github.com/catalyst-team/catalyst/workflows/codestyle/badge.svg?branch=master&event=push)\n![docs](https://github.com/catalyst-team/catalyst/workflows/docs/badge.svg?branch=master&event=push)\n![catalyst](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n![integrations](https://github.com/catalyst-team/catalyst/workflows/integrations/badge.svg?branch=master&event=push)\n\n[![python](https://img.shields.io/badge/python_3.6-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n[![python](https://img.shields.io/badge/python_3.7-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n[![python](https://img.shields.io/badge/python_3.8-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n\n[![os](https://img.shields.io/badge/linux-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n[![os](https://img.shields.io/badge/osx-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n[![os](https://img.shields.io/badge/wsl-passing-success)](https://github.com/catalyst-team/catalyst/workflows/catalyst/badge.svg?branch=master&event=push)\n</div>\n\ncatalyst is a pytorch framework for deep learning research and development.\nit focuses on reproducibility, rapid experimentation, and codebase reuse\nso you can create something new rather than write yet another train loop.\n<br/> break the cycle \u2013 use the catalyst!\n\n- [project manifest](https://github.com/catalyst-team/catalyst/blob/master/manifest.md)\n- [framework architecture](https://miro.com/app/board/o9j_lxbo-2k=/)\n- [catalyst at ai landscape](https://landscape.lfai.foundation/selected=catalyst)\n- part of the [pytorch ecosystem](https://pytorch.org/ecosystem/)\n\n<details>\n<summary>catalyst at pytorch ecosystem day 2021</summary>\n<p>\n\n[![catalyst poster](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst-pted21.png)](https://github.com/catalyst-team/catalyst)\n\n</p>\n</details>\n\n<details>\n<summary>catalyst at pytorch developer day 2021</summary>\n<p>\n\n[![catalyst poster](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst-ptdd21.png)](https://github.com/catalyst-team/catalyst)\n\n</p>\n</details>\n\n----\n\n## getting started\n\n```bash\npip install -u catalyst\n```\n\n```python\nimport os\nfrom torch import nn, optim\nfrom torch.utils.data import dataloader\nfrom catalyst import dl, utils\nfrom catalyst.contrib.datasets import mnist\n\nmodel = nn.sequential(nn.flatten(), nn.linear(28 * 28, 10))\ncriterion = nn.crossentropyloss()\noptimizer = optim.adam(model.parameters(), lr=0.02)\nloaders = {\n    \"train\": dataloader(mnist(os.getcwd(), train=true), batch_size=32),\n    \"valid\": dataloader(mnist(os.getcwd(), train=false), batch_size=32),\n}\n\nrunner = dl.supervisedrunner(\n    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n)\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    num_epochs=1,\n    callbacks=[\n        dl.accuracycallback(input_key=\"logits\", target_key=\"targets\", topk=(1, 3, 5)),\n        dl.precisionrecallf1supportcallback(input_key=\"logits\", target_key=\"targets\"),\n    ],\n    logdir=\"./logs\",\n    valid_loader=\"valid\",\n    valid_metric=\"loss\",\n    minimize_valid_metric=true,\n    verbose=true,\n)\n\n# model evaluation\nmetrics = runner.evaluate_loader(\n    loader=loaders[\"valid\"],\n    callbacks=[dl.accuracycallback(input_key=\"logits\", target_key=\"targets\", topk=(1, 3, 5))],\n)\n\n# model inference\nfor prediction in runner.predict_loader(loader=loaders[\"valid\"]):\n    assert prediction[\"logits\"].detach().cpu().numpy().shape[-1] == 10\n\n# model post-processing\nmodel = runner.model.cpu()\nbatch = next(iter(loaders[\"valid\"]))[0]\nutils.trace_model(model=model, batch=batch)\nutils.quantize_model(model=model)\nutils.prune_model(model=model, pruning_fn=\"l1_unstructured\", amount=0.8)\nutils.onnx_export(model=model, batch=batch, file=\"./logs/mnist.onnx\", verbose=true)\n```\n\n### step-by-step guide\n1. start with [catalyst \u2014 a pytorch framework for accelerated deep learning r&d](https://medium.com/pytorch/catalyst-a-pytorch-framework-for-accelerated-deep-learning-r-d-ad9621e4ca88?source=friends_link&sk=885b4409aecab505db0a63b06f19dcef) introduction.\n1. try [notebook tutorials](#minimal-examples) or check [minimal examples](#minimal-examples) for first deep dive.\n1. read [blog posts](https://catalyst-team.com/post/) with use-cases and guides.\n1. learn machine learning with our [\"deep learning with catalyst\" course](https://catalyst-team.com/#course).\n1. and finally, [join our slack](https://join.slack.com/t/catalyst-team-core/shared_invite/zt-d9miirnn-z86okdzfmklmg4fgfdzafw) if you want to chat with the team and contributors.\n\n\n## table of contents\n- [getting started](#getting-started)\n  - [step-by-step guide](#step-by-step-guide)\n- [table of contents](#table-of-contents)\n- [overview](#overview)\n  - [installation](#installation)\n  - [documentation](#documentation)\n  - [minimal examples](#minimal-examples)\n  - [tests](#tests)\n  - [blog posts](#blog-posts)\n  - [talks](#talks)\n- [community](#community)\n  - [contribution guide](#contribution-guide)\n  - [user feedback](#user-feedback)\n  - [acknowledgments](#acknowledgments)\n  - [trusted by](#trusted-by)\n  - [citation](#citation)\n\n\n## overview\ncatalyst helps you implement compact\nbut full-featured deep learning pipelines with just a few lines of code.\nyou get a training loop with metrics, early-stopping, model checkpointing,\nand other features without the boilerplate.\n\n\n### installation\n\ngeneric installation:\n```bash\npip install -u catalyst\n```\n\n<details>\n<summary>specialized versions, extra requirements might apply</summary>\n<p>\n\n```bash\npip install catalyst[ml]         # installs ml-based catalyst\npip install catalyst[cv]         # installs cv-based catalyst\n# master version installation\npip install git+https://github.com/catalyst-team/catalyst@master --upgrade\n# all available extensions are listed here:\n# https://github.com/catalyst-team/catalyst/blob/master/setup.py\n```\n</p>\n</details>\n\ncatalyst is compatible with: python 3.7+. pytorch 1.4+. <br/>\ntested on ubuntu 16.04/18.04/20.04, macos 10.15, windows 10, and windows subsystem for linux.\n\n### documentation\n- [master](https://catalyst-team.github.io/catalyst/)\n- [22.02](https://catalyst-team.github.io/catalyst/v22.02/index.html)\n\n- <details>\n  <summary>2021 edition</summary>\n  <p>\n\n    - [21.12](https://catalyst-team.github.io/catalyst/v21.12/index.html)\n    - [21.11](https://catalyst-team.github.io/catalyst/v21.11/index.html)\n    - [21.10](https://catalyst-team.github.io/catalyst/v21.10/index.html)\n    - [21.09](https://catalyst-team.github.io/catalyst/v21.09/index.html)\n    - [21.08](https://catalyst-team.github.io/catalyst/v21.08/index.html)\n    - [21.07](https://catalyst-team.github.io/catalyst/v21.07/index.html)\n    - [21.06](https://catalyst-team.github.io/catalyst/v21.06/index.html)\n    - [21.05](https://catalyst-team.github.io/catalyst/v21.05/index.html) ([catalyst \u2014 a pytorch framework for accelerated deep learning r&d](https://medium.com/pytorch/catalyst-a-pytorch-framework-for-accelerated-deep-learning-r-d-ad9621e4ca88?source=friends_link&sk=885b4409aecab505db0a63b06f19dcef))\n    - [21.04/21.04.1](https://catalyst-team.github.io/catalyst/v21.04/index.html), [21.04.2](https://catalyst-team.github.io/catalyst/v21.04.2/index.html)\n    - [21.03](https://catalyst-team.github.io/catalyst/v21.03/index.html), [21.03.1/21.03.2](https://catalyst-team.github.io/catalyst/v21.03.1/index.html)\n\n  </p>\n  </details>\n- <details>\n  <summary>2020 edition</summary>\n  <p>\n\n    - [20.12](https://catalyst-team.github.io/catalyst/v20.12/index.html)\n    - [20.11](https://catalyst-team.github.io/catalyst/v20.11/index.html)\n    - [20.10](https://catalyst-team.github.io/catalyst/v20.10/index.html)\n    - [20.09](https://catalyst-team.github.io/catalyst/v20.09/index.html)\n    - [20.08.2](https://catalyst-team.github.io/catalyst/v20.08.2/index.html)\n    - [20.07](https://catalyst-team.github.io/catalyst/v20.07/index.html) ([dev blog: 20.07 release](https://medium.com/pytorch/catalyst-dev-blog-20-07-release-fb489cd23e14?source=friends_link&sk=7ab92169658fe9a9e1c44068f28cc36c))\n    - [20.06](https://catalyst-team.github.io/catalyst/v20.06/index.html)\n    - [20.05](https://catalyst-team.github.io/catalyst/v20.05/index.html), [20.05.1](https://catalyst-team.github.io/catalyst/v20.05.1/index.html)\n    - [20.04](https://catalyst-team.github.io/catalyst/v20.04/index.html), [20.04.1](https://catalyst-team.github.io/catalyst/v20.04.1/index.html), [20.04.2](https://catalyst-team.github.io/catalyst/v20.04.2/index.html)\n\n  </p>\n  </details>\n\n\n### minimal examples\n\n- [![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/customizing_what_happens_in_train.ipynb) introduction tutorial \"[customizing what happens in `train`](./examples/notebooks/customizing_what_happens_in_train.ipynb)\"\n- [![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/customization_tutorial.ipynb) demo with [customization examples](./examples/notebooks/customization_tutorial.ipynb)\n- [![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/reinforcement_learning.ipynb) [reinforcement learning with catalyst](./examples/notebooks/reinforcement_learning.ipynb)\n- [and more](./examples/)\n\n<details>\n<summary>customrunner \u2013 pytorch for-loop decomposition</summary>\n<p>\n\n```python\nimport os\nfrom torch import nn, optim\nfrom torch.nn import functional as f\nfrom torch.utils.data import dataloader\nfrom catalyst import dl, metrics\nfrom catalyst.contrib.datasets import mnist\n\nmodel = nn.sequential(nn.flatten(), nn.linear(28 * 28, 10))\noptimizer = optim.adam(model.parameters(), lr=0.02)\n\ntrain_data = mnist(os.getcwd(), train=true)\nvalid_data = mnist(os.getcwd(), train=false)\nloaders = {\n    \"train\": dataloader(train_data, batch_size=32),\n    \"valid\": dataloader(valid_data, batch_size=32),\n}\n\nclass customrunner(dl.runner):\n    def predict_batch(self, batch):\n        # model inference step\n        return self.model(batch[0].to(self.engine.device))\n\n    def on_loader_start(self, runner):\n        super().on_loader_start(runner)\n        self.meters = {\n            key: metrics.additivemetric(compute_on_call=false)\n            for key in [\"loss\", \"accuracy01\", \"accuracy03\"]\n        }\n\n    def handle_batch(self, batch):\n        # model train/valid step\n        # unpack the batch\n        x, y = batch\n        # run model forward pass\n        logits = self.model(x)\n        # compute the loss\n        loss = f.cross_entropy(logits, y)\n        # compute the metrics\n        accuracy01, accuracy03 = metrics.accuracy(logits, y, topk=(1, 3))\n        # log metrics\n        self.batch_metrics.update(\n            {\"loss\": loss, \"accuracy01\": accuracy01, \"accuracy03\": accuracy03}\n        )\n        for key in [\"loss\", \"accuracy01\", \"accuracy03\"]:\n            self.meters[key].update(self.batch_metrics[key].item(), self.batch_size)\n        # run model backward pass\n        if self.is_train_loader:\n            self.engine.backward(loss)\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n    def on_loader_end(self, runner):\n        for key in [\"loss\", \"accuracy01\", \"accuracy03\"]:\n            self.loader_metrics[key] = self.meters[key].compute()[0]\n        super().on_loader_end(runner)\n\nrunner = customrunner()\n# model training\nrunner.train(\n    model=model,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=\"./logs\",\n    num_epochs=5,\n    verbose=true,\n    valid_loader=\"valid\",\n    valid_metric=\"loss\",\n    minimize_valid_metric=true,\n)\n# model inference\nfor logits in runner.predict_loader(loader=loaders[\"valid\"]):\n    assert logits.detach().cpu().numpy().shape[-1] == 10\n```\n</p>\n</details>\n\n<details>\n<summary>ml - linear regression</summary>\n<p>\n\n```python\nimport torch\nfrom torch.utils.data import dataloader, tensordataset\nfrom catalyst import dl\n\n# data\nnum_samples, num_features = int(1e4), int(1e1)\nx, y = torch.rand(num_samples, num_features), torch.rand(num_samples)\ndataset = tensordataset(x, y)\nloader = dataloader(dataset, batch_size=32, num_workers=1)\nloaders = {\"train\": loader, \"valid\": loader}\n\n# model, criterion, optimizer, scheduler\nmodel = torch.nn.linear(num_features, 1)\ncriterion = torch.nn.mseloss()\noptimizer = torch.optim.adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.multisteplr(optimizer, [3, 6])\n\n# model training\nrunner = dl.supervisedrunner()\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    logdir=\"./logdir\",\n    valid_loader=\"valid\",\n    valid_metric=\"loss\",\n    minimize_valid_metric=true,\n    num_epochs=8,\n    verbose=true,\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>ml - multiclass classification</summary>\n<p>\n\n```python\nimport torch\nfrom torch.utils.data import dataloader, tensordataset\nfrom catalyst import dl\n\n# sample data\nnum_samples, num_features, num_classes = int(1e4), int(1e1), 4\nx = torch.rand(num_samples, num_features)\ny = (torch.rand(num_samples,) * num_classes).to(torch.int64)\n\n# pytorch loaders\ndataset = tensordataset(x, y)\nloader = dataloader(dataset, batch_size=32, num_workers=1)\nloaders = {\"train\": loader, \"valid\": loader}\n\n# model, criterion, optimizer, scheduler\nmodel = torch.nn.linear(num_features, num_classes)\ncriterion = torch.nn.crossentropyloss()\noptimizer = torch.optim.adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.multisteplr(optimizer, [2])\n\n# model training\nrunner = dl.supervisedrunner(\n    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n)\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    logdir=\"./logdir\",\n    num_epochs=3,\n    valid_loader=\"valid\",\n    valid_metric=\"accuracy03\",\n    minimize_valid_metric=false,\n    verbose=true,\n    callbacks=[\n        dl.accuracycallback(input_key=\"logits\", target_key=\"targets\", num_classes=num_classes),\n        # uncomment for extra metrics:\n        # dl.precisionrecallf1supportcallback(\n        #     input_key=\"logits\", target_key=\"targets\", num_classes=num_classes\n        # ),\n        # dl.auccallback(input_key=\"logits\", target_key=\"targets\"),\n        # catalyst[ml] required ``pip install catalyst[ml]``\n        # dl.confusionmatrixcallback(\n        #     input_key=\"logits\", target_key=\"targets\", num_classes=num_classes\n        # ),\n    ],\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>ml - multilabel classification</summary>\n<p>\n\n```python\nimport torch\nfrom torch.utils.data import dataloader, tensordataset\nfrom catalyst import dl\n\n# sample data\nnum_samples, num_features, num_classes = int(1e4), int(1e1), 4\nx = torch.rand(num_samples, num_features)\ny = (torch.rand(num_samples, num_classes) > 0.5).to(torch.float32)\n\n# pytorch loaders\ndataset = tensordataset(x, y)\nloader = dataloader(dataset, batch_size=32, num_workers=1)\nloaders = {\"train\": loader, \"valid\": loader}\n\n# model, criterion, optimizer, scheduler\nmodel = torch.nn.linear(num_features, num_classes)\ncriterion = torch.nn.bcewithlogitsloss()\noptimizer = torch.optim.adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.multisteplr(optimizer, [2])\n\n# model training\nrunner = dl.supervisedrunner(\n    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n)\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    logdir=\"./logdir\",\n    num_epochs=3,\n    valid_loader=\"valid\",\n    valid_metric=\"accuracy01\",\n    minimize_valid_metric=false,\n    verbose=true,\n    callbacks=[\n        dl.batchtransformcallback(\n            transform=torch.sigmoid,\n            scope=\"on_batch_end\",\n            input_key=\"logits\",\n            output_key=\"scores\"\n        ),\n        dl.auccallback(input_key=\"scores\", target_key=\"targets\"),\n        # uncomment for extra metrics:\n        # dl.multilabelaccuracycallback(input_key=\"scores\", target_key=\"targets\", threshold=0.5),\n        # dl.multilabelprecisionrecallf1supportcallback(\n        #     input_key=\"scores\", target_key=\"targets\", threshold=0.5\n        # ),\n    ]\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>ml - multihead classification</summary>\n<p>\n\n```python\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import dataloader, tensordataset\nfrom catalyst import dl\n\n# sample data\nnum_samples, num_features, num_classes1, num_classes2 = int(1e4), int(1e1), 4, 10\nx = torch.rand(num_samples, num_features)\ny1 = (torch.rand(num_samples,) * num_classes1).to(torch.int64)\ny2 = (torch.rand(num_samples,) * num_classes2).to(torch.int64)\n\n# pytorch loaders\ndataset = tensordataset(x, y1, y2)\nloader = dataloader(dataset, batch_size=32, num_workers=1)\nloaders = {\"train\": loader, \"valid\": loader}\n\nclass custommodule(nn.module):\n    def __init__(self, in_features: int, out_features1: int, out_features2: int):\n        super().__init__()\n        self.shared = nn.linear(in_features, 128)\n        self.head1 = nn.linear(128, out_features1)\n        self.head2 = nn.linear(128, out_features2)\n\n    def forward(self, x):\n        x = self.shared(x)\n        y1 = self.head1(x)\n        y2 = self.head2(x)\n        return y1, y2\n\n# model, criterion, optimizer, scheduler\nmodel = custommodule(num_features, num_classes1, num_classes2)\ncriterion = nn.crossentropyloss()\noptimizer = optim.adam(model.parameters())\nscheduler = optim.lr_scheduler.multisteplr(optimizer, [2])\n\nclass customrunner(dl.runner):\n    def handle_batch(self, batch):\n        x, y1, y2 = batch\n        y1_hat, y2_hat = self.model(x)\n        self.batch = {\n            \"features\": x,\n            \"logits1\": y1_hat,\n            \"logits2\": y2_hat,\n            \"targets1\": y1,\n            \"targets2\": y2,\n        }\n\n# model training\nrunner = customrunner()\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    num_epochs=3,\n    verbose=true,\n    callbacks=[\n        dl.criterioncallback(metric_key=\"loss1\", input_key=\"logits1\", target_key=\"targets1\"),\n        dl.criterioncallback(metric_key=\"loss2\", input_key=\"logits2\", target_key=\"targets2\"),\n        dl.metricaggregationcallback(metric_key=\"loss\", metrics=[\"loss1\", \"loss2\"], mode=\"mean\"),\n        dl.backwardcallback(metric_key=\"loss\"),\n        dl.optimizercallback(metric_key=\"loss\"),\n        dl.schedulercallback(),\n        dl.accuracycallback(\n            input_key=\"logits1\", target_key=\"targets1\", num_classes=num_classes1, prefix=\"one_\"\n        ),\n        dl.accuracycallback(\n            input_key=\"logits2\", target_key=\"targets2\", num_classes=num_classes2, prefix=\"two_\"\n        ),\n        # catalyst[ml] required ``pip install catalyst[ml]``\n        # dl.confusionmatrixcallback(\n        #     input_key=\"logits1\", target_key=\"targets1\", num_classes=num_classes1, prefix=\"one_cm\"\n        # ),\n        # dl.confusionmatrixcallback(\n        #     input_key=\"logits2\", target_key=\"targets2\", num_classes=num_classes2, prefix=\"two_cm\"\n        # ),\n        dl.checkpointcallback(\n            logdir=\"./logs/one\",\n            loader_key=\"valid\", metric_key=\"one_accuracy01\", minimize=false, topk=1\n        ),\n        dl.checkpointcallback(\n            logdir=\"./logs/two\",\n            loader_key=\"valid\", metric_key=\"two_accuracy03\", minimize=false, topk=3\n        ),\n    ],\n    loggers={\"console\": dl.consolelogger(), \"tb\": dl.tensorboardlogger(\"./logs/tb\")},\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>ml \u2013 recsys</summary>\n<p>\n\n```python\nimport torch\nfrom torch.utils.data import dataloader, tensordataset\nfrom catalyst import dl\n\n# sample data\nnum_users, num_features, num_items = int(1e4), int(1e1), 10\nx = torch.rand(num_users, num_features)\ny = (torch.rand(num_users, num_items) > 0.5).to(torch.float32)\n\n# pytorch loaders\ndataset = tensordataset(x, y)\nloader = dataloader(dataset, batch_size=32, num_workers=1)\nloaders = {\"train\": loader, \"valid\": loader}\n\n# model, criterion, optimizer, scheduler\nmodel = torch.nn.linear(num_features, num_items)\ncriterion = torch.nn.bcewithlogitsloss()\noptimizer = torch.optim.adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.multisteplr(optimizer, [2])\n\n# model training\nrunner = dl.supervisedrunner(\n    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n)\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    num_epochs=3,\n    verbose=true,\n    callbacks=[\n        dl.batchtransformcallback(\n            transform=torch.sigmoid,\n            scope=\"on_batch_end\",\n            input_key=\"logits\",\n            output_key=\"scores\"\n        ),\n        dl.criterioncallback(input_key=\"logits\", target_key=\"targets\", metric_key=\"loss\"),\n        # uncomment for extra metrics:\n        # dl.auccallback(input_key=\"scores\", target_key=\"targets\"),\n        # dl.hitratecallback(input_key=\"scores\", target_key=\"targets\", topk=(1, 3, 5)),\n        # dl.mrrcallback(input_key=\"scores\", target_key=\"targets\", topk=(1, 3, 5)),\n        # dl.mapcallback(input_key=\"scores\", target_key=\"targets\", topk=(1, 3, 5)),\n        # dl.ndcgcallback(input_key=\"scores\", target_key=\"targets\", topk=(1, 3, 5)),\n        dl.backwardcallback(metric_key=\"loss\"),\n        dl.optimizercallback(metric_key=\"loss\"),\n        dl.schedulercallback(),\n        dl.checkpointcallback(\n            logdir=\"./logs\", loader_key=\"valid\", metric_key=\"loss\", minimize=true\n        ),\n    ]\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>cv - mnist classification</summary>\n<p>\n\n```python\nimport os\nfrom torch import nn, optim\nfrom torch.utils.data import dataloader\nfrom catalyst import dl\nfrom catalyst.contrib.datasets import mnist\n\nmodel = nn.sequential(nn.flatten(), nn.linear(28 * 28, 10))\ncriterion = nn.crossentropyloss()\noptimizer = optim.adam(model.parameters(), lr=0.02)\n\ntrain_data = mnist(os.getcwd(), train=true)\nvalid_data = mnist(os.getcwd(), train=false)\nloaders = {\n    \"train\": dataloader(train_data, batch_size=32),\n    \"valid\": dataloader(valid_data, batch_size=32),\n}\n\nrunner = dl.supervisedrunner()\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    num_epochs=1,\n    logdir=\"./logs\",\n    valid_loader=\"valid\",\n    valid_metric=\"loss\",\n    minimize_valid_metric=true,\n    verbose=true,\n# uncomment for extra metrics:\n#     callbacks=[\n#         dl.accuracycallback(input_key=\"logits\", target_key=\"targets\", num_classes=10),\n#         dl.precisionrecallf1supportcallback(\n#             input_key=\"logits\", target_key=\"targets\", num_classes=10\n#         ),\n#         dl.auccallback(input_key=\"logits\", target_key=\"targets\"),\n#         # catalyst[ml] required ``pip install catalyst[ml]``\n#         dl.confusionmatrixcallback(\n#             input_key=\"logits\", target_key=\"targets\", num_classes=num_classes\n#         ),\n#     ]\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>cv - mnist segmentation</summary>\n<p>\n\n```python\nimport os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import dataloader\nfrom catalyst import dl\nfrom catalyst.contrib.datasets import mnist\nfrom catalyst.contrib.losses import iouloss\n\n\nmodel = nn.sequential(\n    nn.conv2d(1, 1, 3, 1, 1), nn.relu(),\n    nn.conv2d(1, 1, 3, 1, 1), nn.sigmoid(),\n)\ncriterion = iouloss()\noptimizer = torch.optim.adam(model.parameters(), lr=0.02)\n\ntrain_data = mnist(os.getcwd(), train=true)\nvalid_data = mnist(os.getcwd(), train=false)\nloaders = {\n    \"train\": dataloader(train_data, batch_size=32),\n    \"valid\": dataloader(valid_data, batch_size=32),\n}\n\nclass customrunner(dl.supervisedrunner):\n    def handle_batch(self, batch):\n        x = batch[self._input_key]\n        x_noise = (x + torch.rand_like(x)).clamp_(0, 1)\n        x_ = self.model(x_noise)\n        self.batch = {self._input_key: x, self._output_key: x_, self._target_key: x}\n\nrunner = customrunner(\n    input_key=\"features\", output_key=\"scores\", target_key=\"targets\", loss_key=\"loss\"\n)\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    num_epochs=1,\n    callbacks=[\n        dl.ioucallback(input_key=\"scores\", target_key=\"targets\"),\n        dl.dicecallback(input_key=\"scores\", target_key=\"targets\"),\n        dl.trevskycallback(input_key=\"scores\", target_key=\"targets\", alpha=0.2),\n    ],\n    logdir=\"./logdir\",\n    valid_loader=\"valid\",\n    valid_metric=\"loss\",\n    minimize_valid_metric=true,\n    verbose=true,\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>cv - mnist metric learning</summary>\n<p>\n\n```python\nimport os\nfrom torch.optim import adam\nfrom torch.utils.data import dataloader\nfrom catalyst import dl\nfrom catalyst.contrib.data import hardtripletssampler\nfrom catalyst.contrib.datasets import mnistmldataset, mnistqgdataset\nfrom catalyst.contrib.losses import tripletmarginlosswithsampler\nfrom catalyst.contrib.models import mnistsimplenet\nfrom catalyst.data.sampler import batchbalanceclasssampler\n\n\n# 1. train and valid loaders\ntrain_dataset = mnistmldataset(root=os.getcwd())\nsampler = batchbalanceclasssampler(\n    labels=train_dataset.get_labels(), num_classes=5, num_samples=10, num_batches=10\n)\ntrain_loader = dataloader(dataset=train_dataset, batch_sampler=sampler)\n\nvalid_dataset = mnistqgdataset(root=os.getcwd(), gallery_fraq=0.2)\nvalid_loader = dataloader(dataset=valid_dataset, batch_size=1024)\n\n# 2. model and optimizer\nmodel = mnistsimplenet(out_features=16)\noptimizer = adam(model.parameters(), lr=0.001)\n\n# 3. criterion with triplets sampling\nsampler_inbatch = hardtripletssampler(norm_required=false)\ncriterion = tripletmarginlosswithsampler(margin=0.5, sampler_inbatch=sampler_inbatch)\n\n# 4. training with catalyst runner\nclass customrunner(dl.supervisedrunner):\n    def handle_batch(self, batch) -> none:\n        if self.is_train_loader:\n            images, targets = batch[\"features\"].float(), batch[\"targets\"].long()\n            features = self.model(images)\n            self.batch = {\"embeddings\": features, \"targets\": targets,}\n        else:\n            images, targets, is_query = \\\n                batch[\"features\"].float(), batch[\"targets\"].long(), batch[\"is_query\"].bool()\n            features = self.model(images)\n            self.batch = {\"embeddings\": features, \"targets\": targets, \"is_query\": is_query}\n\ncallbacks = [\n    dl.controlflowcallbackwrapper(\n        dl.criterioncallback(input_key=\"embeddings\", target_key=\"targets\", metric_key=\"loss\"),\n        loaders=\"train\",\n    ),\n    dl.controlflowcallbackwrapper(\n        dl.cmcscorecallback(\n            embeddings_key=\"embeddings\",\n            labels_key=\"targets\",\n            is_query_key=\"is_query\",\n            topk=[1],\n        ),\n        loaders=\"valid\",\n    ),\n    dl.periodicloadercallback(\n        valid_loader_key=\"valid\", valid_metric_key=\"cmc01\", minimize=false, valid=2\n    ),\n]\n\nrunner = customrunner(input_key=\"features\", output_key=\"embeddings\")\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    callbacks=callbacks,\n    loaders={\"train\": train_loader, \"valid\": valid_loader},\n    verbose=false,\n    logdir=\"./logs\",\n    valid_loader=\"valid\",\n    valid_metric=\"cmc01\",\n    minimize_valid_metric=false,\n    num_epochs=10,\n)\n```\n</p>\n</details>\n\n\n<details>\n<summary>cv - mnist gan</summary>\n<p>\n\n```python\nimport os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import dataloader\nfrom catalyst import dl\nfrom catalyst.contrib.datasets import mnist\nfrom catalyst.contrib.layers import globalmaxpool2d, lambda\n\nlatent_dim = 128\ngenerator = nn.sequential(\n    # we want to generate 128 coefficients to reshape into a 7x7x128 map\n    nn.linear(128, 128 * 7 * 7),\n    nn.leakyrelu(0.2, inplace=true),\n    lambda(lambda x: x.view(x.size(0), 128, 7, 7)),\n    nn.convtranspose2d(128, 128, (4, 4), stride=(2, 2), padding=1),\n    nn.leakyrelu(0.2, inplace=true),\n    nn.convtranspose2d(128, 128, (4, 4), stride=(2, 2), padding=1),\n    nn.leakyrelu(0.2, inplace=true),\n    nn.conv2d(128, 1, (7, 7), padding=3),\n    nn.sigmoid(),\n)\ndiscriminator = nn.sequential(\n    nn.conv2d(1, 64, (3, 3), stride=(2, 2), padding=1),\n    nn.leakyrelu(0.2, inplace=true),\n    nn.conv2d(64, 128, (3, 3), stride=(2, 2), padding=1),\n    nn.leakyrelu(0.2, inplace=true),\n    globalmaxpool2d(),\n    nn.flatten(),\n    nn.linear(128, 1),\n)\n\nmodel = nn.moduledict({\"generator\": generator, \"discriminator\": discriminator})\ncriterion = {\"generator\": nn.bcewithlogitsloss(), \"discriminator\": nn.bcewithlogitsloss()}\noptimizer = {\n    \"generator\": torch.optim.adam(generator.parameters(), lr=0.0003, betas=(0.5, 0.999)),\n    \"discriminator\": torch.optim.adam(discriminator.parameters(), lr=0.0003, betas=(0.5, 0.999)),\n}\ntrain_data = mnist(os.getcwd(), train=false)\nloaders = {\"train\": dataloader(train_data, batch_size=32)}\n\nclass customrunner(dl.runner):\n    def predict_batch(self, batch):\n        batch_size = 1\n        # sample random points in the latent space\n        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.engine.device)\n        # decode them to fake images\n        generated_images = self.model[\"generator\"](random_latent_vectors).detach()\n        return generated_images\n\n    def handle_batch(self, batch):\n        real_images, _ = batch\n        batch_size = real_images.shape[0]\n\n        # sample random points in the latent space\n        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.engine.device)\n\n        # decode them to fake images\n        generated_images = self.model[\"generator\"](random_latent_vectors).detach()\n        # combine them with real images\n        combined_images = torch.cat([generated_images, real_images])\n\n        # assemble labels discriminating real from fake images\n        labels = \\\n            torch.cat([torch.ones((batch_size, 1)), torch.zeros((batch_size, 1))]).to(self.engine.device)\n        # add random noise to the labels - important trick!\n        labels += 0.05 * torch.rand(labels.shape).to(self.engine.device)\n\n        # discriminator forward\n        combined_predictions = self.model[\"discriminator\"](combined_images)\n\n        # sample random points in the latent space\n        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.engine.device)\n        # assemble labels that say \"all real images\"\n        misleading_labels = torch.zeros((batch_size, 1)).to(self.engine.device)\n\n        # generator forward\n        generated_images = self.model[\"generator\"](random_latent_vectors)\n        generated_predictions = self.model[\"discriminator\"](generated_images)\n\n        self.batch = {\n            \"combined_predictions\": combined_predictions,\n            \"labels\": labels,\n            \"generated_predictions\": generated_predictions,\n            \"misleading_labels\": misleading_labels,\n        }\n\n\nrunner = customrunner()\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    callbacks=[\n        dl.criterioncallback(\n            input_key=\"combined_predictions\",\n            target_key=\"labels\",\n            metric_key=\"loss_discriminator\",\n            criterion_key=\"discriminator\",\n        ),\n        dl.backwardcallback(metric_key=\"loss_discriminator\"),\n        dl.optimizercallback(\n            optimizer_key=\"discriminator\",\n            metric_key=\"loss_discriminator\",\n        ),\n        dl.criterioncallback(\n            input_key=\"generated_predictions\",\n            target_key=\"misleading_labels\",\n            metric_key=\"loss_generator\",\n            criterion_key=\"generator\",\n        ),\n        dl.backwardcallback(metric_key=\"loss_generator\"),\n        dl.optimizercallback(\n            optimizer_key=\"generator\",\n            metric_key=\"loss_generator\",\n        ),\n    ],\n    valid_loader=\"train\",\n    valid_metric=\"loss_generator\",\n    minimize_valid_metric=true,\n    num_epochs=20,\n    verbose=true,\n    logdir=\"./logs_gan\",\n)\n\n# visualization (matplotlib required):\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n# plt.imshow(runner.predict_batch(none)[0, 0].cpu().numpy())\n```\n</p>\n</details>\n\n\n<details>\n<summary>cv - mnist vae</summary>\n<p>\n\n```python\nimport os\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as f\nfrom torch.utils.data import dataloader\nfrom catalyst import dl, metrics\nfrom catalyst.contrib.datasets import mnist\n\nlog_scale_max = 2\nlog_scale_min = -10\n\ndef normal_sample(loc, log_scale):\n    scale = torch.exp(0.5 * log_scale)\n    return loc + scale * torch.randn_like(scale)\n\nclass vae(nn.module):\n    def __init__(self, in_features, hid_features):\n        super().__init__()\n        self.hid_features = hid_features\n        self.encoder = nn.linear(in_features, hid_features * 2)\n        self.decoder = nn.sequential(nn.linear(hid_features, in_features), nn.sigmoid())\n\n    def forward(self, x, deterministic=false):\n        z = self.encoder(x)\n        bs, z_dim = z.shape\n\n        loc, log_scale = z[:, : z_dim // 2], z[:, z_dim // 2 :]\n        log_scale = torch.clamp(log_scale, log_scale_min, log_scale_max)\n\n        z_ = loc if deterministic else normal_sample(loc, log_scale)\n        z_ = z_.view(bs, -1)\n        x_ = self.decoder(z_)\n\n        return x_, loc, log_scale\n\nclass customrunner(dl.irunner):\n    def __init__(self, hid_features, logdir, engine):\n        super().__init__()\n        self.hid_features = hid_features\n        self._logdir = logdir\n        self._engine = engine\n\n    def get_engine(self):\n        return self._engine\n\n    def get_loggers(self):\n        return {\n            \"console\": dl.consolelogger(),\n            \"csv\": dl.csvlogger(logdir=self._logdir),\n            \"tensorboard\": dl.tensorboardlogger(logdir=self._logdir),\n        }\n\n    @property\n    def num_epochs(self) -> int:\n        return 1\n\n    def get_loaders(self):\n        loaders = {\n            \"train\": dataloader(mnist(os.getcwd(), train=false), batch_size=32),\n            \"valid\": dataloader(mnist(os.getcwd(), train=false), batch_size=32),\n        }\n        return loaders\n\n    def get_model(self):\n        model = self.model if self.model is not none else vae(28 * 28, self.hid_features)\n        return model\n\n    def get_optimizer(self, model):\n        return optim.adam(model.parameters(), lr=0.02)\n\n    def get_callbacks(self):\n        return {\n            \"backward\": dl.backwardcallback(metric_key=\"loss\"),\n            \"optimizer\": dl.optimizercallback(metric_key=\"loss\"),\n            \"checkpoint\": dl.checkpointcallback(\n                self._logdir,\n                loader_key=\"valid\",\n                metric_key=\"loss\",\n                minimize=true,\n                topk=3,\n            ),\n        }\n\n    def on_loader_start(self, runner):\n        super().on_loader_start(runner)\n        self.meters = {\n            key: metrics.additivemetric(compute_on_call=false)\n            for key in [\"loss_ae\", \"loss_kld\", \"loss\"]\n        }\n\n    def handle_batch(self, batch):\n        x, _ = batch\n        x = x.view(x.size(0), -1)\n        x_, loc, log_scale = self.model(x, deterministic=not self.is_train_loader)\n\n        loss_ae = f.mse_loss(x_, x)\n        loss_kld = (\n            -0.5 * torch.sum(1 + log_scale - loc.pow(2) - log_scale.exp(), dim=1)\n        ).mean()\n        loss = loss_ae + loss_kld * 0.01\n\n        self.batch_metrics = {\"loss_ae\": loss_ae, \"loss_kld\": loss_kld, \"loss\": loss}\n        for key in [\"loss_ae\", \"loss_kld\", \"loss\"]:\n            self.meters[key].update(self.batch_metrics[key].item(), self.batch_size)\n\n    def on_loader_end(self, runner):\n        for key in [\"loss_ae\", \"loss_kld\", \"loss\"]:\n            self.loader_metrics[key] = self.meters[key].compute()[0]\n        super().on_loader_end(runner)\n\n    def predict_batch(self, batch):\n        random_latent_vectors = torch.randn(1, self.hid_features).to(self.engine.device)\n        generated_images = self.model.decoder(random_latent_vectors).detach()\n        return generated_images\n\nrunner = customrunner(128, \"./logs\", dl.cpuengine())\nrunner.run()\n# visualization (matplotlib required):\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n# plt.imshow(runner.predict_batch(none)[0].cpu().numpy().reshape(28, 28))\n```\n</p>\n</details>\n\n\n<details>\n<summary>automl - hyperparameters optimization with optuna</summary>\n<p>\n\n```python\nimport os\nimport optuna\nimport torch\nfrom torch import nn\nfrom torch.utils.data import dataloader\nfrom catalyst import dl\nfrom catalyst.contrib.datasets import mnist\n\n\ndef objective(trial):\n    lr = trial.suggest_loguniform(\"lr\", 1e-3, 1e-1)\n    num_hidden = int(trial.suggest_loguniform(\"num_hidden\", 32, 128))\n\n    train_data = mnist(os.getcwd(), train=true)\n    valid_data = mnist(os.getcwd(), train=false)\n    loaders = {\n        \"train\": dataloader(train_data, batch_size=32),\n        \"valid\": dataloader(valid_data, batch_size=32),\n    }\n    model = nn.sequential(\n        nn.flatten(), nn.linear(784, num_hidden), nn.relu(), nn.linear(num_hidden, 10)\n    )\n    optimizer = torch.optim.adam(model.parameters(), lr=lr)\n    criterion = nn.crossentropyloss()\n\n    runner = dl.supervisedrunner(input_key=\"features\", output_key=\"logits\", target_key=\"targets\")\n    runner.train(\n        model=model,\n        criterion=criterion,\n        optimizer=optimizer,\n        loaders=loaders,\n        callbacks={\n            \"accuracy\": dl.accuracycallback(\n                input_key=\"logits\", target_key=\"targets\", num_classes=10\n            ),\n            # catalyst[optuna] required ``pip install catalyst[optuna]``\n            \"optuna\": dl.optunapruningcallback(\n                loader_key=\"valid\", metric_key=\"accuracy01\", minimize=false, trial=trial\n            ),\n        },\n        num_epochs=3,\n    )\n    score = trial.best_score\n    return score\n\nstudy = optuna.create_study(\n    direction=\"maximize\",\n    pruner=optuna.pruners.medianpruner(\n        n_startup_trials=1, n_warmup_steps=0, interval_steps=1\n    ),\n)\nstudy.optimize(objective, n_trials=3, timeout=300)\nprint(study.best_value, study.best_params)\n```\n</p>\n</details>\n\n<details>\n<summary>config api - minimal example</summary>\n<p>\n\n```yaml title=\"example.yaml\"\nrunner:\n  _target_: catalyst.runners.supervisedrunner\n  model:\n    _var_: model\n    _target_: torch.nn.sequential\n    args:\n      - _target_: torch.nn.flatten\n      - _target_: torch.nn.linear\n        in_features: 784  # 28 * 28\n        out_features: 10\n  input_key: features\n  output_key: &output_key logits\n  target_key: &target_key targets\n  loss_key: &loss_key loss\n\nrun:\n  # \u2248 stage 1\n  - _call_: train  # runner.train(...)\n\n    criterion:\n      _target_: torch.nn.crossentropyloss\n\n    optimizer:\n      _target_: torch.optim.adam\n      params:  # model.parameters()\n        _var_: model.parameters\n      lr: 0.02\n\n    loaders:\n      train:\n        _target_: torch.utils.data.dataloader\n        dataset:\n          _target_: catalyst.contrib.datasets.mnist\n          root: data\n          train: y\n        batch_size: 32\n\n      &valid_loader_key valid:\n        &valid_loader\n        _target_: torch.utils.data.dataloader\n        dataset:\n          _target_: catalyst.contrib.datasets.mnist\n          root: data\n          train: n\n        batch_size: 32\n\n    callbacks:\n      - &accuracy_metric\n        _target_: catalyst.callbacks.accuracycallback\n        input_key: *output_key\n        target_key: *target_key\n        topk: [1,3,5]\n      - _target_: catalyst.callbacks.precisionrecallf1supportcallback\n        input_key: *output_key\n        target_key: *target_key\n\n    num_epochs: 1\n    logdir: logs\n    valid_loader: *valid_loader_key\n    valid_metric: *loss_key\n    minimize_valid_metric: y\n    verbose: y\n\n  # \u2248 stage 2\n  - _call_: evaluate_loader  # runner.evaluate_loader(...)\n    loader: *valid_loader\n    callbacks:\n      - *accuracy_metric\n\n```\n\n```sh\ncatalyst-run --config example.yaml\n```\n</p>\n</details>\n\n### tests\nall catalyst code, features, and pipelines [are fully tested](./tests).\nwe also have our own [catalyst-codestyle](https://github.com/catalyst-team/codestyle) and a corresponding pre-commit hook.\nduring testing, we train a variety of different models: image classification,\nimage segmentation, text classification, gans, and much more.\nwe then compare their convergence metrics in order to verify\nthe correctness of the training procedure and its reproducibility.\nas a result, catalyst provides fully tested and reproducible\nbest practices for your deep learning research and development.\n\n### [blog posts](https://catalyst-team.com/post/)\n\n### [talks](https://catalyst-team.com/talk/)\n\n\n## community\n\n### accelerated with catalyst\n\n<details>\n<summary>research papers</summary>\n<p>\n\n- [hierarchical attention for sentiment classification with visualization](https://github.com/neuromation/ml-recipe-hier-attention)\n- [pediatric bone age assessment](https://github.com/neuromation/ml-recipe-bone-age)\n- [implementation of the paper \"tell me where to look: guided attention inference network\"](https://github.com/ngxbac/gain)\n- [implementation of the paper \"filter response normalization layer: eliminating batch dependence in the training of deep neural networks\"](https://github.com/yukkyo/pytorch-filterresponsenormalizationlayer)\n- [implementation of the paper \"utterance-level aggregation for speaker recognition in the wild\"](https://github.com/ptjexio/speaker-recognition)\n- [implementation of the paper \"looking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation\"](https://github.com/vitrioil/speech-separation)\n- [implementation of the paper \"esrgan: enhanced super-resolution generative adversarial networks\"](https://github.com/leverxgroup/esrgan)\n\n</p>\n</details>\n\n<details>\n<summary>blog posts</summary>\n<p>\n\n- [solving the cocktail party problem using pytorch](https://medium.com/pytorch/addressing-the-cocktail-party-problem-using-pytorch-305fb74560ea)\n- [beyond fashion: deep learning with catalyst (config api)](https://evilmartians.com/chronicles/beyond-fashion-deep-learning-with-catalyst)\n- [tutorial from notebook api to config api (ru)](https://github.com/bekovmi/segmentation_tutorial)\n\n</p>\n</details>\n\n<details>\n<summary>competitions</summary>\n<p>\n\n- [kaggle quick, draw! doodle recognition challenge](https://github.com/ngxbac/kaggle-quickdraw) - 11th place\n- [catalyst.rl - neurips 2018: ai for prosthetics challenge](https://github.com/scitator/neurips-18-prosthetics-challenge) \u2013 3rd place\n- [kaggle google landmark 2019](https://github.com/ngxbac/kaggle-google-landmark-2019) - 30th place\n- [imet collection 2019 - fgvc6](https://github.com/ngxbac/kaggle-imet) - 24th place\n- [id r&d anti-spoofing challenge](https://github.com/bagxi/idrnd-anti-spoofing-challenge-solution) - 14th place\n- [neurips 2019: recursion cellular image classification](https://github.com/ngxbac/kaggle-recursion-cellular) - 4th place\n- [miccai 2019: automatic structure segmentation for radiotherapy planning challenge 2019](https://github.com/ngxbac/structseg2019)\n  * 3rd place solution for `task 3: organ-at-risk segmentation from chest ct scans`\n  * and 4th place solution for `task 4: gross target volume segmentation of lung cancer`\n- [kaggle seversteal steel detection](https://github.com/bamps53/kaggle-severstal) - 5th place\n- [rsna intracranial hemorrhage detection](https://github.com/ngxbac/kaggle-rsna) - 5th place\n- [aptos 2019 blindness detection](https://github.com/bloodaxe/kaggle-2019-blindness-detection) \u2013 7th place\n- [catalyst.rl - neurips 2019: learn to move - walk around](https://github.com/scitator/run-skeleton-run-in-3d) \u2013 2nd place\n- [xview2 damage assessment challenge](https://github.com/bloodaxe/xview2-solution) - 3rd place\n\n\n</p>\n</details>\n\n<details>\n<summary>toolkits</summary>\n<p>\n\n- [catalyst.rl](https://github.com/scitator/catalyst-rl-framework) \u2013 a distributed framework for reproducible rl research by [scitator](https://github.com/scitator)\n- [catalyst.classification](https://github.com/catalyst-team/classification) - comprehensive classification pipeline with pseudo-labeling by [bagxi](https://github.com/bagxi) and [pdanilov](https://github.com/pdanilov)\n- [catalyst.segmentation](https://github.com/catalyst-team/segmentation) - segmentation pipelines - binary, semantic and instance, by [bagxi](https://github.com/bagxi)\n- [catalyst.detection](https://github.com/catalyst-team/detection) - anchor-free detection pipeline by [avi2011class](https://github.com/avi2011class) and [tezromach](https://github.com/tezromach)\n- [catalyst.gan](https://github.com/catalyst-team/gan) - reproducible gans pipelines by [asmekal](https://github.com/asmekal)\n- [catalyst.neuro](https://github.com/catalyst-team/neuro) - brain image analysis project, in collaboration with [trends center](https://trendscenter.org)\n- [mlcomp](https://github.com/catalyst-team/mlcomp) \u2013 distributed dag framework for machine learning with ui by [lightforever](https://github.com/lightforever)\n- [pytorch toolbelt](https://github.com/bloodaxe/pytorch-toolbelt) - pytorch extensions for fast r&d prototyping and kaggle farming by [bloodaxe](https://github.com/bloodaxe)\n- [helper functions](https://github.com/ternaus/iglovikov_helper_functions) - an assorted collection of helper functions by [ternaus](https://github.com/ternaus)\n- [bert distillation with catalyst](https://github.com/elephantmipt/bert-distillation) by [elephantmipt](https://github.com/elephantmipt)\n\n</p>\n</details>\n\n\n<details>\n<summary>other</summary>\n<p>\n\n- [camvid segmentation example](https://github.com/bloodaxe/catalyst-camvid-segmentation-example) - example of semantic segmentation for camvid dataset\n- [notebook api tutorial for segmentation in understanding clouds from satellite images competition](https://www.kaggle.com/artgor/segmentation-in-pytorch-using-convenient-tools/)\n- [catalyst.rl - neurips 2019: learn to move - walk around](https://github.com/scitator/learning-to-move-starter-kit) \u2013 starter kit\n- [catalyst.rl - neurips 2019: animal-ai olympics](https://github.com/scitator/animal-olympics-starter-kit) - starter kit\n- [inria segmentation example](https://github.com/bloodaxe/catalyst-inria-segmentation-example) - an example of training segmentation model for inria sattelite segmentation challenge\n- [iglovikov_segmentation](https://github.com/ternaus/iglovikov_segmentation) - semantic segmentation pipeline using catalyst\n- [logging catalyst runs to comet](https://colab.research.google.com/drive/1tag27hcmh2jyrkbgsqrxligufshvycq6?usp=sharing) - an example of how to log metrics, hyperparameters and more from catalyst runs to [comet](https://www.comet.ml/site/data-scientists/)\n\n</p>\n</details>\n\n\nsee other projects at [the github dependency graph](https://github.com/catalyst-team/catalyst/network/dependents).\n\nif your project implements a paper,\na notable use-case/tutorial, or a kaggle competition solution, or\nif your code simply presents interesting results and uses catalyst,\nwe would be happy to add your project to the list above!\ndo not hesitate to send us a pr with a brief description of the project similar to the above.\n\n### contribution guide\n\nwe appreciate all contributions.\nif you are planning to contribute back bug-fixes, there is no need to run that by us; just send a pr.\nif you plan to contribute new features, new utility functions, or extensions,\nplease open an issue first and discuss it with us.\n\n- please see the [contribution guide](contributing.md) for more information.\n- by participating in this project, you agree to abide by its [code of conduct](code_of_conduct.md).\n\n\n### user feedback\n\nwe've created `feedback@catalyst-team.com` as an additional channel for user feedback.\n\n- if you like the project and want to thank us, this is the right place.\n- if you would like to start a collaboration between your team and catalyst team to improve deep learning r&d, you are always welcome.\n- if you don't like github issues and prefer email, feel free to email us.\n- finally, if you do not like something, please, share it with us, and we can see how to improve it.\n\nwe appreciate any type of feedback. thank you!\n\n\n### acknowledgments\n\nsince the beginning of the \u0441atalyst development, a lot of people have influenced it in a lot of different ways.\n\n#### catalyst.team\n- [dmytro doroshenko](https://www.linkedin.com/in/dmytro-doroshenko-05671112a/) ([ditwoo](https://github.com/ditwoo))\n- [eugene kachan](https://www.linkedin.com/in/yauheni-kachan/) ([bagxi](https://github.com/bagxi))\n- [nikita balagansky](https://www.linkedin.com/in/nikita-balagansky-50414a19a/) ([elephantmipt](https://github.com/elephantmipt))\n- [sergey kolesnikov](https://www.scitator.com/) ([scitator](https://github.com/scitator))\n\n#### catalyst.contributors\n- [aleksey grinchuk](https://www.facebook.com/grinchuk.alexey) ([alexgrinch](https://github.com/alexgrinch))\n- [aleksey shabanov](https://linkedin.com/in/aleksey-shabanov-96b351189) ([alekseysh](https://github.com/alekseysh))\n- [alex gaziev](https://www.linkedin.com/in/alexgaziev/) ([gazay](https://github.com/gazay))\n- [andrey zharkov](https://www.linkedin.com/in/andrey-zharkov-8554a1153/) ([asmekal](https://github.com/asmekal))\n- [artem zolkin](https://www.linkedin.com/in/artem-zolkin-b5155571/) ([arquestro](https://github.com/arquestro))\n- [david kuryakin](https://www.linkedin.com/in/dkuryakin/) ([dkuryakin](https://github.com/dkuryakin))\n- [evgeny semyonov](https://www.linkedin.com/in/ewan-semyonov/) ([lightforever](https://github.com/lightforever))\n- [eugene khvedchenya](https://www.linkedin.com/in/cvtalks/) ([bloodaxe](https://github.com/bloodaxe))\n- [ivan stepanenko](https://www.facebook.com/istepanenko)\n- [julia shenshina](https://github.com/julia-shenshina) ([julia-shenshina](https://github.com/julia-shenshina))\n- [nguyen xuan bac](https://www.linkedin.com/in/bac-nguyen-xuan-70340b66/) ([ngxbac](https://github.com/ngxbac))\n- [roman tezikov](http://linkedin.com/in/roman-tezikov/) ([tezromach](https://github.com/tezromach))\n- [valentin khrulkov](https://www.linkedin.com/in/vkhrulkov/) ([khrulkovv](https://github.com/khrulkovv))\n- [vladimir iglovikov](https://www.linkedin.com/in/iglovikov/) ([ternaus](https://github.com/ternaus))\n- [vsevolod poletaev](https://linkedin.com/in/vsevolod-poletaev-468071165) ([hexfaker](https://github.com/hexfaker))\n- [yury kashnitsky](https://www.linkedin.com/in/kashnitskiy/) ([yorko](https://github.com/yorko))\n\n\n### trusted by\n- [awecom](https://www.awecom.com)\n- researchers at the [center for translational research in neuroimaging and data science (trends)](https://trendscenter.org)\n- [deep learning school](https://en.dlschool.org)\n- researchers at [emory university](https://www.emory.edu)\n- [evil martians](https://evilmartians.com)\n- researchers at the [georgia institute of technology](https://www.gatech.edu)\n- researchers at [georgia state university](https://www.gsu.edu)\n- [helios](http://helios.to)\n- [hpcd lab](https://www.hpcdlab.com)\n- [ifarm](https://ifarmproject.com)\n- [kinoplan](http://kinoplan.io/)\n- researchers at the [moscow institute of physics and technology](https://mipt.ru/english/)\n- [neuromation](https://neuromation.io)\n- [poteha labs](https://potehalabs.com/en/)\n- [provectus](https://provectus.com)\n- researchers at the [skolkovo institute of science and technology](https://www.skoltech.ru/en)\n- [softconstruct](https://www.softconstruct.io/)\n- researchers at [tinkoff](https://www.tinkoff.ru/eng/)\n- researchers at [yandex.research](https://research.yandex.com)\n\n\n### citation\n\nplease use this bibtex if you want to cite this repository in your publications:\n\n    @misc{catalyst,\n        author = {kolesnikov, sergey},\n        title = {catalyst - accelerated deep learning r&d},\n        year = {2018},\n        publisher = {github},\n        journal = {github repository},\n        howpublished = {\\url{https://github.com/catalyst-team/catalyst}},\n    }\n\n\n",
  "docs_url": null,
  "keywords": "machine learning,distributed computing,deep learning,reinforcement learning,computer vision,natural language processing,recommendation systems,information retrieval,pytorch",
  "license": "apache license 2.0",
  "name": "catalyst",
  "package_url": "https://pypi.org/project/catalyst/",
  "project_url": "https://pypi.org/project/catalyst/",
  "project_urls": {
    "Bug Tracker": "https://github.com/catalyst-team/catalyst/issues",
    "Documentation": "https://catalyst-team.github.io/catalyst",
    "Download": "https://github.com/catalyst-team/catalyst",
    "Homepage": "https://github.com/catalyst-team/catalyst",
    "Source Code": "https://github.com/catalyst-team/catalyst"
  },
  "release_url": "https://pypi.org/project/catalyst/22.4/",
  "requires_dist": [
    "numpy (>=1.18)",
    "torch (>=1.4.0)",
    "accelerate (>=0.5.1)",
    "hydra-slayer (>=0.4.0)",
    "tqdm (>=4.33.0)",
    "tensorboardX (>=2.1.0)",
    "imageio (>=2.5.0) ; extra == 'all'",
    "opencv-python-headless (>=4.2.0.32) ; extra == 'all'",
    "scikit-image (<0.19.0>=0.16.1) ; extra == 'all'",
    "torchvision (>=0.5.0) ; extra == 'all'",
    "Pillow (>=6.1) ; extra == 'all'",
    "requests ; extra == 'all'",
    "scipy (>=1.4.1) ; extra == 'all'",
    "matplotlib (>=3.1.0) ; extra == 'all'",
    "pandas (>=1.0.0) ; extra == 'all'",
    "scikit-learn (>=1.0) ; extra == 'all'",
    "optuna (>=2.0.0) ; extra == 'all'",
    "comet-ml ; extra == 'comet'",
    "imageio (>=2.5.0) ; extra == 'cv'",
    "opencv-python-headless (>=4.2.0.32) ; extra == 'cv'",
    "scikit-image (<0.19.0>=0.16.1) ; extra == 'cv'",
    "torchvision (>=0.5.0) ; extra == 'cv'",
    "Pillow (>=6.1) ; extra == 'cv'",
    "requests ; extra == 'cv'",
    "deepspeed (>=0.4.0) ; extra == 'deepspeed'",
    "pytest ; extra == 'dev'",
    "sphinx (==2.2.1) ; extra == 'dev'",
    "Jinja2 (<=3.0.3) ; extra == 'dev'",
    "docutils (==0.17.1) ; extra == 'dev'",
    "mock (==3.0.5) ; extra == 'dev'",
    "catalyst-codestyle (==21.09.2) ; extra == 'dev'",
    "black (==21.8b0) ; extra == 'dev'",
    "click (<=8.0.4) ; extra == 'dev'",
    "catalyst-sphinx-theme (==1.2.0) ; extra == 'dev'",
    "tomlkit (==0.7.2) ; extra == 'dev'",
    "pre-commit (==2.13.0) ; extra == 'dev'",
    "path ; extra == 'dev'",
    "scipy (>=1.4.1) ; extra == 'ml'",
    "matplotlib (>=3.1.0) ; extra == 'ml'",
    "pandas (>=1.0.0) ; extra == 'ml'",
    "scikit-learn (>=1.0) ; extra == 'ml'",
    "mlflow ; extra == 'mlflow'",
    "neptune-client (>=0.9.8) ; extra == 'neptune'",
    "onnx ; extra == 'onnx'",
    "onnxruntime ; extra == 'onnx'",
    "onnx ; extra == 'onnx-gpu'",
    "onnxruntime-gpu ; extra == 'onnx-gpu'",
    "optuna (>=2.0.0) ; extra == 'optuna'",
    "torch-tb-profiler ; extra == 'profiler'",
    "wandb ; extra == 'wandb'"
  ],
  "requires_python": ">=3.7.0",
  "summary": "catalyst. accelerated deep learning r&d with pytorch.",
  "version": "22.4",
  "releases": [],
  "developers": [
    "scitator@gmail.com",
    "sergey_kolesnikov"
  ],
  "kwds": "catalyst catalyst_logo catalyst_team catalystteam supervisedrunner",
  "license_kwds": "apache license 2.0",
  "libtype": "pypi",
  "id": "pypi_catalyst",
  "homepage": "https://github.com/catalyst-team/catalyst",
  "release_count": 104,
  "dependency_ids": [
    "pypi_accelerate",
    "pypi_black",
    "pypi_catalyst_codestyle",
    "pypi_catalyst_sphinx_theme",
    "pypi_click",
    "pypi_comet_ml",
    "pypi_deepspeed",
    "pypi_docutils",
    "pypi_hydra_slayer",
    "pypi_imageio",
    "pypi_jinja2",
    "pypi_matplotlib",
    "pypi_mlflow",
    "pypi_mock",
    "pypi_neptune_client",
    "pypi_numpy",
    "pypi_onnx",
    "pypi_onnxruntime",
    "pypi_onnxruntime_gpu",
    "pypi_opencv_python_headless",
    "pypi_optuna",
    "pypi_pandas",
    "pypi_path",
    "pypi_pillow",
    "pypi_pre_commit",
    "pypi_pytest",
    "pypi_requests",
    "pypi_scikit_image",
    "pypi_scikit_learn",
    "pypi_scipy",
    "pypi_sphinx",
    "pypi_tensorboardx",
    "pypi_tomlkit",
    "pypi_torch",
    "pypi_torch_tb_profiler",
    "pypi_torchvision",
    "pypi_tqdm",
    "pypi_wandb"
  ]
}