{
  "classifiers": [
    "development status :: 4 - beta",
    "framework :: pytest",
    "intended audience :: developers",
    "license :: osi approved :: mit license",
    "programming language :: python :: 2",
    "programming language :: python :: 2.7",
    "programming language :: python :: 3",
    "programming language :: python :: 3.2",
    "programming language :: python :: 3.3",
    "programming language :: python :: 3.4",
    "programming language :: python :: 3.5",
    "programming language :: python :: 3.6",
    "topic :: software development :: libraries :: python modules",
    "topic :: software development :: testing"
  ],
  "description": "pytest-spark\n############\n\n.. image:: https://travis-ci.org/malexer/pytest-spark.svg?branch=master\n    :target: https://travis-ci.org/malexer/pytest-spark\n\npytest_ plugin to run the tests with support of pyspark (`apache spark`_).\n\nthis plugin will allow to specify spark_home directory in ``pytest.ini``\nand thus to make \"pyspark\" importable in your tests which are executed\nby pytest.\n\nyou can also define \"spark_options\" in ``pytest.ini`` to customize pyspark,\nincluding \"spark.jars.packages\" option which allows to load external\nlibraries (e.g. \"com.databricks:spark-xml\").\n\npytest-spark provides session scope fixtures ``spark_context`` and\n``spark_session`` which can be used in your tests.\n\n**note:** no need to define spark_home if you've installed pyspark using\npip (e.g. ``pip install pyspark``) - it should be already importable. in\nthis case just don't define spark_home neither in pytest\n(pytest.ini / --spark_home) nor as environment variable.\n\n\ninstall\n=======\n\n.. code-block:: shell\n\n    $ pip install pytest-spark\n\n\nusage\n=====\n\nset spark location\n------------------\n\nto run tests with required spark_home location you need to define it by\nusing one of the following methods:\n\n1. specify command line option \"--spark_home\"::\n\n    $ pytest --spark_home=/opt/spark\n\n2. add \"spark_home\" value to ``pytest.ini`` in your project directory::\n\n    [pytest]\n    spark_home = /opt/spark\n\n3. set the \"spark_home\" environment variable.\n\npytest-spark will try to import ``pyspark`` from provided location.\n\n\n.. note::\n    \"spark_home\" will be read in the specified order. i.e. you can\n    override ``pytest.ini`` value by command line option.\n\n\ncustomize spark_options\n-----------------------\n\njust define \"spark_options\" in your ``pytest.ini``, e.g.::\n\n    [pytest]\n    spark_home = /opt/spark\n    spark_options =\n        spark.app.name: my-pytest-spark-tests\n        spark.executor.instances: 1\n        spark.jars.packages: com.databricks:spark-xml_2.12:0.5.0\n\n\nusing the ``spark_context`` fixture\n-----------------------------------\n\nuse fixture ``spark_context`` in your tests as a regular pyspark fixture.\nsparkcontext instance will be created once and reused for the whole test\nsession.\n\nexample::\n\n    def test_my_case(spark_context):\n        test_rdd = spark_context.parallelize([1, 2, 3, 4])\n        # ...\n\n\nusing the ``spark_session`` fixture (spark 2.0 and above)\n---------------------------------------------------------\n\nuse fixture ``spark_session`` in your tests as a regular pyspark fixture.\na sparksession instance with hive support enabled will be created once and reused for the whole test\nsession.\n\nexample::\n\n    def test_spark_session_dataframe(spark_session):\n        test_df = spark_session.createdataframe([[1,3],[2,4]], \"a: int, b: int\")\n        # ...\n\noverriding default parameters of the ``spark_session`` fixture\n--------------------------------------------------------------\nby default ``spark_session`` will be loaded with the following configurations : \n\nexample::\n\n    {\n        'spark.app.name': 'pytest-spark',\n        'spark.default.parallelism': 1,\n        'spark.dynamicallocation.enabled': 'false',\n        'spark.executor.cores': 1,\n        'spark.executor.instances': 1,\n        'spark.io.compression.codec': 'lz4',\n        'spark.rdd.compress': 'false',\n        'spark.sql.shuffle.partitions': 1,\n        'spark.shuffle.compress': 'false',\n        'spark.sql.catalogimplementation': 'hive',\n    }\n\nyou can override some of these parameters in your ``pytest.ini``. \nfor example, removing hive support for the spark session : \n\nexample::\n\n    [pytest]\n    spark_home = /opt/spark\n    spark_options =\n        spark.sql.catalogimplementation: in-memory\n\ndevelopment\n===========\n\ntests\n-----\n\nrun tests locally::\n\n    $ docker-compose up --build\n\n\n.. _pytest: http://pytest.org/\n.. _apache spark: https://spark.apache.org/\n\n\n",
  "docs_url": null,
  "keywords": "pytest spark pyspark unittest test",
  "license": "mit",
  "name": "pytest-spark",
  "package_url": "https://pypi.org/project/pytest-spark/",
  "project_url": "https://pypi.org/project/pytest-spark/",
  "project_urls": {
    "Homepage": "https://github.com/malexer/pytest-spark"
  },
  "release_url": "https://pypi.org/project/pytest-spark/0.6.0/",
  "requires_dist": [
    "pytest",
    "findspark"
  ],
  "requires_python": "",
  "summary": "pytest plugin to run the tests with support of pyspark.",
  "version": "0.6.0",
  "releases": [],
  "developers": [
    "alex",
    "alex@markovs.me"
  ],
  "kwds": "test_spark_session_dataframe spark_session pyspark spark_options spark_context",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_pytest_spark",
  "homepage": "https://github.com/malexer/pytest-spark",
  "release_count": 15,
  "dependency_ids": [
    "pypi_findspark",
    "pypi_pytest"
  ]
}