{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: developers",
    "license :: osi approved :: gnu general public license (gpl)",
    "operating system :: os independent",
    "programming language :: python :: 2.6",
    "programming language :: python :: 2.7",
    "programming language :: python :: 3",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "\n\npylru\n=====\n\na least recently used (lru) cache for python.\n\nintroduction\n============\n\npylru implements a true lru cache along with several support classes. the cache is efficient and written in pure python. it works with python 2.6+ including the 3.x series. basic operations (lookup, insert, delete) all run in a constant amount of time. pylru provides a cache class with a simple dict interface. it also provides classes to wrap any object that has a dict interface with a cache. both write-through and write-back semantics are supported. pylru also provides classes to wrap functions in a similar way, including a function decorator.\n\nyou can install pylru or you can just copy the source file pylru.py and use it directly in your own project. the rest of this file explains what the pylru module provides and how to use it. if you want to know more examine pylru.py. the code is straightforward and well commented.\n\nusage\n=====\n\nlrucache\n--------\n\nan lrucache object has a dictionary like interface and can be used in the same way::\n\n    import pylru\n\n    size = 100          # size of the cache. the maximum number of key/value\n                        # pairs you want the cache to hold.\n    \n    cache = pylru.lrucache(size)\n                        # create a cache object.\n    \n    value = cache[key]  # lookup a value given its key.\n    cache[key] = value  # insert a key/value pair.\n    del cache[key]      # delete a value given its key.\n                        #\n                        # these three operations affect the order of the cache.\n                        # lookup and insert both move the key/value to the most\n                        # recently used position. delete (obviously) removes a\n                        # key/value from whatever position it was in.\n                        \n    key in cache        # test for membership. does not affect the cache order.\n    \n    value = cache.peek(key)\n                        # lookup a value given its key. does not affect the\n                        # cache order.\n\n    cache.keys()        # return an iterator over the keys in the cache\n    cache.values()      # return an iterator over the values in the cache\n    cache.items()       # return an iterator over the (key, value) pairs in the\n                        # cache.\n                        #\n                        # these calls have no effect on the cache order.\n                        # lrucache is scan resistant when these calls are used.\n                        # the iterators iterate over their respective elements\n                        # in the order of most recently used to least recently\n                        # used.\n                        #\n                        # warning - while these iterators do not affect the\n                        # cache order the lookup, insert, and delete operations\n                        # do. the result of changing the cache's order\n                        # during iteration is undefined. if you really need to\n                        # do something of the sort use list(cache.keys()), then\n                        # loop over the list elements.\n                        \n    for key in cache:   # caches support __iter__ so you can use them directly\n        pass            # in a for loop to loop over the keys just like\n                        # cache.keys()\n\n    cache.size()        # returns the size of the cache\n    cache.size(x)       # changes the size of the cache. x must be greater than\n                        # zero. returns the new size x.\n\n    x = len(cache)      # returns the number of items stored in the cache.\n                        # x will be less than or equal to cache.size()\n\n    cache.clear()       # remove all items from the cache.\n\n\nlrucache takes an optional callback function as a second argument. since the cache has a fixed size, some operations (such as an insertion) may cause the least recently used key/value pair to be ejected. if the optional callback function is given it will be called when this occurs. for example::\n\n    import pylru\n\n    def callback(key, value):\n        print (key, value)    # a dumb callback that just prints the key/value\n\n    size = 100\n    cache = pylru.lrucache(size, callback)\n\n    # use the cache... when it gets full some pairs may be ejected due to\n    # the fixed cache size. but, not before the callback is called to let you\n    # know.\n\nwritethroughcachemanager\n------------------------\n\noften a cache is used to speed up access to some other high latency object. for example, imagine you have a backend storage object that reads/writes from/to a remote server. let us call this object *store*. if store has a dictionary interface a cache manager class can be used to compose the store object and an lrucache. the manager object exposes a dictionary interface. the programmer can then interact with the manager object as if it were the store. the manager object takes care of communicating with the store and caching key/value pairs in the lrucache object.\n\ntwo different semantics are supported, write-through (writethroughcachemanager class) and write-back (writebackcachemanager class). with write-through, lookups from the store are cached for future lookups. insertions and deletions are updated in the cache and written through to the store immediately. write-back works the same way, but insertions are updated only in the cache. these \"dirty\" key/value pair will only be updated to the underlying store when they are ejected from the cache or when a sync is performed. the writebackcachemanager class is discussed more below. \n\nthe writethroughcachemanager class takes as arguments the store object you want to compose and the cache size. it then creates an lru cache and automatically manages it::\n\n    import pylru\n\n    size = 100\n    cached = pylru.writethroughcachemanager(store, size)\n                        # or\n    cached = pylru.lruwrap(store, size)\n                        # this is a factory function that does the same thing.\n\n    # now the object *cached* can be used just like store, except caching is\n    # automatically handled.\n    \n    value = cached[key] # lookup a value given its key.\n    cached[key] = value # insert a key/value pair.\n    del cached[key]     # delete a value given its key.\n    \n    key in cache        # test for membership. does not affect the cache order.\n\n    cached.keys()       # returns store.keys()\n    cached.values()     # returns store.values() \n    cached.items()      # returns store.items()\n                        #\n                        # these calls have no effect on the cache order.\n                        # the iterators iterate over their respective elements\n                        # in the order dictated by store.\n                        \n    for key in cached:  # same as store.keys()\n\n    cached.size()       # returns the size of the cache\n    cached.size(x)      # changes the size of the cache. x must be greater than\n                        # zero. returns the new size x.\n\n    x = len(cached)     # returns the number of items stored in the store.\n\n    cached.clear()      # remove all items from the store and cache.\n\n\nwritebackcachemanager\n---------------------\n\nsimilar to the writethroughcachemanager class except write-back semantics are used to manage the cache. the programmer is responsible for one more thing as well. they must call sync() when they are finished. this ensures that the last of the \"dirty\" entries in the cache are written back. this is not too bad as writebackcachemanager objects can be used in with statements. more about that below::\n\n\n    import pylru\n\n    size = 100\n    cached = pylru.writebackcachemanager(store, size)\n                        # or\n    cached = pylru.lruwrap(store, size, true)\n                        # this is a factory function that does the same thing.\n                        \n    value = cached[key] # lookup a value given its key.\n    cached[key] = value # insert a key/value pair.\n    del cached[key]     # delete a value given its key.\n    \n    key in cache        # test for membership. does not affect the cache order.\n\n                        \n    cached.keys()       # return an iterator over the keys in the cache/store\n    cached.values()     # return an iterator over the values in the cache/store\n    cached.items()      # return an iterator over the (key, value) pairs in the\n                        # cache/store.\n                        #\n                        # the iterators iterate over a consistent view of the\n                        # respective elements. that is, except for the order,\n                        # the elements are the same as those returned if you\n                        # first called sync() then called\n                        # store.keys()[ or values() or items()]\n                        #\n                        # these calls have no effect on the cache order.\n                        # the iterators iterate over their respective elements\n                        # in arbitrary order.\n                        #\n                        # warning - while these iterators do not effect the\n                        # cache order the lookup, insert, and delete operations\n                        # do. the results of changing the cache's order\n                        # during iteration is undefined. if you really need to\n                        # do something of the sort use list(cached.keys()),\n                        # then loop over the list elements.\n                        \n    for key in cached:  # same as cached.keys()\n\n    cached.size()       # returns the size of the cache\n    cached.size(x)      # changes the size of the cache. x must be greater than\n                        # zero. returns the new size x.\n\n    x = len(cached)     # returns the number of items stored in the store.\n                        #\n                        # warning - this method calls sync() internally. if\n                        # that has adverse performance effects for your\n                        # application, you may want to avoid calling this\n                        # method frequently.\n\n    cached.clear()      # remove all items from the store and cache.\n    \n    cached.sync()       # make the store and cache consistent. write all\n                        # cached changes to the store that have not been\n                        # yet.\n                        \n    cached.flush()      # calls sync() then clears the cache.\n    \n\nto help the programmer ensure that the final sync() is called, writebackcachemanager objects can be used in a with statement::\n\n    with pylru.writebackcachemanager(store, size) as cached:\n        # use cached just like you would store. sync() is called automatically\n        # for you when leaving the with statement block.\n\n\nfunctioncachemanager\n---------------------\n\nfunctioncachemanager allows you to compose a function with an lrucache. the resulting object can be called just like the original function, but the results are cached to speed up future calls. the function must have arguments that are hashable. functioncachemanager takes an optional callback function as a third argument::\n\n    import pylru\n\n    def square(x):\n        return x * x\n\n    size = 100\n    cached = pylru.functioncachemanager(square, size)\n\n    y = cached(7)\n\n    # the results of cached are the same as square, but automatically cached\n    # to speed up future calls.\n\n    cached.size()       # returns the size of the cache\n    cached.size(x)      # changes the size of the cache. x must be greater than\n                        # zero. returns the new size x.\n\n    cached.clear()      # remove all items from the cache.\n\n\n\nlrudecorator\n------------\n\npylru also provides a function decorator. this is basically the same functionality as functioncachemanager, but in the form of a decorator. the decorator takes an optional callback function as a second argument::\n\n    from pylru import lrudecorator\n\n    @lrudecorator(100)\n    def square(x):\n        return x * x\n\n    # the results of the square function are cached to speed up future calls.\n\n    square.size()       # returns the size of the cache\n    square.size(x)      # changes the size of the cache. x must be greater than\n                        # zero. returns the new size x.\n\n    square.clear()      # remove all items from the cache.\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "pylru",
  "package_url": "https://pypi.org/project/pylru/",
  "project_url": "https://pypi.org/project/pylru/",
  "project_urls": {
    "Homepage": "https://github.com/jlhutch/pylru"
  },
  "release_url": "https://pypi.org/project/pylru/1.2.1/",
  "requires_dist": [],
  "requires_python": "",
  "summary": "a least recently used (lru) cache implementation",
  "version": "1.2.1",
  "releases": [],
  "developers": [
    "jay_hutchinson",
    "jlhutch+pylru@gmail.com"
  ],
  "kwds": "pylru python py cache caches",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_pylru",
  "homepage": "https://github.com/jlhutch/pylru",
  "release_count": 11,
  "dependency_ids": []
}