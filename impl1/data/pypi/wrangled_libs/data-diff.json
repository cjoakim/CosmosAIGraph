{
  "classifiers": [
    "development status :: 2 - pre-alpha",
    "environment :: console",
    "intended audience :: developers",
    "intended audience :: information technology",
    "intended audience :: system administrators",
    "license :: osi approved :: mit license",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.12",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: database :: database engines/servers",
    "typing :: typed"
  ],
  "description": "<p align=\"center\">\n    <a href=\"https://datafold.com/\"><img alt=\"datafold\" src=\"https://user-images.githubusercontent.com/1799931/196497110-d3de1113-a97f-4322-b531-026d859b867a.png\" width=\"30%\" /></a>\n</p>\n\n<h2 align=\"center\">\ndata-diff: compare datasets fast, within or across sql databases\n\n![data-diff-logo](docs/data-diff-logo.png)\n</h2>\n<br>\n\n> [make sure to join us at our virtual hands-on lab series where our team walks through live how to get set-up with it!](https://www.datafold.com/virtual-hands-on-lab)\n\n# use cases\n\n## data migration & replication testing\ncompare source to target and check for discrepancies when moving data between systems:\n- migrating to a new data warehouse (e.g., oracle > snowflake)\n- converting sql to a new transformation framework (e.g., stored procedures > dbt)\n- continuously replicating data from an oltp db to olap dwh (e.g., mysql > redshift)\n\n\n## data development testing\ntest sql code and preview changes by comparing development/staging environment data to production:\n1. make a change to some sql code\n2. run the sql code to create a new dataset\n3. compare the dataset with its production version or another iteration\n\n  <p align=\"left\">\n  <img alt=\"dbt\" src=\"https://seeklogo.com/images/d/dbt-logo-e4b0ed72a2-seeklogo.com.png\" width=\"10%\" />\n  </p>\n\n<details>\n<summary> data-diff integrates with dbt core to seamlessly compare local development to production datasets\n\n </summary>\n\n![data-development-testing](docs/development_testing.png)\n\n</details>\n\n> [dbt cloud users should check out datafold's out-of-the-box deployment testing integration](https://www.datafold.com/data-deployment-testing)\n\n:eyes: **watch [4-min demo video](https://www.loom.com/share/ad3df969ba6b4298939efb2fbcc14cde)**\n\n**[get started with data-diff & dbt](https://docs.datafold.com/development_testing/open_source)**\n\nreach out on the dbt slack in [#tools-datafold](https://getdbt.slack.com/archives/c03d25a92uu) for advice and support\n\n\n# how it works\n\nwhen comparing the data, `data-diff` utilizes the resources of the underlying databases as much as possible. it has two primary modes of comparison:\n\n## `joindiff`\n- recommended for comparing data within the same database\n- uses the outer join operation to diff the rows as efficiently as possible within the same database\n- fully relies on the underlying database engine for computation\n- requires both datasets to be queryable with a single sql query\n- time complexity approximates join operation and is largely independent of the number of differences in the dataset\n\n## `hashdiff`\n- recommended for comparing datasets across different databases\n- can also be helpful in diffing very large tables with few expected differences within the same database\n- employs a divide-and-conquer algorithm based on hashing and binary search\n- can diff data across distinct database engines, e.g., postgresql <> snowflake\n- time complexity approximates count(*) operation when there are few differences\n- performance degrades when datasets have a large number of differences\n\nmore information about the algorithm and performance considerations can be found [here](https://github.com/datafold/data-diff/blob/master/docs/technical-explanation.md)\n\n# get started\n\n## validating dbt model changes between dev and prod\n\u26a1 looking to use `data-diff` in dbt development? head over to [our `data-diff` + `dbt` documentation](https://docs.datafold.com/development_testing/how_it_works) to get started!\n\n## compare data tables between databases\n\ud83d\udd00 to compare data between databases, install `data-diff` with specific database adapters, e.g.:\n\n```\npip install data-diff 'data-diff[postgresql,snowflake]' -u\n```\n\nrun `data-diff` with connection uris. in the following example, we compare tables between postgresql and snowflake using the hashdiff algorithm:\n\n```bash\ndata-diff \\\n  postgresql://<username>:'<password>'@localhost:5432/<database> \\\n  <table> \\\n  \"snowflake://<username>:<password>@<account>/<database>/<schema>?warehouse=<warehouse>&role=<role>\" \\\n  <table> \\\n  -k <primary key column> \\\n  -c <columns to compare> \\\n  -w <filter condition>\n```\n\nrun `data-diff` with a `toml` configuration file. in the following example, we compare tables between motherduck(hosted duckdb) and snowflake using the hashdiff algorithm:\n\n```toml\n## database connection ##\n[database.duckdb_connection] \n  driver = \"duckdb\"\n  # filepath = \"datafold_demo.duckdb\" # local duckdb file example\n  # filepath = \"md:\" # default motherduck connection example\n  filepath = \"md:datafold_demo?motherduck_token=${motherduck_token}\" # api token recommended for motherduck connection\n  database = \"datafold_demo\"\n\n[database.snowflake_connection]\n  driver = \"snowflake\"\n  database = \"dev\"\n  user = \"sung\"\n  password = \"${snowflake_password}\" # or \"<password_string>\"\n  # the info below is only required for snowflake\n  account = \"${account}\" # by33919\n  schema = \"development\"\n  warehouse = \"demo\"\n  role = \"demo_role\"\n\n## run parameters ##\n[run.default]\n  verbose = true\n\n## example data diff job ##\n[run.demo_xdb_diff]\n  # source 1 (\"left\")\n  1.database = \"duckdb_connection\"\n  1.table = \"development.raw_orders\"\n\n  # source 2 (\"right\")\n  2.database = \"snowflake_connection\"\n  2.table = \"raw_orders\" # note that snowflake table names are case-sensitive\n\n  verbose = false\n```\n\n```bash\n# export relevant environment variables, example below\nexport motherduck_token=<motherduck_token>\n\n# run the configured data-diff job\ndata-diff --conf datadiff.toml \\\n  --run demo_xdb_diff \\\n  -k \"id\" \\\n  -c status\n\n# output example\n- 1, completed\n+ 1, returned\n```\n\ncheck out [documentation](https://docs.datafold.com/reference/open_source/cli) for the full command reference.\n\n\n# supported databases\n\n\n| database      | status | connection string                                                                                                                   |\n|---------------|-------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|\n| postgresql >=10 |  \ud83d\udfe2   | `postgresql://<user>:<password>@<host>:5432/<database>`                                                                             |\n| mysql         |  \ud83d\udfe2     | `mysql://<user>:<password>@<hostname>:5432/<database>`                                                                              |\n| snowflake     |  \ud83d\udfe2     | `\"snowflake://<user>[:<password>]@<account>/<database>/<schema>?warehouse=<warehouse>&role=<role>[&authenticator=externalbrowser]\"` |\n| bigquery      |  \ud83d\udfe2     | `bigquery://<project>/<dataset>`                                                                                                    |\n| redshift      |  \ud83d\udfe2     | `redshift://<username>:<password>@<hostname>:5439/<database>`                                                                       |\n| duckdb        |  \ud83d\udfe2   | `duckdb://<dbname>@<filepath>`                                                                                          |\n| motherduck        |  \ud83d\udfe2   | `duckdb://<dbname>@<filepath>`                                                                                                   |\n| oracle        |  \ud83d\udfe1   | `oracle://<username>:<password>@<hostname>/servive_or_sid`                                                                          |\n| presto        |  \ud83d\udfe1   | `presto://<username>:<password>@<hostname>:8080/<database>`                                                                         |\n| databricks    |  \ud83d\udfe1   | `databricks://<http_path>:<access_token>@<server_hostname>/<catalog>/<schema>`                                                      |\n| trino         |  \ud83d\udfe1   | `trino://<username>:<password>@<hostname>:8080/<database>`                                                                          |\n| clickhouse    |  \ud83d\udfe1   | `clickhouse://<username>:<password>@<hostname>:9000/<database>`                                                                     |\n| vertica       |  \ud83d\udfe1   | `vertica://<username>:<password>@<hostname>:5433/<database>`                                                                        |\n| elasticsearch |  \ud83d\udcdd    |                                                                                                                                     |\n| planetscale   |  \ud83d\udcdd    |                                                                                                                                     |\n| pinot         |  \ud83d\udcdd    |                                                                                                                                     |\n| druid         |  \ud83d\udcdd    |                                                                                                                                     |\n| kafka         |  \ud83d\udcdd    |                                                                                                                                     |\n| sqlite        |  \ud83d\udcdd    |                                                                                                                                     |\n\n* \ud83d\udfe2: implemented and thoroughly tested.\n* \ud83d\udfe1: implemented, but not thoroughly tested yet.\n* \u23f3: implementation in progress.\n* \ud83d\udcdd: implementation planned. contributions welcome.\n\nyour database not listed here?\n\n- contribute a [new database adapter](https://github.com/datafold/data-diff/blob/master/docs/new-database-driver-guide.rst) \u2013\u00a0we accept pull requests!\n- [get in touch](https://www.datafold.com/demo) about enterprise support and adding new adapters and features\n\n\n<br>\n\n## contributors\n\nwe thank everyone who contributed so far!\n\n<a href=\"https://github.com/datafold/data-diff/graphs/contributors\">\n  <img src=\"https://contributors-img.web.app/image?repo=datafold/data-diff\" />\n</a>\n\n<br>\n\n## analytics\n\n* [usage analytics & data privacy](https://github.com/datafold/data-diff/blob/master/docs/usage_analytics.md)\n\n<br>\n\n## license\n\nthis project is licensed under the terms of the [mit license](https://github.com/datafold/data-diff/blob/master/license).\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "data-diff",
  "package_url": "https://pypi.org/project/data-diff/",
  "project_url": "https://pypi.org/project/data-diff/",
  "project_urls": {
    "Homepage": "https://github.com/datafold/data-diff",
    "Repository": "https://github.com/datafold/data-diff"
  },
  "release_url": "https://pypi.org/project/data-diff/0.10.1/",
  "requires_dist": [
    "pydantic (==1.10.12)",
    "dsnparse (<0.2.0)",
    "click (>=8.1,<9.0)",
    "rich",
    "toml (>=0.10.2,<0.11.0)",
    "mysql-connector-python (==8.0.29) ; extra == \"mysql\"",
    "psycopg2 ; extra == \"postgresql\" or extra == \"redshift\"",
    "snowflake-connector-python (>=3.0.2,<4.0.0) ; extra == \"snowflake\"",
    "cryptography ; extra == \"snowflake\"",
    "trino (>=0.314.0,<0.315.0) ; extra == \"trino\"",
    "presto-python-client ; extra == \"presto\"",
    "clickhouse-driver ; extra == \"clickhouse\"",
    "duckdb ; extra == \"duckdb\"",
    "dbt-core (>=1.0.0,<2.0.0)",
    "keyring",
    "tabulate (>=0.9.0,<0.10.0)",
    "preql (>=0.2.19,<0.3.0) ; extra == \"preql\"",
    "vertica-python ; extra == \"vertica\"",
    "urllib3 (<2)",
    "oracledb ; extra == \"oracle\"",
    "pyodbc (>=4.0.39,<5.0.0) ; extra == \"mssql\"",
    "typing-extensions (>=4.0.1)",
    "attrs (>=23.1.0,<24.0.0)",
    "mashumaro[msgpack] (>=2.9,<3.11.0)"
  ],
  "requires_python": ">=3.8.0,<4.0.0",
  "summary": "command-line tool and python library to efficiently diff rows across two different databases.",
  "version": "0.10.1",
  "releases": [],
  "developers": [
    "data-diff@datafold.com",
    "datafold"
  ],
  "kwds": "datadiff demo_xdb_diff datafold databricks migrating",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_data_diff",
  "homepage": "https://github.com/datafold/data-diff",
  "release_count": 72,
  "dependency_ids": [
    "pypi_attrs",
    "pypi_click",
    "pypi_clickhouse_driver",
    "pypi_cryptography",
    "pypi_dbt_core",
    "pypi_dsnparse",
    "pypi_duckdb",
    "pypi_keyring",
    "pypi_mashumaro",
    "pypi_mysql_connector_python",
    "pypi_oracledb",
    "pypi_preql",
    "pypi_presto_python_client",
    "pypi_psycopg2",
    "pypi_pydantic",
    "pypi_pyodbc",
    "pypi_rich",
    "pypi_snowflake_connector_python",
    "pypi_tabulate",
    "pypi_toml",
    "pypi_trino",
    "pypi_typing_extensions",
    "pypi_urllib3",
    "pypi_vertica_python"
  ]
}