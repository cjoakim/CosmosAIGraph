{
  "classifiers": [],
  "description": "microsoft coco caption evaluation\n===================\n\nevaluation codes for ms coco caption generation.\n\n## description ##\nthis repository provides python 3 support for the caption evaluation metrics used for the ms coco dataset.\n\nthe code is derived from the original repository that supports python 2.7: https://github.com/tylin/coco-caption.  \ncaption evaluation depends on the coco api that natively supports python 3.\n\n## requirements ##\n- java 1.8.0\n- python 3\n\n## installation ##\nto install pycocoevalcap and the pycocotools dependency (https://github.com/cocodataset/cocoapi), run:\n```\npip install pycocoevalcap\n```\n\n## usage ##\nsee the example script: [example/coco_eval_example.py](example/coco_eval_example.py)\n\n## files ##\n./\n- eval.py: the file includes cocoeavlcap class that can be used to evaluate results on coco.\n- tokenizer: python wrapper of stanford corenlp ptbtokenizer\n- bleu: bleu evalutation codes\n- meteor: meteor evaluation codes\n- rouge: rouge-l evaluation codes\n- cider: cider evaluation codes\n- spice: spice evaluation codes\n\n## setup ##\n\n- spice requires the download of [stanford corenlp 3.6.0](http://stanfordnlp.github.io/corenlp/index.html) code and models. this will be done automatically the first time the spice evaluation is performed.\n- note: spice will try to create a cache of parsed sentences in ./spice/cache/. this dramatically speeds up repeated evaluations. the cache directory can be moved by setting 'cache_dir' in ./spice. in the same file, caching can be turned off by removing the '-cache' argument to 'spice_cmd'.\n\n## references ##\n\n- [microsoft coco captions: data collection and evaluation server](http://arxiv.org/abs/1504.00325)\n- ptbtokenizer: we use the [stanford tokenizer](http://nlp.stanford.edu/software/tokenizer.shtml) which is included in [stanford corenlp 3.4.1](http://nlp.stanford.edu/software/corenlp.shtml).\n- bleu: [bleu: a method for automatic evaluation of machine translation](http://www.aclweb.org/anthology/p02-1040.pdf)\n- meteor: [project page](http://www.cs.cmu.edu/~alavie/meteor/) with related publications. we use the latest version (1.5) of the [code](https://github.com/mjdenkowski/meteor). changes have been made to the source code to properly aggreate the statistics for the entire corpus.\n- rouge-l: [rouge: a package for automatic evaluation of summaries](http://anthology.aclweb.org/w/w04/w04-1013.pdf)\n- cider: [cider: consensus-based image description evaluation](http://arxiv.org/pdf/1411.5726.pdf)\n- spice: [spice: semantic propositional image caption evaluation](https://arxiv.org/abs/1607.08822)\n\n## developers ##\n- xinlei chen (cmu)\n- hao fang (university of washington)\n- tsung-yi lin (cornell)\n- ramakrishna vedantam (virgina tech)\n\n## acknowledgement ##\n- david chiang (university of norte dame)\n- michael denkowski (cmu)\n- alexander rush (harvard university)\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "pycocoevalcap",
  "package_url": "https://pypi.org/project/pycocoevalcap/",
  "project_url": "https://pypi.org/project/pycocoevalcap/",
  "project_urls": {
    "Homepage": "https://github.com/salaniz/pycocoevalcap"
  },
  "release_url": "https://pypi.org/project/pycocoevalcap/1.2/",
  "requires_dist": [
    "pycocotools (>=2.0.2)"
  ],
  "requires_python": ">=3",
  "summary": "ms-coco caption evaluation for python 3",
  "version": "1.2",
  "releases": [],
  "developers": [
    "salaniz"
  ],
  "kwds": "coco_eval_example cocodataset coco cocoeavlcap captions",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_pycocoevalcap",
  "homepage": "https://github.com/salaniz/pycocoevalcap",
  "release_count": 1,
  "dependency_ids": [
    "pypi_pycocotools"
  ]
}