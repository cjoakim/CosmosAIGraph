{
  "classifiers": [],
  "description": "|banner|\n\n|ci| |pypi| |latest tag| |coverage| |slack|\n\n`website <https://www.getdaft.io>`_ \u2022 `docs <https://www.getdaft.io/projects/docs/>`_ \u2022 `installation`_ \u2022 `10-minute tour of daft <https://www.getdaft.io/projects/docs/en/latest/learn/10-min.html>`_ \u2022 `community and support <https://github.com/eventual-inc/daft/discussions>`_\n\ndaft: the distributed python dataframe for complex data\n=======================================================\n\n\n`daft <https://www.getdaft.io>`_ is a fast, pythonic and scalable open-source dataframe library built for python and machine learning workloads.\n\n  **daft is currently in its beta release phase - please expect bugs and rapid improvements to the project.**\n  **we welcome user feedback/feature requests in our** `discussions forums <https://github.com/eventual-inc/daft/discussions>`_\n\n**table of contents**\n\n* `about daft`_\n* `getting started`_\n* `benchmarks`_\n* `related projects`_\n* `license`_\n\nabout daft\n----------\n\nthe daft dataframe is a table of data with rows and columns. columns can contain any python objects, which allows daft to support rich complex data types such as images, audio, video and more.\n\n1. **any data**: beyond the usual strings/numbers/dates, daft columns can also hold complex multimodal data such as images, embeddings and python objects. ingestion and basic transformations of complex data is extremely easy and performant in daft.\n2. **notebook computing**: daft is built for the interactive developer experience on a notebook - intelligent caching/query optimizations accelerates your experimentation and data exploration.\n3. **distributed computing**: rich complex formats such as images can quickly outgrow your local laptop's computational resources - daft integrates natively with `ray <https://www.ray.io>`_ for running dataframes on large clusters of machines with thousands of cpus/gpus.\n\ngetting started\n---------------\n\ninstallation\n^^^^^^^^^^^^\n\ninstall daft with ``pip install getdaft``.\n\nfor more advanced installations (e.g. installing from source or with extra dependencies such as ray and aws utilities), please see our `installation guide <https://www.getdaft.io/projects/docs/en/latest/install.html>`_\n\nquickstart\n^^^^^^^^^^\n\n  check out our `10-minute quickstart <https://www.getdaft.io/projects/docs/en/latest/learn/10-min.html>`_!\n\nin this example, we load images from an aws s3 bucket's urls and resize each image in the dataframe:\n\n.. code:: python\n\n    import daft\n\n    # load a dataframe from filepaths in an s3 bucket\n    df = daft.from_glob_path(\"s3://daft-public-data/laion-sample-images/*\")\n\n    # 1. download column of image urls as a column of bytes\n    # 2. decode the column of bytes into a column of images\n    df = df.with_column(\"image\", df[\"path\"].url.download().image.decode())\n\n    # resize each image into 32x32\n    df = df.with_column(\"resized\", df[\"image\"].image.resize(32, 32))\n\n    df.show(3)\n\n|quickstart image|\n\n\nbenchmarks\n----------\n|benchmark image|\n\nto see the full benchmarks, detailed setup, and logs, check out our `benchmarking page. <https://www.getdaft.io/projects/docs/en/latest/benchmarks/index.html>`_\n\n\nmore resources\n^^^^^^^^^^^^^^\n\n* `10-minute tour of daft <https://www.getdaft.io/projects/docs/en/latest/learn/10-min.html>`_ - learn more about daft's full range of capabilities including dataloading from urls, joins, user-defined functions (udf), groupby, aggregations and more.\n* `user guide <https://www.getdaft.io/projects/docs/en/latest/learn/user_guides.html>`_ - take a deep-dive into each topic within daft\n* `api reference <https://www.getdaft.io/projects/docs/en/latest/api_docs/index.html>`_ - api reference for public classes/functions of daft\n\ncontributing\n------------\n\nto start contributing to daft, please read `contributing.md <https://github.com/eventual-inc/daft/blob/main/contributing.md>`_\n\ntelemetry\n---------\n\nto help improve daft, we collect non-identifiable data.\n\nto disable this behavior, set the following environment variable: ``daft_analytics_enabled=0``\n\nthe data that we collect is:\n\n1. **non-identifiable:** events are keyed by a session id which is generated on import of daft\n2. **metadata-only:** we do not collect any of our users\u2019 proprietary code or data\n3. **for development only:** we do not buy or sell any user data\n\nplease see our `documentation <https://www.getdaft.io/projects/docs/en/latest/telemetry.html>`_ for more details.\n\nrelated projects\n----------------\n\n+---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+\n| dataframe                                         | query optimizer | complex types | distributed | arrow backed    | vectorized execution engine | out-of-core |\n+===================================================+=================+===============+=============+=================+=============================+=============+\n| daft                                              | yes             | yes           | yes         | yes             | yes                         | yes         |\n+---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+\n| `pandas <https://github.com/pandas-dev/pandas>`_  | no              | python object | no          | optional >= 2.0 | some(numpy)                 | no          |\n+---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+\n| `polars <https://github.com/pola-rs/polars>`_     | yes             | python object | no          | yes             | yes                         | yes         |\n+---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+\n| `modin <https://github.com/modin-project/modin>`_ | eagar           | python object | yes         | no              | some(pandas)                | yes         |\n+---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+\n| `pyspark <https://github.com/apache/spark>`_      | yes             | no            | yes         | pandas udf/io   | pandas udf                  | yes         |\n+---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+\n| `dask df <https://github.com/dask/dask>`_         | no              | python object | yes         | no              | some(pandas)                | yes         |\n+---------------------------------------------------+-----------------+---------------+-------------+-----------------+-----------------------------+-------------+\n\ncheck out our `dataframe comparison page <https://www.getdaft.io/projects/docs/en/latest/dataframe_comparison.html>`_ for more details!\n\nlicense\n-------\n\ndaft has an apache 2.0 license - please see the license file.\n\n.. |quickstart image| image:: https://github.com/eventual-inc/daft/assets/17691182/dea2f515-9739-4f3e-ac58-cd96d51e44a8\n   :alt: dataframe code to load a folder of images from aws s3 and create thumbnails\n   :height: 256\n\n.. |benchmark image| image:: https://github-production-user-asset-6210df.s3.amazonaws.com/2550285/243524430-338e427d-f049-40b3-b555-4059d6be7bfd.png\n   :alt: benchmarks for sf100 tpch\n\n.. |banner| image:: https://user-images.githubusercontent.com/17691182/190476440-28f29e87-8e3b-41c4-9c28-e112e595f558.png\n   :target: https://www.getdaft.io\n   :alt: daft dataframes can load any data such as pdf documents, images, protobufs, csv, parquet and audio files into a table dataframe structure for easy querying\n\n.. |ci| image:: https://github.com/eventual-inc/daft/actions/workflows/python-package.yml/badge.svg\n   :target: https://github.com/eventual-inc/daft/actions/workflows/python-package.yml?query=branch:main\n   :alt: github actions tests\n\n.. |pypi| image:: https://img.shields.io/pypi/v/getdaft.svg?label=pip&logo=pypi&logocolor=white\n   :target: https://pypi.org/project/getdaft\n   :alt: pypi\n\n.. |latest tag| image:: https://img.shields.io/github/v/tag/eventual-inc/daft?label=latest&logo=github\n   :target: https://github.com/eventual-inc/daft/tags\n   :alt: latest tag\n\n.. |coverage| image:: https://codecov.io/gh/eventual-inc/daft/branch/main/graph/badge.svg?token=j430qvfe89\n   :target: https://codecov.io/gh/eventual-inc/daft\n   :alt: coverage\n\n.. |slack| image:: https://img.shields.io/badge/slack-@distdata-purple.svg?logo=slack\n   :target: https://join.slack.com/t/dist-data/shared_invite/zt-1t44ss4za-1rtsjnisqonjlf8blg05yw\n   :alt: slack community\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "getdaft",
  "package_url": "https://pypi.org/project/getdaft/",
  "project_url": "https://pypi.org/project/getdaft/",
  "project_urls": {
    "homepage": "https://www.getdaft.io",
    "repository": "https://github.com/Eventual-Inc/Daft"
  },
  "release_url": "https://pypi.org/project/getdaft/0.2.8/",
  "requires_dist": [
    "pyarrow >=6.0.1",
    "fsspec[http]",
    "psutil",
    "tqdm",
    "typing-extensions >=4.0.0 ; python_version < '3.10'",
    "pickle5 >=0.0.12 ; python_version < '3.8'",
    "getdaft[aws,azure,gcp,ray,pandas,numpy,iceberg] ; extra == 'all'",
    "s3fs ; extra == 'aws'",
    "adlfs ; extra == 'azure'",
    "gcsfs ; extra == 'gcp'",
    "pyiceberg >=0.4.0 ; extra == 'iceberg'",
    "packaging ; extra == 'iceberg'",
    "numpy ; extra == 'numpy'",
    "pandas ; extra == 'pandas'",
    "ray[data,client] >=2.0.0 ; extra == 'ray'",
    "packaging ; extra == 'ray'"
  ],
  "requires_python": ">=3.7",
  "summary": "a distributed dataframe library for large scale complex data processing.",
  "version": "0.2.8",
  "releases": [],
  "developers": [
    "daft@eventualcomputing.com",
    "jay@eventualcomputing.com",
    "sammy@eventualcomputing.com"
  ],
  "kwds": "dataframes pyspark pandas dataframe daft",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_getdaft",
  "homepage": "",
  "release_count": 54,
  "dependency_ids": [
    "pypi_adlfs",
    "pypi_fsspec",
    "pypi_gcsfs",
    "pypi_getdaft",
    "pypi_numpy",
    "pypi_packaging",
    "pypi_pandas",
    "pypi_pickle5",
    "pypi_psutil",
    "pypi_pyarrow",
    "pypi_pyiceberg",
    "pypi_ray",
    "pypi_s3fs",
    "pypi_tqdm",
    "pypi_typing_extensions"
  ]
}