{
  "classifiers": [
    "license :: osi approved :: apache software license",
    "operating system :: os independent",
    "programming language :: python :: 3",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.12",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "programming language :: python :: implementation :: cpython",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "<h1 align=\"center\">\n  <a href=\"https://www.clarifai.com/\"><img alt=\"clarifai\" title=\"clarifai\" src=\"https://upload.wikimedia.org/wikipedia/commons/b/bc/clarifai_logo_fc_web.png\"></a>\n</h1>\n\n<h2 align=\"center\">\nclarifai python sdk</a>\n</h2>\n\n\n\n<p align=\"center\">\n  <a href=\"https://discord.gg/m32v7a7a\" target=\"_blank\"> <img src=\"https://img.shields.io/discord/1145701543228735582\" alt=\"discord\">\n  </a>\n  <a href=\"https://pypi.org/project/clarifai\" target=\"_blank\"> <img src=\"https://img.shields.io/pypi/dm/clarifai\" alt=\"pypi - downloads\">\n  </a>\n</p>\n\n\n\n\nthis is the official python client for interacting with our powerful [api](https://docs.clarifai.com). the clarifai python sdk offers a comprehensive set of tools to integrate clarifai's ai platform to leverage computer vision capabilities like classification , detection ,segementation and natural language capabilities like classification , summarisation , generation , q&a ,etc into your applications. with just a few lines of code, you can leverage cutting-edge artificial intelligence to unlock valuable insights from visual and textual content.\n\n[website](https://www.clarifai.com/) | [demo](https://clarifai.com/demo) | [signup for a free account](https://clarifai.com/signup) | [api docs](https://docs.clarifai.com/) | [clarifai community](https://clarifai.com/explore) | [python sdk docs](https://docs.clarifai.com/python-sdk/api-reference) | [examples](https://github.com/clarifai/examples) | [colab notebooks](https://github.com/clarifai/colab-notebooks) | [discord](https://discord.gg/xape3vtg)\n\n\n---\n\n\n\n## table of contents\n\n* **[installation](#rocket-installation)**\n* **[getting started](#memo-getting-started)**\n* **[interacting with datasets](#floppy_disk-interacting-with-datasets)**\n* **[interacting with inputs](#floppy_disk-interacting-with-inputs)**\n  * [input upload](#input-upload)\n  * [input listing](#input-listing)\n* **[interacting with models](#brain-interacting-with-models)**\n  * [model predict](#model-predict)\n  * [model training](#model-training)\n  * [model listing](#models-listing)\n* **[interacting with workflows](#fire-interacting-with-workflows)**\n  * [workflow predict](#workflow-predict)\n  * [workflow listing](#workflows-listing)\n  * [workflow create](#workflow-create)\n  * [workflow export](#workflow-export)\n* **[search](#mag-search)**\n  * [smart image search](#smart-image-search)\n  * [smart text search](#smart-text-search)\n  * [filters](#filters)\n* **[more examples](#pushpin-more-examples)**\n\n\n\n\n\n\n\n## :rocket: installation\n\n\ninstall from pypi:\n\n```bash\npip install -u clarifai\n```\n\ninstall from source:\n\n```bash\ngit clone https://github.com/clarifai/clarifai-python.git\ncd clarifai-python\npython3 -m venv env\nsource env/bin/activate\npip3 install -r requirements.txt\n```\n\n\n\n## :memo: getting started\nclarifai uses **personal access tokens(pats)** to validate requests. you can create and manage pats under your clarifai account security settings.\n\n* \ud83d\udd17 [create pat:](https://docs.clarifai.com/clarifai-basics/authentication/personal-access-tokens/) ***log into portal &rarr; profile icon &rarr; security settings &rarr; create personal access token &rarr; set the scopes &rarr; confirm***\n\n* \ud83d\udd17 [get user id:](https://help.clarifai.com/hc/en-us/articles/4408131912727-how-do-i-find-my-user-id-app-id-and-pat-) ***log into portal &rarr; profile icon &rarr; account &rarr; profile &rarr; user-id***\n\nexport your pat as an environment variable. then, import and initialize the api client.\n\nset pat as environment variable through terminal:\n\n```cmd\nexport clarifai_pat={your personal access token}\n```\n\n```python\n# note: clarifai_pat must be set as env variable.\nfrom clarifai.client.user import user\nclient = user(user_id=\"user_id\")\n\n# get all apps\napps_generator = client.list_apps()\napps = list(apps_generator)\n```\n\nor <br>\n\npat can be passed as constructor argument\n\n```python\nfrom clarifai.client.user import user\nclient = user(user_id=\"user_id\", pat=\"your personal access token\")\n```\n\n\n## :floppy_disk: interacting with datasets\n\nclarifai datasets help in managing datasets used for model training and evaluation. it provides functionalities like creating datasets,uploading datasets and exporting datasets as .zip files.\n\n```python\n# note: clarifai_pat must be set as env variable.\n\n# create app and dataset\napp = client.create_app(app_id=\"demo_app\", base_workflow=\"universal\")\ndataset = app.create_dataset(dataset_id=\"demo_dataset\")\n\n# execute data upload to clarifai app dataset\nfrom clarifai.datasets.upload.laoders.coco_detection import cocodetectiondataloader\ncoco_dataloader = cocodetectiondataloader(\"images_dir\", \"coco_annotation_filepath\")\ndataset.upload_dataset(dataloader=coco_dataloader, get_upload_status=true)\n\n#upload text from csv\ndataset.upload_from_csv(csv_path='csv_path', input_type='text', csv_type='raw', labels=true)\n\n#upload data from folder\ndataset.upload_from_folder(folder_path='folder_path', input_type='text', labels=true)\n\n# export dataset\nfrom clarifai.client.dataset import dataset\n# note: clarifai-data-protobuf.zip is acquired through exporting datasets within the clarifai platform.\ndataset().export(save_path='output.zip', local_archive_path='clarifai-data-protobuf.zip')\n```\n\n\n\n## :floppy_disk: interacting with inputs\n\nyou can use ***inputs()*** for adding and interacting with input data. inputs can be uploaded directly from a url or a file. you can also view input annotations and concepts.\n\n#### input upload\n```python\n# note: clarifai_pat must be set as env variable.\nfrom clarifai.client.user import user\napp = user(user_id=\"user_id\").app(app_id=\"app_id\")\ninput_obj = app.inputs()\n\n#input upload from url\ninput_obj.upload_from_url(input_id = 'demo', image_url='https://samples.clarifai.com/metro-north.jpg')\n\n#input upload from filename\ninput_obj.upload_from_file(input_id = 'demo', video_file='demo.mp4')\n\n# text upload\ninput_obj.upload_text(input_id = 'demo', raw_text = 'this is a test')\n```\n\n#### input listing\n```python\n#listing inputs\ninput_generator = input_obj.list_inputs(page_no=1,per_page=10,input_type='image')\ninputs_list = list(input_generator)\n\n#listing annotations\nannotation_generator = input_obj.list_annotations(batch_input=inputs_list)\nannotations_list = list(annotation_generator)\n\n#listing concepts\nall_concepts = list(app.list_concepts())\n```\n\n\n\n## :brain: interacting with models\n\nthe **model** class allows you to perform predictions using clarifai models. you can specify which model to use by providing the model url or id. this gives you flexibility in choosing models. the **app** class also allows listing of all available clarifai models for discovery.\nfor greater control over model predictions, you can pass in an `output_config` to modify the model output as demonstrated below.\n#### model predict\n```python\n# note: clarifai_pat must be set as env variable.\nfrom clarifai.client.model import model\n\n\"\"\"\nget model information on details of model(description, usecases..etc) and info on training or\n# other inference parameters(eg: temperature, top_k, max_tokens..etc for llms)\n\"\"\"\ngpt_4_model = model(\"https://clarifai.com/openai/chat-completion/models/gpt-4\")\nprint(gpt_4_model)\n\n\n# model predict\nmodel_prediction = model(\"https://clarifai.com/anthropic/completion/models/claude-v2\").predict_by_bytes(b\"write a tweet on future of ai\", input_type=\"text\")\n\n# customizing model inference output\nmodel_prediction = gpt_4_model.predict_by_bytes(b\"write a tweet on future of ai\", \"text\", inference_params=dict(temperature=str(0.7), max_tokens=30))\n# return predictions having prediction confidence > 0.98\nmodel_prediction = model.predict_by_filepath(filepath=\"local_filepath\", input_type, output_config={\"min_value\": 0.98}) # supports image, text, audio, video\n\n# supports prediction by url\nmodel_prediction = model.predict_by_url(url=\"url\", input_type) # supports image, text, audio, video\n\n# return predictions for specified interval of video\nvideo_input_proto = [input_obj.get_input_from_url(\"input_id\", video_url=beer_video_url)]\nmodel_prediction = model.predict(video_input_proto, input_type=\"video\", output_config={\"sample_ms\": 2000})\n```\n#### model training\n```python\n# note: clarifai_pat must be set as env variable.\nfrom clarifai.client.app import app\nfrom clarifai.client.model import model\n\n\"\"\"\ncreate model with trainable model_type\n\"\"\"\napp = app(user_id=\"user_id\", app_id=\"app_id\")\nmodel = app.create_model(model_id=\"model_id\", model_type_id=\"visual-classifier\")\n               (or)\nmodel = model('url')\n\n\"\"\"\nlist training templates for the model_type\n\"\"\"\ntemplates = model.list_training_templates()\nprint(templates)\n\n\"\"\"\nget parameters for the model.\n\"\"\"\nparams = model.get_params(template='classification_basemodel_v1', save_to='model_params.yaml')\n\n\"\"\"\nupdate the model params yaml and pass it to model.train()\n\"\"\"\nmodel_version_id = model.train('model_params.yaml')\n\n\"\"\"\ntraining status and saving logs\n\"\"\"\nstatus = model.training_status(version_id=model_version_id,training_logs=true)\nprint(status)\n```\n\n#### models listing\n```python\n# note: clarifai_pat must be set as env variable.\n\n# list all model versions\nall_model_versions = list(model.list_versions())\n\n# go to specific model version\nmodel_v1 = client.app(\"app_id\").model(model_id=\"model_id\", model_version_id=\"model_version_id\")\n\n# list all models in an app\nall_models = list(app.list_models())\n\n# list all models in community filtered by model_type, description\nall_llm_community_models = app().list_models(filter_by={\"query\": \"llm\",\n                                                        \"model_type_id\": \"text-to-text\"}, only_in_app=false)\nall_llm_community_models = list(all_llm_community_models)\n```\n\n\n## :fire: interacting with workflows\n\nworkflows offer a versatile framework for constructing the inference pipeline, simplifying the integration of diverse models. you can use the **workflow** class to create and manage workflows using **yaml** configuration.\nfor starting or making quick adjustments to existing clarifai community workflows using an initial yaml configuration, the sdk provides an export feature.\n\n#### workflow predict\n```python\n# note: clarifai_pat must be set as env variable.\nfrom clarifai.client.workflow import workflow\n\n# workflow predict\nworkflow = workflow(\"workflow_url\") # example: https://clarifai.com/clarifai/main/workflows/face-sentiment\nworkflow_prediction = workflow.predict_by_url(url=\"url\", input_type=\"image\") # supports image, text, audio, video\n\n# customizing workflow inference output\nworkflow = workflow(user_id=\"user_id\", app_id=\"app_id\", workflow_id=\"workflow_id\",\n                  output_config={\"min_value\": 0.98}) # return predictions having prediction confidence > 0.98\nworkflow_prediction = workflow.predict_by_filepath(filepath=\"local_filepath\", input_type=\"text\") # supports image, text, audio, video\n```\n\n#### workflows listing\n```python\n# note: clarifai_pat must be set as env variable.\n\n# list all workflow versions\nall_workflow_versions = list(workflow.list_versions())\n\n# go to specific workflow version\nworkflow_v1 = workflow(workflow_id=\"workflow_id\", workflow_version=dict(id=\"workflow_version_id\"), app_id=\"app_id\", user_id=\"user_id\")\n\n# list all workflow in an app\nall_workflow = list(app.list_workflow())\n\n# list all workflow in community filtered by description\nall_face_community_workflows = app().list_workflows(filter_by={\"query\": \"face\"}, only_in_app=false) # get all face related workflows\nall_face_community_workflows = list(all_face_community_workflows)\n```\n#### workflow create\ncreate a new workflow specified by a yaml config file.\n```python\n# note: clarifai_pat must be set as env variable.\nfrom clarifai.client.app import app\napp = app(app_id=\"app_id\", user_id=\"user_id\")\nworkflow = app.create_workflow(config_filepath=\"config.yml\")\n```\n\n#### workflow export\nexport an existing workflow from clarifai as a local yaml file.\n```python\n# note: clarifai_pat must be set as env variable.\nfrom clarifai.client.workflow import workflow\nworkflow = workflow(\"https://clarifai.com/clarifai/main/workflows/demographics\")\nworkflow.export('demographics_workflow.yml')\n```\n\n## :mag: search\n\n#### smart image search\nclarifai's smart search feature leverages vector search capabilities to power the search experience. vector search is a type of search engine that uses vectors to search and retrieve text, images, and videos.\n\ninstead of traditional keyword-based search, where exact matches are sought, vector search allows for searching based on visual and/or semantic similarity by calculating distances between vector embedding representations of the data.\n\nhere is an example of how to use vector search to find similar images:\n\n```python\n# note: clarifai_pat must be set as env variable.\nfrom clarifai.client.search import search\nsearch = search(user_id=\"user_id\", app_id=\"app_id\", top_k=1, metric=\"cosine\")\n\n# search by image url\nresults = search.query(ranks=[{\"image_url\": \"https://samples.clarifai.com/metro-north.jpg\"}])\n\nfor data in results:\n  print(data.hits[0].input.data.image.url)\n```\n\n#### smart text search\nsmart text search is our proprietary feature that uses deep learning techniques to sort, rank, and retrieve text data based on their content and semantic similarity.\n\nhere is an example of how to use smart text search to find similar text:\n\n```python\n# note: clarifai_pat must be set as env variable.\n\n# search by text\nresults = search.query(ranks=[{\"text_raw\": \"i love my dog\"}])\n```\n\n#### filters\n\nyou can use filters to narrow down your search results. filters can be used to filter by concepts, metadata, and geo point.\n\nit is possible to add together multiple search parameters to expand your search. you can even combine negated search terms for more advanced tasks.\n\nfor example, you can combine two concepts as below.\n\n```python\n# query for images that contain concept \"deer\" or \"dog\"\nresults = search.query(ranks=[{\"image_url\": \"https://samples.clarifai.com/metro-north.jpg\"}],\n                        filters=[{\"concepts\": [{\"name\": \"deer\", \"value\":1},\n                                              {\"name\": \"dog\", \"value\":1}]}])\n\n# query for images that contain concepts \"deer\" and \"dog\"\nresults = search.query(ranks=[{\"image_url\": \"https://samples.clarifai.com/metro-north.jpg\"}],\n                        filters=[{\"concepts\": [{\"name\": \"deer\", \"value\":1}],\n                                  \"concepts\": [{\"name\": \"dog\", \"value\":1}]}])\n```\n\ninput filters allows to filter by input_type, status of inputs and by inputs_dataset_id\n\n```python\nresults = search.query(filters=[{'input_types': ['image', 'text']}])\n```\n\n## :pushpin: more examples\n\nsee many more code examples in this [repo](https://github.com/clarifai/examples).\nalso see the official [python sdk docs](https://clarifai-python.readthedocs.io/en/latest/index.html)\n",
  "docs_url": null,
  "keywords": "",
  "license": "apache 2.0",
  "name": "clarifai",
  "package_url": "https://pypi.org/project/clarifai/",
  "project_url": "https://pypi.org/project/clarifai/",
  "project_urls": {
    "Homepage": "https://github.com/Clarifai/clarifai-python"
  },
  "release_url": "https://pypi.org/project/clarifai/9.11.0/",
  "requires_dist": [
    "clarifai-grpc (==9.11.0)",
    "pandas (>=1.3.5)",
    "numpy (>=1.22.0)",
    "tqdm (>=4.65.0)",
    "opencv-python (==4.7.0.68)",
    "tritonclient (==2.34.0)",
    "rich (>=13.4.2)",
    "PyYAML (>=6.0.1)",
    "schema (==0.7.5)",
    "Pillow (>=9.5.0)",
    "pycocotools (==2.0.6) ; extra == 'all'"
  ],
  "requires_python": ">=3.8",
  "summary": "clarifai python sdk",
  "version": "9.11.0",
  "releases": [],
  "developers": [
    "clarifai",
    "support@clarifai.com"
  ],
  "kwds": "clarifai_logo_fc_web clarifai clarifai_pat icon href",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_clarifai",
  "homepage": "https://github.com/clarifai/clarifai-python",
  "release_count": 98,
  "dependency_ids": [
    "pypi_clarifai_grpc",
    "pypi_numpy",
    "pypi_opencv_python",
    "pypi_pandas",
    "pypi_pillow",
    "pypi_pycocotools",
    "pypi_pyyaml",
    "pypi_rich",
    "pypi_schema",
    "pypi_tqdm",
    "pypi_tritonclient"
  ]
}