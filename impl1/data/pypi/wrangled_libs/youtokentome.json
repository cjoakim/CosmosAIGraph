{
  "classifiers": [
    "license :: osi approved :: mit license",
    "programming language :: c++",
    "programming language :: cython",
    "programming language :: python",
    "programming language :: python :: 3",
    "programming language :: python :: 3.5",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: implementation :: cpython"
  ],
  "description": "\n![pypi](https://img.shields.io/pypi/v/youtokentome.svg)\n[![downloads](https://pepy.tech/badge/youtokentome)](https://pepy.tech/project/youtokentome)\n[![code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\n![github](https://img.shields.io/github/license/vkcom/youtokentome.svg)\n[![build status](https://travis-ci.org/vkcom/youtokentome.svg?branch=master)](https://travis-ci.org/vkcom/youtokentome)\n\n# youtokentome \n\nyoutokentome is an unsupervised text tokenizer focused on computational efficiency. it currently implements fast byte pair encoding (bpe) [[sennrich et al.](https://www.aclweb.org/anthology/p16-1162)].\nour implementation is much faster in training and tokenization than [hugging face](https://github.com/huggingface/tokenizers), [fastbpe](https://github.com/glample/fastbpe)\n and [sentencepiece](https://github.com/google/sentencepiece). in some test cases, it is 90 times faster.\n  check out our [benchmark](benchmark.md) results.\n\nkey advantages:\n\n* multithreading for training and tokenization\n* the algorithm has  `o(n)` complexity, where `n` is the length of training data\n* highly efficient implementation in c++\n* python wrapper and command-line interface\n\nextra features:\n* bpe-dropout (as described in [provilkov et al, 2019](https://arxiv.org/abs/1910.13267))\n\nas well as in the algorithm from the original paper, ours does not consider tokens \nthat cross word boundaries. just like in [sentencepiece](https://github.com/google/sentencepiece), all space symbols were replaced by meta symbol \"\u2581\" (u+2581). it allows sequences of tokens to be converted back to text and for word boundaries to be restored.\n\nfor example, the phrase ```blazingly fast tokenization!``` can be tokenized into\n\n`['\u2581bl', 'az', 'ingly', '\u2581fast', '\u2581token', 'ization', '!']`\n\n## installation\n\n```bash\npip install youtokentome\n```\n## python interface \n\n### example\nlet's start with a self-contained example. \n\n```python\nimport random\n\nimport youtokentome as yttm\n\ntrain_data_path = \"train_data.txt\"\nmodel_path = \"example.model\"\n\n# generating random file with training data\n# 10000 lines with 100 characters in each line\nn_lines = 10000\nn_characters = 100\nwith open(train_data_path, \"w\") as fout:\n    for _ in range(n_lines):\n        print(\"\".join([random.choice(\"abcd \") for _ in range(n_characters)]), file=fout)\n\n# generating random text\ntest_text = \"\".join([random.choice(\"abcde \") for _ in range(100)])\n\n# training model\nyttm.bpe.train(data=train_data_path, vocab_size=5000, model=model_path)\n\n# loading model\nbpe = yttm.bpe(model=model_path)\n\n# two types of tokenization\nprint(bpe.encode([test_text], output_type=yttm.outputtype.id))\nprint(bpe.encode([test_text], output_type=yttm.outputtype.subword))\n```\n\n&nbsp;\n### training model\n```python\nyoutokentome.bpe.train(data, model, vocab_size, coverage, n_threads=-1, pad_id=0, unk_id=1, bos_id=2, eos_id=3)\n```\ntrains bpe model and saves to file.\n\n**args:**\n\n* `data`: string, path to file with training data\n* `model`: string, path to where the trained model will be saved\n* `vocab_size`: int, number of tokens in the final vocabulary\n* `coverage`: float, fraction of characters covered by the model. must be in the range [0, 1]. a good value to use is about 0.9999.\n* `n_threads`: int, number of parallel threads used to run. if -1 is passed, then all available threads are going to be used. note that the number of threads is limited by 8 (see [benchmark](benchmark.md#number-of-threads)).\n* `pad_id`: int, reserved id for padding\n* `unk_id`: int, reserved id for unknown symbols\n* `bos_id`: int, reserved id for begin of sentence token\n* `eos_id`: int, reserved id for end of sentence token\n\n**returns**: class `youtokentome.bpe` with the loaded model.\n\n\n&nbsp;\n\n### model loading\n\n```python\nyoutokentome.bpe(model, n_threads=-1)\n```\n\nclass constructor. loads the trained model.\n\n* `model`: string, path to the trained model\n* `n_threads`: int, number of parallel threads used to run. \n    if equal to -1, then the maximum number of threads available will be used.\n\n&nbsp;\n\n### methods\nclass `youtokentome.bpe` has the following methods:\n#### encode \n```python\nencode(self, sentences, output_type=yttm.outputtype.id, bos=false, eos=false, reverse=false, dropout_prob=0)\n```\n\n**args:**\n\n* `sentences`: list of strings, sentences for tokenization.\n* `output_type`: enum, sentence can be tokenized to ids or subwords. use `outputtype.id` for ids and `outputtype.subword` for subwords.\n* `bos`: bool, if true then token \u201cbeginning of sentence\u201d will be added\n* `eos`: bool, if true then token \u201cend of sentence\u201d will be added\n* `reverse`: bool, if true the output sequence of tokens will be reversed\n* `dropout_prob`: float, bpe-dropout probability (the probability of a merge being dropped). must be in the range [0, 1].\n\n\n**returns:** if `output_type` is equal to `youtokentome.outputtype.id` or `youtokentome.outputtype.subword` \n then a list of lists of integers or list of lists of strings will be returned\nrespectively.\n\n&nbsp;\n#### vocab\n\n```python\nvocab(self)\n```\n\n**returns:** a list `vocab_size` strings. the i-th string in the list corresponds\n to i-th subword.\n\n&nbsp;\n#### vocab_size\n\n```python\nvocab_size(self)\n```\n\n**returns:** int. size of vocabulary.\n\n&nbsp;\n#### subword_to_id\n\n```python\nsubword_to_id(self, subword)\n```\n**args:**\n* `subword`: string. \n\n**returns:** \ninteger from the range [0, vocab_size-1]. id of subword or,\n if there is no such subword in the vocabulary, `unk_id` will be \nreturned.\n\n&nbsp;\n#### id_to_subword \n\n```python\nid_to_subword(self, id)\n```\n**args:**\n* `id`: int, must be in the range [0, vocab_size-1]\n\n**returns:** string. subword from vocabulary by id.\n\n&nbsp;\n#### decode \n```python\ndecode(self, ids, ignore_ids=none)\n```  \nconvert each id to subword and concatenate with space symbol.\n\n**args:**\n\n  * `ids`: list of lists of integers. all integers must be in the range [0, vocab_size-1]\n  * `ignore_ids`: collection of integers. these indices would be ignored during the decoding. all integers must be in the range [0, vocab_size-1] [default: none]\n\n\n**returns:** list of strings.  \n\n## command line interface\n\n### example \n\n```bash\n$ yttm bpe --data training_data_file --model output_model_file --vocab_size 2000\n$ yttm encode --model output_model_file --output_type subword < test_data_file > encoded_data \n```\n\n\n### supported commands\n\n`youtokentome` supports the following commands:\n\n```\n$ yttm --help\n\nusage: yttm [options] command [args]...\n\noptions:\n  --help  show this message and exit.\n\ncommands:\n  bpe     train bpe model.\n  decode  decode ids to text.\n  encode  encode text to ids or subwords.\n  vocab   print list of learned subwords.\n```\n\ncommand `bpe` allows you to train byte pair encoding model based on a text file.\n\n```\n$ yttm bpe --help\n\nusage: yttm bpe [options]\n\n  train bpe model.\n\noptions:\n  --data path           training data file path.  [required]\n  --model path          output model file path.  [required]\n  --vocab_size integer  number of tokens in the final vocabulary.  [required]\n  --coverage float      fraction of characters covered by the model.  [default: 1.0]\n  --n_threads integer   number of threads.  [default: -1]\n  --pad_id integer      padding token id.  [default: 0]\n  --unk_id integer      unknown token id.  [default: 1]\n  --bos_id integer      'begin of sentence' token id.  [default: 2]\n  --eos_id integer      'end of sentence' token id.  [default: 3]\n  --help                show this message and exit.\n```\n\n\napply bpe encoding for a corpus of sentences. use `stdin` for input and `stdout` for output.\n\nby default, encoding works in parallel using `n_threads` threads. number of threads is limited by\n8 (see [benchmark](benchmark.md#number-of-threads)).\n\nwith the `--stream` option, `--n_threads` will be ignored and all sentences will be processed one by one.\n each sentence will be tokenized and written to the `stdout` before the next sentence is read.\n\n\n```\n$ yttm encode --help\n\nusage: yttm encode [options]\n\n  encode text to ids or subwords.\n\noptions:\n  --model path         path to file with learned model.  [required]\n  --output_type text   'id' or 'subword'.  [required]\n  --n_threads integer  number of threads.  [default: -1]\n  --bos                add tab 'begin of sentence'.\n  --eos                add tab 'end of sentence'.\n  --reverse            reverse output sequence of tokens.\n  --stream             process each line before reading the next one.\n  --dropout_prob       bpe-dropout probability (the probability of a merge being dropped). [default: 0]\n  --help               show this message and exit.\n```\n\nprint vocabulary. this can be useful for understanding the model.\n\n```\n$ yttm vocab --help\n\nusage: yttm vocab [options]\n\n  print list of learned subwords.\n\noptions:\n  --model path  path to file with learned model.  [required]\n  --verbose     add merging rules.\n  --help        show this message and exit.\n```\n\nconvert ids back to text. use `stdin` for input and `stdout` for output.\n\n```\n$ yttm decode --help\n\nusage: yttm decode [options]\n\n  decode ids to text.\n\noptions:\n  --model path  path to file with learned model.  [required]\n  --ignore_ids  list of indices to ignore for decoding. example: --ignore_ids=1,2,3\n  --help        show this message and exit.\n```\n\n\n\n\n\n\n\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "youtokentome",
  "package_url": "https://pypi.org/project/youtokentome/",
  "project_url": "https://pypi.org/project/youtokentome/",
  "project_urls": {
    "Homepage": "https://github.com/vkcom/youtokentome"
  },
  "release_url": "https://pypi.org/project/youtokentome/1.0.6/",
  "requires_dist": [
    "Click (>=7.0)"
  ],
  "requires_python": ">=3.5.0",
  "summary": "unsupervised text tokenizer focused on computational efficiency",
  "version": "1.0.6",
  "releases": [],
  "developers": [
    "ivan_belonogov"
  ],
  "kwds": "tokenizer tokenizers youtokentome tokenized token",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_youtokentome",
  "homepage": "https://github.com/vkcom/youtokentome",
  "release_count": 8,
  "dependency_ids": [
    "pypi_click"
  ]
}