{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: developers",
    "intended audience :: education",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "operating system :: os independent",
    "programming language :: python :: 3",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "# controlnet auxiliary models\n\nthis is a pypi installable package of [lllyasviel's controlnet annotators](https://github.com/lllyasviel/controlnet/tree/main/annotator)\n\nthe code is copy-pasted from the respective folders in https://github.com/lllyasviel/controlnet/tree/main/annotator and connected to [the \ud83e\udd17 hub](https://huggingface.co/lllyasviel/annotators).\n\nall credit & copyright goes to https://github.com/lllyasviel .\n\n## install\n\n```\npip install controlnet-aux==0.0.6\n```\n\nto support dwpose which is dependent on mmdetection, mmcv and mmpose\n```\npip install -u openmim\nmim install mmengine\nmim install \"mmcv>=2.0.1\"\nmim install \"mmdet>=3.1.0\"\nmim install \"mmpose>=1.1.0\"\n```\n## usage\n\n\nyou can use the processor class, which can load each of the auxiliary models with the following code\n```python\nimport requests\nfrom pil import image\nfrom io import bytesio\n\nfrom controlnet_aux.processor import processor\n\n# load image\nurl = \"https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png\"\n\nresponse = requests.get(url)\nimg = image.open(bytesio(response.content)).convert(\"rgb\").resize((512, 512))\n\n# load processor from processor_id\n# options are:\n# [\"canny\", \"depth_leres\", \"depth_leres++\", \"depth_midas\", \"depth_zoe\", \"lineart_anime\",\n#  \"lineart_coarse\", \"lineart_realistic\", \"mediapipe_face\", \"mlsd\", \"normal_bae\", \"normal_midas\",\n#  \"openpose\", \"openpose_face\", \"openpose_faceonly\", \"openpose_full\", \"openpose_hand\",\n#  \"scribble_hed, \"scribble_pidinet\", \"shuffle\", \"softedge_hed\", \"softedge_hedsafe\",\n#  \"softedge_pidinet\", \"softedge_pidsafe\", \"dwpose\"]\nprocessor_id = 'scribble_hed'\nprocessor = processor(processor_id)\n\nprocessed_image = processor(img, to_pil=true)\n```\n\neach model can be loaded individually by importing and instantiating them as follows\n```python\nfrom pil import image\nimport requests\nfrom io import bytesio\nfrom controlnet_aux import heddetector, midasdetector, mlsddetector, openposedetector, pidinetdetector, normalbaedetector, lineartdetector, lineartanimedetector, cannydetector, contentshuffledetector, zoedetector, mediapipefacedetector, samdetector, leresdetector, dwposedetector\n\n# load image\nurl = \"https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png\"\n\nresponse = requests.get(url)\nimg = image.open(bytesio(response.content)).convert(\"rgb\").resize((512, 512))\n\n# load checkpoints\nhed = heddetector.from_pretrained(\"lllyasviel/annotators\")\nmidas = midasdetector.from_pretrained(\"lllyasviel/annotators\")\nmlsd = mlsddetector.from_pretrained(\"lllyasviel/annotators\")\nopen_pose = openposedetector.from_pretrained(\"lllyasviel/annotators\")\npidi = pidinetdetector.from_pretrained(\"lllyasviel/annotators\")\nnormal_bae = normalbaedetector.from_pretrained(\"lllyasviel/annotators\")\nlineart = lineartdetector.from_pretrained(\"lllyasviel/annotators\")\nlineart_anime = lineartanimedetector.from_pretrained(\"lllyasviel/annotators\")\nzoe = zoedetector.from_pretrained(\"lllyasviel/annotators\")\nsam = samdetector.from_pretrained(\"ybelkada/segment-anything\", subfolder=\"checkpoints\")\nmobile_sam = samdetector.from_pretrained(\"dhkim2810/mobilesam\", model_type=\"vit_t\", filename=\"mobile_sam.pt\")\nleres = leresdetector.from_pretrained(\"lllyasviel/annotators\")\n\n# specify configs, ckpts and device, or it will be downloaded automatically and use cpu by default\n# det_config: ./src/controlnet_aux/dwpose/yolox_config/yolox_l_8xb8-300e_coco.py\n# det_ckpt: https://download.openmmlab.com/mmdetection/v2.0/yolox/yolox_l_8x8_300e_coco/yolox_l_8x8_300e_coco_20211126_140236-d3bd2b23.pth\n# pose_config: ./src/controlnet_aux/dwpose/dwpose_config/dwpose-l_384x288.py\n# pose_ckpt: https://huggingface.co/wanghaofan/dw-ll_ucoco_384/resolve/main/dw-ll_ucoco_384.pth\nimport torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndwpose = dwposedetector(det_config=det_config, det_ckpt=det_ckpt, pose_config=pose_config, pose_ckpt=pose_ckpt, device=device)\n\n# instantiate\ncanny = cannydetector()\ncontent = contentshuffledetector()\nface_detector = mediapipefacedetector()\n\n\n# process\nprocessed_image_hed = hed(img)\nprocessed_image_midas = midas(img)\nprocessed_image_mlsd = mlsd(img)\nprocessed_image_open_pose = open_pose(img, hand_and_face=true)\nprocessed_image_pidi = pidi(img, safe=true)\nprocessed_image_normal_bae = normal_bae(img)\nprocessed_image_lineart = lineart(img, coarse=true)\nprocessed_image_lineart_anime = lineart_anime(img)\nprocessed_image_zoe = zoe(img)\nprocessed_image_sam = sam(img)\nprocessed_image_leres = leres(img)\n\nprocessed_image_canny = canny(img)\nprocessed_image_content = content(img)\nprocessed_image_mediapipe_face = face_detector(img)\nprocessed_image_dwpose = dwpose(img)\n```\n\n\n",
  "docs_url": null,
  "keywords": "deep learning",
  "license": "apache",
  "name": "controlnet-aux",
  "package_url": "https://pypi.org/project/controlnet-aux/",
  "project_url": "https://pypi.org/project/controlnet-aux/",
  "project_urls": {
    "Homepage": "https://github.com/patrickvonplaten/controlnet_aux"
  },
  "release_url": "https://pypi.org/project/controlnet-aux/0.0.7/",
  "requires_dist": [],
  "requires_python": ">=3.7.0",
  "summary": "auxillary models for controlnet",
  "version": "0.0.7",
  "releases": [],
  "developers": [
    "patrick@huggingface.co",
    "the_huggingface_team"
  ],
  "kwds": "controlnet_aux controlnet annotator mmcv lllyasviel",
  "license_kwds": "apache",
  "libtype": "pypi",
  "id": "pypi_controlnet_aux",
  "homepage": "https://github.com/patrickvonplaten/controlnet_aux",
  "release_count": 7,
  "dependency_ids": []
}