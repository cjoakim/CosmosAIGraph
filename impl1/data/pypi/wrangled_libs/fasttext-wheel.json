{
  "classifiers": [
    "development status :: 3 - alpha",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: mit license",
    "operating system :: macos",
    "operating system :: microsoft :: windows",
    "operating system :: posix",
    "operating system :: unix",
    "programming language :: python :: 2.7",
    "programming language :: python :: 3.4",
    "programming language :: python :: 3.5",
    "programming language :: python :: 3.6",
    "topic :: scientific/engineering",
    "topic :: software development"
  ],
  "description": "fasttext |circleci|\n===================\n\n`fasttext <https://fasttext.cc/>`__ is a library for efficient learning\nof word representations and sentence classification.\n\nin this document we present how to use fasttext in python.\n\ntable of contents\n-----------------\n\n-  `requirements <#requirements>`__\n-  `installation <#installation>`__\n-  `usage overview <#usage-overview>`__\n-  `word representation model <#word-representation-model>`__\n-  `text classification model <#text-classification-model>`__\n-  `important: preprocessing data / encoding\n   conventions <#important-preprocessing-data-encoding-conventions>`__\n-  `more examples <#more-examples>`__\n-  `api <#api>`__\n-  `train_unsupervised parameters <#train_unsupervised-parameters>`__\n-  `train_supervised parameters <#train_supervised-parameters>`__\n-  `model object <#model-object>`__\n\nrequirements\n============\n\n`fasttext <https://fasttext.cc/>`__ builds on modern mac os and linux\ndistributions. since it uses c++11 features, it requires a compiler with\ngood c++11 support. you will need `python <https://www.python.org/>`__\n(version 2.7 or \u2265 3.4), `numpy <http://www.numpy.org/>`__ &\n`scipy <https://www.scipy.org/>`__ and\n`pybind11 <https://github.com/pybind/pybind11>`__.\n\ninstallation\n============\n\nto install the latest release, you can do :\n\n.. code:: bash\n\n    $ pip install fasttext\n\nor, to get the latest development version of fasttext, you can install\nfrom our github repository :\n\n.. code:: bash\n\n    $ git clone https://github.com/facebookresearch/fasttext.git\n    $ cd fasttext\n    $ sudo pip install .\n    $ # or :\n    $ sudo python setup.py install\n\nusage overview\n==============\n\nword representation model\n-------------------------\n\nin order to learn word vectors, as `described\nhere <https://fasttext.cc/docs/en/references.html#enriching-word-vectors-with-subword-information>`__,\nwe can use ``fasttext.train_unsupervised`` function like this:\n\n.. code:: py\n\n    import fasttext\n\n    # skipgram model :\n    model = fasttext.train_unsupervised('data.txt', model='skipgram')\n\n    # or, cbow model :\n    model = fasttext.train_unsupervised('data.txt', model='cbow')\n\nwhere ``data.txt`` is a training file containing utf-8 encoded text.\n\nthe returned ``model`` object represents your learned model, and you can\nuse it to retrieve information.\n\n.. code:: py\n\n    print(model.words)   # list of words in dictionary\n    print(model['king']) # get the vector of the word 'king'\n\nsaving and loading a model object\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nyou can save your trained model object by calling the function\n``save_model``.\n\n.. code:: py\n\n    model.save_model(\"model_filename.bin\")\n\nand retrieve it later thanks to the function ``load_model`` :\n\n.. code:: py\n\n    model = fasttext.load_model(\"model_filename.bin\")\n\nfor more information about word representation usage of fasttext, you\ncan refer to our `word representations\ntutorial <https://fasttext.cc/docs/en/unsupervised-tutorial.html>`__.\n\ntext classification model\n-------------------------\n\nin order to train a text classifier using the method `described\nhere <https://fasttext.cc/docs/en/references.html#bag-of-tricks-for-efficient-text-classification>`__,\nwe can use ``fasttext.train_supervised`` function like this:\n\n.. code:: py\n\n    import fasttext\n\n    model = fasttext.train_supervised('data.train.txt')\n\nwhere ``data.train.txt`` is a text file containing a training sentence\nper line along with the labels. by default, we assume that labels are\nwords that are prefixed by the string ``__label__``\n\nonce the model is trained, we can retrieve the list of words and labels:\n\n.. code:: py\n\n    print(model.words)\n    print(model.labels)\n\nto evaluate our model by computing the precision at 1 (p@1) and the\nrecall on a test set, we use the ``test`` function:\n\n.. code:: py\n\n    def print_results(n, p, r):\n        print(\"n\\t\" + str(n))\n        print(\"p@{}\\t{:.3f}\".format(1, p))\n        print(\"r@{}\\t{:.3f}\".format(1, r))\n\n    print_results(*model.test('test.txt'))\n\nwe can also predict labels for a specific text :\n\n.. code:: py\n\n    model.predict(\"which baking dish is best to bake a banana bread ?\")\n\nby default, ``predict`` returns only one label : the one with the\nhighest probability. you can also predict more than one label by\nspecifying the parameter ``k``:\n\n.. code:: py\n\n    model.predict(\"which baking dish is best to bake a banana bread ?\", k=3)\n\nif you want to predict more than one sentence you can pass an array of\nstrings :\n\n.. code:: py\n\n    model.predict([\"which baking dish is best to bake a banana bread ?\", \"why not put knives in the dishwasher?\"], k=3)\n\nof course, you can also save and load a model to/from a file as `in the\nword representation usage <#saving-and-loading-a-model-object>`__.\n\nfor more information about text classification usage of fasttext, you\ncan refer to our `text classification\ntutorial <https://fasttext.cc/docs/en/supervised-tutorial.html>`__.\n\ncompress model files with quantization\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nwhen you want to save a supervised model file, fasttext can compress it\nin order to have a much smaller model file by sacrificing only a little\nbit performance.\n\n.. code:: py\n\n    # with the previously trained `model` object, call :\n    model.quantize(input='data.train.txt', retrain=true)\n\n    # then display results and save the new model :\n    print_results(*model.test(valid_data))\n    model.save_model(\"model_filename.ftz\")\n\n``model_filename.ftz`` will have a much smaller size than\n``model_filename.bin``.\n\nfor further reading on quantization, you can refer to `this paragraph\nfrom our blog\npost <https://fasttext.cc/blog/2017/10/02/blog-post.html#model-compression>`__.\n\nimportant: preprocessing data / encoding conventions\n----------------------------------------------------\n\nin general it is important to properly preprocess your data. in\nparticular our example scripts in the `root\nfolder <https://github.com/facebookresearch/fasttext>`__ do this.\n\nfasttext assumes utf-8 encoded text. all text must be `unicode for\npython2 <https://docs.python.org/2/library/functions.html#unicode>`__\nand `str for\npython3 <https://docs.python.org/3.5/library/stdtypes.html#textseq>`__.\nthe passed text will be `encoded as utf-8 by\npybind11 <https://pybind11.readthedocs.io/en/master/advanced/cast/strings.html?highlight=utf-8#strings-bytes-and-unicode-conversions>`__\nbefore passed to the fasttext c++ library. this means it is important to\nuse utf-8 encoded text when building a model. on unix-like systems you\ncan convert text using `iconv <https://en.wikipedia.org/wiki/iconv>`__.\n\nfasttext will tokenize (split text into pieces) based on the following\nascii characters (bytes). in particular, it is not aware of utf-8\nwhitespace. we advice the user to convert utf-8 whitespace / word\nboundaries into one of the following symbols as appropiate.\n\n-  space\n-  tab\n-  vertical tab\n-  carriage return\n-  formfeed\n-  the null character\n\nthe newline character is used to delimit lines of text. in particular,\nthe eos token is appended to a line of text if a newline character is\nencountered. the only exception is if the number of tokens exceeds the\nmax\\_line\\_size constant as defined in the `dictionary\nheader <https://github.com/facebookresearch/fasttext/blob/master/src/dictionary.h>`__.\nthis means if you have text that is not separate by newlines, such as\nthe `fil9 dataset <http://mattmahoney.net/dc/textdata>`__, it will be\nbroken into chunks with max\\_line\\_size of tokens and the eos token is\nnot appended.\n\nthe length of a token is the number of utf-8 characters by considering\nthe `leading two bits of a\nbyte <https://en.wikipedia.org/wiki/utf-8#description>`__ to identify\n`subsequent bytes of a multi-byte\nsequence <https://github.com/facebookresearch/fasttext/blob/master/src/dictionary.cc>`__.\nknowing this is especially important when choosing the minimum and\nmaximum length of subwords. further, the eos token (as specified in the\n`dictionary\nheader <https://github.com/facebookresearch/fasttext/blob/master/src/dictionary.h>`__)\nis considered a character and will not be broken into subwords.\n\nmore examples\n-------------\n\nin order to have a better knowledge of fasttext models, please consider\nthe main\n`readme <https://github.com/facebookresearch/fasttext/blob/master/readme.md>`__\nand in particular `the tutorials on our\nwebsite <https://fasttext.cc/docs/en/supervised-tutorial.html>`__.\n\nyou can find further python examples in `the doc\nfolder <https://github.com/facebookresearch/fasttext/tree/master/python/doc/examples>`__.\n\nas with any package you can get help on any python function using the\nhelp function.\n\nfor example\n\n::\n\n    +>>> import fasttext\n    +>>> help(fasttext.fasttext)\n\n    help on module fasttext.fasttext in fasttext:\n\n    name\n        fasttext.fasttext\n\n    description\n        # copyright (c) 2017-present, facebook, inc.\n        # all rights reserved.\n        #\n        # this source code is licensed under the mit license found in the\n        # license file in the root directory of this source tree.\n\n    functions\n        load_model(path)\n            load a model given a filepath and return a model object.\n\n        tokenize(text)\n            given a string of text, tokenize it and return a list of tokens\n    [...]\n\napi\n===\n\n``train_unsupervised`` parameters\n---------------------------------\n\n.. code:: python\n\n        input             # training file path (required)\n        model             # unsupervised fasttext model {cbow, skipgram} [skipgram]\n        lr                # learning rate [0.05]\n        dim               # size of word vectors [100]\n        ws                # size of the context window [5]\n        epoch             # number of epochs [5]\n        mincount          # minimal number of word occurences [5]\n        minn              # min length of char ngram [3]\n        maxn              # max length of char ngram [6]\n        neg               # number of negatives sampled [5]\n        wordngrams        # max length of word ngram [1]\n        loss              # loss function {ns, hs, softmax, ova} [ns]\n        bucket            # number of buckets [2000000]\n        thread            # number of threads [number of cpus]\n        lrupdaterate      # change the rate of updates for the learning rate [100]\n        t                 # sampling threshold [0.0001]\n        verbose           # verbose [2]\n\n``train_supervised`` parameters\n-------------------------------\n\n.. code:: python\n\n        input             # training file path (required)\n        lr                # learning rate [0.1]\n        dim               # size of word vectors [100]\n        ws                # size of the context window [5]\n        epoch             # number of epochs [5]\n        mincount          # minimal number of word occurences [1]\n        mincountlabel     # minimal number of label occurences [1]\n        minn              # min length of char ngram [0]\n        maxn              # max length of char ngram [0]\n        neg               # number of negatives sampled [5]\n        wordngrams        # max length of word ngram [1]\n        loss              # loss function {ns, hs, softmax, ova} [softmax]\n        bucket            # number of buckets [2000000]\n        thread            # number of threads [number of cpus]\n        lrupdaterate      # change the rate of updates for the learning rate [100]\n        t                 # sampling threshold [0.0001]\n        label             # label prefix ['__label__']\n        verbose           # verbose [2]\n        pretrainedvectors # pretrained word vectors (.vec file) for supervised learning []\n\n``model`` object\n----------------\n\n``train_supervised``, ``train_unsupervised`` and ``load_model``\nfunctions return an instance of ``_fasttext`` class, that we generaly\nname ``model`` object.\n\nthis object exposes those training arguments as properties : ``lr``,\n``dim``, ``ws``, ``epoch``, ``mincount``, ``mincountlabel``, ``minn``,\n``maxn``, ``neg``, ``wordngrams``, ``loss``, ``bucket``, ``thread``,\n``lrupdaterate``, ``t``, ``label``, ``verbose``, ``pretrainedvectors``.\nso ``model.wordngrams`` will give you the max length of word ngram used\nfor training this model.\n\nin addition, the object exposes several functions :\n\n.. code:: python\n\n        get_dimension           # get the dimension (size) of a lookup vector (hidden layer).\n                                # this is equivalent to `dim` property.\n        get_input_vector        # given an index, get the corresponding vector of the input matrix.\n        get_input_matrix        # get a copy of the full input matrix of a model.\n        get_labels              # get the entire list of labels of the dictionary\n                                # this is equivalent to `labels` property.\n        get_line                # split a line of text into words and labels.\n        get_output_matrix       # get a copy of the full output matrix of a model.\n        get_sentence_vector     # given a string, get a single vector represenation. this function\n                                # assumes to be given a single line of text. we split words on\n                                # whitespace (space, newline, tab, vertical tab) and the control\n                                # characters carriage return, formfeed and the null character.\n        get_subword_id          # given a subword, return the index (within input matrix) it hashes to.\n        get_subwords            # given a word, get the subwords and their indicies.\n        get_word_id             # given a word, get the word id within the dictionary.\n        get_word_vector         # get the vector representation of word.\n        get_words               # get the entire list of words of the dictionary\n                                # this is equivalent to `words` property.\n        is_quantized            # whether the model has been quantized\n        predict                 # given a string, get a list of labels and a list of corresponding probabilities.\n        quantize                # quantize the model reducing the size of the model and it's memory footprint.\n        save_model              # save the model to the given path\n        test                    # evaluate supervised model using file given by path\n        test_label              # return the precision and recall score for each label.\n\nthe properties ``words``, ``labels`` return the words and labels from\nthe dictionary :\n\n.. code:: py\n\n    model.words         # equivalent to model.get_words()\n    model.labels        # equivalent to model.get_labels()\n\nthe object overrides ``__getitem__`` and ``__contains__`` functions in\norder to return the representation of a word and to check if a word is\nin the vocabulary.\n\n.. code:: py\n\n    model['king']       # equivalent to model.get_word_vector('king')\n    'king' in model     # equivalent to `'king' in model.get_words()`\n\njoin the fasttext community\n---------------------------\n\n-  `facebook page <https://www.facebook.com/groups/1174547215919768>`__\n-  `stack\n   overflow <https://stackoverflow.com/questions/tagged/fasttext>`__\n-  `google\n   group <https://groups.google.com/forum/#!forum/fasttext-library>`__\n-  `github <https://github.com/facebookresearch/fasttext>`__\n\n.. |circleci| image:: https://circleci.com/gh/facebookresearch/fasttext/tree/master.svg?style=svg\n   :target: https://circleci.com/gh/facebookresearch/fasttext/tree/master\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "fasttext-wheel",
  "package_url": "https://pypi.org/project/fasttext-wheel/",
  "project_url": "https://pypi.org/project/fasttext-wheel/",
  "project_urls": {
    "Homepage": "https://github.com/facebookresearch/fastText"
  },
  "release_url": "https://pypi.org/project/fasttext-wheel/0.9.2/",
  "requires_dist": [
    "pybind11 (>=2.2)",
    "setuptools (>=0.7.0)",
    "numpy"
  ],
  "requires_python": "",
  "summary": "fasttext python bindings",
  "version": "0.9.2",
  "releases": [],
  "developers": [
    "celebio@fb.com",
    "onur_celebi"
  ],
  "kwds": "fasttext _fasttext get_words text get_sentence_vector",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_fasttext_wheel",
  "homepage": "https://github.com/facebookresearch/fasttext",
  "release_count": 1,
  "dependency_ids": [
    "pypi_numpy",
    "pypi_pybind11",
    "pypi_setuptools"
  ]
}