{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: developers",
    "intended audience :: information technology",
    "intended audience :: science/research",
    "license :: osi approved :: mit license",
    "operating system :: macos",
    "operating system :: microsoft :: windows",
    "operating system :: posix :: linux",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "# recommender utilities\n\nthis package contains functions to simplify common tasks used when developing and evaluating recommender systems. a short description of the submodules is provided below. for more details about what functions are available and how to use them, please review the doc-strings provided with the code or the [online documentation](https://readthedocs.org/projects/microsoft-recommenders/).\n\n# installation\n\n## pre-requisites\nsome dependencies require compilation during pip installation. on linux this can be supported by adding build-essential dependencies:\n```bash\nsudo apt-get install -y build-essential libpython<version>\n``` \nwhere `<version>` should be the python version (e.g. `3.6`).\n\non windows you will need [microsoft c++ build tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/)\n\nfor more details about the software requirements that must be pre-installed on each supported platform, see the [setup guide](https://github.com/microsoft/recommenders/blob/main/setup.md).   \n\n## basic installation\n\nto install core utilities, cpu-based algorithms, and dependencies\n```bash\npip install --upgrade pip setuptools\npip install recommenders\n```\n\n## optional dependencies\n\nby default `recommenders` does not install all dependencies used throughout the code and the notebook examples in this repo. instead we require a bare minimum set of dependencies needed to execute functionality in the `recommenders` package (excluding spark, gpu and jupyter functionality). we also allow the user to specify which groups of dependencies are needed at installation time (or later if updating the pip installation). the following groups are provided:\n\n- examples: dependencies related to jupyter needed to run [example notebooks](https://github.com/microsoft/recommenders/tree/main/examples)\n- gpu: dependencies to enable gpu functionality (pytorch & tensorflow)\n- spark: dependencies to enable apache spark functionality used in dataset, splitting, evaluation and certain algorithms\n- dev: dependencies such as `black` and `pytest` required only for development or testing\n- all: all of the above dependencies\n- experimental: current experimental dependencies that are being evaluated (e.g. libraries that require advanced build requirements or might conflict with libraries from other options)\n- nni: dependencies for nni tuning framework.\n\nnote that, currently, xlearn and vowpal wabbit are in the experimental group.\n\nthese groups can be installed alone or in combination:\n```bash\n# install recommenders with core requirements and support for cpu-based recommender algorithms and notebooks\npip install recommenders[examples]\n\n# add support for running example notebooks and gpu functionality\npip install recommenders[examples,gpu]\n```\n\n## gpu support\n\nyou will need cuda toolkit v11.2 and cudnn v8.1 to enable both tensorflow and pytorch to use the gpu. for example, if you are using a conda environment, this can be installed with\n```bash\nconda install -c conda-forge cudatoolkit=11.2 cudnn=8.1\n```\nfor a virtual environment, you may use a [docker container by nvidia](../setup.md#using-a-virtual-environment). \n\nfor manual installation of the necessary requirements see [tensorflow](https://www.tensorflow.org/install/gpu#software_requirements) and [pytorch](https://pytorch.org/get-started/locally/) installation pages.\n\nwhen installing with gpu support you will need to point to the pytorch index to ensure you are downloading a version of pytorch compiled with cuda support. this can be done using the --find-links or -f option below.\n\n`pip install recommenders[gpu] -f https://download.pytorch.org/whl/cu111/torch_stable.html`\n\n## experimental dependencies\n\nwe are currently evaluating inclusion of the following dependencies:\n\n - vowpalwabbit: current examples show how to use vowpal wabbit after it has been installed on the command line; using the [pypi package](https://pypi.org/project/vowpalwabbit/) with the scikit-learn interface will facilitate easier integration into python environments\n - xlearn: on some platforms, xlearn requires pre-installation of cmake.\n\n## other dependencies\n\nsome dependencies are not available via the recommenders pypi package, but can be installed in the following ways: \n - pymanopt: this dependency is required for the rlrmc and geoimc algorithms; a version of this code compatible with tensorflow 2 can be\n installed with `pip install \"pymanopt@https://github.com/pymanopt/pymanopt/archive/fb36a272cdeecb21992cfd9271eb82baafeb316d.zip\"`. \n\n## nni dependencies\n\nfor nni a more recent version can be installed but is untested.\n\n\n## installing the utilities from a local copy\n\nin case you want to use a version of the source code that is not published on pypi, one alternative is to install from a clone of the source code on your machine. to this end, \na [setup.py](../setup.py) file is provided in order to simplify the installation of the utilities in this repo from the main directory.\n\nthis still requires an environment to be installed as described in the [setup guide](../setup.md). once the necessary dependencies are installed, you can use the following command to install `recommenders` as a python package.\n\n    pip install -e .\n\nit is also possible to install directly from github. or from a specific branch as well.\n\n    pip install -e git+https://github.com/microsoft/recommenders/#egg=pkg\n    pip install -e git+https://github.com/microsoft/recommenders/@staging#egg=pkg\n\n**note** - the pip installation does not install all of the pre-requisites; it is assumed that the environment has already been set up according to the [setup guide](../setup.md), for the utilities to be used.\n\n\n# contents\n\n## [datasets](datasets)\n\ndatasets module includes helper functions for pulling different datasets and formatting them appropriately as well as utilities for splitting data for training / testing.\n\n### data loading\n\nthere are dataloaders for several datasets. for example, the movielens module will allow you to load a dataframe in pandas or spark formats from the movielens dataset, with sizes of 100k, 1m, 10m, or 20m to test algorithms and evaluate performance benchmarks.\n\n```python\ndf = movielens.load_pandas_df(size=\"100k\")\n```\n\n### splitting techniques\n\ncurrently three methods are available for splitting datasets. all of them support splitting by user or item and filtering out minimal samples (for instance users that have not rated enough items, or items that have not been rated by enough users).\n\n- random: this is the basic approach where entries are randomly assigned to each group based on the ratio desired\n- chronological: this uses provided timestamps to order the data and selects a cut-off time that will split the desired ratio of data to train before that time and test after that time\n- stratified: this is similar to random sampling, but the splits are stratified, for example if the datasets are split by user, the splitting approach will attempt to maintain the same ratio of items used in both training and test splits. the converse is true if splitting by item.\n\n## [evaluation](evaluation)\n\nthe evaluation submodule includes functionality for calculating common recommendation metrics directly in python or in a spark environment using pyspark.\n\ncurrently available metrics include:\n\n- root mean squared error\n- mean absolute error\n- r<sup>2</sup>\n- explained variance\n- precision at k\n- recall at k\n- normalized discounted cumulative gain at k\n- mean average precision at k\n- area under curve\n- logistic loss\n\n## [models](models)\n\nthe models submodule contains implementations of various algorithms that can be used in addition to external packages to evaluate and develop new recommender system approaches. a description of all the algorithms can be found on [this table](../readme.md#algorithms). the following is a list of the algorithm utilities:\n\n* cornac\n* deeprec\n  *  convolutional sequence embedding recommendation (caser)\n  *  deep knowledge-aware network (dkn)\n  *  extreme deep factorization machine (xdeepfm)\n  *  gru4rec\n  *  lightgcn\n  *  next item recommendation (nextitnet)\n  *  short-term and long-term preference integrated recommender (sli-rec)\n  *  multi-interest-aware sequential user modeling (sum)\n* fastai\n* geoimc\n* lightfm\n* lightgbm\n* ncf\n* newsrec\n  * neural recommendation with long- and short-term user representations (lstur)\n  * neural recommendation with attentive multi-view learning (naml)\n  * neural recommendation with personalized attention (npa)\n  * neural recommendation with multi-head self-attention (nrms)\n* restricted boltzmann machines (rbm)\n* riemannian low-rank matrix completion (rlrmc)\n* simple algorithm for recommendation (sar)\n* self-attentive sequential recommendation (sasrec)\n* sequential recommendation via personalized transformer (ssept)\n* surprise\n* term frequency - inverse document frequency (tf-idf)\n* variational autoencoders (vae)\n  * multinomial\n  * standard\n* vowpal wabbit (vw)\n* wide and deep\n* xlearn\n  * factorization machine (fm)\n  * field-aware fm (ffm)\n\n## [tuning](tuning)\n\nthis submodule contains utilities for performing hyperparameter tuning.\n\n## [utils](utils)\n\nthis submodule contains high-level utilities for defining constants used in most algorithms as well as helper functions for managing aspects of different frameworks: gpu, spark, jupyter notebook.\n\n\n",
  "docs_url": null,
  "keywords": "recommendations recommendation recommenders recommender system engine machine learning python spark gpu",
  "license": "",
  "name": "recommenders",
  "package_url": "https://pypi.org/project/recommenders/",
  "project_url": "https://pypi.org/project/recommenders/",
  "project_urls": {
    "Documentation": "https://microsoft-recommenders.readthedocs.io/en/stable/",
    "Homepage": "https://github.com/microsoft/recommenders",
    "Wiki": "https://github.com/microsoft/recommenders/wiki"
  },
  "release_url": "https://pypi.org/project/recommenders/1.1.1/",
  "requires_dist": [
    "numpy (>=1.19)",
    "pandas (<2,>1.0.3)",
    "scipy (<2,>=1.0.0)",
    "tqdm (<5,>=4.31.1)",
    "matplotlib (<4,>=2.2.2)",
    "scikit-learn (<1.0.3,>=0.22.1)",
    "numba (<1,>=0.38.1)",
    "lightfm (<2,>=1.15)",
    "lightgbm (>=2.2.1)",
    "memory-profiler (<1,>=0.54.0)",
    "nltk (<4,>=3.4)",
    "seaborn (<1,>=0.8.1)",
    "transformers (<5,>=2.5.0)",
    "bottleneck (<2,>=1.2.1)",
    "category-encoders (<2,>=1.3.0)",
    "jinja2 (<3.1,>=2)",
    "pyyaml (<6,>=5.4.1)",
    "requests (<3,>=2.0.0)",
    "cornac (<2,>=1.1.2)",
    "retrying (>=1.3.3)",
    "pandera[strategies] (>=0.6.5)",
    "scikit-surprise (>=1.0.6)",
    "black (<21,>=18.6b4) ; extra == 'all'",
    "nvidia-ml-py3 (>=7.352.0) ; extra == 'all'",
    "pytest (>=3.6.4) ; extra == 'all'",
    "pytest-mock (>=3.6.1) ; extra == 'all'",
    "tf-slim (>=1.1.0) ; extra == 'all'",
    "pyarrow (<7.0.0,>=0.12.1) ; extra == 'all'",
    "pytest-rerunfailures (>=10.2) ; extra == 'all'",
    "pyspark (<4.0.0,>=2.4.5) ; extra == 'all'",
    "hyperopt (<1,>=0.1.2) ; extra == 'all'",
    "papermill (<3,>=2.1.2) ; extra == 'all'",
    "fastai (<2,>=1.0.46) ; extra == 'all'",
    "pytest-cov (>=2.12.1) ; extra == 'all'",
    "databricks-cli (<1,>=0.8.6) ; extra == 'all'",
    "locust (<2,>=1) ; extra == 'all'",
    "ipykernel (<7,>=4.6.1) ; extra == 'all'",
    "scrapbook (<1.0.0,>=0.5.0) ; extra == 'all'",
    "torch (>=1.8) ; extra == 'all'",
    "azure.mgmt.cosmosdb (<1,>=0.8.0) ; extra == 'all'",
    "jupyter (<2,>=1) ; extra == 'all'",
    "tensorflow (~=2.6.1) ; (python_version == \"3.6\") and extra == 'all'",
    "tensorflow (~=2.7.0) ; (python_version >= \"3.7\") and extra == 'all'",
    "black (<21,>=18.6b4) ; extra == 'dev'",
    "pytest (>=3.6.4) ; extra == 'dev'",
    "pytest-cov (>=2.12.1) ; extra == 'dev'",
    "pytest-mock (>=3.6.1) ; extra == 'dev'",
    "pytest-rerunfailures (>=10.2) ; extra == 'dev'",
    "azure.mgmt.cosmosdb (<1,>=0.8.0) ; extra == 'examples'",
    "hyperopt (<1,>=0.1.2) ; extra == 'examples'",
    "ipykernel (<7,>=4.6.1) ; extra == 'examples'",
    "jupyter (<2,>=1) ; extra == 'examples'",
    "locust (<2,>=1) ; extra == 'examples'",
    "papermill (<3,>=2.1.2) ; extra == 'examples'",
    "scrapbook (<1.0.0,>=0.5.0) ; extra == 'examples'",
    "xlearn (==0.40a1) ; extra == 'experimental'",
    "vowpalwabbit (<9,>=8.9.0) ; extra == 'experimental'",
    "nvidia-ml-py3 (>=7.352.0) ; extra == 'gpu'",
    "tf-slim (>=1.1.0) ; extra == 'gpu'",
    "torch (>=1.8) ; extra == 'gpu'",
    "fastai (<2,>=1.0.46) ; extra == 'gpu'",
    "tensorflow (~=2.6.1) ; (python_version == \"3.6\") and extra == 'gpu'",
    "tensorflow (~=2.7.0) ; (python_version >= \"3.7\") and extra == 'gpu'",
    "nni (==1.5) ; extra == 'nni'",
    "databricks-cli (<1,>=0.8.6) ; extra == 'spark'",
    "pyarrow (<7.0.0,>=0.12.1) ; extra == 'spark'",
    "pyspark (<4.0.0,>=2.4.5) ; extra == 'spark'"
  ],
  "requires_python": ">=3.6, <3.10",
  "summary": "microsoft recommenders - python utilities for building recommender systems",
  "version": "1.1.1",
  "releases": [],
  "developers": [
    "recodev_team_at_microsoft",
    "recodevteam@service.microsoft.com"
  ],
  "kwds": "recommender recommenders recommendation pip recommendations",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_recommenders",
  "homepage": "https://github.com/microsoft/recommenders",
  "release_count": 5,
  "dependency_ids": [
    "pypi_azure.mgmt.cosmosdb",
    "pypi_black",
    "pypi_bottleneck",
    "pypi_category_encoders",
    "pypi_cornac",
    "pypi_databricks_cli",
    "pypi_fastai",
    "pypi_hyperopt",
    "pypi_ipykernel",
    "pypi_jinja2",
    "pypi_jupyter",
    "pypi_lightfm",
    "pypi_lightgbm",
    "pypi_locust",
    "pypi_matplotlib",
    "pypi_memory_profiler",
    "pypi_nltk",
    "pypi_nni",
    "pypi_numba",
    "pypi_numpy",
    "pypi_nvidia_ml_py3",
    "pypi_pandas",
    "pypi_pandera",
    "pypi_papermill",
    "pypi_pyarrow",
    "pypi_pyspark",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_pytest_mock",
    "pypi_pytest_rerunfailures",
    "pypi_pyyaml",
    "pypi_requests",
    "pypi_retrying",
    "pypi_scikit_learn",
    "pypi_scikit_surprise",
    "pypi_scipy",
    "pypi_scrapbook",
    "pypi_seaborn",
    "pypi_tensorflow",
    "pypi_tf_slim",
    "pypi_torch",
    "pypi_tqdm",
    "pypi_transformers",
    "pypi_vowpalwabbit",
    "pypi_xlearn"
  ]
}