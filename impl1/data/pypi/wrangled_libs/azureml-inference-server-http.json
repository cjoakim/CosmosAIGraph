{
  "classifiers": [
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: other/proprietary license",
    "operating system :: macos",
    "operating system :: microsoft :: windows",
    "operating system :: posix :: linux",
    "programming language :: python",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "============================================================================\nazure machine learning inference http server (azureml-inference-server-http)\n============================================================================\n\ncheck our official documentation `azureml inference server - docs <https://docs.microsoft.com/en-us/azure/machine-learning/how-to-inference-server-http>`__.\n\nplease note, the azureml inference server is now open source! the repo is available here: `azureml inference server - github <https://github.com/microsoft/azureml-inference-server>`__.\n\nchangelog\n=========\n\n1.0.0 (2023-09-21)\n~~~~~~~~~~~~~~~~~~\n\nbreaking changes\n----------------\n\n- deprecate python 3.7 support and improve documentation\n\n\n0.8.4.2 (2023-09-07)\n~~~~~~~~~~~~~~~~~~~~\nazureml_inference_server_http 0.8.4.2 (2023-09-07)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nfixes\n-----\n\n- updated inferenceschema from ~=1.5.0 to ~=1.7.0 tp support mlflow ncd\n\n\n0.8.4.1 (2023-08-09)\n~~~~~~~~~~~~~~~~~~~~\n\nfixes\n-----\n\n- fixed pydantic warning at the server startup.\n\n\n0.8.4 (2023-04-19)\n~~~~~~~~~~~~~~~~~~\n\nfeatures\n--------\n\n- added ability to configure seperate dedicated health check port.\n\nfixes\n-----\n\n- restored streaming for score response.\n- updated the error message related to scoring script not found.\n\n\n0.8.3 (2023-03-23)\n~~~~~~~~~~~~~~~~~~\n\nfixes\n-----\n\n- fixed the issue related to compatibility with flask1.\n\n\n0.8.2 (2023-03-20)\n~~~~~~~~~~~~~~~~~~\n\nenhancements\n------------\n\n- warning message will be logged if there are extra keys in the config file which are not supported by the server.\n\n\n0.8.1 (2023-03-06)\n~~~~~~~~~~~~~~~~~~\n\nfeatures\n--------\n\n- supports loading server config from a json file. \n  added support for a new env variable ``azureml_config_file``. refer to readme for detailed usage.\n\nenhancements\n------------\n\n- exception details will not be returned in the http response. check the server logs for details.\n\n\n0.8.0 (2022-12-15)\n~~~~~~~~~~~~~~~~~~\n\nbreaking changes\n----------------\n\n- drop support for python 3.6\n\nenhancements\n------------\n\n- all error responses will be in json. (format: {'message': <error_message>})\n- loggers can be configured by users through a logging.json file in\n  `aml_app_root` or alongside the entry script.\n\n  log message default format has been updated. (format: \"<utc date> <utc time>\n  <log level char> [<pid>] <logger name> - <message>\")\n\n\n0.7.7 (2022-11-01)\n~~~~~~~~~~~~~~~~~~\n\nfixes\n-----\n\n- upgrade ``inference-schema`` dependency to support python 3.9\n\n\n0.7.6 (2022-09-13)\n~~~~~~~~~~~~~~~~~~\n\nfixes\n-----\n\n- ``aml_app_root`` variable is now defaulted to the current working directory\n- ``azureml_entry_script`` is now set to an absolute path to the entry script\n\n\n0.7.5 (2022-08-16)\n~~~~~~~~~~~~~~~~~~\n\nbreaking changes\n----------------\n\n- the header for client request id is renamed from ``x-client-request-id`` to ``x-ms-client-request-id``.\n- server will no longer throw an error when both ``x-ms-request-id`` and ``x-request-id`` are provided. going forward,\n  ``x-ms-request-id`` will be treated as the client request id. however, it is still considered deprecated and users\n  are recommended to use ``x-ms-client-request-id`` for client request id.\n\n  - when neither ``x-ms-request-id`` or ``x-ms-client-request-id`` is set, the server copies the value of\n    ``x-request-id`` to ``x-ms-request-id``. this is done to preserve backwards compatability, ensuring that\n    ``x-ms-request-id`` is not empty. no value is logged to appinsights as \"client request id\".\n  - when only ``x-ms-request-id`` is set, the server returns ``x-ms-request-id`` and ``x-ms-client-request-id`` set to the\n    value. this value is logged to appinsights as \"client request id\".\n  - when only ``x-ms-client-request-id`` is set, the server returns ``x-ms-request-id`` and ``x-ms-client-request-id``\n    set to the value. this value is logged to appinsights as \"client request id\".\n  - when both ``x-ms-request-id`` and ``x-ms-client-request-id`` are set, the values are returned in the respective\n    headers. however, only the value from ``x-ms-client-request-id`` is logged to appinsights as \"client request id\".\n\n\n0.7.4 (2022-07-29)\n~~~~~~~~~~~~~~~~~~\n\nfixes\n-----\n\n- fix an issue where the server would require arguments that have default values in run().\n\n\n0.7.3 (2022-07-18)\n~~~~~~~~~~~~~~~~~~\n\nfeatures\n--------\n\n- cors can be enabled with the environment variable ``aml_cors_origins``. refer to readme for detailed usage.\n- server can now be started with ``python -m azureml_inference_server_http`` in additional to ``azmlinfsrv``.\n- options calls are modified to return ``200 ok`` instead of the previous ``405 method not allowed``.\n- users can bring their own swaggers by placing ``swagger2.json`` and ``swagger3.json`` in ``aml_app_root``.\n\nenhancements\n------------\n\n- swaggers are always generated now, regardless whether the user's run() function is decorated with inference-schema. \n- the x-request-id and x-client-request-id headers are now limited to 100 characters.\n\nfixes\n-----\n\n- fixed an issue that prevents the server from cleanly exiting when the scoring script cannot be initialized. if\n  appinsights is not enabled, users may see ``attributeerror: 'appinsightsclient' object has no attribute 'logger'``.\n\n\n0.7.2 (2022-06-06)\n~~~~~~~~~~~~~~~~~~\n\nenhancements\n------------\n\n- added support for flask 2.1.\n\n- the server now responds with a 400 bad request when it finds invalid inputs.\n\n\n0.7.1 (2022-05-10)\n~~~~~~~~~~~~~~~~~~\n\ndeprecation\n-----------\n\n- the \"x-ms-request-id\" header is deprecated and is being replaced by \"x-request-id\". until \"x-ms-request-id\" is\n  removed, the server will accept either header and respond with both headers set to the same request id. providing two\n  request ids through the headers is not allowed and will be responded with a bad request.\n\n\nenhancements\n------------\n\n- added support for flask 2.0. a compatibility layer is introduced to ensure this upgrade doesn't break users who use\n  ``@rawhttp`` as the methods on the flask request object have slightly changed. specifically,\n\n  * ``request.headers.has_keys()`` was removed\n  * ``request.json`` throws an exception if the content-type is not \"application/json\". previously it returns ``none``.\n\n  the compatibility layer restores these functionalities to their previous behaviors. however, this compatibility layer\n  will be removed in a future date and users are encouraged to audit their score scripts today. to see if your score\n  script is ready for flask 2, run the server with the environment variable ``aml_flask_one_compatibility`` set to\n  ``false``.\n\n  flask's full changelog can be found here: https://flask.palletsprojects.com/en/2.1.x/changes/\n\n- added support for the \"x-request-id\" and \"x-client-request-id\" headers. a new guid is generated for \"x-request-id\" if\n  one is not provided. these values are echoed back to the client in the response headers. \n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "https://aka.ms/azureml-sdk-license",
  "name": "azureml-inference-server-http",
  "package_url": "https://pypi.org/project/azureml-inference-server-http/",
  "project_url": "https://pypi.org/project/azureml-inference-server-http/",
  "project_urls": null,
  "release_url": "https://pypi.org/project/azureml-inference-server-http/1.0.0/",
  "requires_dist": [
    "flask <2.3.0",
    "flask-cors ~=3.0.1",
    "inference-schema ~=1.7.0",
    "opencensus-ext-azure ~=1.1.0",
    "pydantic <1.11,>=1.9",
    "gunicorn ==20.1.0 ; platform_system != \"Windows\"",
    "psutil <6.0.0 ; platform_system == \"Windows\"",
    "waitress ==2.1.2 ; platform_system == \"Windows\"",
    "azure-monitor-query ; extra == 'dev'",
    "black ; extra == 'dev'",
    "coverage ==6.2 ; extra == 'dev'",
    "debugpy ; extra == 'dev'",
    "flake8 ; extra == 'dev'",
    "flake8-comprehensions ; extra == 'dev'",
    "flake8-import-order ; extra == 'dev'",
    "junitparser ==2.0.0 ; extra == 'dev'",
    "numpy ; extra == 'dev'",
    "pandas ; extra == 'dev'",
    "pre-commit ; extra == 'dev'",
    "pytest ; extra == 'dev'",
    "pytest-asyncio ; extra == 'dev'",
    "pytest-benchmark ; extra == 'dev'",
    "pytest-cov ; extra == 'dev'",
    "Pillow ~=9.0.1 ; extra == 'dev'",
    "requests ; extra == 'dev'",
    "towncrier ==21.9.0 ; extra == 'dev'",
    "wheel ; extra == 'dev'"
  ],
  "requires_python": ">=3.8",
  "summary": "azure machine learning inferencing server.",
  "version": "1.0.0",
  "releases": [],
  "developers": [
    "amlinferenceimages@microsoft.com",
    "microsoft_corp"
  ],
  "kwds": "azureml_inference_server_http azureml azureml_entry_script azure azureml_config_file",
  "license_kwds": "https://aka.ms/azureml-sdk-license",
  "libtype": "pypi",
  "id": "pypi_azureml_inference_server_http",
  "homepage": "",
  "release_count": 37,
  "dependency_ids": [
    "pypi_azure_monitor_query",
    "pypi_black",
    "pypi_coverage",
    "pypi_debugpy",
    "pypi_flake8",
    "pypi_flake8_comprehensions",
    "pypi_flake8_import_order",
    "pypi_flask",
    "pypi_flask_cors",
    "pypi_gunicorn",
    "pypi_inference_schema",
    "pypi_junitparser",
    "pypi_numpy",
    "pypi_opencensus_ext_azure",
    "pypi_pandas",
    "pypi_pillow",
    "pypi_pre_commit",
    "pypi_psutil",
    "pypi_pydantic",
    "pypi_pytest",
    "pypi_pytest_asyncio",
    "pypi_pytest_benchmark",
    "pypi_pytest_cov",
    "pypi_requests",
    "pypi_towncrier",
    "pypi_waitress",
    "pypi_wheel"
  ]
}