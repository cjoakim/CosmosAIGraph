{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: developers",
    "license :: public domain",
    "operating system :: os independent",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "\n![ci pipeline](https://github.com/thibtrip/pangres/actions/workflows/main.yml/badge.svg)\n[![codecov](https://codecov.io/gh/thibtrip/pangres/branch/master/graph/badge.svg)](https://codecov.io/gh/thibtrip/pangres)\n[![pypi version](https://img.shields.io/pypi/v/pangres)](https://img.shields.io/pypi/v/pangres)\n[![documentation](https://img.shields.io/badge/wiki-documentation-forestgreen)](https://github.com/thibtrip/pangres/wiki)\n[![made withjupyter](https://img.shields.io/badge/made%20with-jupyter-orange?logo=jupyter)](https://jupyter.org/try)\n\n# pangres\n![pangres logo](https://raw.githubusercontent.com/thibtrip/pangres/master/logo.png)\n\n_thanks to [freesvg.org](https://freesvg.org/) for the logo assets_\n\nupsert with pandas dataframes (<code>on conflict do nothing</code> or <code>on conflict do update</code>) for postgresql, mysql, sqlite and potentially other databases behaving like sqlite (untested) with some additional optional features (see features). upserting can be done with **primary keys** or **unique keys**.\npangres also handles the creation of non-existing sql tables and schemas.\n\n\n# features\n\n1. <i>(optional)</i> automatical column creation (when a column exists in the dataframe but not in the sql table)\n2. <i>(optional)</i> automatical column type alteration for columns that are empty in the sql table (except for sqlite where alteration is limited)\n3. <i>(optional)</i> creates the table if it is missing\n4. <i>(optional)</i> creates missing schemas in postgres (and potentially other databases that have a schema system)\n5. json is supported (with pd.to_sql it does not work) with some exceptions (see [gotchas and caveats](#gotchas-and-caveats))\n6. fast (except for sqlite where some help is needed)\n7. will work even if not all columns defined in the sql table are there\n8. sql injection safe (schema, table and column names are escaped and values are given as parameters)\n9. _new in version 4.1_: **asynchronous support**. tested using `aiosqlite` for sqlite, `asyncpg` for postgresql and `aiomysql` for mysql\n\n# requirements\n\n* sqlite >= 3.24.4\n* python >= 3.6.4\n* see also ./pangres/requirements.txt\n\n## requirements for sqlalchemy>=2.0\n\nfor using `pangres` together with **`sqlalchemy>=2.0`** (sqlalchemy is one of pangres dependencies\nlisted in requirements.txt) - you will need the following base requirements:\n* `alembic>=1.7.2`\n* `pandas>=1.4.0`\n* python >= 3.8 (`pandas>=1.4.0` only supports python >=3.8)\n\n## requirements for asynchronous engines\n\nfor using asynchronous engines (such as `aiosqlite`, `asyncpg` or `aiomysql`) you will need **python >= 3.8**.\n\n# gotchas and caveats\n\n## all flavors\n1. we can't create json columns automatically, but we can insert json like objects (list, dict) in existing json columns.\n\n## postgres\n\n1. \"%\", \")\" and \"(\" in column names will most likely cause errors with postgresql (this is due to psycopg2 and also affect pd.to_sql). use the function pangres.fix_psycopg2_bad_cols to \"clean\" the columns in the dataframe. you'll also have to rename columns in the sql table accordingly (if the table already exists).\n2. even though we only do data type alteration on empty columns, since we don't want to lose column information (e.g. constraints) we use true column alteration (instead of drop+create) so the old data type must be castable to the new data type. postgres seems a bit restrictive in this regard even when the columns are empty (e.g. boolean to timestamp is impossible).\n\n## sqlite\n1. **sqlite must be version 3.24.4 or higher**! upsert syntax did not exist before. \n2. column type alteration is not possible for sqlite.\n3. sqlite inserts can be at worst 5 times slower than pd.to_sql for some reasons. if you can help please contact me!\n4. inserts with 1000 columns (or 32767 columns for sqlite >= 3.32.0) or more are not supported because one could not even insert one row without exceeding the max number of parameters per queries. one way to fix this would inserting the columns progressively but this seems quite tricky. if you know a better way please contact me.\n\n## mysql\n\n1. mysql will often change the order of the primary keys in the sql table when using insert... on conflict.. do nothing/update. this seems to be the expected behavior so nothing we can do about it but please mind that!\n2. you may need to provide sql dtypes e.g. if you have a primary key with text you will need to provide a character length (e.g. varchar(50)) because mysql does not support indices/primary keys with flexible text length. pd.to_sql has the same issue.\n\n\n# notes\n\nthis is a library i was using in production in private with very good results and decided to publish.\n\nideally such features will be integrated into pandas since there is already a [pr on the way](https://github.com/pandas-dev/pandas/pull/29636) and i would like to give the option to add columns via another pr.\n\nthere is also [pandabase](https://github.com/notsambeck/pandabase) which does almost the same thing (plus lots of extra features) but my implementation is different.\nbtw big thanks to pandabase and the sql part of pandas which helped a lot.\n\n# installation\n```\npip install pangres\n```\nadditionally depending on which database you want to work with you will need to install the corresponding library (note that sqlite is included in the standard library):\n\n* postgres\n```\npip install psycopg2\n```\n\n* mysql\n```\npip install pymysql\n```\n\n* postgres (asynchronous)\n```\npip install asyncpg\n```\n\n* mysql (asynchronous)\n```\npip install aiomysql\n```\n\n* sqlite (asynchronous)\n```\npip install aiosqlite\n```\n\n# usage\n\nhead over to [pangres' wiki](https://github.com/thibtrip/pangres/wiki)! note that the wiki is also available\nlocally under the [wiki folder](https://github.com/thibtrip/pangres/tree/master/wiki).\n\nnote:\n\nthe wiki is generated with a command which uses my library [npdoc_to_md](https://github.com/thibtrip/npdoc_to_md).\nit must be installed with `pip install npdoc_to_md` and you will also need the extra dependency `fire` which you\ncan install with `pip install fire`. replace `$destination_folder` with the folder of you choice in the command below:\n\n```bash\nnpdoc-to-md render-folder ./wiki_templates $destination_folder\n```\n\n# contributing\n\npull requests/issues are welcome.\n\n# development\n\ni develop the library inside of **jupyter lab** using the [**jupytext**](https://github.com/mwouts/jupytext) extension.\n\ni recommend using this extension for the best experience.\nit will split code blocks within modules in notebook cells and will allow **interactive development**.\n\nif you wish you can also use the provided **conda environment** (see `environment.yml` file) inside of jupyter lab/notebook\nthanks to [**nb_conda_kernels**](https://github.com/anaconda-platform/nb_conda_kernels).\n\n# testing\n\n## pytest\n\nyou can test one or multiple of the following sql flavors (you will of course need a live database for this): postgresql, sqlite or mysql.\n\nnote: in one of the tests of `pangres` we will try to drop and then create a postgresql schema called `pangres_create_schema_test`. if the schema existed and was not empty an error will be raised.\n\nclone pangres then set your curent working directory to the root of the cloned repository folder. then use the commands below. you will have to replace the following variables in those commands:\n* `sqlite_connection_string`: replace with a sqlite sqlalchemy connection string (e.g. \"sqlite:///test.db\")\n* `async_sqlite_connection_string`: replace with an asynchronous sqlite sqlalchemy connection string (e.g. \"sqlite+aiosqlite:///test.db\")\n* `postgres_connection_string`: replace with a postgres sqlalchemy connection string (e.g. \"postgres:///user:password@localhost:5432/database\"). specifying schema is optional for postgres (will default to public)\n* `async_postgres_connection_string`: replace with an asynchronous postgres sqlalchemy connection string (e.g. \"postgres+asyncpg:///user:password@localhost:5432/database\"). specifying schema is optional for postgres (will default to public)\n* `mysql_connection_string`: replace with a mysql sqlalchemy connection string (e.g. \"mysql+pymysql:///user:password@localhost:3306/database\")\n* `async_mysql_connection_string`: replace with an asynchronous mysql sqlalchemy connection string (e.g. \"mysql+aiomysql:///user:password@localhost:3306/database\")\n* `pg_schema` (optional): schema for postgres (defaults to public)\n\n```bash\n# 1. create and activate the build environment\nconda env create -f environment.yml\nconda activate pangres-dev\n# 2. install pangres in editable mode (changes are reflected upon reimporting)\npip install -e .\n# 3. run pytest\n# -s prints stdout\n# -v prints test parameters\n# --cov=./pangres shows coverage only for pangres\n# --doctest-modules tests with doctest in all modules\n# --benchmark-xxx : these are options for benchmarks tests (see https://pytest-benchmark.readthedocs.io/en/latest/usage.html)\npytest -s -v pangres --cov=pangres --doctest-modules --async_sqlite_conn=$async_sqlite_connection_string --sqlite_conn=$sqlite_connection_string --async_pg_conn=$async_postgres_connection_string --pg_conn=$postgres_connection_string --async_mysql_conn=$async_mysql_connection_string --mysql_conn=$mysql_connection_string --pg_schema=tests --benchmark-group-by=func,param:engine,param:nb_rows --benchmark-columns=min,max,mean,rounds --benchmark-sort=name --benchmark-name=short\n```\n\nadditionally, the following flags could be of interest for you:\n* `-x` for stopping at the first failure\n* `--benchmark-only` for only testing benchmarks\n* `--benchmark-skip` for skipping benchmarks\n\n## flake8\n\nflake8 must run without errors for pipelines to succeed.\nif you are not using the conda environment, you can install flake8 with: `pip install flake8`.\n\nto test flake8 locally you can simply execute this command:\n\n```\nflake8 .\n```\n\n",
  "docs_url": null,
  "keywords": "pandas,postgres,mysql,sqlite",
  "license": "the unlicense",
  "name": "pangres",
  "package_url": "https://pypi.org/project/pangres/",
  "project_url": "https://pypi.org/project/pangres/",
  "project_urls": {
    "Download": "https://github.com/ThibTrip/pangres/archive/v4.2.1.tar.gz",
    "Homepage": "https://github.com/ThibTrip/pangres"
  },
  "release_url": "https://pypi.org/project/pangres/4.2.1/",
  "requires_dist": [
    "pandas>=0.25.3",
    "sqlalchemy>=1.3.12",
    "alembic>=1.3.1",
    "packaging>=21.0"
  ],
  "requires_python": "",
  "summary": "postgres insert update with pandas dataframes.",
  "version": "4.2.1",
  "releases": [],
  "developers": [
    "thibault.betremieux@gmail.com"
  ],
  "kwds": "pandas svg badge forestgreen withjupyter",
  "license_kwds": "the unlicense",
  "libtype": "pypi",
  "id": "pypi_pangres",
  "homepage": "https://github.com/thibtrip/pangres",
  "release_count": 22,
  "dependency_ids": [
    "pypi_alembic",
    "pypi_packaging",
    "pypi_pandas",
    "pypi_sqlalchemy"
  ]
}