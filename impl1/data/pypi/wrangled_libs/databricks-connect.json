{
  "classifiers": [
    "development status :: 5 - production/stable",
    "license :: other/proprietary license",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.12",
    "programming language :: python :: implementation :: cpython",
    "programming language :: python :: implementation :: pypy"
  ],
  "description": "# databricks connect\n\ndatabricks connect is a python library to run pyspark dataframe queries on a remote spark cluster.\ndatabricks connect leverages the power of [spark connect].\nan application using databricks connect runs locally, and when the results of a dataframe query\nneed to be evaluated, the query is run on a configured databricks cluster.\n\nthe following is a simple python code that uses databricks connect and prints out a  number range.\nthe number range query is executed on the databricks cluster.\n\n```python\nfrom databricks.connect import databrickssession\n\nsession = databrickssession.builder.getorcreate()\n\ndf = session.range(1, 10)\ndf.show()\n```\n\n## specifying connection parameters\n\n`databrickssession` offers a few ways to specify the databricks workspace, cluster and user\ncredentials, collectively referred to in the rest of this document as connection parameters.\nthe specified credentials are used to execute the dataframe queries on the cluster. this user must\nhave cluster access permissions and appropriate data access permissions.\n\n*note:* currently, databricks connect only supports credentials based on [personal access\ntoken](https://docs.databricks.com/administration-guide/access-control/tokens.html). other\nauthentication mechanisms are coming soon.\n\nwhen `databrickssession` is initialized with no additional parameters as below, connection\nparameters are picked up from the environment.\n\n```python\nsession = databrickssession.builder.getorcreate()\n```\n\nfirst, the `spark_remote` environment variable is used if it's configured.\n\nif configured, the `spark_remote` environment variable must contain the spark connect connection\nstring.  read more about spark connect [connection string].\n\n```sh\nspark_remote=\"sc://<databricks workspace url>:443/;token=<bearer token>;x-databricks-cluster-id=<cluster id>\"\n```\n\nif this environment variable is not configured, databricks connect will now look for connection\nparameters using the [databricks sdk].\n\nthe databricks python sdk reads these values from two locations - first from environment variables\nthat may be configured. for parameters not configured via environment variables, the 'default'\nprofile, if set up, from the configuration file `.databrickscfg`. databricks python sdk facilitates\noauth token refreshing and enables service principal client credentials support on aws and azure. \nthe details on the authentication process, environment variables, and other configuration options \ncan be found in the [databricks sdk].\n\n> similar to the authentication environment variables, the databricks sdk reads the cluster\n> identifier from the environment variable `databricks_cluster_id` or from the `cluster_id` entry\n> in the config file.\n\nin case specific profile of config file needs to be used it can be achieved as follows:\n\n```python\nfrom databricks.connect import databrickssession\n\nsession = databrickssession.builder.profile(\"profile_name\").getorcreate()\n```\n\nconnection parameters can also be specified directly in code.\n\n```python\nsession = databrickssession.builder.remote(\n    host=\"<databricks workspace url>\",\n    cluster_id=\"<databricks cluster id>\",\n    token=\"<bearer token>\"\n).getorcreate()\n```\n\nalternatively, connection can be initialized based on config object from [databricks sdk]\n```python\nfrom databricks.sdk.core import config\n\nconfig = config(...)\ndatabrickssession.builder.sdkconfig(config).getorcreate()\n```\n\nthe spark connect [connection string] can also be specified directly in code.\n\n```python\nsession = databrickssession.builder\\\n    .remote(\"sc://<databricks workspace url>:443/;token=<bearer token>;x-databricks-cluster-id=<cluster id>\")\\\n    .getorcreate()\n```\n\nin summary, connection parameters are collected in the following order. when all connection\nparameters are available, evaluation is stopped.\n1. specified directly using `remote()`, either as a connection string or as keyword arguments.\n2. specified via the databricks sdk using `sdkconfig()` or using `profile`.\n3. specified in the `spark_remote` environment variable.\n4. specified via the [databricks sdk]'s default authentication.\n\n### debugging\n\ndatabricks connect can generate debug logs in case they are needed for inspection.\n\ndebug logs can be enabled by setting environment variable `spark_connect_log_level=debug`, i.e:\n\n```\n$ spark_connect_log_level=debug python3 myprogram.py\n2023-07-24 14:40:28,505 50147 debug enabled debug logs for databricks-connect\n2023-07-24 14:40:28,505 50147 debug ipython module is present.\n2023-07-24 14:40:28,505 50147 debug falling back to default configuration from the sdk.\n2023-07-24 14:40:28,505 50147 debug loaded from environment\n2023-07-24 14:40:28,505 50147 debug attempting to configure auth: pat\n...\n```\n\n### oauth\n\nthe databricks connect module, via the databricks sdk, supports oauth authentication mechanism.\nthis can be configured via configuration profiles in the `.databrickscfg` file.\nsee [tbd: link here] on how to set up and use configuration profiles.\n\nthe following configuration profile snippet sets up oauth integration via the azure cli, and\nshould be added to the `.databrickscfg` file.\n\n```text\n[azure-cli]\nhost = https://adb-xxx.azuredatabricks.net\nauth_type = azure-cli\ncluster_id = <databricks cluster id>\n```\n\nsimilarly, the following snippet sets up oauth integration via azure active directory (aad) service\nprincipal.\n\n```text\n[azure-aad]\nhost = https://adb-xxx.azuredatabricks.net\nazure_tenant_id = 00000000-0000-0000-0000-000000000001\nazure_client_id = 00000000-0000-0000-0000-000000000002\nazure_client_secret = s0m3p@$$wrd\ncluster_id = yyy\n```\n[spark connect]: https://www.databricks.com/blog/2022/07/07/introducing-spark-connect-the-power-of-apache-spark-everywhere.html\n[connection string]: https://github.com/apache/spark/blob/master/connector/connect/docs/client-connection-string.md\n[databricks sdk]: https://docs.databricks.com/dev-tools/sdk-python.html\n[azure cli]: https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli\n\n### custom headers\n\ndatabricks session supports setting custom headers (in case your remote needs it). you can do it as follows:\n\n`databrickssession.builder.header('x-custom-header', 'value').getorcreate()`\n\nthis can be combined with other session configurations.\n",
  "docs_url": null,
  "keywords": "",
  "license": "databricks proprietary license",
  "name": "databricks-connect",
  "package_url": "https://pypi.org/project/databricks-connect/",
  "project_url": "https://pypi.org/project/databricks-connect/",
  "project_urls": null,
  "release_url": "https://pypi.org/project/databricks-connect/14.2.0/",
  "requires_dist": [
    "py4j ==0.10.9.7",
    "six",
    "pandas >=1.0.5",
    "pyarrow >=4.0.0",
    "grpcio >=1.56.0",
    "grpcio-status >=1.56.0",
    "googleapis-common-protos >=1.56.4",
    "numpy >=1.15",
    "databricks-sdk >=0.1.11",
    "pandas >=1.0.5 ; extra == 'connect'",
    "pyarrow >=4.0.0 ; extra == 'connect'",
    "grpcio >=1.56.0 ; extra == 'connect'",
    "grpcio-status >=1.56.0 ; extra == 'connect'",
    "googleapis-common-protos >=1.56.4 ; extra == 'connect'",
    "numpy >=1.15 ; extra == 'connect'",
    "numpy >=1.15 ; extra == 'ml'",
    "numpy >=1.15 ; extra == 'mllib'",
    "pandas >=1.0.5 ; extra == 'pandas_on_spark'",
    "pyarrow >=4.0.0 ; extra == 'pandas_on_spark'",
    "numpy >=1.15 ; extra == 'pandas_on_spark'",
    "pandas >=1.0.5 ; extra == 'sql'",
    "pyarrow >=4.0.0 ; extra == 'sql'",
    "numpy >=1.15 ; extra == 'sql'"
  ],
  "requires_python": ">=3.10",
  "summary": "databricks connect client",
  "version": "14.2.0",
  "releases": [],
  "developers": [
    "databricks",
    "feedback@databricks.com"
  ],
  "kwds": "databricks_cluster_id spark_remote spark_connect_log_level pyspark databrickscfg",
  "license_kwds": "databricks proprietary license",
  "libtype": "pypi",
  "id": "pypi_databricks_connect",
  "homepage": "",
  "release_count": 88,
  "dependency_ids": [
    "pypi_databricks_sdk",
    "pypi_googleapis_common_protos",
    "pypi_grpcio",
    "pypi_grpcio_status",
    "pypi_numpy",
    "pypi_pandas",
    "pypi_py4j",
    "pypi_pyarrow",
    "pypi_six"
  ]
}