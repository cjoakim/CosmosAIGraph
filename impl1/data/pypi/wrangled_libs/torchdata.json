{
  "classifiers": [
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: bsd license",
    "operating system :: macos :: macos x",
    "operating system :: microsoft :: windows",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "programming language :: python :: implementation :: cpython",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "# torchdata (see note below on current status)\n\n[**why torchdata?**](#why-composable-data-loading) | [**install guide**](#installation) |\n[**what are datapipes?**](#what-are-datapipes) | [**beta usage and feedback**](#beta-usage-and-feedback) |\n[**contributing**](#contributing) | [**future plans**](#future-plans)\n\n**:warning: as of july 2023, we have paused active development on torchdata and have paused new releases. we have learnt a lot from building it and hearing from users, but also believe we need to re-evaluate the technical design and approach given how much the industry has changed since we began the project. during the rest of 2023 we will be re-evaluating our plans in this space. please reach out if you suggestions or comments (please use [#1196](https://github.com/pytorch/data/issues/1196) for feedback).**\n\n`torchdata` is a library of common modular data loading primitives for easily constructing flexible and performant data\npipelines.\n\nthis library introduces composable iterable-style and map-style building blocks called\n[`datapipes`](#what-are-datapipes) that work well out of the box with the pytorch's `dataloader`. these built-in\n`datapipes` have the necessary functionalities to reproduce many different datasets in torchvision and torchtext, namely\nloading files (from local or cloud), parsing, caching, transforming, filtering, and many more utilities. to understand\nthe basic structure of `datapipes`, please see [what are datapipes?](#what-are-datapipes) below, and to see how\n`datapipes` can be practically composed together into datasets, please see our\n[examples](https://pytorch.org/data/main/examples.html).\n\non top of `datapipes`, this library provides a new `dataloader2` that allows the execution of these data pipelines in\nvarious settings and execution backends (`readingservice`). you can learn more about the new version of `dataloader2` in\nour [full dataloader2 documentation](https://pytorch.org/data/main/dataloader2.html#dataloader2). additional features\nare work in progres, such as checkpointing and advanced control of randomness and determinism.\n\nnote that because many features of the original dataloader have been modularized into datapipes, their source codes live\nas [standard datapipes in pytorch/pytorch](https://github.com/pytorch/pytorch/tree/master/torch/utils/data/datapipes)\nrather than torchdata to preserve backward-compatibility support and functional parity within `torch`. regardless, you\ncan to them by importing them from `torchdata`.\n\n## why composable data loading?\n\nover many years of feedback and organic community usage of the pytorch `dataloader` and `dataset`, we've found that:\n\n1. the original `dataloader` bundled too many features together, making them difficult to extend, manipulate, or\n   replace. this has created a proliferation of use-case specific `dataloader` variants in the community rather than an\n   ecosystem of interoperable elements.\n2. many libraries, including each of the pytorch domain libraries, have rewritten the same data loading utilities over\n   and over again. we can save oss maintainers time and effort rewriting, debugging, and maintaining these commonly used\n   elements.\n\nthese reasons inspired the creation of `datapipe` and `dataloader2`, with a goal to make data loading components more\nflexible and reusable.\n\n## installation\n\n### version compatibility\n\nthe following is the corresponding `torchdata` versions and supported python versions.\n\n| `torch`              | `torchdata`        | `python`          |\n| -------------------- | ------------------ | ----------------- |\n| `master` / `nightly` | `main` / `nightly` | `>=3.8`, `<=3.11` |\n| `2.0.0`              | `0.6.0`            | `>=3.8`, `<=3.11` |\n| `1.13.1`             | `0.5.1`            | `>=3.7`, `<=3.10` |\n| `1.12.1`             | `0.4.1`            | `>=3.7`, `<=3.10` |\n| `1.12.0`             | `0.4.0`            | `>=3.7`, `<=3.10` |\n| `1.11.0`             | `0.3.0`            | `>=3.7`, `<=3.10` |\n\n### colab\n\nfollow the instructions\n[in this colab notebook](https://colab.research.google.com/drive/1esvp-eudypj0sd0x_mv9s9vke8rndg1u). the notebook also\ncontains a simple usage example.\n\n### local pip or conda\n\nfirst, set up an environment. we will be installing a pytorch binary as well as torchdata. if you're using conda, create\na conda environment:\n\n```bash\nconda create --name torchdata\nconda activate torchdata\n```\n\nif you wish to use `venv` instead:\n\n```bash\npython -m venv torchdata-env\nsource torchdata-env/bin/activate\n```\n\ninstall torchdata:\n\nusing pip:\n\n```bash\npip install torchdata\n```\n\nusing conda:\n\n```bash\nconda install -c pytorch torchdata\n```\n\nyou can then proceed to run [our examples](https://github.com/pytorch/data/tree/main/examples), such as\n[the imdb one](https://github.com/pytorch/data/blob/main/examples/text/imdb.py).\n\n### from source\n\n```bash\npip install .\n```\n\nif you'd like to include the s3 io datapipes and aws-sdk-cpp, you may also follow\n[the instructions here](https://github.com/pytorch/data/blob/main/torchdata/datapipes/iter/load/readme.md)\n\nin case building torchdata from source fails, install the nightly version of pytorch following the linked guide on the\n[contributing page](https://github.com/pytorch/data/blob/main/contributing.md#install-pytorch-nightly).\n\n### from nightly\n\nthe nightly version of torchdata is also provided and updated daily from main branch.\n\nusing pip:\n\n```bash\npip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n```\n\nusing conda:\n\n```bash\nconda install torchdata -c pytorch-nightly\n```\n\n## what are datapipes?\n\nearly on, we observed widespread confusion between the pytorch `dataset` which represented reusable loading tooling\n(e.g. [torchvision's `imagefolder`](https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py#l272)),\nand those that represented pre-built iterators/accessors over actual data corpora (e.g. torchvision's\n[imagenet](https://github.com/pytorch/vision/blob/main/torchvision/datasets/imagenet.py#l21)). this led to an\nunfortunate pattern of siloed inheritance of data tooling rather than composition.\n\n`datapipe` is simply a renaming and repurposing of the pytorch `dataset` for composed usage. a `datapipe` takes in some\naccess function over python data structures, `__iter__` for `iterdatapipes` and `__getitem__` for `mapdatapipes`, and\nreturns a new access function with a slight transformation applied. for example, take a look at this `jsonparser`, which\naccepts an iterdatapipe over file names and raw streams, and produces a new iterator over the filenames and deserialized\ndata:\n\n```py\nimport json\n\nclass jsonparseriterdatapipe(iterdatapipe):\n    def __init__(self, source_datapipe, **kwargs) -> none:\n        self.source_datapipe = source_datapipe\n        self.kwargs = kwargs\n\n    def __iter__(self):\n        for file_name, stream in self.source_datapipe:\n            data = stream.read()\n            yield file_name, json.loads(data, **self.kwargs)\n\n    def __len__(self):\n        return len(self.source_datapipe)\n```\n\nyou can see in this example how datapipes can be easily chained together to compose graphs of transformations that\nreproduce sophisticated data pipelines, with streamed operation as a first-class citizen.\n\nunder this naming convention, `dataset` simply refers to a graph of `datapipes`, and a dataset module like `imagenet`\ncan be rebuilt as a factory function returning the requisite composed `datapipes`. note that the vast majority of\nbuilt-in features are implemented as `iterdatapipes`, we encourage the usage of built-in `iterdatapipe` as much as\npossible and convert them to `mapdatapipe` only when necessary.\n\n## dataloader2\n\na new, light-weight dataloader2 is introduced to decouple the overloaded data-manipulation functionalities from\n`torch.utils.data.dataloader` to `datapipe` operations. besides, certain features can only be achieved with\n`dataloader2`, such as like checkpointing/snapshotting and switching backend services to perform high-performant\noperations.\n\nplease read the [full documentation here](https://pytorch.org/data/main/dataloader2.html).\n\n## tutorial\n\na tutorial of this library is\n[available here on the documentation site](https://pytorch.org/data/main/dp_tutorial.html). it covers four topics:\n[using datapipes](https://pytorch.org/data/main/dp_tutorial.html#using-datapipes),\n[working with dataloader](https://pytorch.org/data/main/dp_tutorial.html#working-with-dataloader),\n[implementing datapipes](https://pytorch.org/data/main/dp_tutorial.html#implementing-a-custom-datapipe), and\n[working with cloud storage providers](https://pytorch.org/data/main/dp_tutorial.html#working-with-cloud-storage-providers).\n\nthere is also a tutorial available on\n[how to work with the new dataloader2](https://pytorch.org/data/main/dlv2_tutorial.html).\n\n## usage examples\n\nwe provide a simple usage example in this\n[colab notebook](https://colab.research.google.com/drive/1esvp-eudypj0sd0x_mv9s9vke8rndg1u). it can also be downloaded\nand executed locally as a jupyter notebook.\n\nin addition, there are several data loading implementations of popular datasets across different research domains that\nuse `datapipes`. you can find a few [selected examples here](https://pytorch.org/data/main/examples.html).\n\n## frequently asked questions (faq)\n\n<details>\n<summary>\nwhat should i do if the existing set of datapipes does not do what i need?\n</summary>\n\nyou can\n[implement your own custom datapipe](https://pytorch.org/data/main/dp_tutorial.html#implementing-a-custom-datapipe). if\nyou believe your use case is common enough such that the community can benefit from having your custom datapipe added to\nthis library, feel free to open a github issue. we will be happy to discuss!\n\n</details>\n\n<details>\n<summary>\nwhat happens when the <code>shuffler</code> datapipe is used with dataloader?\n</summary>\n\nin order to enable shuffling, you need to add a `shuffler` to your datapipe line. then, by default, shuffling will\nhappen at the point where you specified as long as you do not set `shuffle=false` within dataloader.\n\n</details>\n\n<details>\n<summary>\nwhat happens when the <code>batcher</code> datapipe is used with dataloader?\n</summary>\n\nif you choose to use `batcher` while setting `batch_size > 1` for dataloader, your samples will be batched more than\nonce. you should choose one or the other.\n\n</details>\n\n<details>\n<summary>\nwhy are there fewer built-in <code>mapdatapipes</code> than <code>iterdatapipes</code>?\n</summary>\n\nby design, there are fewer `mapdatapipes` than `iterdatapipes` to avoid duplicate implementations of the same\nfunctionalities as `mapdatapipe`. we encourage users to use the built-in `iterdatapipe` for various functionalities, and\nconvert it to `mapdatapipe` as needed.\n\n</details>\n\n<details>\n<summary>\nhow is multiprocessing handled with datapipes?\n</summary>\n\nmulti-process data loading is still handled by the `dataloader`, see the\n[dataloader documentation for more details](https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading).\nas of pytorch version >= 1.12.0 (torchdata version >= 0.4.0), data sharding is automatically done for datapipes within\nthe `dataloader` as long as a `shardingfilter` datapipe exists in your pipeline. please see the\n[tutorial](https://pytorch.org/data/main/dp_tutorial.html#working-with-dataloader) for an example.\n\n</details>\n\n<details>\n<summary>\nwhat is the upcoming plan for dataloader?\n</summary>\n\n`dataloader2` is in the prototype phase and more features are actively being developed. please see the\n[readme file in `torchdata/dataloader2`](https://github.com/pytorch/data/blob/main/torchdata/dataloader2/readme.md). if\nyou would like to experiment with it (or other prototype features), we encourage you to install the nightly version of\nthis library.\n\n</details>\n\n<details>\n<summary>\nwhy is there an error saying the specified dll could not be found at the time of importing <code>portalocker</code>?\n</summary>\n\nit only happens for people who runs `torchdata` on windows os as a common problem with `pywin32`. and, you can find the\nreason and the solution for it in the\n[link](https://github.com/mhammond/pywin32#the-specified-procedure-could-not-be-found--entry-point-not-found-errors).\n\n</details>\n\n## contributing\n\nwe welcome prs! see the [contributing](contributing.md) file.\n\n## beta usage and feedback\n\nwe'd love to hear from and work with early adopters to shape our designs. please reach out by raising an issue if you're\ninterested in using this tooling for your project.\n\n## license\n\ntorchdata is bsd licensed, as found in the [license](license) file.\n",
  "docs_url": null,
  "keywords": "",
  "license": "bsd",
  "name": "torchdata",
  "package_url": "https://pypi.org/project/torchdata/",
  "project_url": "https://pypi.org/project/torchdata/",
  "project_urls": {
    "Homepage": "https://github.com/pytorch/data"
  },
  "release_url": "https://pypi.org/project/torchdata/0.7.1/",
  "requires_dist": [
    "urllib3 (>=1.25)",
    "requests",
    "torch (>=2)"
  ],
  "requires_python": ">=3.8",
  "summary": "composable data loading modules for pytorch",
  "version": "0.7.1",
  "releases": [],
  "developers": [
    "packages@pytorch.org",
    "pytorch_team"
  ],
  "kwds": "torchdata datapipes datapipe torchvision torch",
  "license_kwds": "bsd",
  "libtype": "pypi",
  "id": "pypi_torchdata",
  "homepage": "https://github.com/pytorch/data",
  "release_count": 11,
  "dependency_ids": [
    "pypi_requests",
    "pypi_torch",
    "pypi_urllib3"
  ]
}