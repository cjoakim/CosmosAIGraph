{
  "classifiers": [
    "development status :: 3 - alpha",
    "intended audience :: developers",
    "license :: osi approved :: mit license",
    "operating system :: macos :: macos x",
    "operating system :: microsoft :: windows",
    "operating system :: posix",
    "programming language :: python",
    "programming language :: python :: 3",
    "topic :: software development :: libraries",
    "topic :: software development :: testing",
    "topic :: utilities"
  ],
  "description": "pytest-cache: working with cross-testrun state\n=====================================================\n\nusage\n---------\n\ninstall via::\n\n    pip install pytest-cache\n\nafter which other plugins can access a new `config.cache`_ object \nwhich helps sharing values between ``py.test`` invocations.\n\nthe plugin provides two options to rerun failures, namely ``--lf`` to\nonly re-run the failures and ``--ff`` to run all tests but the failures\nfrom the last run first.  for cleanup (usually not needed), a\n``--clearcache`` option allows to remove all cross-session cache\ncontents ahead of a test run.\n\n\nrerunning only failures or failures first\n-----------------------------------------------\n\nfirst, let's create 50 test invocation of which only 2 fail::\n\n    # content of test_50.py\n    import pytest\n\n    @pytest.mark.parametrize(\"i\", range(50))\n    def test_num(i):\n        if i in (17,25):\n           pytest.fail(\"bad luck\") \n\nif you run this for the first time you will see two failures::\n\n    $ py.test -q\n    .................f.......f........................\n    =================================== failures ===================================\n    _________________________________ test_num[17] _________________________________\n    \n    i = 17\n    \n        @pytest.mark.parametrize(\"i\", range(50))\n        def test_num(i):\n            if i in (17,25):\n    >          pytest.fail(\"bad luck\")\n    e          failed: bad luck\n    \n    test_50.py:6: failed\n    _________________________________ test_num[25] _________________________________\n    \n    i = 25\n    \n        @pytest.mark.parametrize(\"i\", range(50))\n        def test_num(i):\n            if i in (17,25):\n    >          pytest.fail(\"bad luck\")\n    e          failed: bad luck\n    \n    test_50.py:6: failed\n\nif you then run it with ``--lf`` you will run only the two failing test\nfrom the last run::\n\n    $ py.test --lf\n    ============================= test session starts ==============================\n    platform linux2 -- python 2.7.3 -- pytest-2.3.5\n    run-last-failure: rerun last 2 failures\n    plugins: cache\n    collected 50 items\n    \n    test_50.py ff\n    \n    =================================== failures ===================================\n    _________________________________ test_num[17] _________________________________\n    \n    i = 17\n    \n        @pytest.mark.parametrize(\"i\", range(50))\n        def test_num(i):\n            if i in (17,25):\n    >          pytest.fail(\"bad luck\")\n    e          failed: bad luck\n    \n    test_50.py:6: failed\n    _________________________________ test_num[25] _________________________________\n    \n    i = 25\n    \n        @pytest.mark.parametrize(\"i\", range(50))\n        def test_num(i):\n            if i in (17,25):\n    >          pytest.fail(\"bad luck\")\n    e          failed: bad luck\n    \n    test_50.py:6: failed\n    =================== 2 failed, 48 deselected in 0.02 seconds ====================\n\nthe last line indicates that 48 tests have not been run.\n\nif you run with the ``--ff`` option, all tests will be run but the first\nfailures will be executed first (as can be seen from the series of ``ff`` and\ndots)::\n\n    $ py.test --ff\n    ============================= test session starts ==============================\n    platform linux2 -- python 2.7.3 -- pytest-2.3.5\n    run-last-failure: rerun last 2 failures first\n    plugins: cache\n    collected 50 items\n    \n    test_50.py ff................................................\n    \n    =================================== failures ===================================\n    _________________________________ test_num[17] _________________________________\n    \n    i = 17\n    \n        @pytest.mark.parametrize(\"i\", range(50))\n        def test_num(i):\n            if i in (17,25):\n    >          pytest.fail(\"bad luck\")\n    e          failed: bad luck\n    \n    test_50.py:6: failed\n    _________________________________ test_num[25] _________________________________\n    \n    i = 25\n    \n        @pytest.mark.parametrize(\"i\", range(50))\n        def test_num(i):\n            if i in (17,25):\n    >          pytest.fail(\"bad luck\")\n    e          failed: bad luck\n    \n    test_50.py:6: failed\n    ===================== 2 failed, 48 passed in 0.07 seconds ======================\n\n.. _`config.cache`:\n\nthe new config.cache object\n--------------------------------\n\n.. regendoc:wipe\n\nplugins or conftest.py support code can get a cached value \nusing the pytest ``config`` object.  here is a basic example\nplugin which implements a `funcarg <http://pytest.org/latest/funcargs.html>`_\nwhich re-uses previously created state across py.test invocations::\n\n    # content of test_caching.py\n    import time\n\n    def pytest_funcarg__mydata(request):\n        val = request.config.cache.get(\"example/value\", none)\n        if val is none:\n            time.sleep(9*0.6) # expensive computation :)\n            val = 42\n            request.config.cache.set(\"example/value\", val)\n        return val \n\n    def test_function(mydata):\n        assert mydata == 23\n\nif you run this command once, it will take a while because\nof the sleep::\n\n    $ py.test -q\n    f\n    =================================== failures ===================================\n    ________________________________ test_function _________________________________\n    \n    mydata = 42\n    \n        def test_function(mydata):\n    >       assert mydata == 23\n    e       assert 42 == 23\n    \n    test_caching.py:12: assertionerror\n\nif you run it a second time the value will be retrieved from\nthe cache and this will be quick::\n\n    $ py.test -q\n    f\n    =================================== failures ===================================\n    ________________________________ test_function _________________________________\n    \n    mydata = 42\n    \n        def test_function(mydata):\n    >       assert mydata == 23\n    e       assert 42 == 23\n    \n    test_caching.py:12: assertionerror\n\nconsult the `pytest-cache api <http://packages.python.org/pytest-cache/api.html>`_\nfor more details.\n\n\ninspecting cache content\n-------------------------------\n\nyou can always peek at the content of the cache using the\n``--cache`` command line option::\n\n    $ py.test --cache\n    ============================= test session starts ==============================\n    platform linux2 -- python 2.7.3 -- pytest-2.3.5\n    plugins: cache\n    cachedir: /tmp/doc-exec-6/.cache\n    --------------------------------- cache values ---------------------------------\n    example/value contains:\n      42\n    cache/lastfailed contains:\n      set(['test_caching.py::test_function'])\n    \n    ===============================  in 0.01 seconds ===============================\n\nclearing cache content\n-------------------------------\n\nyou can instruct pytest to clear all cache files and values \nby adding the ``--clearcache`` option like this::\n\n    py.test --clearcache\n\nthis is recommended for invocations from continous integration\nservers where isolation and correctness is more important\nthan speed.\n\nnotes\n-------------\n\nrepository: http://bitbucket.org/hpk42/pytest-cache\n\nissues: repository: http://bitbucket.org/hpk42/pytest-cache/issues\n\nmore info on py.test: http://pytest.org",
  "docs_url": "https://pythonhosted.org/pytest-cache/",
  "keywords": "",
  "license": "mit license",
  "name": "pytest-cache",
  "package_url": "https://pypi.org/project/pytest-cache/",
  "project_url": "https://pypi.org/project/pytest-cache/",
  "project_urls": {
    "Download": "UNKNOWN",
    "Homepage": "http://bitbucket.org/hpk42/pytest-cache/"
  },
  "release_url": "https://pypi.org/project/pytest-cache/1.0/",
  "requires_dist": [],
  "requires_python": null,
  "summary": "pytest plugin with mechanisms for caching across test runs",
  "version": "1.0",
  "releases": [],
  "developers": [
    "holger.krekel@gmail.com",
    "holger_krekel"
  ],
  "kwds": "test_caching pytest pytest_funcarg__mydata test_function pip",
  "license_kwds": "mit license",
  "libtype": "pypi",
  "id": "pypi_pytest_cache",
  "homepage": "http://bitbucket.org/hpk42/pytest-cache/",
  "release_count": 2,
  "dependency_ids": []
}