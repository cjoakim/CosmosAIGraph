{
  "classifiers": [
    "development status :: 4 - beta",
    "license :: osi approved :: apache software license",
    "programming language :: python",
    "programming language :: python :: 2",
    "programming language :: python :: 2.7",
    "programming language :: python :: 3",
    "programming language :: python :: 3.3",
    "programming language :: python :: 3.4",
    "programming language :: python :: 3.5",
    "topic :: system :: logging"
  ],
  "description": "azure-storage-logging\n=====================\n\n.. image:: http://img.shields.io/pypi/v/azure-storage-logging.svg?style=flat\n    :target: https://pypi.python.org/pypi/azure-storage-logging\n\n.. image:: http://img.shields.io/pypi/l/azure-storage-logging.svg?style=flat\n    :target: http://www.apache.org/licenses/license-2.0.html\n\n*azure-storage-logging* provides functionality to send output from\nthe standard python logging apis to microsoft azure storage.\n\ndependencies\n------------\n\n* azure-storage 0.33 or newer\n\ninstallation\n------------\n\ninstall the package via pip: ::\n\n    pip install azure-storage-logging\n\nusage\n-----\n\nthe module **azure_storage_logging.handlers** in the package contains\nthe following logging handler classes. each of them uses a different\ntype of microsoft azure storage to send its output to. they all are subclasses\nof the standard python logging handler classes, so you can make use of them\nin the standard ways of python logging configuration.\n\nin addition to\n`the standard formats for logging <http://docs.python.org/2.7/library/logging.html#logrecord-attributes>`_,\nthe special format ``%(hostname)s`` is also available in your message formatter\nfor the handlers. the format is introduced for ease of identifying the source\nof log messages which come from many computers and go to the same storage.\n\ntablestoragehandler\n~~~~~~~~~~~~~~~~~~~\nthe **tablestoragehandler** class is a subclass of **logging.handler** class,\nand it sends log messages to azure table storage and store them\nas entities in the specified table.\n\nthe handler puts a formatted log message from applications in the *message*\nproperty of a table entity along with some system-defined properties\n(*partitionkey*, *rowkey*, and *timestamp*) like this:\n\n+--------------+-----------+----------------+-------------+\n| partitionkey | rowkey    | timestamp      | message     |\n+==============+===========+================+=============+\n| xxxxx        | xxxxxxxxx | yyyy-mm-dd ... | log message |\n+--------------+-----------+----------------+-------------+\n| xxxxx        | xxxxxxxxx | yyyy-mm-dd ... | log message |\n+--------------+-----------+----------------+-------------+\n| xxxxx        | xxxxxxxxx | yyyy-mm-dd ... | log message |\n+--------------+-----------+----------------+-------------+\n\n* *class* azure_storage_logging.handlers.tablestoragehandler(*account_name=none, account_key=none, protocol='https', table='logs', batch_size=0, extra_properties=none, partition_key_formatter=none, row_key_formatter=none, is_emulated=false*)\n\n    returns a new instance of the **tablestoragehandler** class. \n    the instance is initialized with the name and the key of your\n    azure storage account and some optional parameters.\n\n    the *table* specifies the name of the table that stores log messages.\n    a new table will be created if it doesn't exist. the table name must\n    conform to the naming convention for azure storage table, see\n    `the naming convention for tables <http://msdn.microsoft.com/library/azure/dd179338.aspx>`_\n    for more details.\n\n    the *protocol* specifies the protocol to transfer data between\n    azure storage and your application, ``http`` and ``https``\n    are supported.\n\n    you can specify the *batch_size* in an integer if you want to use\n    batch transaction when creating new log entities. if the *batch_size*\n    is greater than 1, all new log entities will be transferred to the\n    table at a time when the number of new log messages reaches the\n    *batch_size*. otherwise, a new log entity will be transferred to\n    the table every time a logging is performed. the *batch_size* must be\n    up to 100 (maximum number of entities in a batch transaction for\n    azure storage table).\n\n    the *extra_properties* accepts a sequence of\n    `the formats for logging <http://docs.python.org/2.7/library/logging.html#logrecord-attributes>`_.\n    the handler-specific one ``%(hostname)s`` is also acceptable.\n    the handler assigns an entity property for every format specified in\n    *extra_properties*. here is an example of using extra properties:\n\n    ::\n        \n        import logging\n        from azure_storage_logging.handlers import tablestoragehandler\n        \n        # configure the handler and add it to the logger\n        logger = logging.getlogger('example')\n        handler = tablestoragehandler(account_name='mystorageaccountname',\n                                      account_key='mystorageaccountkey',\n                                      extra_properties=('%(hostname)s',\n                                                        '%(levelname)s'))\n        logger.addhandler(handler)\n        \n        # output log messages\n        logger.info('info message')\n        logger.warning('warning message')\n        logger.error('error message')\n\n    and it will create the log entities, that have the extra properties\n    in addition to the regular property *message*, into the table like this:\n\n    +--------------+-----------+----------------+----------+-----------+---------------+\n    | partitionkey | rowkey    | timestamp      | hostname | levelname | message       |\n    +==============+===========+================+==========+===========+===============+\n    | xxxxx        | xxxxxxxxx | yyyy-mm-dd ... | myhost   | info      | info message  |\n    +--------------+-----------+----------------+----------+-----------+---------------+\n    | xxxxx        | xxxxxxxxx | yyyy-mm-dd ... | myhost   | warning   | warn message  |\n    +--------------+-----------+----------------+----------+-----------+---------------+\n    | xxxxx        | xxxxxxxxx | yyyy-mm-dd ... | myhost   | error     | error message |\n    +--------------+-----------+----------------+----------+-----------+---------------+\n\n    you can specify an instance of your custom **logging.formatters**\n    for the *partition_key_formatter* or the *row_key_formatter*\n    if you want to implement your own keys for the table.\n    the default formatters will be used for partition keys and row keys\n    if no custom formatter for them is given to the handler.\n    the default values for partition keys are provided by the format\n    ``%(asctime)s`` and the date format ``%y%m%d%h%m`` (provides a unique\n    value per minute). the default values for row keys are provided by the\n    format ``%(asctime)s%(msecs)03d-%(hostname)s-%(process)d-%(rowno)02d``\n    and the date format ``%y%m%d%h%m%s``.\n\n    note that the format ``%(rowno)d`` is a handler-specific one only\n    available for row keys. it would be formatted to a sequential and\n    unique number in a batch that starts from 0. the format is introduced\n    to avoid collision of row keys generated in a batch, and it would\n    always be formatted to 0 if you don't use batch transaction for logging\n    to the table.\n\n* setpartitionkeyformatter(*fmt*)\n\n    sets the handler's formatter for partition keys to *fmt*.\n\n* setrowkeyformatter(*fmt*)\n\n    sets the handler's formatter for row keys to *fmt*.\n\nqueuestoragehandler\n~~~~~~~~~~~~~~~~~~~\n\nthe **queuestoragehandler** class is a subclass of **logging.handler** class,\nand it pushes log messages to specified azure storage queue.\n\nyou can pop log messages from the queue in other applications\nusing azure storage client libraries.\n\n* *class* azure_storage_logging.handlers.queuestoragehandler(*account_name=none, account_key=none, protocol='https', queue='logs', message_ttl=none, visibility_timeout=none, base64_encoding=false, is_emulated=false*)\n\n    returns a new instance of the **queuestoragehandler** class. \n    the instance is initialized with the name and the key of your\n    azure storage account and some optional parameters.\n\n    the *queue* specifies the name of the queue that log messages are added.\n    a new queue will be created if it doesn't exist. the queue name must\n    conform to the naming convention for azure storage queue, see\n    `the naming convention for queues <http://msdn.microsoft.com/library/azure/dd179349.aspx>`_\n    for more details.\n\n    the *protocol* specifies the protocol to transfer data between\n    azure storage and your application, ``http`` and ``https``\n    are supported.\n\n    the *message_ttl* specifies the time-to-live interval for the message,\n    in seconds. the maximum time-to-live allowed is 7 days. if this \n    parameter is omitted, the default time-to-live is 7 days.\n\n    the *visibility_timeout* specifies the visibility timeout value,\n    in seconds, relative to server time. if not specified, the default\n    value is 0 (makes the message visible immediately). the new value\n    must be larger than or equal to 0, and cannot be larger than 7 days.\n    the *visibility_timeout* cannot be set to a value later than the\n    *message_ttl*, and should be set to a value smaller than the\n    *message_ttl*. \n\n    the *base64_encoding* specifies the necessity for encoding\n    log text in base64. if you set this to ``true``, unicode log text\n    in a message is encoded in utf-8 first and then encoded in base64.\n    some of azure storage client libraries or tools assume that\n    text messages in azure storage queue are encoded in base64,\n    so you can set this to ``true`` to receive log messages correctly\n    with those libraries or tools.\n\nblobstoragerotatingfilehandler\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nthe **blobstoragerotatingfilehandler** class is a subclass of\n**logging.handlers.rotatingfilehandler** class. it performs\nlog file rotation and stores the outdated one in azure blob storage\ncontainer when the current file reaches a certain size.\n\n* *class* azure_storage_logging.handlers.blobstoragerotatingfilehandler(*filename, mode='a', maxbytes=0, encoding=none, delay=false, account_name=none, account_key=none, protocol='https', container='logs', zip_compression=false, max_connections=1, max_retries=5, retry_wait=1.0*, is_emulated=false)\n\n    returns a new instance of the **blobstoragerotatingfilehandler**\n    class. the instance is initialized with the name and the key of your\n    azure storage account and some optional parameters.\n\n    see `rotatingfilehandler <http://docs.python.org/2.7/library/logging.handlers.html#rotatingfilehandler>`_\n    for its basic usage. the handler keeps the latest log file into the\n    local file system. meanwhile, the handler sends the outdated log file\n    to the blob container immediately and then removes it from the local\n    file system.\n\n    the *container* specifies the name of the blob container that stores\n    outdated log files. a new container will be created if it doesn't exist.\n    the container name must conform to the naming convention for\n    azure storage blob container, see\n    `the naming convention for blob containers <http://msdn.microsoft.com/library/azure/dd135715.aspx>`_\n    for more details.\n\n    the *protocol* specifies the protocol to transfer data between\n    azure storage and your application, ``http`` and ``https``\n    are supported.\n\n    the *zip_compression* specifies the necessity for compressing\n    every outdated log file in zip format before putting it in\n    the container.\n\n    the *max_connections* specifies a maximum number of parallel\n    connections to use when the blob size exceeds 64mb.\n    set to 1 to upload the blob chunks sequentially.\n    set to 2 or more to upload the blob chunks in parallel,\n    and this uses more system resources but will upload faster.\n\n    the *max_retries* specifies a number of times to retry\n    upload of blob chunk if an error occurs.\n\n    the *retry_wait* specifies sleep time in secs between retries.\n\n    the only two formatters ``%(hostname)s`` and ``%(process)d`` are\n    acceptable as a part of the *filename* or the *container*. you can save\n    log files in a blob container dedicated to each host or process by\n    naming containers with these formatters, and also can store log files\n    from multiple hosts or processes in a blob container by naming log files\n    with them.\n\n    be careful to use the ``%(process)d`` formatter in the *filename*\n    because inconsistent pids assigned to your application every time it\n    gets started are included as a part of the name of log files to search\n    for rotation. you should use the formatter in the *filename* only when\n    the log file is generated by a long-running application process.\n\n    note that the hander class doesn't take the *backupcount* parameter,\n    unlike rotatingfilehandler does. the number of outdated log files\n    that the handler stores in the container is unlimited, and the files\n    are saved with the extension that indicates the time in utc when\n    they are replaced with a new one. if you want to keep the amount of\n    outdated log files in the container in a certain number, you will\n    need to do that using azure management portal or other tools.\n\nblobstoragetimedrotatingfilehandler\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nthe **blobstoragetimedrotatingfilehandler** class is a subclass of\n**logging.handlers.timedrotatingfilehandler** class. it performs\nlog file rotation and stores the outdated one to azure blob storage\ncontainer at certain timed intervals.\n\n* *class* azure_storage_logging.handlers.blobstoragetimedrotatingfilehandler(*filename, when='h', interval=1, encoding=none, delay=false, utc=false, account_name=none, account_key=none, protocol='https', container='logs', zip_compression=false, max_connections=1, max_retries=5, retry_wait=1.0*, is_emulated=false)\n\n    returns a new instance of the **blobstoragetimedrotatingfilehandler**\n    class. the instance is initialized with the name and the key of your\n    azure storage account and some optional parameters.\n\n    see `timedrotatingfilehandler <http://docs.python.org/2.7/library/logging.handlers.html#timedrotatingfilehandler>`_\n    for its basic usage. the handler keeps the latest log file into the\n    local file system. meanwhile, the handler sends the outdated log file\n    to the blob container immediately and then removes it from the local\n    file system.\n\n    the *container* specifies the name of the blob container that stores\n    outdated log files. a new container will be created if it doesn't exist.\n    the container name must conform to the naming convention for\n    azure storage blob container, see\n    `the naming convention for blob containers <http://msdn.microsoft.com/library/azure/dd135715.aspx>`_\n    for more details.\n\n    the *protocol* specifies the protocol to transfer data between\n    azure storage and your application, ``http`` and ``https``\n    are supported.\n\n    the *zip_compression* specifies the necessity for compressing\n    every outdated log file in zip format before putting it in\n    the container.\n\n    the *max_connections* specifies a maximum number of parallel\n    connections to use when the blob size exceeds 64mb.\n    set to 1 to upload the blob chunks sequentially.\n    set to 2 or more to upload the blob chunks in parallel,\n    and this uses more system resources but will upload faster.\n\n    the *max_retries* specifies a number of times to retry\n    upload of blob chunk if an error occurs.\n\n    the *retry_wait* specifies sleep time in secs between retries.\n\n    the only two formatters ``%(hostname)s`` and ``%(process)d`` are\n    acceptable as a part of the *filename* or the *container*. you can save\n    log files in a blob container dedicated to each host or process by\n    naming containers with these formatters, and also can store log files\n    from multiple hosts or processes in a blob container by naming log files\n    with them.\n\n    be careful to use the ``%(process)d`` formatter in the *filename*\n    because inconsistent pids assigned to your application every time it\n    gets started are included as a part of the name of log files to search\n    for rotation. you should use the formatter in the *filename* only when\n    the log file is generated by a long-running application process.\n\n    note that the hander class doesn't take the *backupcount* parameter,\n    unlike timedrotatingfilehandler does. the number of outdated log files\n    that the handler stores in the container is unlimited.\n    if you want to keep the amount of outdated log files in the container\n    in a certain number, you will need to do that using azure\n    management portal or other tools.\n\nexample\n-------\n\nhere is an example of the configurations and the logging that uses\nthree different types of storage from the logger:\n\n::\n\n    logging = {\n        'version': 1,\n        'formatters': {\n            'simple': {\n                'format': '%(asctime)s %(message)s',\n            },\n            'verbose': {\n                'format': '%(asctime)s %(levelname)s %(hostname)s %(process)d %(message)s',\n            },\n            # this is the same as the default, so you can skip configuring it\n            'partition_key': {\n                'format': '%(asctime)s',\n                'datefmt': '%y%m%d%h%m',\n            },\n            # this is the same as the default, so you can skip configuring it\n            'row_key': {\n                'format': '%(asctime)s%(msecs)03d-%(hostname)s-%(process)d-%(rowno)02d',\n                'datefmt': '%y%m%d%h%m%s',\n            },\n        },\n        'handlers': {\n            'file': {\n                'account_name': 'mystorageaccountname',\n                'account_key': 'mystorageaccountkey',\n                'protocol': 'https',\n                'level': 'debug',\n                'class': 'azure_storage_logging.handlers.blobstoragetimedrotatingfilehandler',\n                'formatter': 'verbose',\n                'filename': 'example.log',\n                'when': 'd',\n                'interval': 1,\n                'container': 'logs-%(hostname)s',\n                'zip_compression': false,\n            },\n            'queue': {\n                'account_name': 'mystorageaccountname',\n                'account_key': 'mystorageaccountkey',\n                'protocol': 'https',\n                'queue': 'logs',\n                'level': 'critical',\n                'class': 'azure_storage_logging.handlers.queuestoragehandler',\n                'formatter': 'verbose',\n            },\n            'table': {\n                'account_name': 'mystorageaccountname',\n                'account_key': 'mystorageaccountkey',\n                'protocol': 'https',\n                'table': 'logs',\n                'level': 'info',\n                'class': 'azure_storage_logging.handlers.tablestoragehandler',\n                'formatter': 'simple',\n                'batch_size': 20,\n                'extra_properties': ['%(hostname)s', '%(levelname)s'],\n                'partition_key_formatter': 'cfg://formatters.partition_key',\n                'row_key_formatter': 'cfg://formatters.row_key',\n            },\n        },\n        'loggers': {\n            'example': {\n                'handlers': ['file', 'queue', 'table'],\n                'level': 'debug',\n            },\n        }\n    }\n    \n    import logging\n    from logging.config import dictconfig\n\n    dictconfig(logging)\n    logger = logging.getlogger('example')\n    logger.debug('debug message')\n    logger.info('info message')\n    logger.warning('warning message')\n    logger.error('error message')\n    logger.critical('critical message') \n\nnotice\n------\n\n* set *is_emulated* to ``true`` at initialization of the logging handlers\n  if you want to use this package with azure storage emulator.\n\nlicense\n-------\n\napache license 2.0\n\ncredits\n-------\n\n-  `michiya takahashi <http://github.com/michiya/>`__",
  "docs_url": null,
  "keywords": "azure logging",
  "license": "apache license 2.0",
  "name": "azure-storage-logging",
  "package_url": "https://pypi.org/project/azure-storage-logging/",
  "project_url": "https://pypi.org/project/azure-storage-logging/",
  "project_urls": {
    "Homepage": "https://github.com/michiya/azure-storage-logging"
  },
  "release_url": "https://pypi.org/project/azure-storage-logging/0.5.1/",
  "requires_dist": [],
  "requires_python": "",
  "summary": "logging handlers to send logs to microsoft azure storage",
  "version": "0.5.1",
  "releases": [],
  "developers": [
    "michiya.takahashi@gmail.com",
    "michiya_takahashi"
  ],
  "kwds": "azure_storage_logging azure logging logs storage",
  "license_kwds": "apache license 2.0",
  "libtype": "pypi",
  "id": "pypi_azure_storage_logging",
  "homepage": "https://github.com/michiya/azure-storage-logging",
  "release_count": 10,
  "dependency_ids": []
}