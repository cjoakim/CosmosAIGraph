{
  "classifiers": [
    "development status :: 7 - inactive",
    "framework :: aws cdk",
    "framework :: aws cdk :: 1",
    "intended audience :: developers",
    "license :: osi approved",
    "operating system :: os independent",
    "programming language :: javascript",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "typing :: typed"
  ],
  "description": "# amazon ec2 auto scaling construct library\n\n<!--begin stability banner-->---\n\n\n![end-of-support](https://img.shields.io/badge/end--of--support-critical.svg?style=for-the-badge)\n\n> aws cdk v1 has reached end-of-support on 2023-06-01.\n> this package is no longer being updated, and users should migrate to aws cdk v2.\n>\n> for more information on how to migrate, see the [*migrating to aws cdk v2* guide](https://docs.aws.amazon.com/cdk/v2/guide/migrating-v2.html).\n\n---\n<!--end stability banner-->\n\nthis module is part of the [aws cloud development kit](https://github.com/aws/aws-cdk) project.\n\n## auto scaling group\n\nan `autoscalinggroup` represents a number of instances on which you run your code. you\npick the size of the fleet, the instance type and the os image:\n\n```python\n# vpc: ec2.vpc\n\n\nautoscaling.autoscalinggroup(self, \"asg\",\n    vpc=vpc,\n    instance_type=ec2.instancetype.of(ec2.instanceclass.burstable2, ec2.instancesize.micro),\n    machine_image=ec2.amazonlinuximage()\n)\n```\n\nnote: autoscalinggroup has an property called `allowalloutbound` (allowing the instances to contact the\ninternet) which is set to `true` by default. be sure to set this to `false`  if you don't want\nyour instances to be able to start arbitrary connections. alternatively, you can specify an existing security\ngroup to attach to the instances that are launched, rather than have the group create a new one.\n\n```python\n# vpc: ec2.vpc\n\n\nmy_security_group = ec2.securitygroup(self, \"securitygroup\", vpc=vpc)\nautoscaling.autoscalinggroup(self, \"asg\",\n    vpc=vpc,\n    instance_type=ec2.instancetype.of(ec2.instanceclass.burstable2, ec2.instancesize.micro),\n    machine_image=ec2.amazonlinuximage(),\n    security_group=my_security_group\n)\n```\n\nalternatively you can create an `autoscalinggroup` from a `launchtemplate`:\n\n```python\n# vpc: ec2.vpc\n# launch_template: ec2.launchtemplate\n\n\nautoscaling.autoscalinggroup(self, \"asg\",\n    vpc=vpc,\n    launch_template=launch_template\n)\n```\n\nto launch a mixture of spot and on-demand instances, and/or with multiple instance types, you can create an `autoscalinggroup` from a mixedinstancespolicy:\n\n```python\n# vpc: ec2.vpc\n# launch_template1: ec2.launchtemplate\n# launch_template2: ec2.launchtemplate\n\n\nautoscaling.autoscalinggroup(self, \"asg\",\n    vpc=vpc,\n    mixed_instances_policy=autoscaling.mixedinstancespolicy(\n        instances_distribution=autoscaling.instancesdistribution(\n            on_demand_percentage_above_base_capacity=50\n        ),\n        launch_template=launch_template1,\n        launch_template_overrides=[autoscaling.launchtemplateoverrides(instance_type=ec2.instancetype(\"t3.micro\")), autoscaling.launchtemplateoverrides(instance_type=ec2.instancetype(\"t3a.micro\")), autoscaling.launchtemplateoverrides(instance_type=ec2.instancetype(\"t4g.micro\"), launch_template=launch_template2)]\n    )\n)\n```\n\n## machine images (amis)\n\namis control the os that gets launched when you start your ec2 instance. the ec2\nlibrary contains constructs to select the ami you want to use.\n\ndepending on the type of ami, you select it a different way.\n\nthe latest version of amazon linux and microsoft windows images are\nselectable by instantiating one of these classes:\n\n```python\n# pick a windows edition to use\nwindows = ec2.windowsimage(ec2.windowsversion.windows_server_2019_english_full_base)\n\n# pick the right amazon linux edition. all arguments shown are optional\n# and will default to these values when omitted.\namzn_linux = ec2.amazonlinuximage(\n    generation=ec2.amazonlinuxgeneration.amazon_linux,\n    edition=ec2.amazonlinuxedition.standard,\n    virtualization=ec2.amazonlinuxvirt.hvm,\n    storage=ec2.amazonlinuxstorage.general_purpose\n)\n\n# for other custom (linux) images, instantiate a `genericlinuximage` with\n# a map giving the ami to in for each region:\n\nlinux = ec2.genericlinuximage({\n    \"us-east-1\": \"ami-97785bed\",\n    \"eu-west-1\": \"ami-12345678\"\n})\n```\n\n> note: the amazon linux images selected will be cached in your `cdk.json`, so that your\n> autoscalinggroups don't automatically change out from under you when you're making unrelated\n> changes. to update to the latest version of amazon linux, remove the cache entry from the `context`\n> section of your `cdk.json`.\n>\n> we will add command-line options to make this step easier in the future.\n\n## autoscaling instance counts\n\nautoscalinggroups make it possible to raise and lower the number of instances in the group,\nin response to (or in advance of) changes in workload.\n\nwhen you create your autoscalinggroup, you specify a `mincapacity` and a\n`maxcapacity`. autoscaling policies that respond to metrics will never go higher\nor lower than the indicated capacity (but scheduled scaling actions might, see\nbelow).\n\nthere are three ways to scale your capacity:\n\n* **in response to a metric** (also known as step scaling); for example, you\n  might want to scale out if the cpu usage across your cluster starts to rise,\n  and scale in when it drops again.\n* **by trying to keep a certain metric around a given value** (also known as\n  target tracking scaling); you might want to automatically scale out and in to\n  keep your cpu usage around 50%.\n* **on a schedule**; you might want to organize your scaling around traffic\n  flows you expect, by scaling out in the morning and scaling in in the\n  evening.\n\nthe general pattern of autoscaling will look like this:\n\n```python\n# vpc: ec2.vpc\n# instance_type: ec2.instancetype\n# machine_image: ec2.imachineimage\n\n\nauto_scaling_group = autoscaling.autoscalinggroup(self, \"asg\",\n    vpc=vpc,\n    instance_type=instance_type,\n    machine_image=machine_image,\n\n    min_capacity=5,\n    max_capacity=100\n)\n```\n\n### step scaling\n\nthis type of scaling scales in and out in deterministics steps that you\nconfigure, in response to metric values. for example, your scaling strategy to\nscale in response to a metric that represents your average worker pool usage\nmight look like this:\n\n```plaintext\n scaling        -1          (no change)          +1       +3\n            \u2502        \u2502                       \u2502        \u2502        \u2502\n            \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n            \u2502        \u2502                       \u2502        \u2502        \u2502\nworker use  0%      10%                     50%       70%     100%\n```\n\n(note that this is not necessarily a recommended scaling strategy, but it's\na possible one. you will have to determine what thresholds are right for you).\n\nnote that in order to set up this scaling strategy, you will have to emit a\nmetric representing your worker utilization from your instances. after that,\nyou would configure the scaling something like this:\n\n```python\n# auto_scaling_group: autoscaling.autoscalinggroup\n\n\nworker_utilization_metric = cloudwatch.metric(\n    namespace=\"myservice\",\n    metric_name=\"workerutilization\"\n)\n\nauto_scaling_group.scale_on_metric(\"scaletocpu\",\n    metric=worker_utilization_metric,\n    scaling_steps=[autoscaling.scalinginterval(upper=10, change=-1), autoscaling.scalinginterval(lower=50, change=+1), autoscaling.scalinginterval(lower=70, change=+3)\n    ],\n\n    # change this to adjustmenttype.percent_change_in_capacity to interpret the\n    # 'change' numbers before as percentages instead of capacity counts.\n    adjustment_type=autoscaling.adjustmenttype.change_in_capacity\n)\n```\n\nthe autoscaling construct library will create the required cloudwatch alarms and\nautoscaling policies for you.\n\n### target tracking scaling\n\nthis type of scaling scales in and out in order to keep a metric around a value\nyou prefer. there are four types of predefined metrics you can track, or you can\nchoose to track a custom metric. if you do choose to track a custom metric,\nbe aware that the metric has to represent instance utilization in some way\n(autoscaling will scale out if the metric is higher than the target, and scale\nin if the metric is lower than the target).\n\nif you configure multiple target tracking policies, autoscaling will use the\none that yields the highest capacity.\n\nthe following example scales to keep the cpu usage of your instances around\n50% utilization:\n\n```python\n# auto_scaling_group: autoscaling.autoscalinggroup\n\n\nauto_scaling_group.scale_on_cpu_utilization(\"keepsparecpu\",\n    target_utilization_percent=50\n)\n```\n\nto scale on average network traffic in and out of your instances:\n\n```python\n# auto_scaling_group: autoscaling.autoscalinggroup\n\n\nauto_scaling_group.scale_on_incoming_bytes(\"limitingressperinstance\",\n    target_bytes_per_second=10 * 1024 * 1024\n)\nauto_scaling_group.scale_on_outgoing_bytes(\"limitegressperinstance\",\n    target_bytes_per_second=10 * 1024 * 1024\n)\n```\n\nto scale on the average request count per instance (only works for\nautoscalinggroups that have been attached to application load\nbalancers):\n\n```python\n# auto_scaling_group: autoscaling.autoscalinggroup\n\n\nauto_scaling_group.scale_on_request_count(\"limitrps\",\n    target_requests_per_second=1000\n)\n```\n\n### scheduled scaling\n\nthis type of scaling is used to change capacities based on time. it works by\nchanging `mincapacity`, `maxcapacity` and `desiredcapacity` of the\nautoscalinggroup, and so can be used for two purposes:\n\n* scale in and out on a schedule by setting the `mincapacity` high or\n  the `maxcapacity` low.\n* still allow the regular scaling actions to do their job, but restrict\n  the range they can scale over (by setting both `mincapacity` and\n  `maxcapacity` but changing their range over time).\n\na schedule is expressed as a cron expression. the `schedule` class has a `cron` method to help build cron expressions.\n\nthe following example scales the fleet out in the morning, going back to natural\nscaling (all the way down to 1 instance if necessary) at night:\n\n```python\n# auto_scaling_group: autoscaling.autoscalinggroup\n\n\nauto_scaling_group.scale_on_schedule(\"prescaleinthemorning\",\n    schedule=autoscaling.schedule.cron(hour=\"8\", minute=\"0\"),\n    min_capacity=20\n)\n\nauto_scaling_group.scale_on_schedule(\"allowdownscalingatnight\",\n    schedule=autoscaling.schedule.cron(hour=\"20\", minute=\"0\"),\n    min_capacity=1\n)\n```\n\n## configuring instances using cloudformation init\n\nit is possible to use the cloudformation init mechanism to configure the\ninstances in the autoscalinggroup. you can write files to it, run commands,\nstart services, etc. see the documentation of\n[aws::cloudformation::init](https://docs.aws.amazon.com/awscloudformation/latest/userguide/aws-resource-init.html)\nand the documentation of cdk's `aws-ec2` library for more information.\n\nwhen you specify a cloudformation init configuration for an autoscalinggroup:\n\n* you *must* also specify `signals` to configure how long cloudformation\n  should wait for the instances to successfully configure themselves.\n* you *should* also specify an `updatepolicy` to configure how instances\n  should be updated when the autoscalinggroup is updated (for example,\n  when the ami is updated). if you don't specify an update policy, a *rolling\n  update* is chosen by default.\n\nhere's an example of using cloudformation init to write a file to the\ninstance hosts on startup:\n\n```python\n# vpc: ec2.vpc\n# instance_type: ec2.instancetype\n# machine_image: ec2.imachineimage\n\n\nautoscaling.autoscalinggroup(self, \"asg\",\n    vpc=vpc,\n    instance_type=instance_type,\n    machine_image=machine_image,\n\n    # ...\n\n    init=ec2.cloudformationinit.from_elements(\n        ec2.initfile.from_string(\"/etc/my_instance\", \"this got written during instance startup\")),\n    signals=autoscaling.signals.wait_for_all(\n        timeout=duration.minutes(10)\n    )\n)\n```\n\n## signals\n\nin normal operation, cloudformation will send a create or update command to\nan autoscalinggroup and proceed with the rest of the deployment without waiting\nfor the *instances in the autoscalinggroup*.\n\nconfigure `signals` to tell cloudformation to wait for a specific number of\ninstances in the autoscalinggroup to have been started (or failed to start)\nbefore moving on. an instance is supposed to execute the\n[`cfn-signal`](https://docs.aws.amazon.com/awscloudformation/latest/userguide/cfn-signal.html)\nprogram as part of its startup to indicate whether it was started\nsuccessfully or not.\n\nif you use cloudformation init support (described in the previous section),\nthe appropriate call to `cfn-signal` is automatically added to the\nautoscalinggroup's userdata. if you don't use the `signals` directly, you are\nresponsible for adding such a call yourself.\n\nthe following type of `signals` are available:\n\n* `signals.waitforall([options])`: wait for all of `desiredcapacity` amount of instances\n  to have started (recommended).\n* `signals.waitformincapacity([options])`: wait for a `mincapacity` amount of instances\n  to have started (use this if waiting for all instances takes too long and you are happy\n  with a minimum count of healthy hosts).\n* `signals.waitforcount(count, [options])`: wait for a specific amount of instances to have\n  started.\n\nthere are two `options` you can configure:\n\n* `timeout`: maximum time a host startup is allowed to take. if a host does not report\n  success within this time, it is considered a failure. default is 5 minutes.\n* `minsuccesspercentage`: percentage of hosts that needs to be healthy in order for the\n  update to succeed. if you set this value lower than 100, some percentage of hosts may\n  report failure, while still considering the deployment a success. default is 100%.\n\n## update policy\n\nthe *update policy* describes what should happen to running instances when the definition\nof the autoscalinggroup is changed. for example, if you add a command to the userdata\nof an autoscalinggroup, do the existing instances get replaced with new instances that\nhave executed the new userdata? or do the \"old\" instances just keep on running?\n\nit is recommended to always use an update policy, otherwise the current state of your\ninstances also depends the previous state of your instances, rather than just on your\nsource code. this degrades the reproducibility of your deployments.\n\nthe following update policies are available:\n\n* `updatepolicy.none()`: leave existing instances alone (not recommended).\n* `updatepolicy.rollingupdate([options])`: progressively replace the existing\n  instances with new instances, in small batches. at any point in time,\n  roughly the same amount of total instances will be running. if the deployment\n  needs to be rolled back, the fresh instances will be replaced with the \"old\"\n  configuration again.\n* `updatepolicy.replacingupdate([options])`: build a completely fresh copy\n  of the new autoscalinggroup next to the old one. once the autoscalinggroup\n  has been successfully created (and the instances started, if `signals` is\n  configured on the autoscalinggroup), the old autoscalinggroup is deleted.\n  if the deployment needs to be rolled back, the new autoscalinggroup is\n  deleted and the old one is left unchanged.\n\n## allowing connections\n\nsee the documentation of the `@aws-cdk/aws-ec2` package for more information\nabout allowing connections between resources backed by instances.\n\n## max instance lifetime\n\nto enable the max instance lifetime support, specify `maxinstancelifetime` property\nfor the `autoscalinggroup` resource. the value must be between 7 and 365 days(inclusive).\nto clear a previously set value, leave this property undefined.\n\n## instance monitoring\n\nto disable detailed instance monitoring, specify `instancemonitoring` property\nfor the `autoscalinggroup` resource as `monitoring.basic`. otherwise detailed monitoring\nwill be enabled.\n\n## monitoring group metrics\n\ngroup metrics are used to monitor group level properties; they describe the group rather than any of its instances (e.g groupmaxsize, the group maximum size). to enable group metrics monitoring, use the `groupmetrics` property.\nall group metrics are reported in a granularity of 1 minute at no additional charge.\n\nsee [ec2 docs](https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html#as-group-metrics) for a list of all available group metrics.\n\nto enable group metrics monitoring using the `groupmetrics` property:\n\n```python\n# vpc: ec2.vpc\n# instance_type: ec2.instancetype\n# machine_image: ec2.imachineimage\n\n\n# enable monitoring of all group metrics\nautoscaling.autoscalinggroup(self, \"asg\",\n    vpc=vpc,\n    instance_type=instance_type,\n    machine_image=machine_image,\n\n    # ...\n\n    group_metrics=[autoscaling.groupmetrics.all()]\n)\n\n# enable monitoring for a subset of group metrics\nautoscaling.autoscalinggroup(self, \"asg\",\n    vpc=vpc,\n    instance_type=instance_type,\n    machine_image=machine_image,\n\n    # ...\n\n    group_metrics=[autoscaling.groupmetrics(autoscaling.groupmetric.min_size, autoscaling.groupmetric.max_size)]\n)\n```\n\n## termination policies\n\nauto scaling uses [termination policies](https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html)\nto determine which instances it terminates first during scale-in events. you\ncan specify one or more termination policies with the `terminationpolicies`\nproperty:\n\n```python\n# vpc: ec2.vpc\n# instance_type: ec2.instancetype\n# machine_image: ec2.imachineimage\n\n\nautoscaling.autoscalinggroup(self, \"asg\",\n    vpc=vpc,\n    instance_type=instance_type,\n    machine_image=machine_image,\n\n    # ...\n\n    termination_policies=[autoscaling.terminationpolicy.oldest_instance, autoscaling.terminationpolicy.default\n    ]\n)\n```\n\n## protecting new instances from being terminated on scale-in\n\nby default, auto scaling can terminate an instance at any time after launch when\nscaling in an auto scaling group, subject to the group's [termination\npolicy](https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html).\n\nhowever, you may wish to protect newly-launched instances from being scaled in\nif they are going to run critical applications that should not be prematurely\nterminated. ec2 capacity providers for amazon ecs requires this attribute be\nset to `true`.\n\n```python\n# vpc: ec2.vpc\n# instance_type: ec2.instancetype\n# machine_image: ec2.imachineimage\n\n\nautoscaling.autoscalinggroup(self, \"asg\",\n    vpc=vpc,\n    instance_type=instance_type,\n    machine_image=machine_image,\n\n    # ...\n\n    new_instances_protected_from_scale_in=true\n)\n```\n\n## configuring instance metadata service (imds)\n\n### toggling imdsv1\n\nyou can configure [ec2 instance metadata service](https://docs.aws.amazon.com/awsec2/latest/userguide/ec2-instance-metadata.html) options to either\nallow both imdsv1 and imdsv2 or enforce imdsv2 when interacting with the imds.\n\nto do this for a single `autoscalinggroup`, you can use set the `requireimdsv2` property.\nthe example below demonstrates imdsv2 being required on a single `autoscalinggroup`:\n\n```python\n# vpc: ec2.vpc\n# instance_type: ec2.instancetype\n# machine_image: ec2.imachineimage\n\n\nautoscaling.autoscalinggroup(self, \"asg\",\n    vpc=vpc,\n    instance_type=instance_type,\n    machine_image=machine_image,\n\n    # ...\n\n    require_imdsv2=true\n)\n```\n\nyou can also use `autoscalinggrouprequireimdsv2aspect` to apply the operation to multiple autoscalinggroups.\nthe example below demonstrates the `autoscalinggrouprequireimdsv2aspect` being used to require imdsv2 for all autoscalinggroups in a stack:\n\n```python\naspect = autoscaling.autoscalinggrouprequireimdsv2aspect()\n\naspects.of(self).add(aspect)\n```\n\n## warm pool\n\nauto scaling offers [a warm pool](https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html) which gives an ability to decrease latency for applications that have exceptionally long boot times. you can create a warm pool with default parameters as below:\n\n```python\n# auto_scaling_group: autoscaling.autoscalinggroup\n\n\nauto_scaling_group.add_warm_pool()\n```\n\nyou can also customize a warm pool by configuring parameters:\n\n```python\n# auto_scaling_group: autoscaling.autoscalinggroup\n\n\nauto_scaling_group.add_warm_pool(\n    min_size=1,\n    reuse_on_scale_in=true\n)\n```\n\n## future work\n\n* [ ] cloudwatch events (impossible to add currently as the autoscalinggroup arn is\n  necessary to make this rule and this cannot be accessed from cloudformation).\n",
  "docs_url": null,
  "keywords": "",
  "license": "apache-2.0",
  "name": "aws-cdk.aws-autoscaling",
  "package_url": "https://pypi.org/project/aws-cdk.aws-autoscaling/",
  "project_url": "https://pypi.org/project/aws-cdk.aws-autoscaling/",
  "project_urls": {
    "Homepage": "https://github.com/aws/aws-cdk",
    "Source": "https://github.com/aws/aws-cdk.git"
  },
  "release_url": "https://pypi.org/project/aws-cdk.aws-autoscaling/1.204.0/",
  "requires_dist": [
    "aws-cdk.aws-autoscaling-common (==1.204.0)",
    "aws-cdk.aws-cloudwatch (==1.204.0)",
    "aws-cdk.aws-ec2 (==1.204.0)",
    "aws-cdk.aws-elasticloadbalancing (==1.204.0)",
    "aws-cdk.aws-elasticloadbalancingv2 (==1.204.0)",
    "aws-cdk.aws-iam (==1.204.0)",
    "aws-cdk.aws-sns (==1.204.0)",
    "aws-cdk.core (==1.204.0)",
    "constructs (<4.0.0,>=3.3.69)",
    "jsii (<2.0.0,>=1.84.0)",
    "publication (>=0.0.3)",
    "typeguard (~=2.13.3)"
  ],
  "requires_python": "~=3.7",
  "summary": "the cdk construct library for aws::autoscaling",
  "version": "1.204.0",
  "releases": [],
  "developers": [
    "amazon_web_services"
  ],
  "kwds": "auto_scaling_group autoscalinggroup autoscaling autoscalinggroups autoscalinggrouprequireimdsv2aspect",
  "license_kwds": "apache-2.0",
  "libtype": "pypi",
  "id": "pypi_aws_cdk.aws_autoscaling",
  "homepage": "https://github.com/aws/aws-cdk",
  "release_count": 258,
  "dependency_ids": [
    "pypi_aws_cdk.aws_autoscaling_common",
    "pypi_aws_cdk.aws_cloudwatch",
    "pypi_aws_cdk.aws_ec2",
    "pypi_aws_cdk.aws_elasticloadbalancing",
    "pypi_aws_cdk.aws_elasticloadbalancingv2",
    "pypi_aws_cdk.aws_iam",
    "pypi_aws_cdk.aws_sns",
    "pypi_aws_cdk.core",
    "pypi_constructs",
    "pypi_jsii",
    "pypi_publication",
    "pypi_typeguard"
  ]
}