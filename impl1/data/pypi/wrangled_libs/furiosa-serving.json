{
  "classifiers": [
    "development status :: 4 - beta",
    "environment :: console",
    "environment :: web environment",
    "intended audience :: developers",
    "intended audience :: system administrators",
    "license :: osi approved :: apache software license",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "# furiosa serving\n\nfuriosa serving is a lightweight library based on [fastapi](https://fastapi.tiangolo.com/) to make a model server running on a furiosa npu.\n\n## dependency\nfuriosa serving depends on followings:\n\n- furiosa npu\n- [furiosa-server](https://github.com/furiosa-ai/furiosa-sdk/tree/main/python/furiosa-server)\n\n## installation\n`furiosa-serving` can be installed from pypi using `pip` (note that the package name is different from the importable name)::\n\n```sh\npip install 'furiosa-sdk[serving]'\n```\n\n## getting started\nthere is one main api called `serveapi`. you can think of `serveapi` as a kind of `fastapi` wrapper.\n\n\n### run server\n\n```python\n# main.py\nfrom fastapi import fastapi\nfrom furiosa.serving import serveapi\n\nserve = serveapi()\n\n# this is fastapi instance\napp: fastapi = serve.app\n```\n\nyou can run [uvicorn](https://www.uvicorn.org/) server via internal `app` variable from `serveapi` instance like [normal fastapi application](https://fastapi.tiangolo.com/tutorial/first-steps/#first-steps)\n\n```sh\n$ uvicorn main:app # or uvicorn main:serve.app\n```\n\n### load model\n\nfrom `serveapi`, you can load your model binary which will be running on a furiosa npu. you should specify model name and uri where to load the model. uri can be one of them below\n\n- local file\n- http\n- [s3](https://docs.aws.amazon.com/s3/index.html)\n\nnote that model binary which is now supported by furiosa npu should be one of them below\n\n- [tensorflow lite (tflite)](https://www.tensorflow.org/lite/)\n- [onnx](https://github.com/onnx/onnx)\n\n```python\nfrom furiosa.common.thread import synchronous\nfrom furiosa.serving import serveapi, servemodel\n\n\nserve = serveapi()\n\n\n# load model from local disk\nimagenet: servemodel = synchronous(serve.model(\"furiosart\"))(\n    'imagenet',\n    location='./examples/assets/models/image_classification.onnx'\n)\n\n# load model from http\nresnet: servemodel = synchronous(serve.model(\"furiosart\"))(\n    'imagenet',\n     location='https://raw.githubusercontent.com/onnx/models/main/vision/classification/resnet/model/resnet50-v1-12.onnx'\n)\n\n# load model from s3 (auth environment variable for aioboto library required)\ndensenet: servemodel = synchronous(serve.model(\"furiosart\"))(\n    'imagenet',\n     location='s3://furiosa/models/93d63f654f0f192cc4ff5691be60fb9379e9d7fd'\n)\n```\n\n### define api\n\nfrom a model you just created, you can define [fastapi path operation decorator](https://fastapi.tiangolo.com/tutorial/first-steps/#define-a-path-operation-decorator) like `post()`, `get()` to expose api endpoints.\n\nyou should follow [fastapi request body concept](https://fastapi.tiangolo.com/tutorial/body/) to correctly define payload.\n\n> :warning: this example below is not actually working as you have to define your own preprocess(), postprocess() functions first.\n\n```python\nfrom typing import dict\n\nfrom fastapi import file, uploadfile\nfrom furiosa.common.thread import synchronous\nfrom furiosa.serving import serveapi, servemodel\nimport numpy as np\n\n\nserve = serveapi()\n\n\nmodel: servemodel = synchronous(serve.model(\"furiosart\"))(\n    'imagenet',\n    location='./examples/assets/models/image_classification.onnx'\n)\n\n@model.post(\"/models/imagenet/infer\")\nasync def infer(image: uploadfile = file(...)) -> dict:\n    # convert image to numpy array with your preprocess() function\n    tensors: list[np.ndarray] = preprocess(image)\n\n    # infer from servemodel\n    result: list[np.ndarray] = await model.predict(tensors)\n\n    # classify model from numpy array with your postprocess() function\n    response: dict = postprocess(result)\n\n    return response\n```\n\nafter running uvicorn server, you can find [documentations](https://fastapi.tiangolo.com/#interactive-api-docs) provided by fastapi at localhost:8000/docs\n\n\n### use sub applications\n\nfuriosa serving provides predefined [fastapi sub applications](https://fastapi.tiangolo.com/advanced/sub-applications/) to give you additional functionalities out of box.\n\nyou can mount the _sub applications_ using `mount()`. we provides several _sub applications_ like below\n\n- **repository**: model repository to list models and load/unload a model dynamically\n- **model**: model metadata, model readiness\n- **health**: server health, server readiness\n\n```python\nfrom fastapi import fastapi\nfrom furiosa.serving import serveapi\nfrom furiosa.serving.apps import health, model, repository\n\n\n# create serveapi with repository instance. this repository maintains models\nserve = serveapi(repository.repository)\n\napp: fastapi = serve.app\n\napp.mount(\"/repository\", repository.app)\napp.mount(\"/models\", model.app)\napp.mount(\"/health\", health.app)\n```\n\nyou can also find documentations for the _sub applications_ at `localhost:8000/{application}/docs`. note that `model` _sub application_ has different default doc api like `localhost:8000/{application}/api/docs` since default doc url conflicts model api.\n\n### use processors for pre/post processing\n\nfuriosa serving provides several _processors_ which are predefined pre/post process functions to convert your data for each model.\n\n```python\nimport numpy as np\nfrom furiosa.common.thread import synchronous\nfrom furiosa.serving import servemodel, serveapi\nfrom furiosa.serving.processors import imagenet\n\n\nserve = serveapi()\n\nmodel: servemodel = synchronous(serve.model(\"furiosart\"))(\n    'imagenet',\n    location='./examples/assets/models/image_classification.onnx'\n)\n\n@model.post(\"/models/imagenet/infer\")\nasync def infer(image: uploadfile = file(...)) -> dict:\n    shape = model.inputs[0].shape\n    input = await imagenet.preprocess(shape, image)\n    output = await model.predict(input)\n    return await imagenet.postprocess(\n        output[0], label='./examples/assets/labels/imagenetlabels.txt'\n    )\n```\n\n### compose models\n\nyou can composite multiple models using [fastapi dependency injection](https://fastapi.tiangolo.com/tutorial/dependencies/).\n\n> :warning: this example below is not actually working as there is no segmentnet in processors yet\n\n```python\nfrom fastapi import depends\nfrom furiosa.common.thread import synchronous\nfrom furiosa.serving import servemodel, serveapi\n\n\nserve = serveapi()\n\nimagenet: servemodel = synchronous(serve.model(\"furiosart\"))(\n    'imagenet',\n    location='./examples/assets/models/image_classification.onnx'\n)\n\nsegmentnet: servemodel = synchronous(serve.model(\"furiosart\"))(\n    'segmentnet',\n    location='./examples/assets/models/image_segmentation.onnx'\n)\n\n# note that no \"imagenet.post()\" here not to expose the endpoint\nasync def classify(image: uploadfile = file(...)) -> list[np.ndarray]:\n    from furiosa.serving.processors.imagenet import preprocess\n\n    tensors: list[np.arrary] = await preprocess(\n        imagenet.inputs[0].shape, image\n    )\n    return await imagenet.predict(tensors)\n\n@segmentnet.post(\"/models/composed/infer\")\nasync def segment(tensors: list[np.ndarray] = depends(classify)) -> dict:\n    from furiosa.serving.processors.segmentnet import postprocess\n\n    tensors = await model.predict(tensors)\n    return await postprocess(tensors)\n```\n\n### example 1\n\nyou can find a complete example at `examples/image_classify.py`\n\n```sh\ncd examples\n\nexamples$ python image_classify.py\ninfo:furiosa_sdk_runtime._api.v1:loaded dynamic library /home/ys/furiosa/compiler/npu-tools/target/x86_64-unknown-linux-gnu/debug/libnux.so (0.4.0-dev d1720b938)\ninfo:     started server process [984608]\ninfo:uvicorn.error:started server process [984608]\ninfo:     waiting for application startup.\ninfo:uvicorn.error:waiting for application startup.\n[1/6] \ud83d\udd0d   compiling from tflite to dfg\ndone in 0.27935523s\n[2/6] \ud83d\udd0d   compiling from dfg to ldfg\n\u25aa\u25aa\u25aa\u25aa\u25aa [1/3] splitting graph...done in 1079.9143s\n\u25aa\u25aa\u25aa\u25aa\u25aa [2/3] lowering...done in 93.315895s\n\u25aa\u25aa\u25aa\u25aa\u25aa [3/3] precalculating operators...done in 45.07178s\ndone in 1218.3285s\n[3/6] \ud83d\udd0d   compiling from ldfg to cdfg\ndone in 0.002127793s\n[4/6] \ud83d\udd0d   compiling from cdfg to gir\ndone in 0.096237786s\n[5/6] \ud83d\udd0d   compiling from gir to lir\ndone in 0.03271749s\n[6/6] \ud83d\udd0d   compiling from lir to enf\ndone in 0.48739022s\n\u2728  finished in 1219.4524s\ninfo:     application startup complete.\ninfo:     uvicorn running on http://127.0.0.1:8000 (press ctrl+c to quit)\n```\n\nyou can find available api in http://localhost:8000/docs#/\n\nsend image to classify a image from server you just launched.\n\n```sh\nexamples$ curl -x 'post' \\\n  'http://127.0.0.1:8000/imagenet/infer' \\\n  -h 'accept: application/json' \\\n  -h 'content-type: multipart/form-data' \\\n  -f 'image=@assets/images/car.jpg'\n\n```\n\n### example 2\n\nin many user scenarios, for each request users may want to split a large image into a number of small images, and process all of them at a time.\nin this use cases, using multiple devices will be able to boost the throughput, eventually leading to lower latency.\nthis example `examples/number_classify.py` shows how to implement this usecase with session pool and python async/await/gather.\n\n```sh\ncd examples\n\nexamples$ python number_classify.py\ninfo:     started server process [57892]\ninfo:     waiting for application startup.\n2022-10-28t05:36:42.468215z  info nux::npu: npu (npu0pe0-1) is being initialized\n2022-10-28t05:36:42.473084z  info nux: nuxinner create with pes: [peid(0)]\n2022-10-28t05:36:42.503103z  info nux::npu: npu (npu1pe0-1) is being initialized\n2022-10-28t05:36:42.507724z  info nux: nuxinner create with pes: [peid(0)]\ninfo:     application startup complete.\ninfo:     uvicorn running on http://0.0.0.0:8000 (press ctrl+c to quit)\n\n\n```\n\nyou can find available api in http://localhost:8000/docs#/\n\nsend image to classify a image from server you just launched.\n\n```sh\nexamples$ curl -x 'post' \\\n  'http://127.0.0.1:8000/infer' \\\n  -h 'accept: application/json' \\\n  -h 'content-type: multipart/form-data' \\\n  -f 'file=@assets/images/1234567890.jpg'\n\n```\n\n## code\n\nthe code and issue tracker are hosted on github:\\\nhttps://github.com/furiosa-ai/furiosa-sdk\n\n## contributing\n\nwe welcome many types of contributions - bug reports, pull requests (code, infrastructure or documentation fixes). for more information about how to contribute to the project, see the ``contributing.md`` file in the repository.\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "furiosa-serving",
  "package_url": "https://pypi.org/project/furiosa-serving/",
  "project_url": "https://pypi.org/project/furiosa-serving/",
  "project_urls": {
    "Bug Tracker": "https://github.com/furiosa-ai/furiosa-sdk/issues",
    "Documentation": "https://furiosa-ai.github.io/docs",
    "Home": "https://furiosa.ai",
    "Source Code": "https://github.com/furiosa-ai/furiosa-sdk"
  },
  "release_url": "https://pypi.org/project/furiosa-serving/0.10.1/",
  "requires_dist": [
    "furiosa-server == 0.10.*",
    "Pillow",
    "python-multipart",
    "httpx",
    "prometheus-client",
    "opentelemetry-instrumentation-fastapi",
    "opentelemetry-instrumentation-logging",
    "opentelemetry-exporter-otlp",
    "opentelemetry-api",
    "opentelemetry-sdk",
    "furiosa-server[openvino] == 0.10.* ; extra == \"openvino\"",
    "transformers ; extra == \"openvino\"",
    "mypy ; extra == \"test\"",
    "pytest ; extra == \"test\"",
    "pytest-asyncio ~= 0.17.2 ; extra == \"test\"",
    "pytest-cov ; extra == \"test\"",
    "ruff ; extra == \"test\"",
    "types-Pillow ; extra == \"test\""
  ],
  "requires_python": "~=3.8",
  "summary": "furiosa serving framework, easy to use inference server.",
  "version": "0.10.1",
  "releases": [],
  "developers": [
    "pkg@furiosa.ai"
  ],
  "kwds": "furiosa_sdk_runtime furiosa furiosart serveapi _api",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_furiosa_serving",
  "homepage": "",
  "release_count": 14,
  "dependency_ids": [
    "pypi_furiosa_server",
    "pypi_httpx",
    "pypi_mypy",
    "pypi_opentelemetry_api",
    "pypi_opentelemetry_exporter_otlp",
    "pypi_opentelemetry_instrumentation_fastapi",
    "pypi_opentelemetry_instrumentation_logging",
    "pypi_opentelemetry_sdk",
    "pypi_pillow",
    "pypi_prometheus_client",
    "pypi_pytest",
    "pypi_pytest_asyncio",
    "pypi_pytest_cov",
    "pypi_python_multipart",
    "pypi_ruff",
    "pypi_transformers",
    "pypi_types_pillow"
  ]
}