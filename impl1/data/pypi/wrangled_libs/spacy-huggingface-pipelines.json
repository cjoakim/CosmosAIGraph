{
  "classifiers": [
    "development status :: 4 - beta",
    "environment :: console",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: mit license",
    "operating system :: macos :: macos x",
    "operating system :: microsoft :: windows",
    "operating system :: posix :: linux",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "<a href=\"https://explosion.ai\"><img src=\"https://explosion.ai/assets/img/logo.svg\" width=\"125\" height=\"125\" align=\"right\" /></a>\n\n# spacy-huggingface-pipelines: use pretrained transformer models for text and token classification\n\nthis package provides [spacy](https://github.com/explosion/spacy) components to\nuse pretrained\n[hugging face transformers pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)\nfor inference only.\n\n[![pypi](https://img.shields.io/pypi/v/spacy-huggingface-pipelines.svg?style=flat-square&logo=pypi&logocolor=white)](https://pypi.python.org/pypi/spacy-huggingface-pipelines)\n[![github](https://img.shields.io/github/release/explosion/spacy-huggingface-pipelines/all.svg?style=flat-square&logo=github)](https://github.com/explosion/spacy-huggingface-pipelines/releases)\n\n## features\n\n- apply pretrained transformers models like\n  [`dslim/bert-base-ner`](https://huggingface.co/dslim/bert-base-ner) and\n  [`distilbert-base-uncased-finetuned-sst-2-english`](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n\n## \ud83d\ude80 installation\n\ninstalling the package from pip will automatically install all dependencies,\nincluding pytorch and spacy.\n\n```bash\npip install -u pip setuptools wheel\npip install spacy-huggingface-pipelines\n```\n\nfor gpu installation, follow the\n[spacy installation quickstart with gpu](https://spacy.io/usage/), e.g.\n\n```bash\npip install -u spacy[cuda-autodetect]\n```\n\nif you are having trouble installing pytorch, follow the\n[instructions](https://pytorch.org/get-started/locally/) on the official website\nfor your specific operating system and requirements.\n\n## \ud83d\udcd6 documentation\n\nthis module provides spacy wrappers for the inference-only transformers\n[`tokenclassificationpipeline`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.tokenclassificationpipeline)\nand\n[`textclassificationpipeline`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.textclassificationpipeline)\npipelines.\n\nthe models are downloaded on initialization from the\n[hugging face hub](https://huggingface.co/models) if they're not already in your\nlocal cache, or alternatively they can be loaded from a local path.\n\nnote that the transformer model data **is not saved with the pipeline** when you\ncall `nlp.to_disk`, so if you are loading pipelines in an environment with\nlimited internet access, make sure the model is available in your\n[transformers cache directory](https://huggingface.co/docs/transformers/main/en/installation#cache-setup)\nand enable offline mode if needed.\n\n### token classification\n\nconfig settings for `hf_token_pipe`:\n\n```ini\n[components.hf_token_pipe]\nfactory = \"hf_token_pipe\"\nmodel = \"dslim/bert-base-ner\"     # model name or path\nrevision = \"main\"                 # model revision\naggregation_strategy = \"average\"  # \"simple\", \"first\", \"average\", \"max\"\nstride = 16                       # if stride >= 0, process long texts in\n                                  # overlapping windows of the model max\n                                  # length. the value is the length of the\n                                  # window overlap in transformer tokenizer\n                                  # tokens, not the length of the stride.\nkwargs = {}                       # any additional arguments for\n                                  # tokenclassificationpipeline\nalignment_mode = \"strict\"         # \"strict\", \"contract\", \"expand\"\nannotate = \"ents\"                 # \"ents\", \"pos\", \"spans\", \"tag\"\nannotate_spans_key = null         # doc.spans key for annotate = \"spans\"\nscorer = null                     # optional scorer\n```\n\n#### `tokenclassificationpipeline` settings\n\n- `model`: the model name or path.\n- `revision`: the model revision. for production use, a specific git commit is\n  recommended instead of the default `main`.\n- `stride`: for `stride >= 0`, the text is processed in overlapping windows\n  where the `stride` setting specifies the number of overlapping tokens between\n  windows (not the stride length). if `stride` is `none`, then the text may be\n  truncated. `stride` is only supported for fast tokenizers.\n- `aggregation_strategy`: the aggregation strategy determines the word-level\n  tags for cases where subwords within one word do not receive the same\n  predicted tag. see:\n  https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.tokenclassificationpipeline.aggregation_strategy\n- `kwargs`: any additional arguments to\n  [`tokenclassificationpipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.tokenclassificationpipeline).\n\n#### spacy settings\n\n- `alignment_mode` determines how transformer predictions are aligned to spacy\n  token boundaries as described for\n  [`doc.char_span`](https://spacy.io/api/doc#char_span).\n- `annotate` and `annotate_spans_key` configure how the annotation is saved to\n  the spacy doc. you can save the output as `token.tag_`, `token.pos_` (only for\n  upos tags), `doc.ents` or `doc.spans`.\n\n#### examples\n\n1. save named entity annotation as `doc.ents`:\n\n```python\nimport spacy\nnlp = spacy.blank(\"en\")\nnlp.add_pipe(\"hf_token_pipe\", config={\"model\": \"dslim/bert-base-ner\"})\ndoc = nlp(\"my name is sarah and i live in london\")\nprint(doc.ents)\n# (sarah, london)\n```\n\n2. save named entity annotation as `doc.spans[spans_key]` and scores as\n   `doc.spans[spans_key].attrs[\"scores\"]`:\n\n```python\nimport spacy\nnlp = spacy.blank(\"en\")\nnlp.add_pipe(\n    \"hf_token_pipe\",\n    config={\n        \"model\": \"dslim/bert-base-ner\",\n        \"annotate\": \"spans\",\n        \"annotate_spans_key\": \"bert-base-ner\",\n    },\n)\ndoc = nlp(\"my name is sarah and i live in london\")\nprint(doc.spans[\"bert-base-ner\"])\n# [sarah, london]\nprint(doc.spans[\"bert-base-ner\"].attrs[\"scores\"])\n# [0.99854773, 0.9996215]\n```\n\n3. save fine-grained tags as `token.tag`:\n\n```python\nimport spacy\nnlp = spacy.blank(\"en\")\nnlp.add_pipe(\n    \"hf_token_pipe\",\n    config={\n        \"model\": \"qcri/bert-base-multilingual-cased-pos-english\",\n        \"annotate\": \"tag\",\n    },\n)\ndoc = nlp(\"my name is sarah and i live in london\")\nprint([t.tag_ for t in doc])\n# ['prp$', 'nn', 'vbz', 'nnp', 'cc', 'prp', 'vbp', 'in', 'nnp']\n```\n\n4. save coarse-grained tags as `token.pos`:\n\n```python\nimport spacy\nnlp = spacy.blank(\"en\")\nnlp.add_pipe(\n    \"hf_token_pipe\",\n    config={\"model\": \"vblagoje/bert-english-uncased-finetuned-pos\", \"annotate\": \"pos\"},\n)\ndoc = nlp(\"my name is sarah and i live in london\")\nprint([t.pos_ for t in doc])\n# ['pron', 'noun', 'aux', 'propn', 'cconj', 'pron', 'verb', 'adp', 'propn']\n```\n\n### text classification\n\nconfig settings for `hf_text_pipe`:\n\n```ini\n[components.hf_text_pipe]\nfactory = \"hf_text_pipe\"\nmodel = \"distilbert-base-uncased-finetuned-sst-2-english\"  # model name or path\nrevision = \"main\"                 # model revision\nkwargs = {}                       # any additional arguments for\n                                  # textclassificationpipeline\nscorer = null                     # optional scorer\n```\n\nthe input texts are truncated according to the transformers model max length.\n\n#### `textclassificationpipeline` settings\n\n- `model`: the model name or path.\n- `revision`: the model revision. for production use, a specific git commit is\n  recommended instead of the default `main`.\n- `kwargs`: any additional arguments to\n  [`textclassificationpipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.textclassificationpipeline).\n\n#### example\n\n```python\nimport spacy\n\nnlp = spacy.blank(\"en\")\nnlp.add_pipe(\n    \"hf_text_pipe\",\n    config={\"model\": \"distilbert-base-uncased-finetuned-sst-2-english\"},\n)\ndoc = nlp(\"this is great!\")\nprint(doc.cats)\n# {'positive': 0.9998694658279419, 'negative': 0.00013048505934420973}\n```\n\n### batching and gpu\n\nboth token and text classification support batching with `nlp.pipe`:\n\n```python\nfor doc in nlp.pipe(texts, batch_size=256):\n    do_something(doc)\n```\n\nif the component runs into an error processing a batch (e.g. on an empty text),\n`nlp.pipe` will back off to processing each text individually. if it runs into\nan error on an individual text, a warning is shown and the doc is returned\nwithout additional annotation.\n\nswitch to gpu:\n\n```python\nimport spacy\nspacy.require_gpu()\n\nfor doc in nlp.pipe(texts):\n    do_something(doc)\n```\n\n## bug reports and issues\n\nplease report bugs in the\n[spacy issue tracker](https://github.com/explosion/spacy/issues) or open a new\nthread on the [discussion board](https://github.com/explosion/spacy/discussions)\nfor other issues.\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "spacy-huggingface-pipelines",
  "package_url": "https://pypi.org/project/spacy-huggingface-pipelines/",
  "project_url": "https://pypi.org/project/spacy-huggingface-pipelines/",
  "project_urls": {
    "Homepage": "https://github.com/explosion/spacy-huggingface-pipelines",
    "Release notes": "https://github.com/explosion/spacy-huggingface-pipelines/releases",
    "Source": "https://github.com/explosion/spacy-huggingface-pipelines"
  },
  "release_url": "https://pypi.org/project/spacy-huggingface-pipelines/0.0.4/",
  "requires_dist": [
    "spacy (<4.0.0,>=3.0.0)",
    "transformers (<5.0.0,>=4.28.0)",
    "torch (>=1.8.0)"
  ],
  "requires_python": ">=3.8",
  "summary": "spacy wrapper for hugging face transformers pipelines",
  "version": "0.0.4",
  "releases": [],
  "developers": [
    "contact@explosion.ai",
    "explosion"
  ],
  "kwds": "tokenizers tokenizer tokenclassificationpipeline spacy huggingface",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_spacy_huggingface_pipelines",
  "homepage": "https://github.com/explosion/spacy-huggingface-pipelines",
  "release_count": 4,
  "dependency_ids": [
    "pypi_spacy",
    "pypi_torch",
    "pypi_transformers"
  ]
}