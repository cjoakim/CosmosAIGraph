{
  "classifiers": [
    "license :: osi approved :: mit license",
    "natural language :: japanese"
  ],
  "description": "[![open in streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/polm/fugashi-streamlit-demo/main/demo.py)\r\n[![current pypi packages](https://badge.fury.io/py/fugashi.svg)](https://pypi.org/project/fugashi/)\r\n![test status](https://github.com/polm/fugashi/workflows/test-manylinux/badge.svg)\r\n[![pypi - downloads](https://img.shields.io/pypi/dm/fugashi)](https://pypi.org/project/fugashi/)\r\n![supported platforms](https://img.shields.io/badge/platforms-linux%20macosx%20windows-blue)\r\n\r\n# fugashi\r\n\r\n<img src=\"https://github.com/polm/fugashi/raw/master/fugashi.png\" width=125 height=125 alt=\"fugashi by irasutoya\" />\r\n\r\nfugashi is a cython wrapper for [mecab](https://taku910.github.io/mecab/), a\r\njapanese tokenizer and morphological analysis tool.  wheels are provided for\r\nlinux, osx (intel), and win64, and unidic is [easy to install](#installing-a-dictionary).\r\n\r\n**issue\u3092\u82f1\u8a9e\u3067\u66f8\u304f\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093\u3002**\r\n\r\ncheck out the [interactive demo][], see the [blog post](https://www.dampfkraft.com/nlp/fugashi.html) for background\r\non why fugashi exists and some of the design decisions, or see [this\r\nguide][guide] for a basic introduction to japanese tokenization.\r\n\r\n[guide]: https://www.dampfkraft.com/nlp/how-to-tokenize-japanese.html\r\n[interactive demo]: https://share.streamlit.io/polm/fugashi-streamlit-demo/main/demo.py\r\n\r\nif you are on a platform for which wheels are not provided, you'll need to\r\ninstall mecab first. it's recommended you install [from\r\nsource](https://github.com/taku910/mecab). if you need to build from source on\r\nwindows, [@chezou's fork](https://github.com/chezou/mecab) is recommended; see\r\n[issue #44](https://github.com/polm/fugashi/issues/44#issuecomment-954426115)\r\nfor an explanation of the problems with the official repo.\r\n\r\nknown platforms without wheels:\r\n\r\n- musl-based distros like alpine [#77](https://github.com/polm/fugashi/issues/77)\r\n- powerpc\r\n- windows 32bit\r\n\r\n## usage\r\n\r\n```python\r\nfrom fugashi import tagger\r\n\r\ntagger = tagger('-owakati')\r\ntext = \"\u9ea9\u83d3\u5b50\u306f\u3001\u9ea9\u3092\u4e3b\u6750\u6599\u3068\u3057\u305f\u65e5\u672c\u306e\u83d3\u5b50\u3002\"\r\ntagger.parse(text)\r\n# => '\u9ea9 \u83d3\u5b50 \u306f \u3001 \u9ea9 \u3092 \u4e3b\u6750 \u6599 \u3068 \u3057 \u305f \u65e5\u672c \u306e \u83d3\u5b50 \u3002'\r\nfor word in tagger(text):\r\n    print(word, word.feature.lemma, word.pos, sep='\\t')\r\n    # \"feature\" is the unidic feature data as a named tuple\r\n```\r\n\r\n## installing a dictionary\r\n\r\nfugashi requires a dictionary. [unidic](https://unidic.ninjal.ac.jp/) is\r\nrecommended, and two easy-to-install versions are provided.\r\n\r\n  - [unidic-lite](https://github.com/polm/unidic-lite), a slightly modified version 2.1.2 of unidic (from 2013) that's relatively small\r\n  - [unidic](https://github.com/polm/unidic-py), the latest unidic 3.1.0, which is 770mb on disk and requires a separate download step\r\n\r\nif you just want to make sure things work you can start with `unidic-lite`, but\r\nfor more serious processing `unidic` is recommended. for production use you'll\r\ngenerally want to generate your own dictionary too; for details see the [mecab\r\ndocumentation](https://taku910.github.io/mecab/learn.html).\r\n\r\nto get either of these dictionaries, you can install them directly using `pip`\r\nor do the below:\r\n\r\n```sh\r\npip install fugashi[unidic-lite]\r\n\r\n# the full version of unidic requires a separate download step\r\npip install fugashi[unidic]\r\npython -m unidic download\r\n```\r\n\r\nfor more information on the different mecab dictionaries available, see [this article](https://www.dampfkraft.com/nlp/japanese-tokenizer-dictionaries.html).\r\n\r\n## dictionary use\r\n\r\nfugashi is written with the assumption you'll use unidic to process japanese,\r\nbut it supports arbitrary dictionaries. \r\n\r\nif you're using a dictionary besides unidic you can use the generictagger like this:\r\n\r\n```python\r\nfrom fugashi import generictagger\r\ntagger = generictagger()\r\n\r\n# parse can be used as normal\r\ntagger.parse('something')\r\n# features from the dictionary can be accessed by field numbers\r\nfor word in tagger(text):\r\n    print(word.surface, word.feature[0])\r\n```\r\n\r\nyou can also create a dictionary wrapper to get feature information as a named tuple. \r\n\r\n```python\r\nfrom fugashi import generictagger, create_feature_wrapper\r\ncustomfeatures = create_feature_wrapper('customfeatures', 'alpha beta gamma')\r\ntagger = generictagger(wrapper=customfeatures)\r\nfor word in tagger.parsetonodelist(text):\r\n    print(word.surface, word.feature.alpha)\r\n```\r\n\r\n## citation\r\n\r\nif you use fugashi in research, it would be appreciated if you cite this paper. you can read it at [the acl anthology](https://www.aclweb.org/anthology/2020.nlposs-1.7/) or [on arxiv](https://arxiv.org/abs/2010.06858).\r\n\r\n    @inproceedings{mccann-2020-fugashi,\r\n        title = \"fugashi, a tool for tokenizing {j}apanese in python\",\r\n        author = \"mccann, paul\",\r\n        booktitle = \"proceedings of second workshop for nlp open source software (nlp-oss)\",\r\n        month = nov,\r\n        year = \"2020\",\r\n        address = \"online\",\r\n        publisher = \"association for computational linguistics\",\r\n        url = \"https://www.aclweb.org/anthology/2020.nlposs-1.7\",\r\n        pages = \"44--51\",\r\n        abstract = \"recent years have seen an increase in the number of large-scale multilingual nlp projects. however, even in such projects, languages with special processing requirements are often excluded. one such language is japanese. japanese is written without spaces, tokenization is non-trivial, and while high quality open source tokenizers exist they can be hard to use and lack english documentation. this paper introduces fugashi, a mecab wrapper for python, and gives an introduction to tokenizing japanese.\",\r\n    }\r\n\r\n## alternatives\r\n\r\nif you have a problem with fugashi feel free to open an issue. however, there\r\nare some cases where it might be better to use a different library.\r\n\r\n- if you don't want to deal with installing mecab at all, try [sudachipy](https://github.com/worksapplications/sudachi.rs).\r\n- if you need to work with korean, try [pymecab-ko](https://github.com/nounique/pymecab-ko) or [konlpy](https://konlpy.org/en/latest/).\r\n\r\n## license and copyright notice\r\n\r\nfugashi is released under the terms of the [mit license](./license). please\r\ncopy it far and wide.\r\n\r\nfugashi is a wrapper for mecab, and fugashi wheels include mecab binaries.\r\nmecab is copyrighted free software by taku kudo `<taku@chasen.org>` and nippon\r\ntelegraph and telephone corporation, and is redistributed under the [bsd\r\nlicense](./license.mecab).\r\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "fugashi",
  "package_url": "https://pypi.org/project/fugashi/",
  "project_url": "https://pypi.org/project/fugashi/",
  "project_urls": {
    "Homepage": "https://github.com/polm/fugashi"
  },
  "release_url": "https://pypi.org/project/fugashi/1.3.0/",
  "requires_dist": [
    "unidic ; extra == 'unidic'",
    "unidic-lite ; extra == 'unidic-lite'"
  ],
  "requires_python": ">=3.7",
  "summary": "a cython mecab wrapper for fast, pythonic japanese tokenization.",
  "version": "1.3.0",
  "releases": [],
  "developers": [
    "paul_o",
    "polm@dampfkraft.com"
  ],
  "kwds": "streamlit_badge_black_white fugashi badges badge streamlit",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_fugashi",
  "homepage": "https://github.com/polm/fugashi",
  "release_count": 67,
  "dependency_ids": [
    "pypi_unidic",
    "pypi_unidic_lite"
  ]
}