{
  "classifiers": [
    "license :: osi approved :: apache software license"
  ],
  "description": "# intel\u00ae extension for pytorch\\*\n\nintel\u00ae extension for pytorch\\* extends pytorch\\* with up-to-date features optimizations for an extra performance boost on intel hardware. optimizations take advantage of avx-512 vector neural network instructions (avx512 vnni) and intel\u00ae advanced matrix extensions (intel\u00ae amx) on intel cpus as well as intel x<sup>e</sup> matrix extensions (xmx) ai engines on intel discrete gpus. moreover, through pytorch\\* `xpu` device, intel\u00ae extension for pytorch\\* provides easy gpu acceleration for intel discrete gpus with pytorch\\*.\n\nintel\u00ae extension for pytorch\\* provides optimizations for both eager mode and graph mode, however, compared to eager mode, graph mode in pytorch\\* normally yields better performance from optimization techniques, such as operation fusion. intel\u00ae extension for pytorch\\* amplifies them with more comprehensive graph optimizations. therefore we recommend you to take advantage of intel\u00ae extension for pytorch\\* with [torchscript](https://pytorch.org/docs/stable/jit.html) whenever your workload supports it. you could choose to run with `torch.jit.trace()` function or `torch.jit.script()` function, but based on our evaluation, `torch.jit.trace()` supports more workloads so we recommend you to use `torch.jit.trace()` as your first choice.\n\nthe extension can be loaded as a python module for python programs or linked as a c++ library for c++ programs. in python scripts users can enable it dynamically by importing `intel_extension_for_pytorch`.\n\nin the current technological landscape, generative ai (genai) workloads and models have gained widespread attention and popularity. large language models (llms) have emerged as the dominant models driving these genai applications. starting from 2.1.0, specific optimizations for certain llm models are introduced in the intel\u00ae extension for pytorch\\*. check [llm optimizations](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/llm.html) for details.\n\n* check [cpu tutorial](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/) for detailed information of intel\u00ae extension for pytorch\\* for intel\u00ae cpus. source code is available at the [main branch](https://github.com/intel/intel-extension-for-pytorch/tree/main).\n* check [gpu tutorial](https://intel.github.io/intel-extension-for-pytorch/xpu/latest/) for detailed information of intel\u00ae extension for pytorch\\* for intel\u00ae gpus. source code is available at the [xpu-main branch](https://github.com/intel/intel-extension-for-pytorch/tree/xpu-main).\n\n## installation\n\n### cpu version\n\nyou can use either of the following 2 commands to install intel\u00ae extension for pytorch\\* cpu version.\n\n```python\npython -m pip install intel_extension_for_pytorch\n```\n\n```python\npython -m pip install intel_extension_for_pytorch --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/cpu/us/\n```\n\n**note:** intel\u00ae extension for pytorch\\* has pytorch version requirement. please check more detailed information via the url below.\n\nmore installation methods can be found at [cpu installation guide](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html).\n\ncompilation instruction of the latest cpu code base `main` branch can be found at [installation guide](https://github.com/intel/intel-extension-for-pytorch/blob/main/docs/tutorials/installation.md#install-via-compiling-from-source).\n\n### gpu version\n\nyou can install intel\u00ae extension for pytorch\\* for gpu via command below.\n\n```python\npython -m pip install torch==2.0.1a0 torchvision==0.15.2a0 intel_extension_for_pytorch==2.0.110+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\n```\n\n**note:** the patched pytorch 2.0.1a0 is required to work with intel\u00ae extension for pytorch\\* on intel\u00ae graphics card for now.\n\nmore installation methods can be found at [gpu installation guide](https://intel.github.io/intel-extension-for-pytorch/xpu/latest/tutorials/installation.html).\n\ncompilation instruction of the latest gpu code base `xpu-main` branch can be found at [installation guide](https://github.com/intel/intel-extension-for-pytorch/blob/xpu-main/docs/tutorials/installation.md#install-via-compiling-from-source).\n\n## getting started\n\nminor code changes are required for users to get start with intel\u00ae extension for pytorch\\*. both pytorch imperative mode and torchscript mode are supported. you just need to import intel\u00ae extension for pytorch\\* package and apply its optimize function against the model object. if it is a training workload, the optimize function also needs to be applied against the optimizer object.\n\nthe following code snippet shows an inference code with fp32 data type. more examples on cpu, including training and c++ examples, are available at [cpu example page](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/examples.html). more examples on gpu are available at [gpu example page](https://intel.github.io/intel-extension-for-pytorch/xpu/latest/tutorials/examples.html).\n\n**note:** more detailed information about `torch.compile()` with `ipex` backend can be found at [tutorial features page](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features.html#torch-compile-experimental-new-feature-from-2-0-0).\n\n### inference on cpu\n\n```python\nimport torch\nimport torchvision.models as models\n\nmodel = models.resnet50(pretrained=true)\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize(model)\n\nwith torch.no_grad():\n  model(data)\n```\n\n### inference on gpu\n\n```python\nimport torch\nimport torchvision.models as models\n\nmodel = models.resnet50(pretrained=true)\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\nimport intel_extension_for_pytorch as ipex\nmodel = model.to('xpu')\ndata = data.to('xpu')\nmodel = ipex.optimize(model)\n\nwith torch.no_grad():\n  model(data)\n```\n\n## intel\u00ae ai reference models\n\nuse cases that had already been optimized by intel engineers are available at [intel\u00ae ai reference models](https://github.com/intelai/models/tree/pytorch-r2.1.100-models) (former model zoo). a bunch of pytorch use cases for benchmarking are also available on the [github page](https://github.com/intelai/models/tree/pytorch-r2.1.100-models/benchmarks#pytorch-use-cases). you can get performance benefits out-of-box by simply running scipts in the model zoo.\n\n## license\n\n_apache license_, version _2.0_. as found in [license](https://github.com/intel/intel-extension-for-pytorch/blob/main/license) file.\n\n## security\n\nsee intel's [security center](https://www.intel.com/content/www/us/en/security-center/default.html)\nfor information on how to report a potential security issue or vulnerability.\n\nsee also: [security policy](security.md)\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "https://www.apache.org/licenses/license-2.0",
  "name": "intel-extension-for-pytorch",
  "package_url": "https://pypi.org/project/intel-extension-for-pytorch/",
  "project_url": "https://pypi.org/project/intel-extension-for-pytorch/",
  "project_urls": {
    "Homepage": "https://github.com/intel/intel-extension-for-pytorch"
  },
  "release_url": "https://pypi.org/project/intel-extension-for-pytorch/2.1.100/",
  "requires_dist": [
    "psutil",
    "numpy",
    "packaging"
  ],
  "requires_python": "",
  "summary": "intel\u00ae extension for pytorch*",
  "version": "2.1.100",
  "releases": [],
  "developers": [
    "intel_corp"
  ],
  "kwds": "intel_extension_for_pytorch pytorch intel torchscript benchmarks",
  "license_kwds": "https://www.apache.org/licenses/license-2.0",
  "libtype": "pypi",
  "id": "pypi_intel_extension_for_pytorch",
  "homepage": "https://github.com/intel/intel-extension-for-pytorch",
  "release_count": 14,
  "dependency_ids": [
    "pypi_numpy",
    "pypi_packaging",
    "pypi_psutil"
  ]
}