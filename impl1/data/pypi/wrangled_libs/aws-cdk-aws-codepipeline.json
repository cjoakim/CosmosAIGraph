{
  "classifiers": [
    "development status :: 7 - inactive",
    "framework :: aws cdk",
    "framework :: aws cdk :: 1",
    "intended audience :: developers",
    "license :: osi approved",
    "operating system :: os independent",
    "programming language :: javascript",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "typing :: typed"
  ],
  "description": "# aws codepipeline construct library\n\n<!--begin stability banner-->---\n\n\n![end-of-support](https://img.shields.io/badge/end--of--support-critical.svg?style=for-the-badge)\n\n> aws cdk v1 has reached end-of-support on 2023-06-01.\n> this package is no longer being updated, and users should migrate to aws cdk v2.\n>\n> for more information on how to migrate, see the [*migrating to aws cdk v2* guide](https://docs.aws.amazon.com/cdk/v2/guide/migrating-v2.html).\n\n---\n<!--end stability banner-->\n\n## pipeline\n\nto construct an empty pipeline:\n\n```python\n# construct an empty pipeline\npipeline = codepipeline.pipeline(self, \"myfirstpipeline\")\n```\n\nto give the pipeline a nice, human-readable name:\n\n```python\n# give the pipeline a nice, human-readable name\npipeline = codepipeline.pipeline(self, \"myfirstpipeline\",\n    pipeline_name=\"mypipeline\"\n)\n```\n\nbe aware that in the default configuration, the `pipeline` construct creates\nan aws key management service (aws kms) customer master key (cmk) for you to\nencrypt the artifacts in the artifact bucket, which incurs a cost of\n**$1/month**. this default configuration is necessary to allow cross-account\nactions.\n\nif you do not intend to perform cross-account deployments, you can disable\nthe creation of the customer master keys by passing `crossaccountkeys: false`\nwhen defining the pipeline:\n\n```python\n# don't create customer master keys\npipeline = codepipeline.pipeline(self, \"myfirstpipeline\",\n    cross_account_keys=false\n)\n```\n\nif you want to enable key rotation for the generated kms keys,\nyou can configure it by passing `enablekeyrotation: true` when creating the pipeline.\nnote that key rotation will incur an additional cost of **$1/month**.\n\n```python\n# enable key rotation for the generated kms key\npipeline = codepipeline.pipeline(self, \"myfirstpipeline\",\n    # ...\n    enable_key_rotation=true\n)\n```\n\n## stages\n\nyou can provide stages when creating the pipeline:\n\n```python\n# provide a stage when creating a pipeline\npipeline = codepipeline.pipeline(self, \"myfirstpipeline\",\n    stages=[codepipeline.stageprops(\n        stage_name=\"source\",\n        actions=[]\n    )\n    ]\n)\n```\n\nor append a stage to an existing pipeline:\n\n```python\n# append a stage to an existing pipeline\n# pipeline: codepipeline.pipeline\n\nsource_stage = pipeline.add_stage(\n    stage_name=\"source\",\n    actions=[]\n)\n```\n\nyou can insert the new stage at an arbitrary point in the pipeline:\n\n```python\n# insert a new stage at an arbitrary point\n# pipeline: codepipeline.pipeline\n# another_stage: codepipeline.istage\n# yet_another_stage: codepipeline.istage\n\n\nsome_stage = pipeline.add_stage(\n    stage_name=\"somestage\",\n    placement=codepipeline.stageplacement(\n        # note: you can only specify one of the below properties\n        right_before=another_stage,\n        just_after=yet_another_stage\n    )\n)\n```\n\nyou can disable transition to a stage:\n\n```python\n# disable transition to a stage\n# pipeline: codepipeline.pipeline\n\n\nsome_stage = pipeline.add_stage(\n    stage_name=\"somestage\",\n    transition_to_enabled=false,\n    transition_disabled_reason=\"manual transition only\"\n)\n```\n\nthis is useful if you don't want every executions of the pipeline to flow into\nthis stage automatically. the transition can then be \"manually\" enabled later on.\n\n## actions\n\nactions live in a separate package, `@aws-cdk/aws-codepipeline-actions`.\n\nto add an action to a stage, you can provide it when creating the stage,\nin the `actions` property,\nor you can use the `istage.addaction()` method to mutate an existing stage:\n\n```python\n# use the `istage.addaction()` method to mutate an existing stage.\n# source_stage: codepipeline.istage\n# some_action: codepipeline.action\n\nsource_stage.add_action(some_action)\n```\n\n## custom action registration\n\nto make your own custom codepipeline action requires registering the action provider. look to the `jenkinsprovider` in `@aws-cdk/aws-codepipeline-actions` for an implementation example.\n\n```python\n# make a custom codepipeline action\ncodepipeline.customactionregistration(self, \"genericgitsourceproviderresource\",\n    category=codepipeline.actioncategory.source,\n    artifact_bounds=codepipeline.actionartifactbounds(min_inputs=0, max_inputs=0, min_outputs=1, max_outputs=1),\n    provider=\"genericgitsource\",\n    version=\"1\",\n    entity_url=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-custom-action.html\",\n    execution_url=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-custom-action.html\",\n    action_properties=[codepipeline.customactionproperty(\n        name=\"branch\",\n        required=true,\n        key=false,\n        secret=false,\n        queryable=false,\n        description=\"git branch to pull\",\n        type=\"string\"\n    ), codepipeline.customactionproperty(\n        name=\"giturl\",\n        required=true,\n        key=false,\n        secret=false,\n        queryable=false,\n        description=\"ssh git clone url\",\n        type=\"string\"\n    )\n    ]\n)\n```\n\n## cross-account codepipelines\n\n> cross-account pipeline actions require that the pipeline has *not* been\n> created with `crossaccountkeys: false`.\n\nmost pipeline actions accept an aws resource object to operate on. for example:\n\n* `s3deployaction` accepts an `s3.ibucket`.\n* `codebuildaction` accepts a `codebuild.iproject`.\n* etc.\n\nthese resources can be either newly defined (`new s3.bucket(...)`) or imported\n(`s3.bucket.frombucketattributes(...)`) and identify the resource that should\nbe changed.\n\nthese resources can be in different accounts than the pipeline itself. for\nexample, the following action deploys to an imported s3 bucket from a\ndifferent account:\n\n```python\n# deploy an imported s3 bucket from a different account\n# stage: codepipeline.istage\n# input: codepipeline.artifact\n\nstage.add_action(codepipeline_actions.s3deployaction(\n    bucket=s3.bucket.from_bucket_attributes(self, \"bucket\",\n        account=\"123456789012\"\n    ),\n    input=input,\n    action_name=\"s3-deploy-action\"\n))\n```\n\nactions that don't accept a resource object accept an explicit `account` parameter:\n\n```python\n# actions that don't accept a resource objet accept an explicit `account` parameter\n# stage: codepipeline.istage\n# template_path: codepipeline.artifactpath\n\nstage.add_action(codepipeline_actions.cloudformationcreateupdatestackaction(\n    account=\"123456789012\",\n    template_path=template_path,\n    admin_permissions=false,\n    stack_name=stack.of(self).stack_name,\n    action_name=\"cloudformation-create-update\"\n))\n```\n\nthe `pipeline` construct automatically defines an **iam role** for you in the\ntarget account which the pipeline will assume to perform that action. this\nrole will be defined in a **support stack** named\n`<pipelinestackname>-support-<account>`, that will automatically be deployed\nbefore the stack containing the pipeline.\n\nif you do not want to use the generated role, you can also explicitly pass a\n`role` when creating the action. in that case, the action will operate in the\naccount the role belongs to:\n\n```python\n# explicitly pass in a `role` when creating an action.\n# stage: codepipeline.istage\n# template_path: codepipeline.artifactpath\n\nstage.add_action(codepipeline_actions.cloudformationcreateupdatestackaction(\n    template_path=template_path,\n    admin_permissions=false,\n    stack_name=stack.of(self).stack_name,\n    action_name=\"cloudformation-create-update\",\n    # ...\n    role=iam.role.from_role_arn(self, \"actionrole\", \"...\")\n))\n```\n\n## cross-region codepipelines\n\nsimilar to how you set up a cross-account action, the aws resource object you\npass to actions can also be in different *regions*. for example, the\nfollowing action deploys to an imported s3 bucket from a different region:\n\n```python\n# deploy to an imported s3 bucket from a different region.\n# stage: codepipeline.istage\n# input: codepipeline.artifact\n\nstage.add_action(codepipeline_actions.s3deployaction(\n    bucket=s3.bucket.from_bucket_attributes(self, \"bucket\",\n        region=\"us-west-1\"\n    ),\n    input=input,\n    action_name=\"s3-deploy-action\"\n))\n```\n\nactions that don't take an aws resource will accept an explicit `region`\nparameter:\n\n```python\n# actions that don't take an aws resource will accept an explicit `region` parameter.\n# stage: codepipeline.istage\n# template_path: codepipeline.artifactpath\n\nstage.add_action(codepipeline_actions.cloudformationcreateupdatestackaction(\n    template_path=template_path,\n    admin_permissions=false,\n    stack_name=stack.of(self).stack_name,\n    action_name=\"cloudformation-create-update\",\n    # ...\n    region=\"us-west-1\"\n))\n```\n\nthe `pipeline` construct automatically defines a **replication bucket** for\nyou in the target region, which the pipeline will replicate artifacts to and\nfrom. this bucket will be defined in a **support stack** named\n`<pipelinestackname>-support-<region>`, that will automatically be deployed\nbefore the stack containing the pipeline.\n\nif you don't want to use these support stacks, and already have buckets in\nplace to serve as replication buckets, you can supply these at pipeline definition\ntime using the `crossregionreplicationbuckets` parameter. example:\n\n```python\n# supply replication buckets for the pipeline instead of using the generated support stack\npipeline = codepipeline.pipeline(self, \"myfirstpipeline\",\n    # ...\n\n    cross_region_replication_buckets={\n        # note that a physical name of the replication bucket must be known at synthesis time\n        \"us-west-1\": s3.bucket.from_bucket_attributes(self, \"uswest1replicationbucket\",\n            bucket_name=\"my-us-west-1-replication-bucket\",\n            # optional kms key\n            encryption_key=kms.key.from_key_arn(self, \"uswest1replicationkey\", \"arn:aws:kms:us-west-1:123456789012:key/1234-5678-9012\")\n        )\n    }\n)\n```\n\nsee [the aws docs here](https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html)\nfor more information on cross-region codepipelines.\n\n### creating an encrypted replication bucket\n\nif you're passing a replication bucket created in a different stack,\nlike this:\n\n```python\n# passing a replication bucket created in a different stack.\napp = app()\nreplication_stack = stack(app, \"replicationstack\",\n    env=environment(\n        region=\"us-west-1\"\n    )\n)\nkey = kms.key(replication_stack, \"replicationkey\")\nreplication_bucket = s3.bucket(replication_stack, \"replicationbucket\",\n    # like was said above - replication buckets need a set physical name\n    bucket_name=physicalname.generate_if_needed,\n    encryption_key=key\n)\n\n# later...\ncodepipeline.pipeline(replication_stack, \"pipeline\",\n    cross_region_replication_buckets={\n        \"us-west-1\": replication_bucket\n    }\n)\n```\n\nwhen trying to encrypt it\n(and note that if any of the cross-region actions happen to be cross-account as well,\nthe bucket *has to* be encrypted - otherwise the pipeline will fail at runtime),\nyou cannot use a key directly - kms keys don't have physical names,\nand so you can't reference them across environments.\n\nin this case, you need to use an alias in place of the key when creating the bucket:\n\n```python\n# passing an encrypted replication bucket created in a different stack.\napp = app()\nreplication_stack = stack(app, \"replicationstack\",\n    env=environment(\n        region=\"us-west-1\"\n    )\n)\nkey = kms.key(replication_stack, \"replicationkey\")\nalias = kms.alias(replication_stack, \"replicationalias\",\n    # aliasname is required\n    alias_name=physicalname.generate_if_needed,\n    target_key=key\n)\nreplication_bucket = s3.bucket(replication_stack, \"replicationbucket\",\n    bucket_name=physicalname.generate_if_needed,\n    encryption_key=alias\n)\n```\n\n## variables\n\nthe library supports the codepipeline variables feature.\neach action class that emits variables has a separate variables interface,\naccessed as a property of the action instance called `variables`.\nyou instantiate the action class and assign it to a local variable;\nwhen you want to use a variable in the configuration of a different action,\nyou access the appropriate property of the interface returned from `variables`,\nwhich represents a single variable.\nexample:\n\n```python\n# myaction is some action type that produces variables, like ecrsourceaction\nmy_action = myaction(\n    # ...\n    action_name=\"myaction\"\n)\notheraction(\n    # ...\n    config=my_action.variables.my_variable,\n    action_name=\"otheraction\"\n)\n```\n\nthe namespace name that will be used will be automatically generated by the pipeline construct,\nbased on the stage and action name;\nyou can pass a custom name when creating the action instance:\n\n```python\n# myaction is some action type that produces variables, like ecrsourceaction\nmy_action = myaction(\n    # ...\n    variables_namespace=\"mynamespace\",\n    action_name=\"myaction\"\n)\n```\n\nthere are also global variables available,\nnot tied to any action;\nthese are accessed through static properties of the `globalvariables` class:\n\n```python\n# otheraction is some action type that produces variables, like ecrsourceaction\notheraction(\n    # ...\n    config=codepipeline.globalvariables.execution_id,\n    action_name=\"otheraction\"\n)\n```\n\ncheck the documentation of the `@aws-cdk/aws-codepipeline-actions`\nfor details on how to use the variables for each action class.\n\nsee the [codepipeline documentation](https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-variables.html)\nfor more details on how to use the variables feature.\n\n## events\n\n### using a pipeline as an event target\n\na pipeline can be used as a target for a cloudwatch event rule:\n\n```python\n# a pipeline being used as a target for a cloudwatch event rule.\nimport aws_cdk.aws_events_targets as targets\nimport aws_cdk.aws_events as events\n\n# pipeline: codepipeline.pipeline\n\n\n# kick off the pipeline every day\nrule = events.rule(self, \"daily\",\n    schedule=events.schedule.rate(duration.days(1))\n)\nrule.add_target(targets.codepipeline(pipeline))\n```\n\nwhen a pipeline is used as an event target, the\n\"codepipeline:startpipelineexecution\" permission is granted to the aws\ncloudwatch events service.\n\n### event sources\n\npipelines emit cloudwatch events. to define event rules for events emitted by\nthe pipeline, stages or action, use the `onxxx` methods on the respective\nconstruct:\n\n```python\n# define event rules for events emitted by the pipeline\nimport aws_cdk.aws_events as events\n\n# my_pipeline: codepipeline.pipeline\n# my_stage: codepipeline.istage\n# my_action: codepipeline.action\n# target: events.iruletarget\n\nmy_pipeline.on_state_change(\"mypipelinestatechange\", target=target)\nmy_stage.on_state_change(\"mystagestatechange\", target)\nmy_action.on_state_change(\"myactionstatechange\", target)\n```\n\n## codestar notifications\n\nto define codestar notification rules for pipelines, use one of the `notifyonxxx()` methods.\nthey are very similar to `onxxx()` methods for cloudwatch events:\n\n```python\n# define codestar notification rules for pipelines\nimport aws_cdk.aws_chatbot as chatbot\n\n# pipeline: codepipeline.pipeline\n\ntarget = chatbot.slackchannelconfiguration(self, \"myslackchannel\",\n    slack_channel_configuration_name=\"your_channel_name\",\n    slack_workspace_id=\"your_slack_workspace_id\",\n    slack_channel_id=\"your_slack_channel_id\"\n)\nrule = pipeline.notify_on_execution_state_change(\"notifyonexecutionstatechange\", target)\n```\n",
  "docs_url": null,
  "keywords": "",
  "license": "apache-2.0",
  "name": "aws-cdk.aws-codepipeline",
  "package_url": "https://pypi.org/project/aws-cdk.aws-codepipeline/",
  "project_url": "https://pypi.org/project/aws-cdk.aws-codepipeline/",
  "project_urls": {
    "Homepage": "https://github.com/aws/aws-cdk",
    "Source": "https://github.com/aws/aws-cdk.git"
  },
  "release_url": "https://pypi.org/project/aws-cdk.aws-codepipeline/1.204.0/",
  "requires_dist": [
    "aws-cdk.aws-codestarnotifications (==1.204.0)",
    "aws-cdk.aws-events (==1.204.0)",
    "aws-cdk.aws-iam (==1.204.0)",
    "aws-cdk.aws-kms (==1.204.0)",
    "aws-cdk.aws-s3 (==1.204.0)",
    "aws-cdk.core (==1.204.0)",
    "constructs (<4.0.0,>=3.3.69)",
    "jsii (<2.0.0,>=1.84.0)",
    "publication (>=0.0.3)",
    "typeguard (~=2.13.3)"
  ],
  "requires_python": "~=3.7",
  "summary": "better interface to aws code pipeline",
  "version": "1.204.0",
  "releases": [],
  "developers": [
    "amazon_web_services"
  ],
  "kwds": "aws_cdk codepipeline pipeline_name codepipelines pipeline",
  "license_kwds": "apache-2.0",
  "libtype": "pypi",
  "id": "pypi_aws_cdk.aws_codepipeline",
  "homepage": "https://github.com/aws/aws-cdk",
  "release_count": 258,
  "dependency_ids": [
    "pypi_aws_cdk.aws_codestarnotifications",
    "pypi_aws_cdk.aws_events",
    "pypi_aws_cdk.aws_iam",
    "pypi_aws_cdk.aws_kms",
    "pypi_aws_cdk.aws_s3",
    "pypi_aws_cdk.core",
    "pypi_constructs",
    "pypi_jsii",
    "pypi_publication",
    "pypi_typeguard"
  ]
}