{
  "classifiers": [
    "development status :: 4 - beta",
    "intended audience :: developers",
    "license :: osi approved :: apache software license",
    "natural language :: english",
    "operating system :: os independent",
    "programming language :: python",
    "programming language :: python :: 2.6",
    "programming language :: python :: 2.7",
    "programming language :: python :: 3",
    "programming language :: python :: 3.2",
    "topic :: scientific/engineering"
  ],
  "description": "scikit-learn integration package for apache spark\n=================================================\n\nthis package contains some tools to integrate the `spark computing framework <https://spark.apache.org/>`_\nwith the popular `scikit-learn machine library <https://scikit-learn.org/stable/>`_. among other things, it can:\n\n- train and evaluate multiple scikit-learn models in parallel. it is a distributed analog to the\n  `multicore implementation <https://pythonhosted.org/joblib/parallel.html>`_ included by default in ``scikit-learn``\n- convert spark's dataframes seamlessly into numpy ``ndarray`` or sparse matrices\n- (experimental) distribute scipy's sparse matrices as a dataset of sparse vectors\n\nit focuses on problems that have a small amount of data and that can be run in parallel.\nfor small datasets, it distributes the search for estimator parameters (``gridsearchcv`` in scikit-learn),\nusing spark. for datasets that do not fit in memory, we recommend using the `distributed implementation in\n`spark mllib <https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html>`_.\n\nthis package distributes simple tasks like grid-search cross-validation.\nit does not distribute individual learning algorithms (unlike spark mllib).\n\ninstallation\n------------\n\nthis package is available on pypi:\n\n::\n\n\tpip install spark-sklearn\n\nthis project is also available as `spark package <https://spark-packages.org/package/databricks/spark-sklearn>`_.\n\nthe developer version has the following requirements:\n\n- scikit-learn 0.18 or 0.19. later versions may work, but tests currently are incompatible with 0.20.\n- spark >= 2.1.1. spark may be downloaded from the `spark website <https://spark.apache.org/>`_.\n  in order to use this package, you need to use the pyspark interpreter or another spark-compliant python\n  interpreter. see the `spark guide <https://spark.apache.org/docs/latest/programming-guide.html#overview>`_\n  for more details.\n- `nose <https://nose.readthedocs.org>`_ (testing dependency only)\n- pandas, if using the pandas integration or testing. pandas==0.18 has been tested.\n\nif you want to use a developer version, you just need to make sure the ``python/`` subdirectory is in the\n``pythonpath`` when launching the pyspark interpreter:\n\n::\n\n\tpythonpath=$pythonpath:./python:$spark_home/bin/pyspark\n\nyou can directly run tests:\n\n::\n\n    cd python && ./run-tests.sh\n\nthis requires the environment variable ``spark_home`` to point to your local copy of spark.\n\nexample\n-------\n\nhere is a simple example that runs a grid search with spark. see the `installation <#installation>`_ section\non how to install the package.\n\n.. code:: python\n\n    from sklearn import svm, datasets\n    from spark_sklearn import gridsearchcv\n    iris = datasets.load_iris()\n    parameters = {'kernel':('linear', 'rbf'), 'c':[1, 10]}\n    svr = svm.svc(gamma='auto')\n    clf = gridsearchcv(sc, svr, parameters)\n    clf.fit(iris.data, iris.target)\n\nthis classifier can be used as a drop-in replacement for any scikit-learn classifier, with the same api.\n\ndocumentation\n-------------\n\n`api documentation <http://databricks.github.io/spark-sklearn-docs>`_ is currently hosted on github pages. to\nbuild the docs yourself, see the instructions in ``docs/``.\n\n.. image:: https://travis-ci.org/databricks/spark-sklearn.svg?branch=master\n    :target: https://travis-ci.org/databricks/spark-sklearn",
  "docs_url": null,
  "keywords": "spark,scikit-learn,distributed computing,machine learning",
  "license": "apache 2.0",
  "name": "spark-sklearn",
  "package_url": "https://pypi.org/project/spark-sklearn/",
  "project_url": "https://pypi.org/project/spark-sklearn/",
  "project_urls": {
    "Homepage": "https://github.com/databricks/spark-sklearn"
  },
  "release_url": "https://pypi.org/project/spark-sklearn/0.3.0/",
  "requires_dist": [],
  "requires_python": "",
  "summary": "integration tools for running scikit-learn on spark",
  "version": "0.3.0",
  "releases": [],
  "developers": [
    "joseph@databricks.com",
    "joseph_bradley",
    "tim_hunter",
    "timhunter@databricks.com"
  ],
  "kwds": "spark_sklearn pyspark multicore databricks scikit",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_spark_sklearn",
  "homepage": "https://github.com/databricks/spark-sklearn",
  "release_count": 8,
  "dependency_ids": []
}