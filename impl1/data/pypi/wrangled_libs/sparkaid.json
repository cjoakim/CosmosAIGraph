{
  "classifiers": [
    "development status :: 4 - beta",
    "intended audience :: developers",
    "license :: osi approved :: gnu lesser general public license v3 or later (lgplv3+)",
    "programming language :: python :: 3",
    "topic :: software development :: build tools"
  ],
  "description": "# spark - dataframe with complex schema\n\n# changelog\n### version 1.0.0:\n* **_[breaking changes]_** `flatten()` now stops the unpacking of nested data at arraytype\n  (i.e: any field with datatype = arraytype will have its nested elements as-is).\n  to have the same result as in the previous version - flatten all array fields, add the param `arrays_to_unpack = [\"*\"]`.\n* added `snake_case()`\n* added `json_schema_to_spark_schema()`\n* added support for providing the param `nested_struct_separator` to `flatten()`.\n  example: when provided with the value \"___\", the raw schema `{\"parent\": {\"child\": \"some_value\"}}` will be unpacked to `{\"parrent___child\": \"some_value\"}`\n\n# problem description\na spark dataframe can have a simple schema, where every single column is of a simple datatype like `integertype, booleantype, stringtype`,...\nhowever, a column can be of one of the complex types: `arraytype`, `maptype`, or `structtype`. the schema itself is, actually, an instance of the type `structtype`. so, when a schema has column(s) with datatype is structtype, we have a nested schema.\n\nworking with nested schema is not always easy. some notable problems are:\n* complex sql queries\n* difficult to rename/cast datatype of nested columns\n* unnecessary high io when reading only some nested columns from parquet files (_https://issues.apache.org/jira/browse/spark-17636_)\n\nthe page *https://docs.databricks.com/delta/data-transformation/complex-types.html* provides a lot of useful tips on dealing with dataframes having a complex schema.<br>\n\nthis page will provide some further tips/utils to work on dataframes with more complex schema:\n* renaming or casting/parsing nested columns\n* flattening\n\n_please note the term **flattening** in this post **only** means getting rid of structtype column(s) in our dataframe. this does not include the act of eliminating arraytype and maptype in the schema, which is usually called **exploding** in spark documents. also, spark 2.4 introduced the function flatten [https://spark.apache.org/docs/2.4.0/api/java/org/apache/spark/sql/functions.html#flatten-org.apache.spark.sql.column], which is used on a nested array (an array with 2 layers, an array of arrays of datatype) to make it flat (an array with 1 layer)._\n\n# solutions\n## renaming nested columns\nrenaming a column at root level is simple: use the function `withcolumnrenamed`.\nhowever, with a nested column, that function does not give any error, but also does not make any effect:\n\n\tdf_struct = spark.createdataframe([row(structa=row(field1=10, field2=1.5), structb=row(field3=\"one\",field4=false))])\n    df_struct.printschema()\n\t\n\troot\n     |-- structa: struct (nullable = true)\n     |    |-- field1: long (nullable = true)\n     |    |-- field2: double (nullable = true)\n     |-- structb: struct (nullable = true)\n     |    |-- field3: string (nullable = true)\n     |    |-- field4: boolean (nullable = true)\n    \n    df_struct.withcolumnrenamed(\"structa.field1\", \"structa.newfield1\") \\\n        .withcolumnrenamed(\"structb\", \"newstructb\") \\\n        .printschema()\n\n\troot\n     |-- structa: struct (nullable = true)\n     |    |-- field1: long (nullable = true)\n     |    |-- field2: double (nullable = true)\n     |-- newstructb: struct (nullable = true)\n     |    |-- field3: string (nullable = true)\n     |    |-- field4: boolean (nullable = true)\n\nto change the names of nested columns, there are some options:\n1. by building a new struct column on the flight with the `struct()` function:\n\n\t\tfrom pyspark.sql.functions import struct, col\n\t\tdf_renamed = df_struct.withcolumn(\"structa\", struct(col(\"structa.field1\").alias(\"newfield1\"),\n\t\t                                                    col(\"structa.field2\")))\n\n2. by creating a new *schema* (a `structtype()` object) and use type casting on the original struct column:\n\n\t\tfrom pyspark.sql.types import *\t\t\n\t\tnewstructaschema = structtype([\n\t                            structfield(\"newfield1\", longtype()),\n\t                            structfield(\"field2\", doubletype())\n\t                        ])\n\t    df_renamed = df_struct.withcolumn(\"structa\", col(\"structa\").cast(newstructaschema)).printschema()\n\t    \nboth options yield the same schema\n\n\t    root\n\t     |-- structa: struct (nullable = true)\n\t     |    |-- newfield1: long (nullable = true)\n\t     |    |-- field2: double (nullable = true)\n\t     |-- structb: struct (nullable = true)\n\t     |    |-- field3: string (nullable = true)\n\t     |    |-- field4: boolean (nullable = true)\n\nthe 2nd option is more convenient when building a recursive function to recreate the multi-layer nested schema with new columns names.\n\n---\n\n## flattening\n### structtype\n\nsample dataframe:\n\n    from pyspark.sql import row\n    from pyspark.sql.functions import col\n\n    df_struct = spark.createdataframe([row(structa=row(field1=10, field2=1.5),\n                                           structb=row(field3=\"one\",field4=false))])\n\tdf_struct.printschema()\n\n\troot\n\t |-- structa: struct (nullable = true)\n\t |    |-- field1: long (nullable = true)\n\t |    |-- field2: double (nullable = true)\n\t |-- structb: struct (nullable = true)\n\t |    |-- field3: string (nullable = true)\n\t |    |-- field4: boolean (nullable = true)\n\nspark allows selecting nested columns by using the dot `.` notation:\n\n\tdf_struct.select(\"structa.*\", \"structb.field3\").printschema()\n\t\n\troot\n     |-- field1: long (nullable = true)\n     |-- field2: double (nullable = true)\n     |-- field3: string (nullable = true)\nplease note here that the current spark implementation (2.4.3 or below) doesn't keep the outer layer fieldname (e.g: structa) in the output dataframe\n\n### arraytype\nto select only some elements from an arraytype column, either *`getitem()`* or using brackets (as selecting elements from a legacy array: `[]` in python `()` in scala)  would do the trick:\n\n\tdf_array = spark.createdataframe([row(arraya=[1,2,3,4,5],fieldb=\"foo\")])\n\tdf_array.select(col(\"arraya\").getitem(0).alias(\"element0\"), col(\"arraya\")[4].alias(\"element5\"), col(\"fieldb\")).show()\n\t\n\t+--------+--------+------+\n    |element0|element5|fieldb|\n    +--------+--------+------+\n    |       1|       5|   foo|\n    +--------+--------+------+\n\n### maptype\nelements from a maptype column can be selected the same way as in the case of arraytype, but using the key instead of index number. the dot notation (`.`) could also be used instead of `getitem()` or brackets:\n\n    df_map = spark.createdataframe([row(mapa={2: \"two\", 3: \"three\", 0: \"zero\"}, fieldb=\"foo\")])\n    df_map.select(col(\"mapa\")[3].alias(\"element3\"), col(\"mapa\").getitem(2).alias(\"element2\"), col(\"mapa.0\").alias(\"element0\"), col(\"mapa\").getitem(1).alias(\"element1\")).show()\n\t\n\t+--------+--------+--------+--------+\n    |element3|element2|element0|element1|\n    +--------+--------+--------+--------+\n    |   three|     two|    zero|    null|\n    +--------+--------+--------+--------+\n\n### structtype nested in structtype\nas spark dataframe.select() supports passing an array of columns to be selected, to fully unflatten a multi-layer nested dataframe, a recursive call would do the trick.\n\nhere is a detailed discussion on stackoverflow on how to do this:\nhttps://stackoverflow.com/questions/37471346/automatically-and-elegantly-flatten-dataframe-in-spark-sql\n\n### structtype nested in arraytype\n\tdf_nested = spark.createdataframe([\n\t    row(\n\t        arraya=[\n\t            row(childstructb=row(field1=1, field2=\"foo\")),\n\t            row(childstructb=row(field1=2, field2=\"bar\"))\n\t        ]\n\t    )])\n\tdf_nested.printschema()\n\t\n\troot\n     |-- arraya: array (nullable = true)\n     |    |-- element: struct (containsnull = true)\n     |    |    |-- childstructb: struct (nullable = true)\n     |    |    |    |-- field1: long (nullable = true)\n     |    |    |    |-- field2: string (nullable = true)\n     \n    df_nested.show(1, false)\n    \n    +------------------------+\n    |arraya                  |\n    +------------------------+\n    |[[[1, foo]], [[2, bar]]]|\n    +------------------------+\n     \nselecting *field1* or *field2* can be done as with normal structs (not nested inside an array), by using that dot `.` annotation. the result would be of the type `arraytype[childfieldtype]`, which has been **_vertically sliced_** from the original array\n\n\tdf_child = df_nested.select(\"arraya.childstructb.field1\", \"arraya.childstructb.field2\")\n\tdf_child.printschema()\n\t\n\troot\n     |-- field1: array (nullable = true)\n     |    |-- element: long (containsnull = true)\n     |-- field2: array (nullable = true)\n     |    |-- element: string (containsnull = true)\n     \n    df_child.show()\n\n    +------+----------+\n    |field1|    field2|\n    +------+----------+\n    |[1, 2]|[foo, bar]|\n    +------+----------+\n\n### structtype nested in maptype\nas each maptype column has two components, the keys and the values, selecting nested column inside a maptype column is not straight forward - we cannot just use that `.` to take the nested fields as that has already been used for denoting the key. \n\n    df_map_nested = spark.createdataframe([row(mapa={\"2\": row(type_name=\"arabic number\", equivalent=2), \"three\": row(type_name=\"english text\", equivalent=3)}, fieldb=\"foo\")])\n    df_map_nested.select(col(\"mapa.type_name\"), col(\"mapa.three.type_name\")).show()\n    \n    +---------+------------+\n    |type_name|   type_name|\n    +---------+------------+\n    |     null|english text|\n    +---------+------------+\n    \na solution for this is to use the builtin function `map_values()` which has been introduced since spark 2.3. note the type of the result column: arraytype\n\n    from pyspark.sql.functions import map_values\n    result = df_map_nested.select(map_values(\"mapa\")[\"type_name\"], col(\"mapa.three.type_name\"))\n    result.show(2,false)\n    result.printschema()\n\n    +-----------------------------+------------+\n    |map_values(mapa).type_name   |type_name   |\n    +-----------------------------+------------+\n    |[arabic number, english text]|english text|\n    +-----------------------------+------------+\n    \n    root\n     |-- map_values(mapa).type_name: array (nullable = true)\n     |    |-- element: string (containsnull = true)\n     |-- type_name: string (nullable = true)\n     \n\t\n## hurdles\nthe above steps would work well for most of dataframes. the only dataframes that it fails (as of spark 2.4.3 or lower) are the ones with a structtype nested inside more than one layers of arraytype.\nlike this one:\n\n\tdf_nested_b = spark.createdataframe([\n        row(\n            arraya=[[\n                row(childstructb=row(field1=1, field2=\"foo\")),\n                row(childstructb=row(field1=2, field2=\"bar\"))\n            ]]\n        )])\n    df_nested_b.printschema()\n    \n    root\n     |-- arraya: array (nullable = true)\n     |    |-- element: array (containsnull = true)\n     |    |    |-- element: struct (containsnull = true)\n     |    |    |    |-- childstructb: struct (nullable = true)\n     |    |    |    |    |-- field1: long (nullable = true)\n     |    |    |    |    |-- field2: string (nullable = true)\n     \nor this one\n\n\tdf_nested_c = spark.createdataframe([\n        row(\n            arraya=[\n                row(childstructb=row(childarrayc=[row(field1=1, field2=\"foo\")])),\n                row(childstructb=row(childarrayc=[row(field1=2, field2=\"bar\")])),\n            ]\n        )])\n    df_nested_c.printschema()\n    \n    root\n     |-- arraya: array (nullable = true)\n     |    |-- element: struct (containsnull = true)\n     |    |    |-- childstructb: struct (nullable = true)\n     |    |    |    |-- childarrayc: array (nullable = true)\n     |    |    |    |    |-- element: struct (containsnull = true)\n     |    |    |    |    |    |-- field1: long (nullable = true)\n     |    |    |    |    |    |-- field2: string (nullable = true)\n     \nselecting `arraya.childstructb.field1` from `df_nested_b` fails with the error message: `analysisexception: no such struct field field1 in childstructb`.<br>\nwhile selecting `arraya.childstructb.childarrayc.field1` from `df_nested_c` throws the `analysisexception`: `cannot resolve 'arraya.childstructb.childarrayc['field1']' due to data type mismatch: argument 2 requires integral type, however, ''field1'' is of string type.`\n\n## (more) solutions\nwith the introduction of the sql function `transform` in spark 2.4, the error above can be solved by applying `transform` on every layer of the array.\n\na comprehensive implementation of a flatten function can be found in the python package `sparkaid`:\n\n\tfrom sparkaid import flatten\n\t\n\tflatten(df_nested_b).printschema()\n\t\n\troot\n\t |-- arraya__childstructb_field1: array (nullable = true)\n\t |    |-- element: array (containsnull = true)\n\t |    |    |-- element: long (containsnull = true)\n\t |-- arraya__childstructb_field2: array (nullable = true)\n\t |    |-- element: array (containsnull = true)\n\t |    |    |-- element: string (containsnull = true)\n<p>\n\n\tflatten(df_nested_b).show()\n\t\n\t+---------------------------+---------------------------+\n    |arraya__childstructb_field1|arraya__childstructb_field2|\n    +---------------------------+---------------------------+\n    |                   [[1, 2]]|               [[foo, bar]]|\n    +---------------------------+---------------------------+\n<p>\n  \n    flatten(df_nested_c).printschema()\n\n    root\n     |-- arraya_childstructb_childarrayc_field1: array (nullable = true)\n     |    |-- element: array (containsnull = true)\n     |    |    |-- element: long (containsnull = true)\n     |-- arraya_childstructb_childarrayc_field2: array (nullable = true)\n     |    |-- element: array (containsnull = true)\n     |    |    |-- element: string (containsnull = true)\n\n[]: https://spark.apache.org/docs/2.4.0/api/java/org/apache/spark/sql/functions.html#flatten-org.apache.spark.sql.column\n",
  "docs_url": null,
  "keywords": "spark,dataframe,manipulate",
  "license": "",
  "name": "sparkaid",
  "package_url": "https://pypi.org/project/sparkaid/",
  "project_url": "https://pypi.org/project/sparkaid/",
  "project_urls": {
    "Homepage": "https://github.com/lvhuyen/SparkAid"
  },
  "release_url": "https://pypi.org/project/sparkaid/1.0.0/",
  "requires_dist": [
    "pyspark (>=2.4.0)",
    "pip-tools ; extra == 'dev'",
    "pytest ; extra == 'dev'"
  ],
  "requires_python": ">=3.4",
  "summary": "utils for working with spark",
  "version": "1.0.0",
  "releases": [],
  "developers": [
    "lvhuyen@yahoo.com"
  ],
  "kwds": "json_schema_to_spark_schema arrays_to_unpack arraya__childstructb_field1 arraya__childstructb_field2 arraya_childstructb_childarrayc_field1",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_sparkaid",
  "homepage": "",
  "release_count": 2,
  "dependency_ids": [
    "pypi_pip_tools",
    "pypi_pyspark",
    "pypi_pytest"
  ]
}