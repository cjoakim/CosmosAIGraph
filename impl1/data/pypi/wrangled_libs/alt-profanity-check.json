{
  "classifiers": [
    "development status :: 5 - production/stable",
    "license :: osi approved :: mit license",
    "natural language :: english",
    "operating system :: os independent",
    "programming language :: python :: 3"
  ],
  "description": "# alt-profanity-check\n\nalt profanity check is a drop-in replacement of the `profanity-check` library for the not so well\nmaintained <https://github.com/vzhou842/profanity-check>:\n\n> a fast, robust python library to check for profanity or offensive language in strings.\n> read more about how and why `profanity-check` was built in\n> [this blog post](https://victorzhou.com/blog/better-profanity-detection-with-scikit-learn/).\n\nour aim is to follow scikit-learn's (main dependency) versions and post models trained with the\nsame version number, example alt-profanity-check version 1.2.3.4 should be trained with the\n1.2.3.4 version of the scikit-learn library.\n\nfor joblib which is the next major dependency we will be using the latest one which was available\nwhen we trained the models.\n\nlast but not least we aim to clean up the codebase a bit and **maybe** introduce some features or\ndatasets.\n\n| learn python from the maintainer of alt-profanity-check \ud83c\udf93\ud83e\uddd1\u200d\ud83d\udcbb\ufe0f\u2328\ufe0f                                                                                                                                                                                                            |\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| i am teaching python through mentorcruise, aiming both to beginners and seasoned developers who want to get to the next level in their learning journey: <https://mentorcruise.com/mentor/dimitriosmistriotis/>. please mention that you found me through this repository. |\n\n## changelog\n\nsee\n[changelog.md](https://github.com/dimitrismistriotis/alt-profanity-check/blob/master/changelog.md)\n\n## how it works\n\n`profanity-check` uses a linear svm model trained on 200k human-labeled samples of clean and\nprofane text strings. its model is simple but surprisingly effective, meaning\n**`profanity-check` is both robust and extremely performant**.\n\n## why use profanity-check?\n\n### no explicit blacklist\n\nmany profanity detection libraries use a hard-coded list of bad words to detect and filter\nprofanity. for example, [profanity](https://pypi.org/project/profanity/) uses\n[this wordlist](https://github.com/ben174/profanity/blob/master/profanity/data/wordlist.txt),\nand even [better-profanity](https://pypi.org/project/better-profanity/) still uses\n[a wordlist](https://github.com/snguyenthanh/better_profanity/blob/master/better_profanity/profanity_wordlist.txt).\nthere are obviously glaring issues with this approach, and, while they might be performant,\n**these libraries are not accurate at all**.\n\na simple example for which `profanity-check` is better is the phrase\n\n- \"you cocksucker\"\\* - `profanity` thinks this is clean because it doesn't have\n- \"cocksucker\"\\* in its wordlist.\n\n### performance\n\nother libraries like [profanity-filter](https://github.com/rominf/profanity-filter)\nuse more sophisticated methods that are much more accurate but at the cost of performance.\na benchmark (performed december 2018 on a new 2018 macbook pro) using\n[a kaggle dataset of wikipedia comments](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data) yielded roughly\nthe following results:\n\n| package          | 1 prediction (ms) | 10 predictions (ms) | 100 predictions (ms) |\n| ---------------- | ----------------- | ------------------- | -------------------- |\n| profanity-check  | 0.2               | 0.5                 | 3.5                  |\n| profanity-filter | 60                | 1200                | 13000                |\n| profanity        | 0.3               | 1.2                 | 24                   |\n\n`profanity-check` is anywhere from **300 - 4000 times faster** than `profanity-filter` in this\nbenchmark!\n\n### accuracy\n\nthis table speaks for itself:\n\n| package          | test accuracy | balanced test accuracy | precision | recall | f1 score |\n| ---------------- | ------------- | ---------------------- | --------- | ------ | -------- |\n| profanity-check  | 95.0%         | 93.0%                  | 86.1%     | 89.6%  | 0.88     |\n| profanity-filter | 91.8%         | 83.6%                  | 85.4%     | 70.2%  | 0.77     |\n| profanity        | 85.6%         | 65.1%                  | 91.7%     | 30.8%  | 0.46     |\n\nsee the how section below for more details on the dataset used for these results.\n\n## installation\n\n```\n$ pip install alt-profanity-check\n```\n\n### for older python versions\n\n#### python 3.7\n\nfrom scikit-learn's [github page](https://github.com/scikit-learn/scikit-learn):\n\n> scikit-learn 1.0 and later require python 3.7 or newer.\n> scikit-learn 1.1 and later require python 3.8 or newer.\n\nwhich means that from 1.1.2 and later, python 3.7 is not supported, hence:\nif you are using 3.6 pin alt-profanity-check to **1.0.2.1**.\n\n#### python 3.6\n\nfollowing scikit-learn, **python3.6** is not supported after its 1.0 version if you are using 3.6 pin\nalt-profanity-check to **0.24.2**.\n\n## usage\n\nyou can test from the command line:\n\n```shell\nprofanity_check \"check something\" \"check something else\"\n```\n\n```python\nfrom profanity_check import predict, predict_prob\n\npredict(['predict() takes an array and returns a 1 for each string if it is offensive, else 0.'])\n# [0]\n\npredict(['fuck you'])\n# [1]\n\npredict_prob(['predict_prob() takes an array and returns the probability each string is offensive'])\n# [0.08686173]\n\npredict_prob(['go to hell, you scum'])\n# [0.7618861]\n```\n\nnote that both `predict()` and `predict_prob` return [`numpy`](https://pypi.org/project/numpy/)\narrays.\n\n## more on how/why it works\n\n### how\n\nspecial thanks to the authors of the datasets used in this project. `profanity-check` hence also\n`alt-profanity-check` is trained on a combined dataset from 2 sources:\n\n- [t-davidson/hate-speech-and-offensive-language](https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data),\n  used in their paper _automated hate speech detection and the problem of offensive language_\n- the [toxic comment classification challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data) on kaggle.\n\n`profanity-check` relies heavily on the excellent [`scikit-learn`](https://scikit-learn.org/)\nlibrary. it's mostly powered by `scikit-learn` classes\n[`countvectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.countvectorizer.html),\n[`linearsvc`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.linearsvc.html), and\n[`calibratedclassifiercv`](https://scikit-learn.org/stable/modules/generated/sklearn.calibration.calibratedclassifiercv.html).\nit uses a [bag-of-words model](https://en.wikipedia.org/wiki/bag-of-words_model)\nto vectorize input strings before feeding them to a linear classifier.\n\n### why\n\none simplified way you could think about why `profanity-check` works is this:\nduring the training process, the model learns which words are \"bad\" and how \"bad\" they are\nbecause those words will appear more often in offensive texts. thus, it's as if the training\nprocess is picking out the \"bad\" words out of all possible words and using those to make future\npredictions. this is better than just relying on arbitrary word blacklists chosen by humans!\n\n## caveats\n\nthis library is far from perfect. for example, it has a hard time picking up on less common\nvariants of swear words like _\"f4ck you\"_ or _\"you b1tch\"_ because they don't appear often\nenough in the training corpus. **never treat any prediction from this library as\nunquestionable truth, because it does and will make mistakes.** instead, use this library as a\nheuristic.\n\n## developer notes\n\n- create a virtual environment from the project\n- `pip install -r development_requirements.txt`\n\n### retraining data\n\nwith the above in place:\n\n```shell\ncd profanity_check/data\npython train_model.py\n```\n\n### uploading to pypi\n\ncurrently trying to automate it using github actions; see:\n`.github/workflows/package_release_dry_run.yml`.\n\nsetup:\n\n- set up your \"~/.pypirc\" with the appropriate token\n- `pip install -r requirements_for_uploading.txt` which installs twine\n\nnew version:\n\nwith `x.y.z` as the version to be uploaded:\n\nfirst tag:\n\n```shell\ngit tag -a vx.y.z -m \"version x.y.z\"\ngit push --tags\n```\n\nthen upload:\n\n```shell\npython setup.py sdist\ntwine upload dist/alt-profanity-check-x.y.z.tar.gz\n```\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "alt-profanity-check",
  "package_url": "https://pypi.org/project/alt-profanity-check/",
  "project_url": "https://pypi.org/project/alt-profanity-check/",
  "project_urls": {
    "Homepage": "https://github.com/dimitrismistriotis/alt-profanity-check"
  },
  "release_url": "https://pypi.org/project/alt-profanity-check/1.3.2/",
  "requires_dist": [],
  "requires_python": ">=3.8",
  "summary": "a fast, robust library to check for offensive language in strings. dropdown replacement of \"profanity-check\".",
  "version": "1.3.2",
  "releases": [],
  "developers": [
    "dimitrios@mistriotis.com",
    "victor_zhou"
  ],
  "kwds": "profanity_check profanity_wordlist better_profanity scikit profanity",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_alt_profanity_check",
  "homepage": "https://github.com/dimitrismistriotis/alt-profanity-check",
  "release_count": 14,
  "dependency_ids": []
}