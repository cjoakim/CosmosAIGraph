{
  "classifiers": [
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "# evojax: hardware-accelerated neuroevolution\n\nevojax is a scalable, general purpose, hardware-accelerated [neuroevolution](https://en.wikipedia.org/wiki/neuroevolution) toolkit. built on top of the jax library, this toolkit enables neuroevolution algorithms to work with neural networks running in parallel across multiple tpu/gpus. evojax achieves very high performance by implementing the evolution algorithm, neural network and task all in numpy, which is compiled just-in-time to run on accelerators.\n\nthis repo also includes several extensible examples of evojax for a wide range of tasks, including supervised learning, reinforcement learning and generative art, demonstrating how evojax can run your evolution experiments within minutes on a single accelerator, compared to hours or days when using cpus.\n\nevojax paper: https://arxiv.org/abs/2202.05008 (presentation [video](https://youtu.be/tmkft3wwpb8))\n\nplease use this bibtex if you wish to cite this project in your publications:\n\n```\n@article{evojax2022,\n  title={evojax: hardware-accelerated neuroevolution},\n  author={tang, yujin and tian, yingtao and ha, david},\n  journal={arxiv preprint arxiv:2202.05008},\n  year={2022}\n}\n```\n\nlist of publications using evojax (please open a pr to add missing entries):\n\n- [modern evolution strategies for creativity: fitting concrete images and abstract concepts](https://es-clip.github.io/) (neurips creativity workshop 2021, evomusart 2022)\n\n## installation\n\nevojax is implemented in [jax](https://github.com/google/jax) which needs to be installed first.\n\n**install jax**: \nplease first follow jax's [installation instruction](https://github.com/google/jax#installation) with optional gpu/tpu backend support.\nin case jax is not set up, evojax installation will still try pulling a cpu-only version of jax.\nnote that colab runtimes come with jax pre-installed.\n\n\n**install evojax**:\n```shell\n# install from pypi.\npip install evojax\n\n# or, install from our github repo.\npip install git+https://github.com/google/evojax.git@main\n```\n\nif you also want to install the extra dependencies required for certain optional functionalities, use\n```shell\npip install evojax[extra]\n# or\npip install git+https://github.com/google/evojax.git@main#egg=evojax[extra]\n```\n\n## code overview\n\nevojax is a framework with three major components, which we expect the users to extend.\n1. **neuroevolution algorithms** all neuroevolution algorithms should implement the `evojax.algo.base.nealgorithm` interface and reside in `evojax/algo/`.\nsee [here](https://github.com/google/evojax/blob/main/evojax/algo/readme.md) for the available algorithms in evojax.\n2. **policy networks** all neural networks should implement the `evojax.policy.base.policynetwork` interface and be saved in `evojax/policy/`.\nin this repo, we give example implementations of the mlp, convnet, seq2seq and [permutationinvariant](https://attentionneuron.github.io/) models.\n3. **tasks** all tasks should implement `evojax.task.base.vectorizedtask` and be in `evojax/task/`.\n\nthese components can be used either independently, or orchestrated by `evojax.trainer` and `evojax.sim_mgr` that manage the training pipeline.\nwhile they should be sufficient for the currently provided policies and tasks, we plan to extend their functionality in the future as the need arises.\n\n## examples\n\nas a quickstart, we provide non-trivial examples (scripts in `examples/` and notebooks in `examples/notebooks`) to illustrate the usage of evojax.\nwe provide example commands to start the training process at the top of each script.\nthese scripts and notebooks are run with tpus and/or nvidia v100 gpu(s):\n\n### supervised learning tasks\n\n*while one would obviously use gradient-descent for such tasks in practice, the point is to show that neuroevolution can also solve them to some degree of accuracy within a short amount of time, which will be useful when these models are adapted within a more complicated task where gradient-based approaches may not work.*\n\n<img width=\"100%\" src=\"img/evojax_supervised.png\"></img>\n\n* [mnist classification](https://github.com/google/evojax/blob/main/examples/train_mnist.py) -\nwe show that evojax trains a convnet policy to achieve >98% test accuracy within 5 min on a single gpu.\n* [seq2seq learning](https://github.com/google/evojax/blob/main/examples/train_seq2seq.py) -\nwe demonstrate that evojax is capable of learning a large network with hundreds of thousands parameters to accomplish a seq2seq task.\n\n### classic control tasks\n\n*the purpose of including control tasks are two-fold: 1) unlike supervised learning tasks, control tasks in evojax have undetermined number of steps, we thus use these examples to demonstrate the efficiency of our task roll-out loops. 2) we wish to show the speed-up benefit of implementing tasks in jax and illustrate how to implement one from scratch.*\n\n<img width=\"100%\" src=\"img/evojax_control.png\"></img>\n\n* [locomotion](https://github.com/google/evojax/blob/main/examples/notebooks/braxtasks.ipynb) -\n[brax](https://github.com/google/brax) is a differentiable physics engine implemented in jax.\nwe wrap it as a task and train with evojax on gpus/tpus. it takes evojax tens of minutes to solve a locomotion task in brax.\n* [cart-pole swing up](https://github.com/google/evojax/blob/main/examples/train_cartpole.py) -\nwe illustrate how the classic control task can be implemented in jax and be integrated into evojax's pipeline for significant speed up training.\n\n### novel tasks\n\n*in this last category, we go beyond simple illustrations and show examples of novel tasks that are more practical and attractive to researchers in the genetic and evolutionary computation area, with the goal of helping them try out ideas in evojax.*\n\n<table width=\"100%\">\n  <tr>\n    <td width=\"30%\">\n      <img width=\"100%\" src=\"https://media.giphy.com/media/tg05twwrdaxpoqkg1s/giphy.gif\"></img>\n    </td>\n    <td width=\"30%\">\n      <img width=\"100%\" src=\"https://media.giphy.com/media/zxsbpuaxdaxiifbdi4/giphy.gif\"></img>\n    </td>\n    <td width=\"40%\">\n      <img width=\"100%\" src=\"https://media.giphy.com/media/apiwufjx9fkehyifbh/giphy.gif\"></img>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      multi-agent waterworld\n    </td>\n    <td>\n      es-clip: <i>\u201ca drawing of a cat\u201d</i>\n    </td>\n    <td>\n      slime volleyball\n    </td>\n  </tr>\n</table>\n\n* [waterworld](https://github.com/google/evojax/blob/main/examples/train_waterworld.py) -\nin this [task](https://cs.stanford.edu/people/karpathy/reinforcejs/waterworld.html), an agent tries to get as much food as possible while avoiding poisons.\nevojax is able to train the agent in tens of minutes on a single gpu.\nmoreover, we demonstrate that [multi-agents training](https://github.com/google/evojax/blob/main/examples/train_waterworld_ma.py) in evojax is possible, which is beneficial for learning policies that can deal with environmental complexity and uncertainties.\n* [abstract paintings](https://es-clip.github.io/) ([notebook 1](https://github.com/google/evojax/blob/main/examples/notebooks/abstractpainting01.ipynb) and [notebook 2](https://github.com/google/evojax/blob/main/examples/notebooks/abstractpainting02.ipynb)) -\nwe reproduce the results from this [computational creativity work](https://es-clip.github.io/) and show how the original work, whose implementation requires multiple cpus and gpus, could be accelerated on a single gpu efficiently using evojax, which was not possible before.\nmoreover, with multiple gpus/tpus, evojax can further speed up the mentioned work almost linearly.\nwe also show that the modular design of evojax allows its components to be used independently -- in this case it is possible to use only the es algorithms from evojax while leveraging one's own training loops and environment implantation.\n* [neural slime volleyball](https://github.com/google/evojax/blob/main/examples/train_slimevolley.py) -\nin this [task](https://otoro.net/slimevolley/), the agent's goal is to get the ball to land on the ground of its opponent's side, causing its opponent to lose a life. the episode ends when either agent loses all five lives, or after the time limit. an agent receives a reward of +1 when its opponent loses or -1 when it loses a life.\nevojax is able to train the agent in under 5 minutes on a single gpu, compared hours on multiple cpus.\nthis implementation is based on the [slime volleyball gym environment](https://github.com/hardmaru/slimevolleygym), which is a python port of the original javascript version of the [game](https://otoro.net/slimevolley/) that you can play in the web browser. in all of these versions, the built-in ai opponent and the less-than-ideal physics are identical.\n\n## call for contributions\n\nthe goal of evojax is to get evolutionary computation to able to work on a vast array of tasks using accelerators.\n\none issue before was that many evolution algorithms were only optimized for one particular task for some paper. this is the reason we focused only on one single algorithm (pgpe) in the first release of evojax, while creating 6+ different tasks in diverse domains, ensuring that one single algorithm works for all of the tasks without any issues. see [table](https://github.com/google/evojax/blob/main/evojax/algo/readme.md) of contributed algorithms.\n\n### evolutionary algorithms\n\nwe welcome new evolution algorithms to be added to this toolkit. it would be great if you can show that your implementation can perform on cart-pole swing-up (hardmode), brax, waterworld, and mnist, before submitting a pull request.\n\nideas for evolutionary algorithm candidates:\n\n- your favorite genetic algorithm.\n- [cma-es](https://en.wikipedia.org/wiki/cma-es) (bare version, and improved versions such as [bipop-cma-es](https://hal.inria.fr/hal-00818596v1/document))\n- augmented random search ([paper](https://arxiv.org/abs/1803.07055))\n- amalgam-idea ([paper](https://homepages.cwi.nl/~bosman/publications/2013_benchmarkingparameterfree.pdf))\n\nwe suggest the below performance guidelines for new algorithms:\n\n1. mnist: 90%+\n2. cartpole: 900+ (easy), 600+ (hard)\n3. waterworld: 6+ (single-agent), 2+ (multiiagent)\n4. brax ant: 3000+\n\nnote that these are not hard requirements, but just rough guidelines.\n\nplease use the [benchmark script](https://github.com/google/evojax/tree/main/scripts/benchmarks) to evaluate your algorithm before sending us a pr, let us know if you are unable to test on some tasks due to hardware limitations.\nsee this [example](https://github.com/google/evojax/pull/5#issuecomment-1043879609) pull request thread of a genetic algorithm that has been merged into evojax to see how it should be done. \n\nfeel free to reach out to evojax-dev@google.com if you wish to discuss further.\n\n### new tasks\n\nwe also welcome new tasks and examples (see [here](https://github.com/google/evojax/tree/main/evojax/task) for all tasks in evojax). some suggestions:\n\n- train a [neural turing machine](https://en.wikipedia.org/wiki/neural_turing_machine) using evolution to come up with a sorting algorithm.\n- soccer via self-play ([example](https://mobile.aau.at/~welmenre/papers/fehervari-2010-evolving_neural_network_controllers_for_a_team_of_self-organizing_robots.pdf))\n- evolving hebbian learning-capable plastic networks that can remember the map of a maze from the agent\u2019s recent experience.\n- [adaptive computation time for rnns](https://arxiv.org/abs/1603.08983) performing a task that requires an unknown number of steps.\n- tasks that make use of hard attention.\n\n## sister projects\n\nthere is a growing number of researchers working with evolutionary computation who are using jax. here is a list of related efforts:\n\n- qdax: accelerated quality-diversity. a tool that uses jax to help accelerate quality-diveristy (qd) algorithms through hardware accelerators and massive parallelism. ([github](https://github.com/adaptive-intelligent-robotics/qdax) | [paper](https://arxiv.org/abs/2202.01258))\n\n- evosax: a jax-based library of evolution strategies focusing on jax-composable ask-tell functionality and strategy diversity. more than 10 es algorithms implemented. ([github](https://github.com/roberttlange/evosax))\n\n## disclaimer\nthis is not an official google product.\n",
  "docs_url": null,
  "keywords": "",
  "license": "apache 2.0",
  "name": "evojax",
  "package_url": "https://pypi.org/project/evojax/",
  "project_url": "https://pypi.org/project/evojax/",
  "project_urls": {
    "Homepage": "https://github.com/google/evojax"
  },
  "release_url": "https://pypi.org/project/evojax/0.2.16/",
  "requires_dist": [
    "flax <0.7.0",
    "jax >=0.2.17",
    "jaxlib >=0.1.65",
    "Pillow",
    "cma",
    "matplotlib",
    "pyyaml",
    "evosax ; extra == 'extra'",
    "torchvision ; extra == 'extra'",
    "pandas ; extra == 'extra'",
    "procgen ; extra == 'extra'",
    "brax ; extra == 'extra'"
  ],
  "requires_python": ">=3.8",
  "summary": "evojax: hardware-accelerated neuroevolution.",
  "version": "0.2.16",
  "releases": [],
  "developers": [
    "evojax-dev@google.com",
    "google"
  ],
  "kwds": "evojax neuroevolution evojax_supervised evojax_control implementations",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_evojax",
  "homepage": "https://github.com/google/evojax",
  "release_count": 22,
  "dependency_ids": [
    "pypi_brax",
    "pypi_cma",
    "pypi_evosax",
    "pypi_flax",
    "pypi_jax",
    "pypi_jaxlib",
    "pypi_matplotlib",
    "pypi_pandas",
    "pypi_pillow",
    "pypi_procgen",
    "pypi_pyyaml",
    "pypi_torchvision"
  ]
}