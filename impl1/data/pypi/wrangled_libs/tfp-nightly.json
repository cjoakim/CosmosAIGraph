{
  "classifiers": [
    "development status :: 4 - beta",
    "intended audience :: developers",
    "intended audience :: education",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: scientific/engineering :: mathematics",
    "topic :: software development",
    "topic :: software development :: libraries",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "# tensorflow probability\n\ntensorflow probability is a library for probabilistic reasoning and statistical\nanalysis in tensorflow. as part of the tensorflow ecosystem, tensorflow\nprobability provides integration of probabilistic methods with deep networks,\ngradient-based inference via automatic differentiation, and scalability to\nlarge datasets and models via hardware acceleration (e.g., gpus) and distributed\ncomputation.\n\n__tfp also works as \"tensor-friendly probability\" in pure jax!__:\n`from tensorflow_probability.substrates import jax as tfp` --\nlearn more [here](https://www.tensorflow.org/probability/examples/tensorflow_probability_on_jax).\n\nour probabilistic machine learning tools are structured as follows.\n\n__layer 0: tensorflow.__ numerical operations. in particular, the linearoperator\nclass enables matrix-free implementations that can exploit special structure\n(diagonal, low-rank, etc.) for efficient computation. it is built and maintained\nby the tensorflow probability team and is now part of\n[`tf.linalg`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/ops/linalg)\nin core tf.\n\n__layer 1: statistical building blocks__\n\n* distributions ([`tfp.distributions`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/distributions)):\n  a large collection of probability\n  distributions and related statistics with batch and\n  [broadcasting](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n  semantics. see the\n  [distributions tutorial](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/tensorflow_distributions_tutorial.ipynb).\n* bijectors ([`tfp.bijectors`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/bijectors)):\n  reversible and composable transformations of random variables. bijectors\n  provide a rich class of transformed distributions, from classical examples\n  like the\n  [log-normal distribution](https://en.wikipedia.org/wiki/log-normal_distribution)\n  to sophisticated deep learning models such as\n  [masked autoregressive flows](https://arxiv.org/abs/1705.07057).\n\n__layer 2: model building__\n\n* joint distributions (e.g., [`tfp.distributions.jointdistributionsequential`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/distributions/joint_distribution_sequential.py)):\n    joint distributions over one or more possibly-interdependent distributions.\n    for an introduction to modeling with tfp's `jointdistribution`s, check out\n    [this colab](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/modeling_with_jointdistribution.ipynb)\n* probabilistic layers ([`tfp.layers`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/layers)):\n  neural network layers with uncertainty over the functions they represent,\n  extending tensorflow layers.\n\n__layer 3: probabilistic inference__\n\n* markov chain monte carlo ([`tfp.mcmc`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/mcmc)):\n  algorithms for approximating integrals via sampling. includes\n  [hamiltonian monte carlo](https://en.wikipedia.org/wiki/hamiltonian_monte_carlo),\n  random-walk metropolis-hastings, and the ability to build custom transition\n  kernels.\n* variational inference ([`tfp.vi`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/vi)):\n  algorithms for approximating integrals via optimization.\n* optimizers ([`tfp.optimizer`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/optimizer)):\n  stochastic optimization methods, extending tensorflow optimizers. includes\n  [stochastic gradient langevin dynamics](http://www.icml-2011.org/papers/398_icmlpaper.pdf).\n* monte carlo ([`tfp.monte_carlo`](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/python/monte_carlo)):\n  tools for computing monte carlo expectations.\n\ntensorflow probability is under active development. interfaces may change at any\ntime.\n\n## examples\n\nsee [`tensorflow_probability/examples/`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/examples/)\nfor end-to-end examples. it includes tutorial notebooks such as:\n\n* [linear mixed effects models](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/linear_mixed_effects_models.ipynb).\n  a hierarchical linear model for sharing statistical strength across examples.\n* [eight schools](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/eight_schools.ipynb).\n  a hierarchical normal model for exchangeable treatment effects.\n* [hierarchical linear models](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/hlm_tfp_r_stan.ipynb).\n  hierarchical linear models compared among tensorflow probability, r, and stan.\n* [bayesian gaussian mixture models](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/bayesian_gaussian_mixture_model.ipynb).\n  clustering with a probabilistic generative model.\n* [probabilistic principal components analysis](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/probabilistic_pca.ipynb).\n  dimensionality reduction with latent variables.\n* [gaussian copulas](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/gaussian_copula.ipynb).\n  probability distributions for capturing dependence across random variables.\n* [tensorflow distributions: a gentle introduction](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/tensorflow_distributions_tutorial.ipynb).\n  introduction to tensorflow distributions.\n* [understanding tensorflow distributions shapes](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/understanding_tensorflow_distributions_shapes.ipynb).\n  how to distinguish between samples, batches, and events for arbitrarily shaped\n  probabilistic computations.\n* [tensorflow probability case study: covariance estimation](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/tensorflow_probability_case_study_covariance_estimation.ipynb).\n  a user's case study in applying tensorflow probability to estimate covariances.\n\nit also includes example scripts such as:\n\n  representation learning with a latent code and variational inference.\n* [vector-quantized autoencoder](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/examples/vq_vae.py).\n  discrete representation learning with vector quantization.\n* [disentangled sequential variational autoencoder](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/examples/disentangled_vae.py)\n  disentangled representation learning over sequences with variational inference.\n* [bayesian neural networks](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/examples/bayesian_neural_network.py).\n  neural networks with uncertainty over their weights.\n* [bayesian logistic regression](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/examples/logistic_regression.py).\n  bayesian inference for binary classification.\n\n## installation\n\nfor additional details on installing tensorflow, guidance installing\nprerequisites, and (optionally) setting up virtual environments, see the\n[tensorflow installation guide](https://www.tensorflow.org/install).\n\n### stable builds\n\nto install the latest stable version, run the following:\n\n```shell\n# notes:\n\n# - the `--upgrade` flag ensures you'll get the latest version.\n# - the `--user` flag ensures the packages are installed to your user directory\n#   rather than the system directory.\n# - tensorflow 2 packages require a pip >= 19.0\npython -m pip install --upgrade --user pip\npython -m pip install --upgrade --user tensorflow tensorflow_probability\n```\n\nfor cpu-only usage (and a smaller install), install with `tensorflow-cpu`.\n\nto use a pre-2.0 version of tensorflow, run:\n\n```shell\npython -m pip install --upgrade --user \"tensorflow<2\" \"tensorflow_probability<0.9\"\n```\n\nnote: since [tensorflow](https://www.tensorflow.org/install) is *not* included\nas a dependency of the tensorflow probability package (in `setup.py`), you must\nexplicitly install the tensorflow package (`tensorflow` or `tensorflow-cpu`).\nthis allows us to maintain one package instead of separate packages for cpu and\ngpu-enabled tensorflow. see the\n[tfp release notes](https://github.com/tensorflow/probability/releases) for more\ndetails about dependencies between tensorflow and tensorflow probability.\n\n\n### nightly builds\n\nthere are also nightly builds of tensorflow probability under the pip package\n`tfp-nightly`, which depends on one of `tf-nightly` or `tf-nightly-cpu`.\nnightly builds include newer features, but may be less stable than the\nversioned releases. both stable and nightly docs are available\n[here](https://www.tensorflow.org/probability/api_docs/python/tfp?version=nightly).\n\n```shell\npython -m pip install --upgrade --user tf-nightly tfp-nightly\n```\n\n### installing from source\n\nyou can also install from source. this requires the [bazel](\nhttps://bazel.build/) build system. it is highly recommended that you install\nthe nightly build of tensorflow (`tf-nightly`) before trying to build\ntensorflow probability from source. the most recent version of bazel that tfp\ncurrently supports is 6.4.0; support for 7.0.0+ is wip.\n\n```shell\n# sudo apt-get install bazel git python-pip  # ubuntu; others, see above links.\npython -m pip install --upgrade --user tf-nightly\ngit clone https://github.com/tensorflow/probability.git\ncd probability\nbazel build --copt=-o3 --copt=-march=native :pip_pkg\npkgdir=$(mktemp -d)\n./bazel-bin/pip_pkg $pkgdir\npython -m pip install --upgrade --user $pkgdir/*.whl\n```\n\n## community\n\nas part of tensorflow, we're committed to fostering an open and welcoming\nenvironment.\n\n* [stack overflow](https://stackoverflow.com/questions/tagged/tensorflow): ask\n  or answer technical questions.\n* [github](https://github.com/tensorflow/probability/issues): report bugs or\n  make feature requests.\n* [tensorflow blog](https://blog.tensorflow.org/): stay up to date on content\n  from the tensorflow team and best articles from the community.\n* [youtube channel](http://youtube.com/tensorflow/): follow tensorflow shows.\n* [tfprobability@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfprobability):\n  open mailing list for discussion and questions.\n\nsee the [tensorflow community](https://www.tensorflow.org/community/) page for\nmore details. check out our latest publicity here:\n\n+ [coffee with a googler: probabilistic machine learning in tensorflow](\n  https://www.youtube.com/watch?v=bjukl8dfh5q)\n+ [introducing tensorflow probability](\n  https://medium.com/tensorflow/introducing-tensorflow-probability-dca4c304e245)\n\n## contributing\n\nwe're eager to collaborate with you! see [`contributing.md`](contributing.md)\nfor a guide on how to contribute. this project adheres to tensorflow's\n[code of conduct](code_of_conduct.md). by participating, you are expected to\nuphold this code.\n\n## references\n\nif you use tensorflow probability in a paper, please cite:\n\n+ _tensorflow distributions._ joshua v. dillon, ian langmore, dustin tran,\neugene brevdo, srinivas vasudevan, dave moore, brian patton, alex alemi, matt\nhoffman, rif a. saurous.\n[arxiv preprint arxiv:1711.10604, 2017](https://arxiv.org/abs/1711.10604).\n\n(we're aware there's a lot more to tensorflow probability than distributions, but the distributions paper lays out our vision and is a fine thing to cite for now.)\n",
  "docs_url": null,
  "keywords": "tensorflow probability statistics bayesian machine learning",
  "license": "apache 2.0",
  "name": "tfp-nightly",
  "package_url": "https://pypi.org/project/tfp-nightly/",
  "project_url": "https://pypi.org/project/tfp-nightly/",
  "project_urls": {
    "Homepage": "http://github.com/tensorflow/probability"
  },
  "release_url": "https://pypi.org/project/tfp-nightly/0.24.0.dev20231226/",
  "requires_dist": [
    "absl-py",
    "six >=1.10.0",
    "numpy >=1.13.3",
    "decorator",
    "cloudpickle >=1.3",
    "gast >=0.3.2",
    "dm-tree",
    "jax ; extra == 'jax'",
    "jaxlib ; extra == 'jax'",
    "tfds-nightly ; extra == 'tfds'"
  ],
  "requires_python": ">=3.9",
  "summary": "probabilistic modeling and statistical inference in tensorflow",
  "version": "0.24.0.dev20231226",
  "releases": [],
  "developers": [
    "google_llc",
    "no-reply@google.com"
  ],
  "kwds": "tensorflow_probability_on_jax tensorflow_probability tensorflow _tensorflow tensorflow_distributions_tutorial",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_tfp_nightly",
  "homepage": "http://github.com/tensorflow/probability",
  "release_count": 2041,
  "dependency_ids": [
    "pypi_absl_py",
    "pypi_cloudpickle",
    "pypi_decorator",
    "pypi_dm_tree",
    "pypi_gast",
    "pypi_jax",
    "pypi_jaxlib",
    "pypi_numpy",
    "pypi_six",
    "pypi_tfds_nightly"
  ]
}