{
  "classifiers": [
    "license :: osi approved :: mit license",
    "programming language :: python",
    "programming language :: python :: 3",
    "topic :: database",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "# [nats-bench: benchmarking nas algorithms for architecture topology and size](https://arxiv.org/abs/2009.00437)\n\nxuanyi dong, lu liu, katarzyna musial, bogdan gabrys\n\nin ieee transactions on pattern analysis and machine intelligence (tpami), 2021\n\n**abstract**: neural architecture search (nas) has attracted a lot of attention and has been illustrated to bring tangible benefits in a large number of applications in the past few years. network topology and network size have been regarded as two of the most important aspects for the performance of deep learning models and the community has spawned lots of searching algorithms for both of those aspects of the neural architectures. however, the performance gain from these searching algorithms is achieved under different search spaces and training setups. this makes the overall performance of the algorithms incomparable and the improvement from a sub-module of the searching model unclear.\nin this paper, we propose nats-bench, a unified benchmark on searching for both topology and size, for (almost) any up-to-date nas algorithm.\nnats-bench includes the search space of 15,625 neural cell candidates for architecture topology and 32,768 for architecture size on three datasets.\nwe analyze the validity of our benchmark in terms of various criteria and performance comparison of all candidates in the search space.\nwe also show the versatility of nats-bench by benchmarking 13 recent state-of-the-art nas algorithms on it. all logs and diagnostic information trained using the same setup for each candidate are provided.\nthis facilitates a much larger community of researchers to focus on developing better nas algorithms in a more comparable and computationally effective environment.\n\n**you can use `pip install nats_bench` to install the library of nats-bench\nor install from source by `pip install .`.**\n\nif you are seeking how to re-create nats-bench from scratch or reproduce benchmarked results, please see use [autodl-projects](https://github.com/d-x-y/autodl-projects) and see these [instructions](https://github.com/d-x-y/nats-bench#how-to-re-create-nats-bench-from-scratch).\n\nif you have questions, please ask at [here](https://github.com/d-x-y/nats-bench/issues) or [email me](mailto:dongxuanyi888@gmail.com) :)\n\nthis figure is the main difference between `nats-bench`, `nas-bench-101`, and `nas-bench-201`. the `topology search space` (`$\\mathcal{s}_t$`) in `nats-bench` is the same as `nas-bench-201`, while we upgrade with results of more runs for the architecture candidates, and the benchmarked nas algorithms have better hyperparameters.\n<p align=\"center\">\n<img src=\"https://xuanyidong.com/resources/images/nats-compare.png\" width=\"700\"/>\n</p>\n\n\n## preparation and download\n\n**step-1: download raw vision datasets.** (you can skip this one if you do not use weight-sharing nas or re-create nats-bench).\n\nin nats-bench, we (create and) use three image datasets -- cifar-10, cifar-100, and imagenet16-120.\nfor more details, please see sec-3.2 in [the nats-bench paper](https://arxiv.org/pdf/2009.00437.pdf). to download these three datasets, please find them at [google drive](https://drive.google.com/drive/folders/1t3uiyzxuhmmiujlobmiykasjknatrro4?usp=sharing).\nto create the `imagenet16-120` pytorch dataset, please call [autodl-projects/lib/datasets/imagenet16](https://github.com/d-x-y/autodl-projects/blob/main/xautodl/datasets/get_dataset_with_transform.py#l222-l225), by using:\n```\ntrain_data = imagenet16(root, true , train_transform, 120)\ntest_data  = imagenet16(root, false, test_transform , 120)\n```\n\n**step-2: download benchmark files of nats-bench.**\n\nthe **latest** benchmark file of nats-bench can be downloaded from [google drive](https://drive.google.com/drive/folders/1zjb6wmanikwb2a1yil2hq8h_qyese2yt?usp=sharing).\nafter download `nats-[tss/sss]-[version]-[md5sum]-simple.tar`, please uncompress it by using `tar xvf [file_name]`.\nwe highly recommend to put the downloaded benchmark file (`nats-sss-v1_0-50262.pickle.pbz2` / `nats-tss-v1_0-3ffb9.pickle.pbz2`) or uncompressed archive (`nats-sss-v1_0-50262-simple` / `nats-tss-v1_0-3ffb9-simple`) into `$torch_home`.\nin this way, our api will automatically find the path for these benchmark files, which are convenient for the users. otherwise, you need to indicate the file when creating the benchmark instance manually.\n\nthe history of benchmark files is as follows, `tss` indicates the topology search space and `sss` indicates the size search space.\nthe benchmark file is used when creating the nats-bench instance with `fast_mode=false`.\nthe archive is used when `fast_mode=true`, where `archive` is a directory containing 15,625 files for tss or contains 32,768 files for sss.\neach file contains all the information for a specific architecture candidate.\nthe `full archive` is similar to `archive`, while each file in `full archive` contains **the trained weights**.\nsince the full archive is too large, we use `split -b 30g file_name file_name` to split it into multiple 30g chunks.\nto merge the chunks into the original full archive, you can use `cat file_name* > file_name`.\n\n|   date     |  benchmark file (tss) | (unpacked benchmark file) archive (tss) | full archive (tss) |       benchmark file (sss)      | (unpacked benchmark file) archive (sss)        | full archive (sss) |\n|:-----------|:---------------------:|:-------------:|:------------------:|:-------------------------------:|:--------------------------:|:------------------:|\n| 2020.08.31 | [nats-tss-v1_0-3ffb9.pickle.pbz2](https://drive.google.com/file/d/1vzyk0uvh2d3ftpa1_dswnp1gvgpaxrul/view?usp=sharing) | [nats-tss-v1_0-3ffb9-simple.tar](https://drive.google.com/file/d/17_sacsj_krkjlcblojepntzpxarmcqxu/view?usp=sharing) | [nats-tss-v1_0-3ffb9-full](https://drive.google.com/drive/folders/17s2xg_rvkuul4kujdq0wawouudbo8zkb?usp=sharing) | [nats-sss-v1_0-50262.pickle.pbz2](https://drive.google.com/file/d/1iabivzwedddawicbzfttcmxxywpioiox/view?usp=sharing) | [nats-sss-v1_0-50262-simple.tar](https://drive.google.com/file/d/1scomtuwcqhama_imedp9ltzwmgqhlgga/view?usp=sharing) | [nats-sss-v1_0-50262-full](https://drive.google.com/drive/folders/1xutpqj4bhouv1emarspd0c1buqvtmuyy?usp=sharing) |\n| 2021.04.22 (baidu-pan) | [nats-tss-v1_0-3ffb9.pickle.pbz2](https://pan.baidu.com/s/10z20f5s2rrpzgwro40fltw) (code: 8duj) | [nats-tss-v1_0-3ffb9-simple.tar](https://pan.baidu.com/s/1vonrhlxcb4y8cxudrhuyag) (code: tu1e) | [nats-tss-v1_0-3ffb9-full](https://pan.baidu.com/s/1qbpnli8y1i29qmdxto_yca) (code:ssub) | [nats-sss-v1_0-50262.pickle.pbz2](https://pan.baidu.com/s/1m1uaxr6y1d_rqeyg95yjca) (code: za2h) | [nats-sss-v1_0-50262-simple.tar](https://pan.baidu.com/s/1ek-b89pw2qdm9mp6kkkera) (code: e4t9) | [nats-sss-v1_0-50262-full](https://pan.baidu.com/s/1biruqd9ppeyarej_wttg_a) (code: htif) |\n\nthese benchmark files (without pretrained weights) can also be downloaded from [dropbox](https://www.dropbox.com/sh/ceeo70u1buow681/aac2m-sbkoxiiqpb0ucgxnxja?dl=0), [onedrive](https://1drv.ms/u/s!aqkc27lrowwdf6svuiksxx0uqai?e=nfvm5r) or [baidu-pan (extract code: h6pm)](https://pan.baidu.com/s/144vc2bdm6ixbavzmupqo7a).\n\nfor the full checkpoints in `nats-*ss-*-full`, we split the file into multiple parts (`nats-*ss-*-full.tara*`) since they are too large to upload.\neach file is about `30gb`. for baidu pan, since they restrict the maximum size of each file, we further split `nats-*ss-*-full.tara*` into `nats-*ss-*-full.tara*-aa` and `nats-*ss-*-full.tara*-ab`.\nall splits are created by the command `split`.\n\n**note:** if you encounter the `quota exceed erros` when download from google drive, please try to (1) login your personal google account, (2) right-click-copy the files to your personal google drive, and (3) download from your personal google drive.\n\n## usage\n\nsee more examples at [notebooks](notebooks).\n\n#### 1, create the benchmark instance:\n```\nfrom nats_bench import create\n# create the api instance for the size search space in nats\napi = create(none, 'sss', fast_mode=true, verbose=true)\n\n# create the api instance for the topology search space in nats\napi = create(none, 'tss', fast_mode=true, verbose=true)\n```\n\n#### 2, query the performance:\n```\n# show the architecture topology string of the 12-th architecture\n# for the topology search space, the string is interpreted as\n# arch = '|{}~0|+|{}~0|{}~1|+|{}~0|{}~1|{}~2|'.format(\n#         edge_node_0_to_node_1,\n#         edge_node_0_to_node_2,\n#         edge_node_1_to_node_2,\n#         edge_node_0_to_node_3,\n#         edge_node_1_to_node_3,\n#         edge_node_2_to_node_3,\n#         )\n# for the size search space, the string is interpreted as\n# arch = '{}:{}:{}:{}:{}'.format(out_channel_of_1st_conv_layer,\n#                                out_channel_of_1st_cell_stage,\n#                                out_channel_of_1st_residual_block,\n#                                out_channel_of_2nd_cell_stage,\n#                                out_channel_of_2nd_residual_block,\n#                                )\narchitecture_str = api.arch(12)\nprint(architecture_str)\n\n# query the loss / accuracy / time for 1234-th candidate architecture on cifar-10\n# info is a dict, where you can easily figure out the meaning by key\ninfo = api.get_more_info(1234, 'cifar10')\n\n# query the flops, params, latency. info is a dict.\ninfo = api.get_cost_info(12, 'cifar10')\n\n# simulate the training of the 1224-th candidate:\nvalidation_accuracy, latency, time_cost, current_total_time_cost = api.simulate_train_eval(1224, dataset='cifar10', hp='12')\n```\n\n#### 3, create the instance of an architecture candidate in `nats-bench`:\n```\n# create the instance of th 12-th candidate for cifar-10.\n# to keep nats-bench repo concise, we did not include any model-related codes here because they rely on pytorch.\n# the package of [models] is defined at https://github.com/d-x-y/autodl-projects\n#   so that one need to first import this package.\nimport xautodl\nfrom xautodl.models import get_cell_based_tiny_net\nconfig = api.get_net_config(12, 'cifar10')\nnetwork = get_cell_based_tiny_net(config)\n\n# load the pre-trained weights: params is a dict, where the key is the seed and value is the weights.\nparams = api.get_net_param(12, 'cifar10', none)\nnetwork.load_state_dict(next(iter(params.values())))\n```\n\n#### 4, others:\n```\n# clear the parameters of the 12-th candidate.\napi.clear_params(12)\n\n# reload all information of the 12-th candidate.\napi.reload(index=12)\n\n```\n\nplease see [`api_test.py`](https://github.com/d-x-y/nats-bench/blob/main/tests/api_test.py) for more examples.\n```\nfrom nats_bench import api_test\napi_test.test_nats_bench_tss('nats-tss-v1_0-3ffb9-simple')\napi_test.test_nats_bench_tss('nats-sss-v1_0-50262-simple')\n```\n\n\n\n## how to re-create nats-bench from scratch\n\n**you need to use the [autodl-projects](https://github.com/d-x-y/autodl-projects) repo to re-create nats-bench from scratch.**\n\n### the size search space\n\nthe following command will train all architecture candidate in the size search space with 90 epochs and use the random seed of `777`.\nif you want to use a different number of training epochs, please replace `90` with it, such as `01` or `12`.\n```\nbash ./scripts/nats-bench/train-shapes.sh 00000-32767 90 777\n```\nthe checkpoint of all candidates are located at `output/nats-bench-size` by default.\n\nafter training these candidate architectures, please use the following command to re-organize all checkpoints into the official benchmark file.\n```\npython exps/nats-bench/sss-collect.py\n```\n\n### the topology search space\n\nthe following command will train all architecture candidate in the topology search space with 200 epochs and use the random seed of `777`/`888`/`999`.\nif you want to use a different number of training epochs, please replace `200` with it, such as `12`.\n```\nbash scripts/nats-bench/train-topology.sh 00000-15624 200 '777 888 999'\n```\nthe checkpoint of all candidates are located at `output/nats-bench-topology` by default.\n\nafter training these candidate architectures, please use the following command to re-organize all checkpoints into the official benchmark file.\n```\npython exps/nats-bench/tss-collect.py\n```\n\n\n## to reproduce 13 baseline nas algorithms in nats-bench\n\n**you need to use the [autodl-projects](https://github.com/d-x-y/autodl-projects) repo to run 13 baseline nas methods.** here are a brief introduction on how to run each algorithm ([nats-algos](https://github.com/d-x-y/autodl-projects/tree/main/exps/nats-algos)).\n\n### reproduce nas methods on the topology search space\n\nplease use the following commands to run different nas methods on the topology search space:\n```\nfour multi-trial based methods:\npython ./exps/nats-algos/reinforce.py       --dataset cifar100 --search_space tss --learning_rate 0.01\npython ./exps/nats-algos/regularized_ea.py  --dataset cifar100 --search_space tss --ea_cycles 200 --ea_population 10 --ea_sample_size 3\npython ./exps/nats-algos/random_wo_share.py --dataset cifar100 --search_space tss\npython ./exps/nats-algos/bohb.py            --dataset cifar100 --search_space tss --num_samples 4 --random_fraction 0.0 --bandwidth_factor 3\n\ndarts (first order):\npython ./exps/nats-algos/search-cell.py --dataset cifar10  --data_path $torch_home/cifar.python --algo darts-v1\npython ./exps/nats-algos/search-cell.py --dataset cifar100 --data_path $torch_home/cifar.python --algo darts-v1\npython ./exps/nats-algos/search-cell.py --dataset imagenet16-120 --data_path $torch_home/cifar.python/imagenet16 --algo darts-v1\n\ndarts (second order):\npython ./exps/nats-algos/search-cell.py --dataset cifar10  --data_path $torch_home/cifar.python --algo darts-v2\npython ./exps/nats-algos/search-cell.py --dataset cifar100 --data_path $torch_home/cifar.python --algo darts-v2\npython ./exps/nats-algos/search-cell.py --dataset imagenet16-120 --data_path $torch_home/cifar.python/imagenet16 --algo darts-v2\n\ngdas:\npython ./exps/nats-algos/search-cell.py --dataset cifar10  --data_path $torch_home/cifar.python --algo gdas\npython ./exps/nats-algos/search-cell.py --dataset cifar100 --data_path $torch_home/cifar.python --algo gdas\npython ./exps/nats-algos/search-cell.py --dataset imagenet16-120 --data_path $torch_home/cifar.python/imagenet16\n\nsetn:\npython ./exps/nats-algos/search-cell.py --dataset cifar10  --data_path $torch_home/cifar.python --algo setn\npython ./exps/nats-algos/search-cell.py --dataset cifar100 --data_path $torch_home/cifar.python --algo setn\npython ./exps/nats-algos/search-cell.py --dataset imagenet16-120 --data_path $torch_home/cifar.python/imagenet16 --algo setn\n\nrandom search with weight sharing:\npython ./exps/nats-algos/search-cell.py --dataset cifar10  --data_path $torch_home/cifar.python --algo random\npython ./exps/nats-algos/search-cell.py --dataset cifar100 --data_path $torch_home/cifar.python --algo random\npython ./exps/nats-algos/search-cell.py --dataset imagenet16-120 --data_path $torch_home/cifar.python/imagenet16 --algo random\n\nenas:\npython ./exps/nats-algos/search-cell.py --dataset cifar10  --data_path $torch_home/cifar.python --algo enas --arch_weight_decay 0 --arch_learning_rate 0.001 --arch_eps 0.001\npython ./exps/nats-algos/search-cell.py --dataset cifar100 --data_path $torch_home/cifar.python --algo enas --arch_weight_decay 0 --arch_learning_rate 0.001 --arch_eps 0.001\npython ./exps/nats-algos/search-cell.py --dataset imagenet16-120 --data_path $torch_home/cifar.python/imagenet16 --algo enas --arch_weight_decay 0 --arch_learning_rate 0.001 --arch_eps 0.001\n```\n\n### reproduce nas methods on the size search space\n\nplease use the following commands to run different nas methods on the size search space:\n```\nfour multi-trial based methods:\npython ./exps/nats-algos/reinforce.py       --dataset cifar100 --search_space sss --learning_rate 0.01\npython ./exps/nats-algos/regularized_ea.py  --dataset cifar100 --search_space sss --ea_cycles 200 --ea_population 10 --ea_sample_size 3\npython ./exps/nats-algos/random_wo_share.py --dataset cifar100 --search_space sss\npython ./exps/nats-algos/bohb.py            --dataset cifar100 --search_space sss --num_samples 4 --random_fraction 0.0 --bandwidth_factor 3\n\n\nrun transformable architecture search (tas), proposed in network pruning via transformable architecture search, neurips 2019\n\npython ./exps/nats-algos/search-size.py --dataset cifar10  --data_path $torch_home/cifar.python --algo tas --rand_seed 777\npython ./exps/nats-algos/search-size.py --dataset cifar100 --data_path $torch_home/cifar.python --algo tas --rand_seed 777\npython ./exps/nats-algos/search-size.py --dataset imagenet16-120 --data_path $torch_home/cifar.python/imagenet16 --algo tas --rand_seed 777\n\n\nrun the channel search strategy in fbnet-v2 -- masking + gumbel-softmax :\n\npython ./exps/nats-algos/search-size.py --dataset cifar10  --data_path $torch_home/cifar.python --algo mask_gumbel --rand_seed 777\npython ./exps/nats-algos/search-size.py --dataset cifar100 --data_path $torch_home/cifar.python --algo mask_gumbel --rand_seed 777\npython ./exps/nats-algos/search-size.py --dataset imagenet16-120 --data_path $torch_home/cifar.python/imagenet16 --algo mask_gumbel --rand_seed 777\n\n\nrun the channel search strategy in tunas -- masking + sampling :\n\npython ./exps/nats-algos/search-size.py --dataset cifar10  --data_path $torch_home/cifar.python --algo mask_rl --arch_weight_decay 0 --rand_seed 777 --use_api 0\npython ./exps/nats-algos/search-size.py --dataset cifar100 --data_path $torch_home/cifar.python --algo mask_rl --arch_weight_decay 0 --rand_seed 777\npython ./exps/nats-algos/search-size.py --dataset imagenet16-120 --data_path $torch_home/cifar.python/imagenet16 --algo mask_rl --arch_weight_decay 0 --rand_seed 777\n```\n\n### final discovered architectures for each algorithm\n\nthe architecture index can be found by use `api.query_index_by_arch(architecture_string)`.\n\nthe final discovered architecture id on cifar-10:\n```\ndarts (first order):\n|skip_connect~0|+|skip_connect~0|skip_connect~1|+|skip_connect~0|skip_connect~1|skip_connect~2|\n|skip_connect~0|+|skip_connect~0|skip_connect~1|+|skip_connect~0|skip_connect~1|skip_connect~2|\n|skip_connect~0|+|skip_connect~0|skip_connect~1|+|skip_connect~0|skip_connect~1|skip_connect~2|\n\ndarts (second order):\n|skip_connect~0|+|skip_connect~0|skip_connect~1|+|skip_connect~0|skip_connect~1|skip_connect~2|\n|skip_connect~0|+|skip_connect~0|skip_connect~1|+|skip_connect~0|skip_connect~1|skip_connect~2|\n|skip_connect~0|+|skip_connect~0|skip_connect~1|+|skip_connect~0|skip_connect~1|skip_connect~2|\n\ngdas:\n|nor_conv_3x3~0|+|nor_conv_3x3~0|none~1|+|nor_conv_1x1~0|nor_conv_3x3~1|nor_conv_3x3~2|\n|nor_conv_3x3~0|+|nor_conv_3x3~0|none~1|+|nor_conv_3x3~0|nor_conv_3x3~1|nor_conv_3x3~2|\n|avg_pool_3x3~0|+|nor_conv_3x3~0|skip_connect~1|+|nor_conv_3x3~0|nor_conv_1x1~1|nor_conv_1x1~2|\n```\n\nthe final discovered architecture id on cifar-100:\n```\ndarts (v1):\n|none~0|+|skip_connect~0|none~1|+|skip_connect~0|nor_conv_1x1~1|none~2|\n|none~0|+|skip_connect~0|none~1|+|skip_connect~0|nor_conv_1x1~1|none~2|\n|skip_connect~0|+|skip_connect~0|none~1|+|skip_connect~0|nor_conv_1x1~1|nor_conv_3x3~2|\n\ndarts (v2):\n|none~0|+|skip_connect~0|none~1|+|skip_connect~0|nor_conv_1x1~1|skip_connect~2|\n|skip_connect~0|+|nor_conv_3x3~0|none~1|+|skip_connect~0|none~1|none~2|\n|skip_connect~0|+|nor_conv_1x1~0|none~1|+|nor_conv_3x3~0|skip_connect~1|none~2|\n\ngdas:\n|nor_conv_3x3~0|+|nor_conv_1x1~0|none~1|+|avg_pool_3x3~0|nor_conv_3x3~1|nor_conv_3x3~2|\n|avg_pool_3x3~0|+|nor_conv_1x1~0|none~1|+|nor_conv_3x3~0|avg_pool_3x3~1|nor_conv_1x1~2|\n|avg_pool_3x3~0|+|nor_conv_3x3~0|none~1|+|nor_conv_3x3~0|nor_conv_1x1~1|nor_conv_1x1~2|\n```\n\nthe final discovered architecture id on imagenet-16-120:\n```\ndarts (v1):\n|none~0|+|skip_connect~0|none~1|+|skip_connect~0|none~1|nor_conv_3x3~2|\n|none~0|+|skip_connect~0|none~1|+|skip_connect~0|none~1|nor_conv_3x3~2|\n|none~0|+|skip_connect~0|none~1|+|skip_connect~0|none~1|nor_conv_1x1~2|\n\ndarts (v2):\n|none~0|+|skip_connect~0|none~1|+|skip_connect~0|none~1|skip_connect~2|\n\ngdas:\n|none~0|+|none~0|none~1|+|nor_conv_3x3~0|none~1|none~2|\n|none~0|+|none~0|none~1|+|nor_conv_3x3~0|none~1|none~2|\n|none~0|+|none~0|none~1|+|nor_conv_3x3~0|none~1|none~2|\n```\n\n## others\n\nwe use [`black`](https://github.com/psf/black) for python code formatter.\nplease use `black . -l 120`.\n\n## citation\n\nif you find that nats-bench helps your research, please consider citing it:\n```\n@article{dong2021nats,\n  title   = {{nats-bench}: benchmarking nas algorithms for architecture topology and size},\n  author  = {dong, xuanyi and liu, lu and musial, katarzyna and gabrys, bogdan},\n  doi     = {10.1109/tpami.2021.3054824},\n  journal = {ieee transactions on pattern analysis and machine intelligence (tpami)},\n  year    = {2021},\n  note    = {\\mbox{doi}:\\url{10.1109/tpami.2021.3054824}}\n}\n@inproceedings{dong2020nasbench201,\n  title     = {{nas-bench-201}: extending the scope of reproducible neural architecture search},\n  author    = {dong, xuanyi and yang, yi},\n  booktitle = {international conference on learning representations (iclr)},\n  url       = {https://openreview.net/forum?id=hjxyzkbkdr},\n  year      = {2020}\n}\n```\n\n\n",
  "docs_url": null,
  "keywords": "nas dataset api deeplearning",
  "license": "mit licence",
  "name": "nats-bench",
  "package_url": "https://pypi.org/project/nats-bench/",
  "project_url": "https://pypi.org/project/nats-bench/",
  "project_urls": {
    "Homepage": "https://github.com/D-X-Y/NATS-Bench"
  },
  "release_url": "https://pypi.org/project/nats-bench/1.8/",
  "requires_dist": [
    "numpy (>=1.16.5)"
  ],
  "requires_python": ">=3.6",
  "summary": "api for nats-bench (a dataset/benchmark for neural architecture topology and size).",
  "version": "1.8",
  "releases": [],
  "developers": [
    "dongxuanyi888@gmail.com",
    "xuanyi_dong"
  ],
  "kwds": "nas searching imagenet benchmark benchmarked",
  "license_kwds": "mit licence",
  "libtype": "pypi",
  "id": "pypi_nats_bench",
  "homepage": "https://github.com/d-x-y/nats-bench",
  "release_count": 9,
  "dependency_ids": [
    "pypi_numpy"
  ]
}