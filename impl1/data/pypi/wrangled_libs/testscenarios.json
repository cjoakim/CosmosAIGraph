{
  "classifiers": [
    "development status :: 6 - mature",
    "intended audience :: developers",
    "license :: osi approved :: apache software license",
    "license :: osi approved :: bsd license",
    "operating system :: os independent",
    "programming language :: python",
    "programming language :: python :: 3",
    "topic :: software development :: quality assurance",
    "topic :: software development :: testing"
  ],
  "description": "*****************************************************************\ntestscenarios: extensions to python unittest to support scenarios\n*****************************************************************\n\n  copyright (c) 2009, robert collins <robertc@robertcollins.net>\n  \n  licensed under either the apache license, version 2.0 or the bsd 3-clause\n  license at the users choice. a copy of both licenses are available in the\n  project source as apache-2.0 and bsd. you may not use this file except in\n  compliance with one of these two licences.\n  \n  unless required by applicable law or agreed to in writing, software\n  distributed under these licenses is distributed on an \"as is\" basis, without\n  warranties or conditions of any kind, either express or implied.  see the\n  license you chose for the specific language governing permissions and\n  limitations under that license.\n\n\ntestscenarios provides clean dependency injection for python unittest style\ntests. this can be used for interface testing (testing many implementations via\na single test suite) or for classic dependency injection (provide tests with\ndependencies externally to the test code itself, allowing easy testing in\ndifferent situations).\n\ndependencies\n============\n\n* python 2.6+\n* testtools <https://launchpad.net/testtools>\n\n\nwhy testscenarios\n=================\n\nstandard python unittest.py provides on obvious method for running a single\ntest_foo method with two (or more) scenarios: by creating a mix-in that\nprovides the functions, objects or settings that make up the scenario. this is\nhowever limited and unsatisfying. firstly, when two projects are cooperating\non a test suite (for instance, a plugin to a larger project may want to run\nthe standard tests for a given interface on its implementation), then it is\neasy for them to get out of sync with each other: when the list of testcase\nclasses to mix-in with changes, the plugin will either fail to run some tests\nor error trying to run deleted tests. secondly, its not as easy to work with\nruntime-created-subclasses (a way of dealing with the aforementioned skew)\nbecause they require more indirection to locate the source of the test, and will\noften be ignored by e.g. pyflakes pylint etc.\n\nit is the intent of testscenarios to make dynamically running a single test\nin multiple scenarios clear, easy to debug and work with even when the list\nof scenarios is dynamically generated.\n\n\ndefining scenarios\n==================\n\na **scenario** is a tuple of a string name for the scenario, and a dict of\nparameters describing the scenario.  the name is appended to the test name, and\nthe parameters are made available to the test instance when it's run.\n\nscenarios are presented in **scenario lists** which are typically python lists\nbut may be any iterable.\n\n\ngetting scenarios applied\n=========================\n\nat its heart the concept is simple. for a given test object with a list of\nscenarios we prepare a new test object for each scenario. this involves:\n\n* clone the test to a new test with a new id uniquely distinguishing it.\n* apply the scenario to the test by setting each key, value in the scenario\n  as attributes on the test object.\n\nthere are some complicating factors around making this happen seamlessly. these\nfactors are in two areas:\n\n* choosing what scenarios to use. (see setting scenarios for a test).\n* getting the multiplication to happen. \n\nsubclasssing\n++++++++++++\n\nif you can subclass testwithscenarios, then the ``run()`` method in\ntestwithscenarios will take care of test multiplication. it will at test\nexecution act as a generator causing multiple tests to execute. for this to \nwork reliably testwithscenarios must be first in the mro and you cannot\noverride run() or __call__. this is the most robust method, in the sense\nthat any test runner or test loader that obeys the python unittest protocol\nwill run all your scenarios.\n\nmanual generation\n+++++++++++++++++\n\nif you cannot subclass testwithscenarios (e.g. because you are using\ntwistedtestcase, or testcasewithresources, or any one of a number of other\nuseful test base classes, or need to override run() or __call__ yourself) then \nyou can cause scenario application to happen later by calling\n``testscenarios.generate_scenarios()``. for instance::\n\n  >>> import unittest\n  >>> try:\n  ...     from stringio import stringio\n  ... except importerror:\n  ...     from io import stringio\n  >>> from testscenarios.scenarios import generate_scenarios\n\nthis can work with loaders and runners from the standard library, or possibly other\nimplementations::\n\n  >>> loader = unittest.testloader()\n  >>> test_suite = unittest.testsuite()\n  >>> runner = unittest.texttestrunner(stream=stringio())\n\n  >>> mytests = loader.loadtestsfromnames(['doc.test_sample'])\n  >>> test_suite.addtests(generate_scenarios(mytests))\n  >>> runner.run(test_suite)\n  <unittest...texttestresult run=1 errors=0 failures=0>\n\ntestloaders\n+++++++++++\n\nsome test loaders support hooks like ``load_tests`` and ``test_suite``.\nensuring your tests have had scenario application done through these hooks can\nbe a good idea - it means that external test runners (which support these hooks\nlike ``nose``, ``trial``, ``tribunal``) will still run your scenarios. (of\ncourse, if you are using the subclassing approach this is already a surety).\nwith ``load_tests``::\n\n  >>> def load_tests(standard_tests, module, loader):\n  ...     result = loader.suiteclass()\n  ...     result.addtests(generate_scenarios(standard_tests))\n  ...     return result\n\nas a convenience, this is available in ``load_tests_apply_scenarios``, so a\nmodule using scenario tests need only say ::\n\n  >>> from testscenarios import load_tests_apply_scenarios as load_tests\n\npython 2.7 and greater support a different calling convention for `load_tests``\n<https://bugs.launchpad.net/bzr/+bug/607412>.  `load_tests_apply_scenarios`\ncopes with both.\n\nwith ``test_suite``::\n\n  >>> def test_suite():\n  ...     loader = testloader()\n  ...     tests = loader.loadtestsfromname(__name__)\n  ...     result = loader.suiteclass()\n  ...     result.addtests(generate_scenarios(tests))\n  ...     return result\n\n\nsetting scenarios for a test\n============================\n\na sample test using scenarios can be found in the doc/ folder.\n\nsee `pydoc testscenarios` for details.\n\non the testcase\n+++++++++++++++\n\nyou can set a scenarios attribute on the test case::\n\n  >>> class mytest(unittest.testcase):\n  ...\n  ...     scenarios = [\n  ...         ('scenario1', dict(param=1)),\n  ...         ('scenario2', dict(param=2)),]\n\nthis provides the main interface by which scenarios are found for a given test.\nsubclasses will inherit the scenarios (unless they override the attribute).\n\nafter loading\n+++++++++++++\n\ntest scenarios can also be generated arbitrarily later, as long as the test has\nnot yet run. simply replace (or alter, but be aware that many tests may share a\nsingle scenarios attribute) the scenarios attribute. for instance in this\nexample some third party tests are extended to run with a custom scenario. ::\n\n  >>> import testtools\n  >>> class testtransport:\n  ...     \"\"\"hypothetical test case for bzrlib transport tests\"\"\"\n  ...     pass\n  ...\n  >>> stock_library_tests = unittest.testloader().loadtestsfromnames(\n  ...     ['doc.test_sample'])\n  ...\n  >>> for test in testtools.iterate_tests(stock_library_tests):\n  ...     if isinstance(test, testtransport):\n  ...         test.scenarios = test.scenarios + [my_vfs_scenario]\n  ...\n  >>> suite = unittest.testsuite()\n  >>> suite.addtests(generate_scenarios(stock_library_tests))\n\ngenerated tests don't have a ``scenarios`` list, because they don't normally\nrequire any more expansion.  however, you can add a ``scenarios`` list back on\nto them, and then run them through ``generate_scenarios`` again to generate the\ncross product of tests. ::\n\n  >>> class crossproductdemo(unittest.testcase):\n  ...     scenarios = [('scenario_0_0', {}),\n  ...                  ('scenario_0_1', {})]\n  ...     def test_foo(self):\n  ...         return\n  ...\n  >>> suite = unittest.testsuite()\n  >>> suite.addtests(generate_scenarios(crossproductdemo(\"test_foo\")))\n  >>> for test in testtools.iterate_tests(suite):\n  ...     test.scenarios = [\n  ...         ('scenario_1_0', {}), \n  ...         ('scenario_1_1', {})]\n  ...\n  >>> suite2 = unittest.testsuite()\n  >>> suite2.addtests(generate_scenarios(suite))\n  >>> print(suite2.counttestcases())\n  4\n\ndynamic scenarios\n+++++++++++++++++\n\na common use case is to have the list of scenarios be dynamic based on plugins\nand available libraries. an easy way to do this is to provide a global scope\nscenarios somewhere relevant to the tests that will use it, and then that can\nbe customised, or dynamically populate your scenarios from a registry etc.\nfor instance::\n\n  >>> hash_scenarios = []\n  >>> try:\n  ...     from hashlib import md5\n  ... except importerror:\n  ...     pass\n  ... else:\n  ...     hash_scenarios.append((\"md5\", dict(hash=md5)))\n  >>> try:\n  ...     from hashlib import sha1\n  ... except importerror:\n  ...     pass\n  ... else:\n  ...     hash_scenarios.append((\"sha1\", dict(hash=sha1)))\n  ...\n  >>> class testhashcontract(unittest.testcase):\n  ...\n  ...     scenarios = hash_scenarios\n  ...\n  >>> class testhashperformance(unittest.testcase):\n  ...\n  ...     scenarios = hash_scenarios\n\n\nforcing scenarios\n+++++++++++++++++\n\nthe ``apply_scenarios`` function can be useful to apply scenarios to a test\nthat has none applied. ``apply_scenarios`` is the workhorse for\n``generate_scenarios``, except it takes the scenarios passed in rather than\nintrospecting the test object to determine the scenarios. the\n``apply_scenarios`` function does not reset the test scenarios attribute,\nallowing it to be used to layer scenarios without affecting existing scenario\nselection.\n\n\ngenerating scenarios\n====================\n\nsome functions (currently one :-) are available to ease generation of scenario\nlists for common situations.\n\ntesting per implementation module\n+++++++++++++++++++++++++++++++++\n\nit is reasonably common to have multiple python modules that provide the same\ncapabilities and interface, and to want apply the same tests to all of them.\n\nin some cases, not all of the statically defined implementations will be able\nto be used in a particular testing environment.  for example, there may be both\na c and a pure-python implementation of a module.  you want to test the c\nmodule if it can be loaded, but also to have the tests pass if the c module has\nnot been compiled.\n\nthe ``per_module_scenarios`` function generates a scenario for each named\nmodule. the module object of the imported module is set in the supplied\nattribute name of the resulting scenario.\nmodules which raise ``importerror`` during import will have the\n``sys.exc_info()`` of the exception set instead of the module object. tests\ncan check for the attribute being a tuple to decide what to do (e.g. to skip).\n\nnote that for the test to be valid, all access to the module under test must go\nthrough the relevant attribute of the test object.  if one of the\nimplementations is also directly imported by the test module or any other,\ntestscenarios will not magically stop it being used.\n\n\nadvice on writing scenarios\n===========================\n\nif a parameterised test is because of a bug run without being parameterized,\nit should fail rather than running with defaults, because this can hide bugs.\n\n\nproducing scenarios\n===================\n\nthe `multiply_scenarios` function produces the cross-product of the scenarios\npassed in::\n\n  >>> from testscenarios.scenarios import multiply_scenarios\n  >>> \n  >>> scenarios = multiply_scenarios(\n  ...      [('scenario1', dict(param1=1)), ('scenario2', dict(param1=2))],\n  ...      [('scenario2', dict(param2=1))],\n  ...      )\n  >>> scenarios == [('scenario1,scenario2', {'param2': 1, 'param1': 1}),\n  ...               ('scenario2,scenario2', {'param2': 1, 'param1': 2})]\n  true",
  "docs_url": null,
  "keywords": "",
  "license": "unknown",
  "name": "testscenarios",
  "package_url": "https://pypi.org/project/testscenarios/",
  "project_url": "https://pypi.org/project/testscenarios/",
  "project_urls": {
    "Download": "UNKNOWN",
    "Homepage": "https://launchpad.net/testscenarios"
  },
  "release_url": "https://pypi.org/project/testscenarios/0.5.0/",
  "requires_dist": [],
  "requires_python": null,
  "summary": "testscenarios, a pyunit extension for dependency injection",
  "version": "0.5.0",
  "releases": [],
  "developers": [
    "testing",
    "testing-cabal@lists.launchpad.net"
  ],
  "kwds": "load_tests_apply_scenarios test_suite load_tests python apache",
  "license_kwds": "unknown",
  "libtype": "pypi",
  "id": "pypi_testscenarios",
  "homepage": "https://launchpad.net/testscenarios",
  "release_count": 5,
  "dependency_ids": []
}