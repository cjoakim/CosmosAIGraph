{
  "classifiers": [
    "development status :: 4 - beta",
    "intended audience :: developers",
    "license :: osi approved :: apache software license",
    "natural language :: english",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "programming language :: python :: implementation :: cpython",
    "programming language :: python :: implementation :: pypy"
  ],
  "description": "segments\n========\n\n[![build status](https://github.com/cldf/segments/workflows/tests/badge.svg)](https://github.com/cldf/segments/actions?query=workflow%3atests)\n[![codecov](https://codecov.io/gh/cldf/segments/branch/master/graph/badge.svg)](https://codecov.io/gh/cldf/segments)\n[![pypi](https://img.shields.io/pypi/v/segments.svg)](https://pypi.org/project/segments)\n\n\n[![doi](https://zenodo.org/badge/doi/10.5281/zenodo.1051157.svg)](https://doi.org/10.5281/zenodo.1051157)\n\nthe segments package provides unicode standard tokenization routines and orthography segmentation,\nimplementing the linear algorithm described in the orthography profile specification from \n*the unicode cookbook* (moran and cysouw 2018 [![doi](https://zenodo.org/badge/doi/10.5281/zenodo.1296780.svg)](https://doi.org/10.5281/zenodo.1296780)).\n\n\ncommand line usage\n------------------\n\ncreate a text file:\n```\n$ echo \"a\u00e4aa\u00f6aa\u00fcaa\" > text.txt\n```\n\nnow look at the profile:\n```\n$ cat text.txt | segments profile\ngrapheme        frequency       mapping\na       7       a\na\u0308       1       a\u0308\nu\u0308       1       u\u0308\no\u0308       1       o\u0308\n```\n\nwrite the profile to a file:\n```\n$ cat text.txt | segments profile > profile.prf\n```\n\nedit the profile:\n\n```\n$ more profile.prf\ngrapheme        frequency       mapping\naa      0       x\na       7       a\na\u0308       1       a\u0308\nu\u0308       1       u\u0308\no\u0308       1       o\u0308\n```\n\nnow tokenize the text without profile:\n```\n$ cat text.txt | segments tokenize\na a\u0308 a a o\u0308 a a u\u0308 a a\n```\n\nand with profile:\n```\n$ cat text.txt | segments --profile=profile.prf tokenize\na a\u0308 aa o\u0308 aa u\u0308 aa\n\n$ cat text.txt | segments --mapping=mapping --profile=profile.prf tokenize\na a\u0308 x o\u0308 x u\u0308 x\n```\n\n\napi\n---\n\n```python\n>>> from segments import profile, tokenizer\n>>> t = tokenizer()\n>>> t('abcd')\n'a b c d'\n>>> prf = profile({'grapheme': 'ab', 'mapping': 'x'}, {'grapheme': 'cd', 'mapping': 'y'})\n>>> print(prf)\ngrapheme\tmapping\nab\tx\ncd\ty\n>>> t = tokenizer(profile=prf)\n>>> t('abcd')\n'ab cd'\n>>> t('abcd', column='mapping')\n'x y'\n```\n\n\n",
  "docs_url": null,
  "keywords": "tokenizer",
  "license": "apache 2.0",
  "name": "segments",
  "package_url": "https://pypi.org/project/segments/",
  "project_url": "https://pypi.org/project/segments/",
  "project_urls": {
    "Homepage": "https://github.com/cldf/segments"
  },
  "release_url": "https://pypi.org/project/segments/2.2.1/",
  "requires_dist": [
    "clldutils (>=1.7.3)",
    "csvw (>=1.5.6)",
    "regex",
    "flake8 ; extra == 'dev'",
    "twine ; extra == 'dev'",
    "wheel ; extra == 'dev'",
    "pytest-cov ; extra == 'test'",
    "pytest-mock ; extra == 'test'",
    "pytest (>=5) ; extra == 'test'"
  ],
  "requires_python": "",
  "summary": "",
  "version": "2.2.1",
  "releases": [],
  "developers": [
    "steven.moran@uzh.ch",
    "steven_moran_and_robert_forkel"
  ],
  "kwds": "segments tokenizer tokenization segmentation tokenize",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_segments",
  "homepage": "https://github.com/cldf/segments",
  "release_count": 17,
  "dependency_ids": [
    "pypi_clldutils",
    "pypi_csvw",
    "pypi_flake8",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_pytest_mock",
    "pypi_regex",
    "pypi_twine",
    "pypi_wheel"
  ]
}