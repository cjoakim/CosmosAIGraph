{
  "classifiers": [
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: mit license",
    "operating system :: macos",
    "operating system :: microsoft :: windows",
    "operating system :: posix",
    "operating system :: unix",
    "programming language :: python",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "topic :: scientific/engineering",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "<img src=\"images/logo.png\" width=\"70%\" height=\"70%\"/>\n\n[![pypi - python](https://img.shields.io/badge/python-3.6%20|%203.7%20|%203.8-blue.svg)](https://pypi.org/project/keybert/)\n[![pypi - license](https://img.shields.io/badge/license-mit-green.svg)](https://github.com/maartengr/keybert/blob/master/license)\n[![pypi - pypi](https://img.shields.io/pypi/v/polyfuzz)](https://pypi.org/project/polyfuzz/)\n[![build](https://img.shields.io/github/workflow/status/maartengr/polyfuzz/code%20checks/master)](https://pypi.org/project/polyfuzz/)\n[![docs](https://img.shields.io/badge/docs-passing-green.svg)](https://maartengr.github.io/polyfuzz/)  \n**`polyfuzz`** performs fuzzy string matching, string grouping, and contains extensive evaluation functions. \npolyfuzz is meant to bring fuzzy string matching techniques together within a single framework.\n\ncurrently, methods include a variety of edit distance measures, a character-based n-gram tf-idf, word embedding\ntechniques such as fasttext and glove, and \u00f0\u00ff\u00a4\u2014 transformers embeddings.  \n\ncorresponding medium post can be found [here](https://towardsdatascience.com/string-matching-with-bert-tf-idf-and-more-274bb3a95136?source=friends_link&sk=0f765b76ceaba49363829c13dfdc9d98).\n\n\n<a name=\"installation\"/></a>\n## installation\nyou can install **`polyfuzz`** via pip:\n \n```bash\npip install polyfuzz\n```\n\nyou may want to install more depending on the transformers and language backends that you will be using. the possible installations are:\n\n```python\npip install polyfuzz[sbert]\npip install polyfuzz[flair]\npip install polyfuzz[gensim]\npip install polyfuzz[spacy]\npip install polyfuzz[use]\n```\n\nif you want to speed up the cosine similarity comparison and decrease memory usage when using embedding models, \nyou can use `sparse_dot_topn` which is installed via:\n\n```bash\npip install polyfuzz[fast]\n```\n\n<details>\n<summary>installation issues</summary>\n\nyou might run into installation issues with `sparse_dot_topn`. if so, one solution that has worked for many \nis by installing it via conda first before installing polyfuzz:\n\n```bash\nconda install -c conda-forge sparse_dot_topn\n```\n\nif that does not work, i would advise you to look through their \nissues](https://github.com/ing-bank/sparse_dot_topn/issues) page or continue to use polyfuzz without `sparse_dot_topn`. \n\n</details>  \n\n\n<a name=\"gettingstarted\"/></a>\n## getting started\n\nfor an in-depth overview of the possibilities of `polyfuzz` \nyou can check the full documentation [here](https://maartengr.github.io/polyfuzz/) or you can follow along \nwith the notebook [here](https://github.com/maartengr/polyfuzz/blob/master/notebooks/overview.ipynb).\n\n### quick start\n\nthe main goal of `polyfuzz` is to allow the user to perform different methods for matching strings. \nwe start by defining two lists, one to map from and one to map to. we are going to be using `tf-idf` to create \nn-grams on a character level in order to compare similarity between strings. then, we calculate the similarity \nbetween strings by calculating the cosine similarity between vector representations. \n\nwe only have to instantiate `polyfuzz` with `tf-idf` and match the lists:\n\n```python\nfrom polyfuzz import polyfuzz\n\nfrom_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"]\nto_list = [\"apple\", \"apples\", \"mouse\"]\n\nmodel = polyfuzz(\"tf-idf\")\nmodel.match(from_list, to_list)\n```  \n\nthe resulting matches can be accessed through `model.get_matches()`:\n\n```python\n>>> model.get_matches()\n         from      to    similarity\n0       apple   apple    1.000000\n1      apples  apples    1.000000\n2        appl   apple    0.783751\n3       recal    none    0.000000\n4       house   mouse    0.587927\n5  similarity    none    0.000000\n\n``` \n\n**note 1**: if you want to compare distances within a single list, you can simply pass that list as such: `model.match(from_list)`\n\n**note 2**: when instantiating `polyfuzz` we also could have used \"editdistance\" or \"embeddings\" to quickly \naccess levenshtein and fasttext (english) respectively. \n\n### production\nthe `.match` function allows you to quickly extract similar strings. however, after selecting the right models to be used, you may want to use polyfuzz \nin production to match incoming strings. to do so, we can make use of the familiar `fit`, `transform`, and `fit_transform` functions. \n\nlet's say that we have a list of words that we know to be correct called `train_words`. we want to any incoming word to mapped to one of the words in `train_words`. \nin other words, we `fit` on `train_words` and we use `transform` on any incoming words:\n\n```python\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import countvectorizer\nfrom polyfuzz import polyfuzz\n\ntrain_words = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"]\nunseen_words = [\"apple\", \"apples\", \"mouse\"]\n\n# fit\nmodel = polyfuzz(\"tf-idf\")\nmodel.fit(train_words)\n\n# transform\nresults = model.transform(unseen_words)\n```\n\nin the above example, we are using `fit` on `train_words` to calculate the tf-idf representation of those words which are saved to be used again in `transform`. \nthis speeds up `transform` quite a bit since all tf-idf representations are stored when applying `fit`. \n\nthen, we apply save and load the model as follows to be used in production:\n\n```python\n# save the model\nmodel.save(\"my_model\")\n\n# load the model\nloaded_model = polyfuzz.load(\"my_model\")\n```\n\n### group matches\nwe can group the matches `to` as there might be significant overlap in strings in our to_list. \nto do this, we calculate the similarity within strings in to_list and use `single linkage` to then \ngroup the strings with a high similarity.\n\nwhen we extract the new matches, we can see an additional column `group` in which all the `to` matches were grouped to:\n\n```python\n>>> model.group(link_min_similarity=0.75)\n>>> model.get_matches()\n\t      from\tto\t\tsimilarity\tgroup\n0\t     apple\tapple\t1.000000\tapples\n1\t    apples\tapples\t1.000000\tapples\n2\t      appl\tapple\t0.783751\tapples\n3\t     recal\tnone\t0.000000\tnone\n4\t     house\tmouse\t0.587927\tmouse\n5\tsimilarity\tnone\t0.000000\tnone\n```\n\nas can be seen above, we grouped apple and apples together to `apple` such that when a string is mapped to `apple` it \nwill fall in the cluster of `[apples, apple]` and will be mapped to the first instance in the cluster which is `apples`.\n\n### precision-recall curve  \nnext, we would like to see how well our model is doing on our data. we express our results as \n**`precision`** and **`recall`** where precision is defined as the minimum similarity score before a match is correct and \nrecall the percentage of matches found at a certain minimum similarity score.  \n\ncreating the visualizations is as simple as:\n\n```\nmodel.visualize_precision_recall()\n```\n<img src=\"images/tfidf.png\" width=\"100%\" height=\"100%\"/> \n\n## models\ncurrently, the following models are implemented in polyfuzz:\n* tf-idf\n* editdistance (you can use any distance measure, see [documentation](https://maartengr.github.io/polyfuzz/tutorial/models/#editdistance))\n* fasttext and glove\n* \u00f0\u00ff\u00a4\u2014 transformers\n\nwith `flair`, we can use all \u00f0\u00ff\u00a4\u2014 transformers [models](https://huggingface.co/transformers/pretrained_models.html). \nwe simply have to instantiate any flair wordembedding method and pass it through polyfuzzy.\n\nall models listed above can be found in `polyfuzz.models` and can be used to create and compare different models:\n\n```python\nfrom polyfuzz.models import editdistance, tfidf, embeddings\nfrom flair.embeddings import transformerwordembeddings\n\nembeddings = transformerwordembeddings('bert-base-multilingual-cased')\nbert = embeddings(embeddings, min_similarity=0, model_id=\"bert\")\ntfidf = tfidf(min_similarity=0)\nedit = editdistance()\n\nstring_models = [bert, tfidf, edit]\nmodel = polyfuzz(string_models)\nmodel.match(from_list, to_list)\n```\n\nto access the results, we again can call `get_matches` but since we have multiple models we get back a dictionary \nof dataframes back. \n\nin order to access the results of a specific model, call `get_matches` with the correct id: \n\n```python\n>>> model.get_matches(\"bert\")\n        from\t    to          similarity\n0\tapple\t    apple\t1.000000\n1\tapples\t    apples\t1.000000\n2\tappl\t    apple\t0.928045\n3\trecal\t    apples\t0.825268\n4\thouse\t    mouse\t0.887524\n5\tsimilarity  mouse\t0.791548\n``` \n\nfinally, visualize the results to compare the models:\n\n```python\nmodel.visualize_precision_recall(kde=true)\n```\n\n<img src=\"images/multiple_models.png\" width=\"100%\" height=\"100%\"/>\n\n## custom grouper\nwe can even use one of the `polyfuzz.models` to be used as the grouper in case you would like to use \nsomething else than the standard tf-idf model:\n\n```python\nmodel = polyfuzz(\"tf-idf\")\nmodel.match(from_list, to_list)\n\nedit_grouper = editdistance(n_jobs=1)\nmodel.group(edit_grouper)\n```\n\n## custom models\nalthough the options above are a great solution for comparing different models, what if you have developed your own? \nif you follow the structure of polyfuzz's `basematcher`  \nyou can quickly implement any model you would like.\n\nbelow, we are implementing the ratio similarity measure from rapidfuzz.\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom rapidfuzz import fuzz\nfrom polyfuzz.models import basematcher\n\n\nclass mymodel(basematcher):\n    def match(self, from_list, to_list, **kwargs):\n        # calculate distances\n        matches = [[fuzz.ratio(from_string, to_string) / 100 for to_string in to_list] \n                    for from_string in from_list]\n        \n        # get best matches\n        mappings = [to_list[index] for index in np.argmax(matches, axis=1)]\n        scores = np.max(matches, axis=1)\n        \n        # prepare dataframe\n        matches = pd.dataframe({'from': from_list,'to': mappings, 'similarity': scores})\n        return matches\n```\nthen, we can simply create an instance of mymodel and pass it through polyfuzz:\n```python\ncustom_model = mymodel()\nmodel = polyfuzz(custom_model)\n```\n\n## citation\nto cite polyfuzz in your work, please use the following bibtex reference:\n\n```bibtex\n@misc{grootendorst2020polyfuzz,\n  author       = {maarten grootendorst},\n  title        = {polyfuzz: fuzzy string matching, grouping, and evaluation.},\n  year         = 2020,\n  publisher    = {zenodo},\n  version      = {v0.2.2},\n  doi          = {10.5281/zenodo.4461050},\n  url          = {https://doi.org/10.5281/zenodo.4461050}\n}\n```\n\n## references\nbelow, you can find several resources that were used for or inspired by when developing polyfuzz:  \n  \n**edit distance algorithms**:  \nthese algorithms focus primarily on edit distance measures and can be used in `polyfuzz.models.editdistance`:\n\n* https://github.com/jamesturk/jellyfish\n* https://github.com/ztane/python-levenshtein\n* https://github.com/seatgeek/fuzzywuzzy\n* https://github.com/maxbachmann/rapidfuzz\n* https://github.com/roy-ht/editdistance\n\n**other interesting repos**:\n\n* https://github.com/ing-bank/sparse_dot_topn\n    * used in polyfuzz for fast cosine similarity between sparse matrices\n\n\n",
  "docs_url": null,
  "keywords": "nlp string matching embeddings levenshtein tfidf bert",
  "license": "",
  "name": "polyfuzz",
  "package_url": "https://pypi.org/project/polyfuzz/",
  "project_url": "https://pypi.org/project/polyfuzz/",
  "project_urls": {
    "Documentation": "https://maartengr.github.io/polyfuzz/",
    "Homepage": "https://github.com/MaartenGr/PolyFuzz",
    "Issue Tracker": "https://github.com/MaartenGr/PolyFuzz/issues",
    "Source Code": "https://github.com/MaartenGr/PolyFuzz/"
  },
  "release_url": "https://pypi.org/project/polyfuzz/0.4.2/",
  "requires_dist": [
    "numpy (>=1.20.0)",
    "scipy (>=1.3.1)",
    "pandas (>=0.25.3)",
    "tqdm (>=4.41.1)",
    "joblib (>=0.14.0)",
    "matplotlib (>=3.2.2)",
    "seaborn (>=0.11.0)",
    "rapidfuzz (>=0.13.1)",
    "scikit-learn (>=0.22.2.post1)",
    "mkdocs (==1.1) ; extra == 'dev'",
    "mkdocs-material (==4.6.3) ; extra == 'dev'",
    "mkdocstrings (==0.8.0) ; extra == 'dev'",
    "pytest (>=5.4.3) ; extra == 'dev'",
    "pytest-cov (>=2.6.1) ; extra == 'dev'",
    "torch (>=1.4.0) ; extra == 'dev'",
    "flair (>=0.7) ; extra == 'dev'",
    "sparse-dot-topn (>=0.2.9) ; extra == 'dev'",
    "sentence-transformers (>=0.4.1) ; extra == 'dev'",
    "spacy (>=3.0.1) ; extra == 'dev'",
    "tensorflow ; extra == 'dev'",
    "tensorflow-hub ; extra == 'dev'",
    "tensorflow-text ; extra == 'dev'",
    "mkdocs (==1.1) ; extra == 'docs'",
    "mkdocs-material (==4.6.3) ; extra == 'docs'",
    "mkdocstrings (==0.8.0) ; extra == 'docs'",
    "sparse-dot-topn (>=0.2.9) ; extra == 'fast'",
    "torch (>=1.4.0) ; extra == 'flair'",
    "flair (>=0.7) ; extra == 'flair'",
    "gensim (>=4.0.0) ; extra == 'gensim'",
    "sentence-transformers (>=0.4.1) ; extra == 'sbert'",
    "pytest (>=5.4.3) ; extra == 'test'",
    "pytest-cov (>=2.6.1) ; extra == 'test'",
    "tensorflow ; extra == 'use'",
    "tensorflow-hub ; extra == 'use'",
    "tensorflow-text ; extra == 'use'"
  ],
  "requires_python": ">=3.6",
  "summary": "polyfuzz performs fuzzy string matching, grouping, and evaluation.",
  "version": "0.4.2",
  "releases": [],
  "developers": [
    "maarten_grootendorst",
    "maartengrootendorst@gmail.com"
  ],
  "kwds": "bert embeddings embedding wordembedding transformerwordembeddings",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_polyfuzz",
  "homepage": "https://github.com/maartengr/polyfuzz",
  "release_count": 12,
  "dependency_ids": [
    "pypi_flair",
    "pypi_gensim",
    "pypi_joblib",
    "pypi_matplotlib",
    "pypi_mkdocs",
    "pypi_mkdocs_material",
    "pypi_mkdocstrings",
    "pypi_numpy",
    "pypi_pandas",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_rapidfuzz",
    "pypi_scikit_learn",
    "pypi_scipy",
    "pypi_seaborn",
    "pypi_sentence_transformers",
    "pypi_spacy",
    "pypi_sparse_dot_topn",
    "pypi_tensorflow",
    "pypi_tensorflow_hub",
    "pypi_tensorflow_text",
    "pypi_torch",
    "pypi_tqdm"
  ]
}