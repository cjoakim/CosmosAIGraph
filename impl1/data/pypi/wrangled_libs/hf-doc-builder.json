{
  "classifiers": [],
  "description": "# doc-builder\n\nthis is the package we use to build the documentation of our hugging face repos.\n\n## installation\n\nyou can install from pypi with\n\n```bash\npip install hf-doc-builder\n```\n\nto install from source, clone this repository then\n\n```bash\ncd doc-builder\npip install -e .\n```\n\n## previewing\n\nto preview the docs, use the following command:\n\n```bash\ndoc-builder preview {package_name} {path_to_docs}\n```\n\nfor example:\n```bash\ndoc-builder preview datasets ~/desktop/datasets/docs/source/\n```\n\n**`preview` command only works with existing doc files. when you add a completely new file, you need to update `_toctree.yml` & restart `preview` command (`ctrl-c` to stop it & call `doc-builder preview ...` again).\n## doc building\n\nto build the documentation of a given package, use the following command:\n\n```bash\ndoc-builder build {package_name} {path_to_docs} --build_dir {build_dir}\n```\n\nfor instance, here is how you can build the datasets documentation (requires `pip install datasets[dev]`) if you have cloned the repo in `~/git/datasets`:\n\n```bash\ndoc-builder datasets ~/git/datasets/docs/source --build_dir ~/tmp/test-build\n```\n\nthis will generate mdx files that you can preview like any markdown file in your favorite editor. to have a look at the documentation in html, you need to install node version 14 or higher. then you can run (still with the example on datasets)\n\n```bash\ndoc-builder datasets ~/git/datasets/docs/source --build_dir ~/tmp/test-build --html\n```\nwhich will build html files in `~/tmp/test-build`. you can then inspect those files in your browser.\n\n`doc-builder` can also automatically convert some of the documentation guides or tutorials into notebooks. this requires two steps:\n- add `[[open-in-colab]]` in the tutorial for which you want to build a notebook\n- add `--notebook_dir {path_to_notebook_folder}` to the build command.\n\n## templates for github actions\n\n`doc-builder` provides templates for github actions, so you can build your documentation with every pull request, push to some branch etc. to use them in your project, simply create the following three files in the `.github/workflows/` directory:\n\n* `build_main_documentation.yml`: responsible for building the docs for the `main` branch, releases etc.\n* `build_pr_documentation.yml`: responsible for building the docs on each pr\n* `delete_doc_comment.yml`: responsible for removing the comments from the `huggingfacedocbuilderdev` bot that provides a url to the pr docs.\n\nwithin each workflow, the main thing to include is a pointer from the `uses` field to the corresponding workflow in `doc-builder`. for example, this is what the pr workflow looks like in the `datasets` library:\n\n```yaml\nname: build pr documentation\n\non:\n  pull_request:\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\n  cancel-in-progress: true\n\njobs:\n  build:\n    uses: huggingface/doc-builder/.github/workflows/build_pr_documentation.yml@main # runs this doc-builder workflow\n    with:\n      commit_sha: ${{ github.event.pull_request.head.sha }}\n      pr_number: ${{ github.event.number }}\n      package: datasets # replace this with your package name\n```\n\nnote the use of special arguments like `pr_number` and `package` under the `with` field. you can find the various options by inspecting each of the `doc-builder` [workflow files](https://github.com/huggingface/doc-builder/tree/main/.github/workflows).\n\n### enabling multilingual documentation\n\n`doc-builder` can also convert documentation that's been translated from the english source into one or more languages. to enable the conversion, the documentation directories should be structured as follows:\n\n```\ndoc_folder\n\u251c\u2500\u2500 en\n\u2502   \u251c\u2500\u2500 _toctree.yml\n\u2502   ...\n\u2514\u2500\u2500 es\n    \u251c\u2500\u2500 _toctree.yml\n    ...\n```\n\nnote that each language directory has it's own table of contents file `_toctree.yml` and that all languages are arranged under a single `doc_folder` directory - see the [`course`](https://github.com/huggingface/course/tree/main/chapters) repo for an example. you can then build the individual language subsets as follows:\n\n```bash\ndoc-builder {package_name} {path_to_docs} --build_dir {build_dir} --language {lang_id}\n```\n\nto automatically build the documentation for all languages via the github actions templates, simply provide the `languages` argument to your workflow, with a space-separated list of the languages you wish to build, e.g. `languages: en es`.\n\n\n## writing documentation for hugging face libraries\n\n`doc-builder` expects markdown so you should write any new documentation in `\".mdx\"` files for tutorials, guides, api documentations. for docstrings, we follow the [google format](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html) with the main difference that you should use markdown instead of restructured text (hopefully, that will be easier!)\n\nvalues that should be put in `code` should either be surrounded by backticks: \\`like so\\`. note that argument names\nand objects like true, none or any strings should usually be put in `code`.\n\nwhen mentioning a class, function or method, it is recommended to use the following syntax for internal links so that our tool\nautomarically adds a link to its documentation: \\[\\`xxxclass\\`\\] or \\[\\`function\\`\\]. this requires the class or \nfunction to be in the main package.\n\nif you want to create a link to some internal class or function, you need to\nprovide its path. for instance, in the transformers documentation \\[\\`file_utils.modeloutput\\`\\] will create a link to the documnetation of `modeloutput`. this link will have `file_utils.modeloutput` in the description. to get rid of the path and only keep the name of the object you are\nlinking to in the description, add a ~: \\[\\`~file_utils.modeloutput\\`\\] will generate a link with `modeloutput` in the description.\n\nthe same works for methods so you can either use \\[\\`xxxclass.method\\`\\] or \\[~\\`xxxclass.method\\`\\].\n\nmulti-line code blocks can be useful for displaying examples. they are done between two lines of three backticks as usual in markdown:\n\n````\n```\n# first line of code\n# second line\n# etc\n```\n````\n\nwe follow the [doctest](https://docs.python.org/3/library/doctest.html) syntax for the examples to automatically test\nthe results stay consistent with the library.\n\nto write a block that you'd like to see highlighted as a note or warning, place your content between the following\nmarkers:\n\n```\n<tip>\n\nwrite your note here\n\n</tip>\n```\n\nfor warnings, change the introduction to `<tip warning={true}>`.\n\nif your documentation has a block that is framework-dependent (pytorch vs tensorflow vs flax), you can use the\nfollowing syntax:\n\n```\n<frameworkcontent>\n<pt>\npytorch content goes here\n</pt>\n<tf>\ntensorflow content goes here\n</tf>\n<flax>\nflax content goes here\n</flax>\n</frameworkcontent>\n```\n\nnote that all frameworks are optional (you can write a pytorch-only block for instance) and the order does not matter.\n\nanchor links for markdown headings are generated automatically (with the following rule: 1. lowercase, 2. replace space with dash `-`, 3. strip [^a-z0-9-]):\n```\n## my awesome section\n// the anchor link is: `my-awesome-section`\n```\nmoreover, there is a way to customize the anchor link. example:\n```\n## my awesome section[[some-section]]\n// the anchor link is: `some-section`\n```\n\n### writing api documentation\n\nto show the full documentation of any object of the library you are documenting, use the `[[autodoc]]` marker:\n\n```\n[[autodoc]] someobject\n```\n\nif the object is a class, this will include every public method of it that is documented. if for some reason you wish for a method\nnot to be displayed in the documentation, you can do so by specifying which methods should be in the docs, here is an example:\n\n```\n[[autodoc]] xxxtokenizer\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary\n```\n\nif you just want to add a method that is not documented (for instance magic method like `__call__` are not documented\nby default) you can put the list of methods to add in a list that contains `all`:\n\n```\n## xxxtokenizer\n\n[[autodoc]] xxxtokenizer\n    - all\n    - __call__\n```\n\nyou can create a code-block by referencing a file excerpt with `<literalinclude>` (sphinx-inspired) syntax. \nthere should be json between `<literalinclude>` open & close tags.\n```\n<literalinclude>\n{\"path\": \"./data/convert_literalinclude_dummy.txt\", # relative path\n\"language\": \"python\", # defaults to \" (empty str)\n\"start-after\": \"start python_import\",  # defaults to start of file\n\"end-before\": \"end python_import\",  # defaults to end of file\n\"dedent\": 7 # defaults to 0\n}\n</literalinclude>\n```\n\n### writing source documentation\n\narguments of a function/class/method should be defined with the `args:` (or `arguments:` or `parameters:`) prefix, followed by a line return and\nan indentation. the argument should be followed by its type, with its shape if it is a tensor, a colon and its\ndescription:\n\n```\n    args:\n        n_layers (`int`): the number of layers of the model.\n```\n\nif the description is too long to fit in one line, another indentation is necessary before writing the description\nafter th argument.\n\nhere's an example showcasing everything so far:\n\n```\n    args:\n        input_ids (`torch.longtensor` of shape `(batch_size, sequence_length)`):\n            indices of input sequence tokens in the vocabulary.\n\n            indices can be obtained using [`alberttokenizer`]. see [`~pretrainedtokenizer.encode`] and\n            [`~pretrainedtokenizer.__call__`] for details.\n\n            [what are input ids?](../glossary#input-ids)\n```\n\nyou can check the full example it comes from [here](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/bert/modeling_bert.py#l794-l841)\n\nif a class is similar to that of a dataclass but the parameters do not align to the available attributes of the class, such as in the below example, `attributes` instance should be rewritten as `**attributes**` in order to have the documentation properly render these. otherwise it will assume that `attributes` is synonymous to `parameters`.\n\n```diff\n  class someclass:\n      \"\"\"\n      docstring\n-     attributes:\n+     **attributes**:\n          - **attr_a** (`type_a`) -- doc a\n          - **attr_b** (`type_b`) -- doc b\n      \"\"\"\n      def __init__(self, param_a, param_b):\n          ...\n```\n\nfor optional arguments or arguments with defaults we follow the following syntax. imagine we have a function with the\nfollowing signature:\n\n```\ndef my_function(x: str = none, a: float = 1):\n```\n\nthen its documentation should look like this:\n\n```\n    args:\n        x (`str`, *optional*):\n            this argument controls ...\n        a (`float`, *optional*, defaults to 1):\n            this argument is used to ...\n```\n\nnote that we always omit the \"defaults to \\`none\\`\" when none is the default for any argument. also note that even\nif the first line describing your argument type and its default gets long, you can't break it on several lines. you can\nhowever write as many lines as you want in the indented description (see the example above with `input_ids`).\n\nif your argument has for type a class defined in the package, you can use the syntax we saw earlier to link to its\ndocumentation:\n\n```\n    args:\n         config ([`bertconfig`]):\n            model configuration class with all the parameters of the model.\n\n            initializing with a config file does not load the weights associated with the model, only the\n            configuration. check out the [`~pretrainedmodel.from_pretrained`] method to load the model weights.\n```\n\nthe return block should be introduced with the `returns:` prefix, followed by a line return and an indentation.\nthe first line should be the type of the return, followed by a line return. no need to indent further for the elements\nbuilding the return.\n\nhere's an example for a single value return:\n\n```\n    returns:\n        `list[int]`: a list of integers in the range [0, 1] --- 1 for a special token, 0 for a sequence token.\n```\n\nhere's an example for tuple return, comprising several objects:\n\n```\n    returns:\n        `tuple(torch.floattensor)` comprising various elements depending on the configuration ([`bertconfig`]) and inputs:\n        - ** loss** (*optional*, returned when `masked_lm_labels` is provided) `torch.floattensor` of shape `(1,)` --\n          total loss as the sum of the masked language modeling loss and the next sequence prediction (classification) loss.\n        - **prediction_scores** (`torch.floattensor` of shape `(batch_size, sequence_length, config.vocab_size)`) --\n          prediction scores of the language modeling head (scores for each vocabulary token before softmax).\n```\n\nthere are directives for `added`, `changed`, & `deprecated`.\nhere's an example:\n```\n    args:\n        cache_dir (`str`, *optional*): directory to cache data.\n        config_name (`str`, *optional*): name of the dataset configuration.\n            it affects the data generated on disk: different configurations will have their own subdirectories and\n            versions.\n            if not provided, the default configuration is used (if it exists).\n\n            <added version=\"2.3.0\">\n\n            `name` was renamed to `config_name`.\n\n            </added>\n        name (`str`): configuration name for the dataset.\n\n            <deprecated version=\"2.3.0\">\n\n            use `config_name` instead.\n\n            </deprecated>\n```\n\n",
  "docs_url": null,
  "keywords": "doc documentation doc-builder huggingface hugging face",
  "license": "",
  "name": "hf-doc-builder",
  "package_url": "https://pypi.org/project/hf-doc-builder/",
  "project_url": "https://pypi.org/project/hf-doc-builder/",
  "project_urls": {
    "Homepage": "https://github.com/huggingface/doc-builder"
  },
  "release_url": "https://pypi.org/project/hf-doc-builder/0.4.0/",
  "requires_dist": [
    "GitPython",
    "tqdm",
    "pyyaml",
    "packaging",
    "nbformat",
    "gql[requests]",
    "requests",
    "pytest ; extra == 'all'",
    "pytest-xdist ; extra == 'all'",
    "torch ; extra == 'all'",
    "transformers ; extra == 'all'",
    "tokenizers ; extra == 'all'",
    "black (~=22.0) ; extra == 'all'",
    "isort (>=5.5.4) ; extra == 'all'",
    "flake8 (>=3.8.3) ; extra == 'all'",
    "pytest ; extra == 'dev'",
    "pytest-xdist ; extra == 'dev'",
    "torch ; extra == 'dev'",
    "transformers ; extra == 'dev'",
    "tokenizers ; extra == 'dev'",
    "black (~=22.0) ; extra == 'dev'",
    "isort (>=5.5.4) ; extra == 'dev'",
    "flake8 (>=3.8.3) ; extra == 'dev'",
    "black (~=22.0) ; extra == 'quality'",
    "isort (>=5.5.4) ; extra == 'quality'",
    "flake8 (>=3.8.3) ; extra == 'quality'",
    "pytest ; extra == 'testing'",
    "pytest-xdist ; extra == 'testing'",
    "torch ; extra == 'testing'",
    "transformers ; extra == 'testing'",
    "tokenizers ; extra == 'testing'",
    "transformers[dev] ; extra == 'transformers'"
  ],
  "requires_python": "",
  "summary": "doc building utility",
  "version": "0.4.0",
  "releases": [],
  "developers": [
    "hugging_face",
    "sylvain@huggingface.co"
  ],
  "kwds": "build_main_documentation build_pr_documentation path_to_docs doc_folder sphinxcontrib",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_hf_doc_builder",
  "homepage": "https://github.com/huggingface/doc-builder",
  "release_count": 5,
  "dependency_ids": [
    "pypi_black",
    "pypi_flake8",
    "pypi_gitpython",
    "pypi_gql",
    "pypi_isort",
    "pypi_nbformat",
    "pypi_packaging",
    "pypi_pytest",
    "pypi_pytest_xdist",
    "pypi_pyyaml",
    "pypi_requests",
    "pypi_tokenizers",
    "pypi_torch",
    "pypi_tqdm",
    "pypi_transformers"
  ]
}