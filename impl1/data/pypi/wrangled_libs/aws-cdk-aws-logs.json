{
  "classifiers": [
    "development status :: 7 - inactive",
    "framework :: aws cdk",
    "framework :: aws cdk :: 1",
    "intended audience :: developers",
    "license :: osi approved",
    "operating system :: os independent",
    "programming language :: javascript",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "typing :: typed"
  ],
  "description": "# amazon cloudwatch logs construct library\n\n<!--begin stability banner-->---\n\n\n![end-of-support](https://img.shields.io/badge/end--of--support-critical.svg?style=for-the-badge)\n\n> aws cdk v1 has reached end-of-support on 2023-06-01.\n> this package is no longer being updated, and users should migrate to aws cdk v2.\n>\n> for more information on how to migrate, see the [*migrating to aws cdk v2* guide](https://docs.aws.amazon.com/cdk/v2/guide/migrating-v2.html).\n\n---\n<!--end stability banner-->\n\nthis library supplies constructs for working with cloudwatch logs.\n\n## log groups/streams\n\nthe basic unit of cloudwatch is a *log group*. every log group typically has the\nsame kind of data logged to it, in the same format. if there are multiple\napplications or services logging into the log group, each of them creates a new\n*log stream*.\n\nevery log operation creates a \"log event\", which can consist of a simple string\nor a single-line json object. json objects have the advantage that they afford\nmore filtering abilities (see below).\n\nthe only configurable attribute for log streams is the retention period, which\nconfigures after how much time the events in the log stream expire and are\ndeleted.\n\nthe default retention period if not supplied is 2 years, but it can be set to\none of the values in the `retentiondays` enum to configure a different\nretention period (including infinite retention).\n\n```python\n# configure log group for short retention\nlog_group = loggroup(stack, \"loggroup\",\n    retention=retentiondays.one_week\n)# configure log group for infinite retention\nlog_group = loggroup(stack, \"loggroup\",\n    retention=infinity\n)\n```\n\n## logretention\n\nthe `logretention` construct is a way to control the retention period of log groups that are created outside of the cdk. the construct is usually\nused on log groups that are auto created by aws services, such as [aws\nlambda](https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html).\n\nthis is implemented using a [cloudformation custom\nresource](https://docs.aws.amazon.com/awscloudformation/latest/userguide/aws-resource-cfn-customresource.html)\nwhich pre-creates the log group if it doesn't exist, and sets the specified log retention period (never expire, by default).\n\nby default, the log group will be created in the same region as the stack. the `loggroupregion` property can be used to configure\nlog groups in other regions. this is typically useful when controlling retention for log groups auto-created by global services that\npublish their log group to a specific region, such as aws chatbot creating a log group in `us-east-1`.\n\n## resource policy\n\ncloudwatch resource policies allow other aws services or iam principals to put log events into the log groups.\na resource policy is automatically created when `addtoresourcepolicy` is called on the loggroup for the first time:\n\n```python\nlog_group = logs.loggroup(self, \"loggroup\")\nlog_group.add_to_resource_policy(iam.policystatement(\n    actions=[\"logs:createlogstream\", \"logs:putlogevents\"],\n    principals=[iam.serviceprincipal(\"es.amazonaws.com\")],\n    resources=[log_group.log_group_arn]\n))\n```\n\nor more conveniently, write permissions to the log group can be granted as follows which gives same result as in the above example.\n\n```python\nlog_group = logs.loggroup(self, \"loggroup\")\nlog_group.grant_write(iam.serviceprincipal(\"es.amazonaws.com\"))\n```\n\nbe aware that any arns or tokenized values passed to the resource policy will be converted into aws account ids.\nthis is because cloudwatch logs resource policies do not accept arns as principals, but they do accept\naccount id strings. non-arn principals, like service principals or any princpals, are accepted by cloudwatch.\n\n## encrypting log groups\n\nby default, log group data is always encrypted in cloudwatch logs. you have the\noption to encrypt log group data using a aws kms customer master key (cmk) should\nyou not wish to use the default aws encryption. keep in mind that if you decide to\nencrypt a log group, any service or iam identity that needs to read the encrypted\nlog streams in the future will require the same cmk to decrypt the data.\n\nhere's a simple example of creating an encrypted log group using a kms cmk.\n\n```python\nimport aws_cdk.aws_kms as kms\n\n\nlogs.loggroup(self, \"loggroup\",\n    encryption_key=kms.key(self, \"key\")\n)\n```\n\nsee the aws documentation for more detailed information about [encrypting cloudwatch\nlogs](https://docs.aws.amazon.com/amazoncloudwatch/latest/logs/encrypt-log-data-kms.html).\n\n## subscriptions and destinations\n\nlog events matching a particular filter can be sent to either a lambda function\nor a kinesis stream.\n\nif the kinesis stream lives in a different account, a `crossaccountdestination`\nobject needs to be added in the destination account which will act as a proxy\nfor the remote kinesis stream. this object is automatically created for you\nif you use the cdk kinesis library.\n\ncreate a `subscriptionfilter`, initialize it with an appropriate `pattern` (see\nbelow) and supply the intended destination:\n\n```python\nimport aws_cdk.aws_logs_destinations as destinations\n# fn: lambda.function\n# log_group: logs.loggroup\n\n\nlogs.subscriptionfilter(self, \"subscription\",\n    log_group=log_group,\n    destination=destinations.lambdadestination(fn),\n    filter_pattern=logs.filterpattern.all_terms(\"error\", \"mainthread\")\n)\n```\n\n## metric filters\n\ncloudwatch logs can extract and emit metrics based on a textual log stream.\ndepending on your needs, this may be a more convenient way of generating metrics\nfor you application than making calls to cloudwatch metrics yourself.\n\na `metricfilter` either emits a fixed number every time it sees a log event\nmatching a particular pattern (see below), or extracts a number from the log\nevent and uses that as the metric value.\n\nexample:\n\n```python\nmetricfilter(self, \"metricfilter\",\n    log_group=log_group,\n    metric_namespace=\"myapp\",\n    metric_name=\"latency\",\n    filter_pattern=filterpattern.exists(\"$.latency\"),\n    metric_value=\"$.latency\"\n)\n```\n\nremember that if you want to use a value from the log event as the metric value,\nyou must mention it in your pattern somewhere.\n\na very simple metricfilter can be created by using the `loggroup.extractmetric()`\nhelper function:\n\n```python\n# log_group: logs.loggroup\n\nlog_group.extract_metric(\"$.jsonfield\", \"namespace\", \"metricname\")\n```\n\nwill extract the value of `jsonfield` wherever it occurs in json-structed\nlog records in the loggroup, and emit them to cloudwatch metrics under\nthe name `namespace/metricname`.\n\n### exposing metric on a metric filter\n\nyou can expose a metric on a metric filter by calling the `metricfilter.metric()` api.\nthis has a default of `statistic = 'avg'` if the statistic is not set in the `props`.\n\n```python\n# log_group: logs.loggroup\n\nmf = logs.metricfilter(self, \"metricfilter\",\n    log_group=log_group,\n    metric_namespace=\"myapp\",\n    metric_name=\"latency\",\n    filter_pattern=logs.filterpattern.exists(\"$.latency\"),\n    metric_value=\"$.latency\"\n)\n\n# expose a metric from the metric filter\nmetric = mf.metric()\n\n# you can use the metric to create a new alarm\ncloudwatch.alarm(self, \"alarm from metric filter\",\n    metric=metric,\n    threshold=100,\n    evaluation_periods=2\n)\n```\n\n## patterns\n\npatterns describe which log events match a subscription or metric filter. there\nare three types of patterns:\n\n* text patterns\n* json patterns\n* space-delimited table patterns\n\nall patterns are constructed by using static functions on the `filterpattern`\nclass.\n\nin addition to the patterns above, the following special patterns exist:\n\n* `filterpattern.allevents()`: matches all log events.\n* `filterpattern.literal(string)`: if you already know what pattern expression to\n  use, this function takes a string and will use that as the log pattern. for\n  more information, see the [filter and pattern\n  syntax](https://docs.aws.amazon.com/amazoncloudwatch/latest/logs/filterandpatternsyntax.html).\n\n### text patterns\n\ntext patterns match if the literal strings appear in the text form of the log\nline.\n\n* `filterpattern.allterms(term, term, ...)`: matches if all of the given terms\n  (substrings) appear in the log event.\n* `filterpattern.anyterm(term, term, ...)`: matches if all of the given terms\n  (substrings) appear in the log event.\n* `filterpattern.anytermgroup([term, term, ...], [term, term, ...], ...)`: matches if\n  all of the terms in any of the groups (specified as arrays) matches. this is\n  an or match.\n\nexamples:\n\n```python\n# search for lines that contain both \"error\" and \"mainthread\"\npattern1 = logs.filterpattern.all_terms(\"error\", \"mainthread\")\n\n# search for lines that either contain both \"error\" and \"mainthread\", or\n# both \"warn\" and \"deadlock\".\npattern2 = logs.filterpattern.any_term_group([\"error\", \"mainthread\"], [\"warn\", \"deadlock\"])\n```\n\n## json patterns\n\njson patterns apply if the log event is the json representation of an object\n(without any other characters, so it cannot include a prefix such as timestamp\nor log level). json patterns can make comparisons on the values inside the\nfields.\n\n* **strings**: the comparison operators allowed for strings are `=` and `!=`.\n  string values can start or end with a `*` wildcard.\n* **numbers**: the comparison operators allowed for numbers are `=`, `!=`,\n  `<`, `<=`, `>`, `>=`.\n\nfields in the json structure are identified by identifier the complete object as `$`\nand then descending into it, such as `$.field` or `$.list[0].field`.\n\n* `filterpattern.stringvalue(field, comparison, string)`: matches if the given\n  field compares as indicated with the given string value.\n* `filterpattern.numbervalue(field, comparison, number)`: matches if the given\n  field compares as indicated with the given numerical value.\n* `filterpattern.isnull(field)`: matches if the given field exists and has the\n  value `null`.\n* `filterpattern.notexists(field)`: matches if the given field is not in the json\n  structure.\n* `filterpattern.exists(field)`: matches if the given field is in the json\n  structure.\n* `filterpattern.booleanvalue(field, boolean)`: matches if the given field\n  is exactly the given boolean value.\n* `filterpattern.all(jsonpattern, jsonpattern, ...)`: matches if all of the\n  given json patterns match. this makes an and combination of the given\n  patterns.\n* `filterpattern.any(jsonpattern, jsonpattern, ...)`: matches if any of the\n  given json patterns match. this makes an or combination of the given\n  patterns.\n\nexample:\n\n```python\n# search for all events where the component field is equal to\n# \"httpserver\" and either error is true or the latency is higher\n# than 1000.\npattern = logs.filterpattern.all(\n    logs.filterpattern.string_value(\"$.component\", \"=\", \"httpserver\"),\n    logs.filterpattern.any(\n        logs.filterpattern.boolean_value(\"$.error\", true),\n        logs.filterpattern.number_value(\"$.latency\", \">\", 1000)))\n```\n\n## space-delimited table patterns\n\nif the log events are rows of a space-delimited table, this pattern can be used\nto identify the columns in that structure and add conditions on any of them. the\ncanonical example where you would apply this type of pattern is apache server\nlogs.\n\ntext that is surrounded by `\"...\"` quotes or `[...]` square brackets will\nbe treated as one column.\n\n* `filterpattern.spacedelimited(column, column, ...)`: construct a\n  `spacedelimitedtextpattern` object with the indicated columns. the columns\n  map one-by-one the columns found in the log event. the string `\"...\"` may\n  be used to specify an arbitrary number of unnamed columns anywhere in the\n  name list (but may only be specified once).\n\nafter constructing a `spacedelimitedtextpattern`, you can use the following\ntwo members to add restrictions:\n\n* `pattern.wherestring(field, comparison, string)`: add a string condition.\n  the rules are the same as for json patterns.\n* `pattern.wherenumber(field, comparison, number)`: add a numerical condition.\n  the rules are the same as for json patterns.\n\nmultiple restrictions can be added on the same column; they must all apply.\n\nexample:\n\n```python\n# search for all events where the component is \"httpserver\" and the\n# result code is not equal to 200.\npattern = logs.filterpattern.space_delimited(\"time\", \"component\", \"...\", \"result_code\", \"latency\").where_string(\"component\", \"=\", \"httpserver\").where_number(\"result_code\", \"!=\", 200)\n```\n\n## logs insights query definition\n\ncreates a query definition for cloudwatch logs insights.\n\nexample:\n\n```python\nlogs.querydefinition(self, \"querydefinition\",\n    query_definition_name=\"myquery\",\n    query_string=logs.querystring(\n        fields=[\"@timestamp\", \"@message\"],\n        sort=\"@timestamp desc\",\n        limit=20\n    )\n)\n```\n\n## notes\n\nbe aware that log group arns will always have the string `:*` appended to\nthem, to match the behavior of [the cloudformation `aws::logs::loggroup`\nresource](https://docs.aws.amazon.com/awscloudformation/latest/userguide/aws-resource-logs-loggroup.html#aws-resource-logs-loggroup-return-values).\n",
  "docs_url": null,
  "keywords": "",
  "license": "apache-2.0",
  "name": "aws-cdk.aws-logs",
  "package_url": "https://pypi.org/project/aws-cdk.aws-logs/",
  "project_url": "https://pypi.org/project/aws-cdk.aws-logs/",
  "project_urls": {
    "Homepage": "https://github.com/aws/aws-cdk",
    "Source": "https://github.com/aws/aws-cdk.git"
  },
  "release_url": "https://pypi.org/project/aws-cdk.aws-logs/1.204.0/",
  "requires_dist": [
    "aws-cdk.aws-cloudwatch (==1.204.0)",
    "aws-cdk.aws-iam (==1.204.0)",
    "aws-cdk.aws-kms (==1.204.0)",
    "aws-cdk.aws-s3-assets (==1.204.0)",
    "aws-cdk.core (==1.204.0)",
    "aws-cdk.cx-api (==1.204.0)",
    "constructs (<4.0.0,>=3.3.69)",
    "jsii (<2.0.0,>=1.84.0)",
    "publication (>=0.0.3)",
    "typeguard (~=2.13.3)"
  ],
  "requires_python": "~=3.7",
  "summary": "the cdk construct library for aws::logs",
  "version": "1.204.0",
  "releases": [],
  "developers": [
    "amazon_web_services"
  ],
  "kwds": "cloudwatchlogs cloudwatch aws_cdk aws_logs_destinations aws_kms",
  "license_kwds": "apache-2.0",
  "libtype": "pypi",
  "id": "pypi_aws_cdk.aws_logs",
  "homepage": "https://github.com/aws/aws-cdk",
  "release_count": 258,
  "dependency_ids": [
    "pypi_aws_cdk.aws_cloudwatch",
    "pypi_aws_cdk.aws_iam",
    "pypi_aws_cdk.aws_kms",
    "pypi_aws_cdk.aws_s3_assets",
    "pypi_aws_cdk.core",
    "pypi_aws_cdk.cx_api",
    "pypi_constructs",
    "pypi_jsii",
    "pypi_publication",
    "pypi_typeguard"
  ]
}