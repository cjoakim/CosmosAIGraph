{
  "classifiers": [
    "development status :: 3 - alpha",
    "environment :: console",
    "intended audience :: developers",
    "license :: osi approved :: apache software license",
    "natural language :: english",
    "operating system :: os independent",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/1402048/151947958-0bcadf38-3a82-4b4e-96b4-a38d3721d737.png\" align=\"right\" height=\"255px\" /></p>\n\n# \ud83d\udc5f trainer\nan opinionated general purpose model trainer on pytorch with a simple code base.\n\n## installation\n\nfrom github:\n\n```console\ngit clone https://github.com/coqui-ai/trainer\ncd trainer\nmake install\n```\n\nfrom pypi:\n\n```console\npip install trainer\n```\n\nprefer installing from github as it is more stable.\n\n## implementing a model\nsubclass and overload the functions in the [```trainermodel()```](trainer/model.py)\n\n\n## training a model with auto-optimization\nsee the [mnist example](examples/train_mnist.py).\n\n\n## training a model with advanced optimization\nwith \ud83d\udc5f you can define the whole optimization cycle as you want as the in gan example below. it enables more\nunder-the-hood control and flexibility for more advanced training loops.\n\nyou just have to use the ```scaled_backward()``` function to handle mixed precision training.\n\n```python\n...\n\ndef optimize(self, batch, trainer):\n    imgs, _ = batch\n\n    # sample noise\n    z = torch.randn(imgs.shape[0], 100)\n    z = z.type_as(imgs)\n\n    # train discriminator\n    imgs_gen = self.generator(z)\n    logits = self.discriminator(imgs_gen.detach())\n    fake = torch.zeros(imgs.size(0), 1)\n    fake = fake.type_as(imgs)\n    loss_fake = trainer.criterion(logits, fake)\n\n    valid = torch.ones(imgs.size(0), 1)\n    valid = valid.type_as(imgs)\n    logits = self.discriminator(imgs)\n    loss_real = trainer.criterion(logits, valid)\n    loss_disc = (loss_real + loss_fake) / 2\n\n    # step dicriminator\n    _, _ = self.scaled_backward(loss_disc, none, trainer, trainer.optimizer[0])\n\n    if trainer.total_steps_done % trainer.grad_accum_steps == 0:\n        trainer.optimizer[0].step()\n        trainer.optimizer[0].zero_grad()\n\n    # train generator\n    imgs_gen = self.generator(z)\n\n    valid = torch.ones(imgs.size(0), 1)\n    valid = valid.type_as(imgs)\n\n    logits = self.discriminator(imgs_gen)\n    loss_gen = trainer.criterion(logits, valid)\n\n    # step generator\n    _, _ = self.scaled_backward(loss_gen, none, trainer, trainer.optimizer[1])\n    if trainer.total_steps_done % trainer.grad_accum_steps == 0:\n        trainer.optimizer[1].step()\n        trainer.optimizer[1].zero_grad()\n    return {\"model_outputs\": logits}, {\"loss_gen\": loss_gen, \"loss_disc\": loss_disc}\n\n...\n```\n\nsee the [gan training example](examples/train_simple_gan.py) with gradient accumulation\n\n\n## training with batch size finder\nsee the test script [here](tests/test_train_batch_size_finder.py) for training with batch size finder.\n\n\nthe batch size finder starts at a default bs(defaults to 2048 but can also be user defined) and searches for the largest batch size that can fit on your hardware. you should expect for it to run multiple trainings until it finds it. to use it instead of calling ```trainer.fit()``` youll call ```trainer.fit_with_largest_batch_size(starting_batch_size=2048)``` with ```starting_batch_size``` being the batch the size you want to start the search with. very useful if you are wanting to use as much gpu mem as possible.\n\n## training with ddp\n\n```console\n$ python -m trainer.distribute --script path/to/your/train.py --gpus \"0,1\"\n```\n\nwe don't use ```.spawn()``` to initiate multi-gpu training since it causes certain limitations.\n\n- everything must the pickable.\n- ```.spawn()``` trains the model in subprocesses and the model in the main process is not updated.\n- dataloader with n processes gets really slow when the n is large.\n\n## training with [accelerate](https://huggingface.co/docs/accelerate/index)\n\nsetting `use_accelerate` in `trainingargs` to `true` will enable training with accelerate.\n\nyou can also use it for multi-gpu or distributed training.\n\n```console\ncuda_visible_devices=\"0,1,2\" accelerate launch --multi_gpu --num_processes 3 train_recipe_autoregressive_prompt.py\n```\n\nsee the [accelerate docs](https://huggingface.co/docs/accelerate/basic_tutorials/launch).\n\n## adding a callback\n\ud83d\udc5f supports callbacks to customize your runs. you can either set callbacks in your model implementations or give them\nexplicitly to the trainer.\n\nplease check `trainer.utils.callbacks` to see available callbacks.\n\nhere is how you provide an explicit call back to a \ud83d\udc5ftrainer object for weight reinitialization.\n\n```python\ndef my_callback(trainer):\n    print(\" > my callback was called.\")\n\ntrainer = trainer(..., callbacks={\"on_init_end\": my_callback})\ntrainer.fit()\n```\n\n## profiling example\n\n- create the torch profiler as you like and pass it to the trainer.\n    ```python\n    import torch\n    profiler = torch.profiler.profile(\n        activities=[\n            torch.profiler.profileractivity.cpu,\n            torch.profiler.profileractivity.cuda,\n        ],\n        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n        on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./profiler/\"),\n        record_shapes=true,\n        profile_memory=true,\n        with_stack=true,\n    )\n    prof = trainer.profile_fit(profiler, epochs=1, small_run=64)\n    then run tensorboard\n    ```\n- run the tensorboard.\n    ```console\n    tensorboard --logdir=\"./profiler/\"\n    ```\n\n## supported experiment loggers\n- [tensorboard](https://www.tensorflow.org/tensorboard) - actively maintained\n- [clearml](https://clear.ml/) - actively maintained\n- [mlflow](https://mlflow.org/)\n- [aim](https://aimstack.io/)\n- [wanddb](https://wandb.ai/)\n\nto add a new logger, you must subclass [basedashboardlogger](trainer/logging/base_dash_logger.py) and overload its functions.\n\n## anonymized telemetry\nwe constantly seek to improve \ud83d\udc38 for the community. to understand the community's needs better and address them accordingly, we collect stripped-down anonymized usage stats when you run the trainer.\n\nof course, if you don't want, you can opt out by setting the environment variable `trainer_telemetry=0`.\n",
  "docs_url": null,
  "keywords": "",
  "license": "apache2",
  "name": "trainer",
  "package_url": "https://pypi.org/project/trainer/",
  "project_url": "https://pypi.org/project/trainer/",
  "project_urls": {
    "Discussions": "https://github.com/coqui-ai/Trainer/discussions",
    "Documentation": "https://github.com/coqui-ai/Trainer/",
    "Homepage": "https://github.com/coqui-ai/Trainer",
    "Repository": "https://github.com/coqui-ai/Trainer",
    "Tracker": "https://github.com/coqui-ai/Trainer/issues"
  },
  "release_url": "https://pypi.org/project/trainer/0.0.36/",
  "requires_dist": [
    "torch >=1.7",
    "coqpit",
    "psutil",
    "fsspec",
    "tensorboard",
    "soundfile",
    "torch >=1.7 ; extra == 'all'",
    "coqpit ; extra == 'all'",
    "psutil ; extra == 'all'",
    "fsspec ; extra == 'all'",
    "tensorboard ; extra == 'all'",
    "soundfile ; extra == 'all'",
    "black ; extra == 'all'",
    "coverage ; extra == 'all'",
    "isort ; extra == 'all'",
    "pytest ; extra == 'all'",
    "pylint ; extra == 'all'",
    "accelerate ; extra == 'all'",
    "torchvision ; extra == 'all'",
    "black ; extra == 'dev'",
    "coverage ; extra == 'dev'",
    "isort ; extra == 'dev'",
    "pytest ; extra == 'dev'",
    "pylint ; extra == 'dev'",
    "accelerate ; extra == 'dev'",
    "torchvision ; extra == 'test'"
  ],
  "requires_python": ">=3.6.0, <3.12",
  "summary": "general purpose model trainer for pytorch that is more flexible than it should be, by \ud83d\udc38coqui.",
  "version": "0.0.36",
  "releases": [],
  "developers": [
    "egolge@coqui.ai"
  ],
  "kwds": "trainer trainermodel trainer_telemetry train_mnist trainingargs",
  "license_kwds": "apache2",
  "libtype": "pypi",
  "id": "pypi_trainer",
  "homepage": "https://github.com/coqui-ai/trainer",
  "release_count": 32,
  "dependency_ids": [
    "pypi_accelerate",
    "pypi_black",
    "pypi_coqpit",
    "pypi_coverage",
    "pypi_fsspec",
    "pypi_isort",
    "pypi_psutil",
    "pypi_pylint",
    "pypi_pytest",
    "pypi_soundfile",
    "pypi_tensorboard",
    "pypi_torch",
    "pypi_torchvision"
  ]
}