{
  "classifiers": [
    "development status :: 3 - alpha",
    "intended audience :: developers",
    "intended audience :: education",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "operating system :: os independent",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "[![examples](https://github.com/huggingface/optimum-graphcore/actions/workflows/test-examples.yml/badge.svg)](https://github.com/huggingface/optimum-graphcore/actions/workflows/test-examples.yml) [![pipelines](https://github.com/huggingface/optimum-graphcore/actions/workflows/test-pipelines.yml/badge.svg)](https://github.com/huggingface/optimum-graphcore/actions/workflows/test-pipelines.yml)\n\n<p align=\"center\">\n    <img src=\"readme_logo.png\" />\n</p>\n\n# optimum graphcore\n\n\ud83e\udd17 optimum graphcore is the interface between the \ud83e\udd17 transformers library and [graphcore ipus](https://www.graphcore.ai/products/ipu).\nit provides a set of tools enabling model parallelization and loading on ipus, training, fine-tuning and inference on all the tasks already supported by \ud83e\udd17 transformers while being compatible with the \ud83e\udd17 hub and every model available on it out of the box.\n\n## what is an intelligence processing unit (ipu)?\nquote from the hugging face [blog post](https://huggingface.co/blog/graphcore#what-is-an-intelligence-processing-unit):\n>ipus are the processors that power graphcore\u2019s ipu-pod datacenter compute systems. this new type of processor is designed to support the very specific computational requirements of ai and machine learning. characteristics such as fine-grained parallelism, low precision arithmetic, and the ability to handle sparsity have been built into our silicon.\n\n> instead of adopting a simd/simt architecture like gpus, graphcore\u2019s ipu uses a massively parallel, mimd architecture, with ultra-high bandwidth memory placed adjacent to the processor cores, right on the silicon die.\n\n> this design delivers high performance and new levels of efficiency, whether running today\u2019s most popular models, such as bert and efficientnet, or exploring next-generation ai applications.\n\n## poplar sdk setup\na poplar sdk environment needs to be enabled to use this library. please refer to graphcore's [getting started](https://docs.graphcore.ai/en/latest/getting-started.html) guides.\n\n## install\nto install the latest release of this package:\n\n`pip install optimum-graphcore`\n\noptimum graphcore is a fast-moving project, and you may want to install from source.\n\n`pip install git+https://github.com/huggingface/optimum-graphcore.git`\n\n### installing in developer mode\n\nif you are working on the `optimum-graphcore` code then you should use an editable install\nby cloning and installing `optimum` and `optimum-graphcore`:\n\n```\ngit clone https://github.com/huggingface/optimum --branch v1.6.1-release\ngit clone https://github.com/huggingface/optimum-graphcore\npip install -e optimum -e optimum-graphcore\n```\n\nnow whenever you change the code, you'll be able to run with those changes instantly.\n\n\n## running the examples\n\nthere are a number of examples provided in the `examples` directory. each of these contains a readme with command lines for running them on ipus with optimum graphcore.\n\nplease install the requirements for every example:\n\n```\ncd <example-folder>\npip install -r requirements.txt\n```\n\n## how to use optimum graphcore\n\ud83e\udd17 optimum graphcore was designed with one goal in mind: **make training and evaluation straightforward for any \ud83e\udd17 transformers user while leveraging the complete power of ipus.**\nit requires minimal changes if you are already using \ud83e\udd17 transformers.\n\nto immediately use a model on a given input (text, image, audio, ...), we support the `pipeline` api:\n\n```diff\n->>> from transformers import pipeline\n+>>> from optimum.graphcore import pipeline\n\n# allocate a pipeline for sentiment-analysis\n->>> classifier = pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n+>>> classifier = pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\", ipu_config = \"graphcore/distilbert-base-ipu\")\n>>> classifier('we are very happy to introduce pipeline to the transformers repository.')\n[{'label': 'positive', 'score': 0.9996947050094604}]\n```\n\nit is also super easy to use the `trainer` api:\n\n```diff\n-from transformers import trainer, trainingarguments\n+from optimum.graphcore import ipuconfig, iputrainer, iputrainingarguments\n\n-training_args = trainingarguments(\n+training_args = iputrainingarguments(\n     per_device_train_batch_size=4,\n     learning_rate=1e-4,\n+    # any ipuconfig on the hub or stored locally\n+    ipu_config_name=\"graphcore/bert-base-ipu\",\n+)\n+\n+# loading the ipuconfig needed by the iputrainer to compile and train the model on ipus\n+ipu_config = ipuconfig.from_pretrained(\n+    training_args.ipu_config_name,\n )\n\n # initialize our trainer\n-trainer = trainer(\n+trainer = iputrainer(\n     model=model,\n+    ipu_config=ipu_config,\n     args=training_args,\n     train_dataset=train_dataset if training_args.do_train else none,\n     ...  # other arguments\n```\n\nfor more information, refer to the full [\ud83e\udd17 optimum graphcore documentation](https://huggingface.co/docs/optimum/graphcore_index).\n\n## supported models\nthe following model architectures and tasks are currently supported by \ud83e\udd17 optimum graphcore:\n|            | pre-training | masked lm | causal lm | seq2seq lm (summarization, translation, etc) | sequence classification | token classification | question answering | multiple choice | image classification | ctc |\n|------------|--------------|-----------|-----------|----------------------------------------------|-------------------------|----------------------|--------------------|-----------------|----------------------| ------------ |\n| bart       | \u2705            |           | \u274c         | \u2705                                            | \u2705                       |                      | \u274c                  |                 |                      |             |\n| bert       | \u2705            | \u2705         | \u274c         |                                              | \u2705                       | \u2705                    | \u2705                  | \u2705               |                      |             |\n| convnext   | \u2705            |           |           |                                              |                         |                      |                    |                 | \u2705                    |             |\n| deberta    | \u2705            | \u2705         |           |                                              | \u2705                       | \u2705                    | \u2705                  |                 |                      |             |\n| distilbert | \u274c            | \u2705         |           |                                              | \u2705                       | \u2705                    | \u2705                  | \u2705               |                      |             |\n| gpt-2      | \u2705            |           | \u2705         |                                              | \u2705                       | \u2705                    |                    |                 |                      |             |\n| [groupbert](https://arxiv.org/abs/2106.05822)   | \u2705            | \u2705         | \u274c         |                                              | \u2705                       | \u2705                    | \u2705                  | \u2705               |                      |             |\n| hubert     | \u274c            |           |           |                                              | \u2705                       |                      |                    |                 |                      |       \u2705      |\n| lxmert     | \u274c            |           |           |                                              |                         |                      | \u2705                  |                 |                      |             |\n| roberta    | \u2705            | \u2705         | \u274c         |                                              | \u2705                       | \u2705                    | \u2705                  | \u2705               |                      |             |\n| t5         | \u2705            |           |           | \u2705                                            |                         |                      |                    |                 |                      |             |\n| vit        | \u274c            |           |           |                                              |                         |                      |                    |                 | \u2705                    |             |\n| wav2vec2   | \u2705            |           |           |                                              |                         |                      |                    |                 |                      |      \u2705        |\n| whisper   |    \u274c          |           |           |                    \u2705                           |                          |                      |                    |                 |                      |              |\n\n\nif you find any issue while using those, please open an issue or a pull request.\n",
  "docs_url": null,
  "keywords": "transformers,quantization,pruning,training,ipu",
  "license": "apache",
  "name": "optimum-graphcore",
  "package_url": "https://pypi.org/project/optimum-graphcore/",
  "project_url": "https://pypi.org/project/optimum-graphcore/",
  "project_urls": {
    "Homepage": "https://huggingface.co/hardware"
  },
  "release_url": "https://pypi.org/project/optimum-graphcore/0.7.1/",
  "requires_dist": [
    "transformers (==4.29.2)",
    "optimum (==1.6.1)",
    "diffusers[torch] (==0.12.1)",
    "cppimport (==22.8.2)",
    "datasets",
    "tokenizers",
    "typeguard",
    "sentencepiece",
    "scipy",
    "pillow",
    "black (~=23.1) ; extra == 'quality'",
    "isort (>=5.5.4) ; extra == 'quality'",
    "ruff (<=0.0.259,>=0.0.241) ; extra == 'quality'",
    "filelock ; extra == 'testing'",
    "GitPython ; extra == 'testing'",
    "parameterized ; extra == 'testing'",
    "psutil ; extra == 'testing'",
    "pytest ; extra == 'testing'",
    "pytest-pythonpath ; extra == 'testing'",
    "pytest-xdist ; extra == 'testing'",
    "librosa ; extra == 'testing'",
    "soundfile ; extra == 'testing'"
  ],
  "requires_python": "",
  "summary": "optimum library is an extension of the hugging face transformers library, providing a framework to integrate third-party libraries from hardware partners and interface with their specific functionality.",
  "version": "0.7.1",
  "releases": [],
  "developers": [
    "hardware@huggingface.co",
    "huggingface_inc"
  ],
  "kwds": "graphcore pipeline parallelization pipelines efficientnet",
  "license_kwds": "apache",
  "libtype": "pypi",
  "id": "pypi_optimum_graphcore",
  "homepage": "https://huggingface.co/hardware",
  "release_count": 18,
  "dependency_ids": [
    "pypi_black",
    "pypi_cppimport",
    "pypi_datasets",
    "pypi_diffusers",
    "pypi_filelock",
    "pypi_gitpython",
    "pypi_isort",
    "pypi_librosa",
    "pypi_optimum",
    "pypi_parameterized",
    "pypi_pillow",
    "pypi_psutil",
    "pypi_pytest",
    "pypi_pytest_pythonpath",
    "pypi_pytest_xdist",
    "pypi_ruff",
    "pypi_scipy",
    "pypi_sentencepiece",
    "pypi_soundfile",
    "pypi_tokenizers",
    "pypi_transformers",
    "pypi_typeguard"
  ]
}