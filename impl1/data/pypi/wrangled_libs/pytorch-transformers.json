{
  "classifiers": [
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "programming language :: python :: 3",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "# \ud83d\udc7e pytorch-transformers\n\n[![circleci](https://circleci.com/gh/huggingface/pytorch-transformers.svg?style=svg)](https://circleci.com/gh/huggingface/pytorch-transformers)\n\npytorch-transformers (formerly known as `pytorch-pretrained-bert`) is a library of state-of-the-art pre-trained models for natural language processing (nlp).\n\nthe library currently contains pytorch implementations, pre-trained model weights, usage scripts and conversion utilities for the following models:\n\n1. **[bert](https://github.com/google-research/bert)** (from google) released with the paper [bert: pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805) by jacob devlin, ming-wei chang, kenton lee and kristina toutanova.\n2. **[gpt](https://github.com/openai/finetune-transformer-lm)** (from openai) released with the paper [improving language understanding by generative pre-training](https://blog.openai.com/language-unsupervised/) by alec radford, karthik narasimhan, tim salimans and ilya sutskever.\n3. **[gpt-2](https://blog.openai.com/better-language-models/)** (from openai) released with the paper [language models are unsupervised multitask learners](https://blog.openai.com/better-language-models/) by alec radford*, jeffrey wu*, rewon child, david luan, dario amodei** and ilya sutskever**.\n4. **[transformer-xl](https://github.com/kimiyoung/transformer-xl)** (from google/cmu) released with the paper [transformer-xl: attentive language models beyond a fixed-length context](https://arxiv.org/abs/1901.02860) by zihang dai*, zhilin yang*, yiming yang, jaime carbonell, quoc v. le, ruslan salakhutdinov.\n5. **[xlnet](https://github.com/zihangdai/xlnet/)** (from google/cmu) released with the paper [\u200bxlnet: generalized autoregressive pretraining for language understanding](https://arxiv.org/abs/1906.08237) by zhilin yang*, zihang dai*, yiming yang, jaime carbonell, ruslan salakhutdinov, quoc v. le.\n6. **[xlm](https://github.com/facebookresearch/xlm/)** (from facebook) released together with the paper [cross-lingual language model pretraining](https://arxiv.org/abs/1901.07291) by guillaume lample and alexis conneau.\n7. **[roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta)** (from facebook), released together with the paper a [robustly optimized bert pretraining approach](https://arxiv.org/abs/1907.11692) by yinhan liu, myle ott, naman goyal, jingfei du, mandar joshi, danqi chen, omer levy, mike lewis, luke zettlemoyer, veselin stoyanov.\n8. **[distilbert](https://github.com/huggingface/pytorch-transformers/tree/master/examples/distillation)** (from huggingface), released together with the blogpost [smaller, faster, cheaper, lighter: introducing distilbert, a distilled version of\u00a0bert](https://medium.com/huggingface/distilbert-8cf3380435b5\n) by victor sanh, lysandre debut and thomas wolf.\n\nthese implementations have been tested on several datasets (see the example scripts) and should match the performances of the original implementations (e.g. ~93 f1 on squad for bert whole-word-masking, ~88 f1 on rocstories for openai gpt, ~18.3 perplexity on wikitext 103 for transformer-xl, ~0.916 peason r coefficient on sts-b for xlnet). you can find more details on the performances in the examples section of the [documentation](https://huggingface.co/pytorch-transformers/examples.html).\n\n| section | description |\n|-|-|\n| [installation](#installation) | how to install the package |\n| [quick tour: usage](#quick-tour) | tokenizers & models usage: bert and gpt-2 |\n| [quick tour: fine-tuning/usage scripts](#quick-tour-of-the-fine-tuningusage-scripts) | using provided scripts: glue, squad and text generation |\n| [migrating from pytorch-pretrained-bert to pytorch-transformers](#migrating-from-pytorch-pretrained-bert-to-pytorch-transformers) | migrating your code from pytorch-pretrained-bert to pytorch-transformers |\n| [documentation](https://huggingface.co/pytorch-transformers/) | full api documentation and more |\n\n## installation\n\nthis repo is tested on python 2.7 and 3.5+ (examples are tested only on python 3.5+) and pytorch 1.0.0+\n\n### with pip\n\npytorch-transformers can be installed by pip as follows:\n\n```bash\npip install pytorch-transformers\n```\n\n### from source\n\nclone the repository and run:\n\n```bash\npip install [--editable] .\n```\n\n### tests\n\na series of tests is included for the library and the example scripts. library tests can be found in the [tests folder](https://github.com/huggingface/pytorch-transformers/tree/master/pytorch_transformers/tests) and examples tests in the [examples folder](https://github.com/huggingface/pytorch-transformers/tree/master/examples).\n\nthese tests can be run using `pytest` (install pytest if needed with `pip install pytest`).\n\nyou can run the tests from the root of the cloned repository with the commands:\n\n```bash\npython -m pytest -sv ./pytorch_transformers/tests/\npython -m pytest -sv ./examples/\n```\n\n### do you want to run a transformer model on a mobile device?\n\nyou should check out our [`swift-coreml-transformers`](https://github.com/huggingface/swift-coreml-transformers) repo.\n\nit contains an example of a conversion script from a pytorch trained transformer model (here, `gpt-2`) to a coreml model that runs on ios devices.\n\nat some point in the future, you'll be able to seamlessly move from pre-training or fine-tuning models in pytorch to productizing them in coreml,\nor prototype a model or an app in coreml then research its hyperparameters or architecture from pytorch. super exciting!\n\n\n## quick tour\n\nlet's do a very quick overview of pytorch-transformers. detailed examples for each model architecture (bert, gpt, gpt-2, transformer-xl, xlnet and xlm) can be found in the [full documentation](https://huggingface.co/pytorch-transformers/).\n\n```python\nimport torch\nfrom pytorch_transformers import *\n\n# pytorch-transformers has a unified api\n# for 7 transformer architectures and 30 pretrained weights.\n#          model          | tokenizer          | pretrained weights shortcut\nmodels = [(bertmodel,       berttokenizer,      'bert-base-uncased'),\n          (openaigptmodel,  openaigpttokenizer, 'openai-gpt'),\n          (gpt2model,       gpt2tokenizer,      'gpt2'),\n          (transfoxlmodel,  transfoxltokenizer, 'transfo-xl-wt103'),\n          (xlnetmodel,      xlnettokenizer,     'xlnet-base-cased'),\n          (xlmmodel,        xlmtokenizer,       'xlm-mlm-enfr-1024'),\n          (robertamodel,    robertatokenizer,   'roberta-base')]\n\n# let's encode some text in a sequence of hidden-states using each model:\nfor model_class, tokenizer_class, pretrained_weights in models:\n    # load pretrained model/tokenizer\n    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n    model = model_class.from_pretrained(pretrained_weights)\n\n    # encode text\n    input_ids = torch.tensor([tokenizer.encode(\"here is some text to encode\", add_special_tokens=true)])  # add special tokens takes care of adding [cls], [sep], <s>... tokens in the right way for each model.\n    with torch.no_grad():\n        last_hidden_states = model(input_ids)[0]  # models outputs are now tuples\n\n# each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.\nbert_model_classes = [bertmodel, bertforpretraining, bertformaskedlm, bertfornextsentenceprediction,\n                      bertforsequenceclassification, bertformultiplechoice, bertfortokenclassification,\n                      bertforquestionanswering]\n\n# all the classes for an architecture can be initiated from pretrained weights for this architecture\n# note that additional weights added for fine-tuning are only initialized\n# and need to be trained on the down-stream task\ntokenizer = berttokenizer.from_pretrained('bert-base-uncased')\nfor model_class in bert_model_classes:\n    # load pretrained model/tokenizer\n    model = model_class.from_pretrained('bert-base-uncased')\n\n# models can return full list of hidden-states & attentions weights at each layer\nmodel = model_class.from_pretrained(pretrained_weights,\n                                    output_hidden_states=true,\n                                    output_attentions=true)\ninput_ids = torch.tensor([tokenizer.encode(\"let's see all hidden-states and attentions on this text\")])\nall_hidden_states, all_attentions = model(input_ids)[-2:]\n\n# models are compatible with torchscript\nmodel = model_class.from_pretrained(pretrained_weights, torchscript=true)\ntraced_model = torch.jit.trace(model, (input_ids,))\n\n# simple serialization for models and tokenizers\nmodel.save_pretrained('./directory/to/save/')  # save\nmodel = model_class.from_pretrained('./directory/to/save/')  # re-load\ntokenizer.save_pretrained('./directory/to/save/')  # save\ntokenizer = tokenizer_class.from_pretrained('./directory/to/save/')  # re-load\n\n# sota examples for glue, squad, text generation...\n```\n\n## quick tour of the fine-tuning/usage scripts\n\nthe library comprises several example scripts with sota performances for nlu and nlg tasks:\n\n- `run_glue.py`: an example fine-tuning bert, xlnet and xlm on nine different glue tasks (*sequence-level classification*)\n- `run_squad.py`: an example fine-tuning bert, xlnet and xlm on the question answering dataset squad 2.0 (*token-level classification*)\n- `run_generation.py`: an example using gpt, gpt-2, transformer-xl and xlnet for conditional language generation\n- other model-specific examples (see the documentation).\n\nhere are three quick usage examples for these scripts:\n\n### `run_glue.py`: fine-tuning on glue tasks for sequence classification\n\nthe [general language understanding evaluation (glue) benchmark](https://gluebenchmark.com/) is a collection of nine sentence- or sentence-pair language understanding tasks for evaluating and analyzing natural language understanding systems.\n\nbefore running anyone of these glue tasks you should download the\n[glue data](https://gluebenchmark.com/tasks) by running\n[this script](https://gist.github.com/w4ngatang/60c2bdb54d156a41194446737ce03e2e)\nand unpack it to some directory `$glue_dir`.\n\nyou should also install the additional packages required by the examples:\n\n```shell\npip install -r ./examples/requirements.txt\n```\n\n```shell\nexport glue_dir=/path/to/glue\nexport task_name=mrpc\n\npython ./examples/run_glue.py \\\n    --model_type bert \\\n    --model_name_or_path bert-base-uncased \\\n    --task_name $task_name \\\n    --do_train \\\n    --do_eval \\\n    --do_lower_case \\\n    --data_dir $glue_dir/$task_name \\\n    --max_seq_length 128 \\\n    --per_gpu_eval_batch_size=8   \\\n    --per_gpu_train_batch_size=8   \\\n    --learning_rate 2e-5 \\\n    --num_train_epochs 3.0 \\\n    --output_dir /tmp/$task_name/\n```\n\nwhere task name can be one of cola, sst-2, mrpc, sts-b, qqp, mnli, qnli, rte, wnli.\n\nthe dev set results will be present within the text file 'eval_results.txt' in the specified output_dir. in case of mnli, since there are two separate dev sets, matched and mismatched, there will be a separate output folder called '/tmp/mnli-mm/' in addition to '/tmp/mnli/'.\n\n#### fine-tuning xlnet model on the sts-b regression task\n\nthis example code fine-tunes xlnet on the sts-b corpus using parallel training on a server with 4 v100 gpus.\nparallel training is a simple way to use several gpus (but is slower and less flexible than distributed training, see below).\n\n```shell\nexport glue_dir=/path/to/glue\n\npython ./examples/run_glue.py \\\n    --model_type xlnet \\\n    --model_name_or_path xlnet-large-cased \\\n    --do_train  \\\n    --do_eval   \\\n    --task_name=sts-b     \\\n    --data_dir=${glue_dir}/sts-b  \\\n    --output_dir=./proc_data/sts-b-110   \\\n    --max_seq_length=128   \\\n    --per_gpu_eval_batch_size=8   \\\n    --per_gpu_train_batch_size=8   \\\n    --gradient_accumulation_steps=1 \\\n    --max_steps=1200  \\\n    --model_name=xlnet-large-cased   \\\n    --overwrite_output_dir   \\\n    --overwrite_cache \\\n    --warmup_steps=120\n```\n\non this machine we thus have a batch size of 32, please increase `gradient_accumulation_steps` to reach the same batch size if you have a smaller machine. these hyper-parameters should result in a pearson correlation coefficient of `+0.917` on the development set.\n\n#### fine-tuning bert model on the mrpc classification task\n\nthis example code fine-tunes the bert whole word masking model on the microsoft research paraphrase corpus (mrpc) corpus using distributed training on 8 v100 gpus to reach a f1 > 92.\n\n```bash\npython -m torch.distributed.launch --nproc_per_node 8 ./examples/run_glue.py   \\\n    --model_type bert \\\n    --model_name_or_path bert-large-uncased-whole-word-masking \\\n    --task_name mrpc \\\n    --do_train   \\\n    --do_eval   \\\n    --do_lower_case   \\\n    --data_dir $glue_dir/mrpc/   \\\n    --max_seq_length 128   \\\n    --per_gpu_eval_batch_size=8   \\\n    --per_gpu_train_batch_size=8   \\\n    --learning_rate 2e-5   \\\n    --num_train_epochs 3.0  \\\n    --output_dir /tmp/mrpc_output/ \\\n    --overwrite_output_dir   \\\n    --overwrite_cache \\\n```\n\ntraining with these hyper-parameters gave us the following results:\n\n```bash\n  acc = 0.8823529411764706\n  acc_and_f1 = 0.901702786377709\n  eval_loss = 0.3418912578906332\n  f1 = 0.9210526315789473\n  global_step = 174\n  loss = 0.07231863956341798\n```\n\n### `run_squad.py`: fine-tuning on squad for question-answering\n\nthis example code fine-tunes bert on the squad dataset using distributed training on 8 v100 gpus and bert whole word masking uncased model to reach a f1 > 93 on squad:\n\n```bash\npython -m torch.distributed.launch --nproc_per_node=8 ./examples/run_squad.py \\\n    --model_type bert \\\n    --model_name_or_path bert-large-uncased-whole-word-masking \\\n    --do_train \\\n    --do_eval \\\n    --do_lower_case \\\n    --train_file $squad_dir/train-v1.1.json \\\n    --predict_file $squad_dir/dev-v1.1.json \\\n    --learning_rate 3e-5 \\\n    --num_train_epochs 2 \\\n    --max_seq_length 384 \\\n    --doc_stride 128 \\\n    --output_dir ../models/wwm_uncased_finetuned_squad/ \\\n    --per_gpu_eval_batch_size=3   \\\n    --per_gpu_train_batch_size=3   \\\n```\n\ntraining with these hyper-parameters gave us the following results:\n\n```bash\npython $squad_dir/evaluate-v1.1.py $squad_dir/dev-v1.1.json ../models/wwm_uncased_finetuned_squad/predictions.json\n{\"exact_match\": 86.91579943235573, \"f1\": 93.1532499015869}\n```\n\nthis is the model provided as `bert-large-uncased-whole-word-masking-finetuned-squad`.\n\n### `run_generation.py`: text generation with gpt, gpt-2, transformer-xl and xlnet\n\na conditional generation script is also included to generate text from a prompt.\nthe generation script includes the [tricks](https://github.com/rusiaaman/xlnet-gen#methodology) proposed by by aman rusia to get high quality generation with memory models like transformer-xl and xlnet (include a predefined text to make short inputs longer).\n\nhere is how to run the script with the small version of openai gpt-2 model:\n\n```shell\npython ./examples/run_generation.py \\\n    --model_type=gpt2 \\\n    --length=20 \\\n    --model_name_or_path=gpt2 \\\n```\n\n## migrating from pytorch-pretrained-bert to pytorch-transformers\n\nhere is a quick summary of what you should take care of when migrating from `pytorch-pretrained-bert` to `pytorch-transformers`\n\n### models always output `tuples`\n\nthe main breaking change when migrating from `pytorch-pretrained-bert` to `pytorch-transformers` is that the models forward method always outputs a `tuple` with various elements depending on the model and the configuration parameters.\n\nthe exact content of the tuples for each model are detailed in the models' docstrings and the [documentation](https://huggingface.co/pytorch-transformers/).\n\nin pretty much every case, you will be fine by taking the first element of the output as the output you previously used in `pytorch-pretrained-bert`.\n\nhere is a `pytorch-pretrained-bert` to `pytorch-transformers` conversion example for a `bertforsequenceclassification` classification model:\n\n```python\n# let's load our model\nmodel = bertforsequenceclassification.from_pretrained('bert-base-uncased')\n\n# if you used to have this line in pytorch-pretrained-bert:\nloss = model(input_ids, labels=labels)\n\n# now just use this line in pytorch-transformers to extract the loss from the output tuple:\noutputs = model(input_ids, labels=labels)\nloss = outputs[0]\n\n# in pytorch-transformers you can also have access to the logits:\nloss, logits = outputs[:2]\n\n# and even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)\nmodel = bertforsequenceclassification.from_pretrained('bert-base-uncased', output_attentions=true)\noutputs = model(input_ids, labels=labels)\nloss, logits, attentions = outputs\n```\n\n### serialization\n\nbreaking change in the `from_pretrained()`method:\n\n1. models are now set in evaluation mode by default when instantiated with the `from_pretrained()` method. to train them don't forget to set them back in training mode (`model.train()`) to activate the dropout modules.\n\n2. the additional `*input` and `**kwargs` arguments supplied to the `from_pretrained()` method used to be directly passed to the underlying model's class `__init__()` method. they are now used to update the model configuration attribute instead which can break derived model classes build based on the previous `bertforsequenceclassification` examples. we are working on a way to mitigate this breaking change in [#866](https://github.com/huggingface/pytorch-transformers/pull/866) by forwarding the the model `__init__()` method (i) the provided positional arguments and (ii) the keyword arguments which do not match any configuration class attributes.\n\nalso, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method `save_pretrained(save_directory)` if you were using any other serialization method before.\n\nhere is an example:\n\n```python\n### let's load a model and tokenizer\nmodel = bertforsequenceclassification.from_pretrained('bert-base-uncased')\ntokenizer = berttokenizer.from_pretrained('bert-base-uncased')\n\n### do some stuff to our model and tokenizer\n# ex: add new tokens to the vocabulary and embeddings of our model\ntokenizer.add_tokens(['[special_token_1]', '[special_token_2]'])\nmodel.resize_token_embeddings(len(tokenizer))\n# train our model\ntrain(model)\n\n### now let's save our model and tokenizer to a directory\nmodel.save_pretrained('./my_saved_model_directory/')\ntokenizer.save_pretrained('./my_saved_model_directory/')\n\n### reload the model and the tokenizer\nmodel = bertforsequenceclassification.from_pretrained('./my_saved_model_directory/')\ntokenizer = berttokenizer.from_pretrained('./my_saved_model_directory/')\n```\n\n### optimizers: bertadam & openaiadam are now adamw, schedules are standard pytorch schedules\n\nthe two optimizers previously included, `bertadam` and `openaiadam`, have been replaced by a single `adamw` optimizer which has a few differences:\n\n- it only implements weights decay correction,\n- schedules are now externals (see below),\n- gradient clipping is now also external (see below).\n\nthe new optimizer `adamw` matches pytorch `adam` optimizer api and let you use standard pytorch or apex methods for the schedule and clipping.\n\nthe schedules are now standard [pytorch learning rate schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) and not part of the optimizer anymore.\n\nhere is a conversion examples from `bertadam` with a linear warmup and decay schedule to `adamw` and the same schedule:\n\n```python\n# parameters:\nlr = 1e-3\nmax_grad_norm = 1.0\nnum_total_steps = 1000\nnum_warmup_steps = 100\nwarmup_proportion = float(num_warmup_steps) / float(num_total_steps)  # 0.1\n\n### previously bertadam optimizer was instantiated like this:\noptimizer = bertadam(model.parameters(), lr=lr, schedule='warmup_linear', warmup=warmup_proportion, t_total=num_total_steps)\n### and used like this:\nfor batch in train_data:\n    loss = model(batch)\n    loss.backward()\n    optimizer.step()\n\n### in pytorch-transformers, optimizer and schedules are splitted and instantiated like this:\noptimizer = adamw(model.parameters(), lr=lr, correct_bias=false)  # to reproduce bertadam specific behavior set correct_bias=false\nscheduler = warmuplinearschedule(optimizer, warmup_steps=num_warmup_steps, t_total=num_total_steps)  # pytorch scheduler\n### and used like this:\nfor batch in train_data:\n    loss = model(batch)\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # gradient clipping is not in adamw anymore (so you can use amp without issue)\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n```\n\n## citation\n\nat the moment, there is no paper associated to pytorch-transformers but we are working on preparing one. in the meantime, please include a mention of the library and a link to the present repository if you use this work in a published or open-source project.\n\n\n",
  "docs_url": null,
  "keywords": "nlp deep learning transformer pytorch bert gpt gpt-2 google openai cmu",
  "license": "apache",
  "name": "pytorch-transformers",
  "package_url": "https://pypi.org/project/pytorch-transformers/",
  "project_url": "https://pypi.org/project/pytorch-transformers/",
  "project_urls": {
    "Homepage": "https://github.com/huggingface/pytorch-transformers"
  },
  "release_url": "https://pypi.org/project/pytorch-transformers/1.2.0/",
  "requires_dist": [
    "torch (>=1.0.0)",
    "numpy",
    "boto3",
    "requests",
    "tqdm",
    "regex",
    "sentencepiece",
    "sacremoses"
  ],
  "requires_python": "",
  "summary": "repository of pre-trained nlp transformer models: bert & roberta, gpt & gpt-2, transformer-xl, xlnet and xlm",
  "version": "1.2.0",
  "releases": [],
  "developers": [
    "thomas@huggingface.co",
    "thomas_wolf"
  ],
  "kwds": "bertforpretraining pytorch_transformers bert_model_classes bert bertfornextsentenceprediction",
  "license_kwds": "apache",
  "libtype": "pypi",
  "id": "pypi_pytorch_transformers",
  "homepage": "https://github.com/huggingface/pytorch-transformers",
  "release_count": 4,
  "dependency_ids": [
    "pypi_boto3",
    "pypi_numpy",
    "pypi_regex",
    "pypi_requests",
    "pypi_sacremoses",
    "pypi_sentencepiece",
    "pypi_torch",
    "pypi_tqdm"
  ]
}