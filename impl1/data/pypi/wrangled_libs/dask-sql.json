{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: mit license",
    "operating system :: os independent",
    "programming language :: python",
    "programming language :: python :: 3",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "programming language :: rust",
    "topic :: scientific/engineering",
    "topic :: system :: distributed computing"
  ],
  "description": "[![conda](https://img.shields.io/conda/v/conda-forge/dask-sql)](https://anaconda.org/conda-forge/dask-sql)\n[![pypi](https://img.shields.io/pypi/v/dask-sql?logo=pypi)](https://pypi.python.org/pypi/dask-sql/)\n[![github workflow status](https://img.shields.io/github/actions/workflow/status/dask-contrib/dask-sql/test.yml?branch=main)](https://github.com/dask-contrib/dask-sql/actions/workflows/test.yml?query=branch%3amain)\n[![read the docs](https://img.shields.io/readthedocs/dask-sql)](https://dask-sql.readthedocs.io/en/latest/)\n[![codecov](https://img.shields.io/codecov/c/github/dask-contrib/dask-sql?logo=codecov)](https://codecov.io/gh/dask-contrib/dask-sql)\n[![github](https://img.shields.io/github/license/dask-contrib/dask-sql)](https://github.com/dask-contrib/dask-sql/blob/main/license.txt)\n[![binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/dask-contrib/dask-sql-binder/main?urlpath=lab)\n\n<div align=\"center\">\n    <img src=\"./.github/heart.png\" alt=\"sql + python\">\n</div>\n\n`dask-sql` is a distributed sql query engine in python.\nit allows you to query and transform your data using a mixture of\ncommon sql operations and python code and also scale up the calculation easily\nif you need it.\n\n* **combine the power of python and sql**: load your data with python, transform it with sql, enhance it with python and query it with sql - or the other way round.\n  with `dask-sql` you can mix the well known python dataframe api of `pandas` and `dask` with common sql operations, to\n  process your data in exactly the way that is easiest for you.\n* **infinite scaling**: using the power of the great `dask` ecosystem, your computations can scale as you need it - from your laptop to your super cluster - without changing any line of sql code. from k8s to cloud deployments, from batch systems to yarn - if `dask` [supports it](https://docs.dask.org/en/latest/setup.html), so will `dask-sql`.\n* **your data - your queries**: use python user-defined functions (udfs) in sql without any performance drawback and extend your sql queries with the large number of python libraries, e.g. machine learning, different complicated input formats, complex statistics.\n* **easy to install and maintain**: `dask-sql` is just a pip/conda install away (or a docker run if you prefer).\n* **use sql from wherever you like**: `dask-sql` integrates with your jupyter notebook, your normal python module or can be used as a standalone sql server from any bi tool. it even integrates natively with [apache hue](https://gethue.com/).\n* **gpu support**: `dask-sql` supports running sql queries on cuda-enabled gpus by utilizing [rapids](https://rapids.ai) libraries like [`cudf`](https://github.com/rapidsai/cudf), enabling accelerated compute for sql.\n\nread more in the [documentation](https://dask-sql.readthedocs.io/en/latest/).\n\n<div align=\"center\">\n    <img src=\"./.github/animation.gif\" alt=\"dask-sql gif\">\n</div>\n\n---\n\n## example\n\nfor this example, we use some data loaded from disk and query them with a sql command from our python code.\nany pandas or dask dataframe can be used as input and ``dask-sql`` understands a large amount of formats (csv, parquet, json,...) and locations (s3, hdfs, gcs,...).\n\n```python\nimport dask.dataframe as dd\nfrom dask_sql import context\n\n# create a context to hold the registered tables\nc = context()\n\n# load the data and register it in the context\n# this will give the table a name, that we can use in queries\ndf = dd.read_csv(\"...\")\nc.create_table(\"my_data\", df)\n\n# now execute a sql query. the result is again dask dataframe.\nresult = c.sql(\"\"\"\n    select\n        my_data.name,\n        sum(my_data.x)\n    from\n        my_data\n    group by\n        my_data.name\n\"\"\", return_futures=false)\n\n# show the result\nprint(result)\n```\n\n## quickstart\n\nhave a look into the [documentation](https://dask-sql.readthedocs.io/en/latest/) or start the example notebook on [binder](https://mybinder.org/v2/gh/dask-contrib/dask-sql-binder/main?urlpath=lab).\n\n\n> `dask-sql` is currently under development and does so far not understand all sql commands (but a large fraction).\nwe are actively looking for feedback, improvements and contributors!\n\n## installation\n\n`dask-sql` can be installed via `conda` (preferred) or `pip` - or in a development environment.\n\n### with `conda`\n\ncreate a new conda environment or use your already present environment:\n\n    conda create -n dask-sql\n    conda activate dask-sql\n\ninstall the package from the `conda-forge` channel:\n\n    conda install dask-sql -c conda-forge\n\n### with `pip`\n\nyou can install the package with\n\n    pip install dask-sql\n\n### for development\n\nif you want to have the newest (unreleased) `dask-sql` version or if you plan to do development on `dask-sql`, you can also install the package from sources.\n\n    git clone https://github.com/dask-contrib/dask-sql.git\n\ncreate a new conda environment and install the development environment:\n\n    conda env create -f continuous_integration/environment-3.9-dev.yaml\n\nit is not recommended to use `pip` instead of `conda` for the environment setup.\n\nafter that, you can install the package in development mode\n\n    pip install -e \".[dev]\"\n\nthe rust datafusion bindings are built as part of the `pip install`.\nnote that if changes are made to the rust source in `src/`, another build must be run to recompile the bindings.\nthis repository uses [pre-commit](https://pre-commit.com/) hooks. to install them, call\n\n    pre-commit install\n\n## testing\n\nyou can run the tests (after installation) with\n\n    pytest tests\n\ngpu-specific tests require additional dependencies specified in `continuous_integration/gpuci/environment.yaml`.\nthese can be added to the development environment by running\n\n```\nconda env update -n dask-sql -f continuous_integration/gpuci/environment.yaml\n```\n\nand gpu-specific tests can be run with\n\n```\npytest tests -m gpu --rungpu\n```\n\n## sql server\n\n`dask-sql` comes with a small test implementation for a sql server.\ninstead of rebuilding a full odbc driver, we re-use the [presto wire protocol](https://github.com/prestodb/presto/wiki/http-protocol).\nit is - so far - only a start of the development and missing important concepts, such as\nauthentication.\n\nyou can test the sql presto server by running (after installation)\n\n    dask-sql-server\n\nor by using the created docker image\n\n    docker run --rm -it -p 8080:8080 nbraun/dask-sql\n\nin one terminal. this will spin up a server on port 8080 (by default)\nthat looks similar to a normal presto database to any presto client.\n\nyou can test this for example with the default [presto client](https://prestosql.io/docs/current/installation/cli.html):\n\n    presto --server localhost:8080\n\nnow you can fire simple sql queries (as no data is loaded by default):\n\n    => select 1 + 1;\n     expr$0\n    --------\n        2\n    (1 row)\n\nyou can find more information in the [documentation](https://dask-sql.readthedocs.io/en/latest/pages/server.html).\n\n## cli\n\nyou can also run the cli `dask-sql` for testing out sql commands quickly:\n\n    dask-sql --load-test-data --startup\n\n    (dask-sql) > select * from timeseries limit 10;\n\n## how does it work?\n\nat the core, `dask-sql` does two things:\n\n- translate the sql query using [datafusion](https://arrow.apache.org/datafusion) into a relational algebra, which is represented as a logical query plan - similar to many other sql engines (hive, flink, ...)\n- convert this description of the query into dask api calls (and execute them) - returning a dask dataframe.\n\nfor the first step, arrow datafusion needs to know about the columns and types of the dask dataframes, therefore some rust code to store this information for dask dataframes are defined in `dask_planner`.\nafter the translation to a relational algebra is done (using `dasksqlcontext.logical_relational_algebra`), the python methods defined in `dask_sql.physical` turn this into a physical dask execution plan by converting each piece of the relational algebra one-by-one.\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "dask-sql",
  "package_url": "https://pypi.org/project/dask-sql/",
  "project_url": "https://pypi.org/project/dask-sql/",
  "project_urls": {
    "Documentation": "https://dask-sql.readthedocs.io",
    "Homepage": "https://github.com/dask-contrib/dask-sql",
    "Source": "https://github.com/dask-contrib/dask-sql"
  },
  "release_url": "https://pypi.org/project/dask-sql/2023.11.0/",
  "requires_dist": [
    "dask[dataframe] >=2022.3.0, <=2023.11.0",
    "distributed >=2022.3.0, <=2023.11.0",
    "pandas >=1.4.0",
    "fastapi >=0.92.0",
    "httpx >=0.24.1",
    "uvicorn >=0.13.4",
    "tzlocal >=2.1",
    "prompt_toolkit >=3.0.8",
    "pygments >=2.7.1",
    "tabulate",
    "pytest >=6.0.1 ; extra == 'dev'",
    "pytest-cov >=2.10.1 ; extra == 'dev'",
    "mock >=4.0.3 ; extra == 'dev'",
    "sphinx >=3.2.1 ; extra == 'dev'",
    "pyarrow >=6.0.2 ; extra == 'dev'",
    "scikit-learn >=1.0.0 ; extra == 'dev'",
    "intake >=0.6.0 ; extra == 'dev'",
    "pre-commit ; extra == 'dev'",
    "black ==22.10.0 ; extra == 'dev'",
    "isort ==5.12.0 ; extra == 'dev'",
    "fugue >=0.7.3 ; extra == 'fugue'",
    "triad <0.9.2 ; extra == 'fugue'"
  ],
  "requires_python": ">=3.8",
  "summary": "sql query layer for dask",
  "version": "2023.11.0",
  "releases": [],
  "developers": [
    "nilslennartbraun@gmail.com"
  ],
  "kwds": "dask_sql dask_planner dasksqlcontext dask workflows",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_dask_sql",
  "homepage": "",
  "release_count": 38,
  "dependency_ids": [
    "pypi_black",
    "pypi_dask",
    "pypi_distributed",
    "pypi_fastapi",
    "pypi_fugue",
    "pypi_httpx",
    "pypi_intake",
    "pypi_isort",
    "pypi_mock",
    "pypi_pandas",
    "pypi_pre_commit",
    "pypi_prompt_toolkit",
    "pypi_pyarrow",
    "pypi_pygments",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_scikit_learn",
    "pypi_sphinx",
    "pypi_tabulate",
    "pypi_triad",
    "pypi_tzlocal",
    "pypi_uvicorn"
  ]
}