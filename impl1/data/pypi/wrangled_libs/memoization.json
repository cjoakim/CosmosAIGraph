{
  "classifiers": [
    "development status :: 5 - production/stable",
    "license :: osi approved :: mit license",
    "operating system :: os independent",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.4",
    "programming language :: python :: 3.5",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: software development :: libraries",
    "topic :: software development :: libraries :: python modules",
    "typing :: typed"
  ],
  "description": "# python-memoization\n\n[![repository][repositorysvg]][repository] [![build status][travismaster]][travis] [![codacy badge][codacysvg]][codacy]\n[![coverage status][coverallssvg]][coveralls] [![downloads][downloadssvg]][repository]\n<br>\n[![prs welcome][prsvg]][pr] [![license][licensesvg]][license] [![supports python][pythonsvg]][python]\n\n\na powerful caching library for python, with ttl support and multiple algorithm options.\n\nif you like this work, please [star](https://github.com/lonelyenvoy/python-memoization) it on github.\n\n## why choose this library?\n\nperhaps you know about [```functools.lru_cache```](https://docs.python.org/3/library/functools.html#functools.lru_cache)\nin python 3, and you may be wondering why we are reinventing the wheel.\n\nwell, actually not. this lib is based on ```functools```. please find below the comparison with ```lru_cache```.\n\n|features|```functools.lru_cache```|```memoization```|\n|--------|-------------------|-----------|\n|configurable max size|\u2714\ufe0f|\u2714\ufe0f|\n|thread safety|\u2714\ufe0f|\u2714\ufe0f|\n|flexible argument typing (typed & untyped)|\u2714\ufe0f|always typed|\n|cache statistics|\u2714\ufe0f|\u2714\ufe0f|\n|lru (least recently used) as caching algorithm|\u2714\ufe0f|\u2714\ufe0f|\n|lfu (least frequently used) as caching algorithm|no support|\u2714\ufe0f|\n|fifo (first in first out) as caching algorithm|no support|\u2714\ufe0f|\n|extensibility for new caching algorithms|no support|\u2714\ufe0f|\n|ttl (time-to-live) support|no support|\u2714\ufe0f|\n|support for unhashable arguments (dict, list, etc.)|no support|\u2714\ufe0f|\n|custom cache keys|no support|\u2714\ufe0f|\n|on-demand partial cache clearing|no support|\u2714\ufe0f|\n|iterating through the cache|no support|\u2714\ufe0f|\n|python version|3.2+|3.4+|\n\n```memoization``` solves some drawbacks of ```functools.lru_cache```:\n\n1. ```lru_cache``` does not support __unhashable types__, which means function arguments cannot contain dict or list.\n\n```python\n>>> from functools import lru_cache\n>>> @lru_cache()\n... def f(x): return x\n... \n>>> f([1, 2])  # unsupported\ntraceback (most recent call last):\n  file \"<stdin>\", line 1, in <module>\ntypeerror: unhashable type: 'list'\n```\n\n2. ```lru_cache``` is vulnerable to [__hash collision attack__](https://learncryptography.com/hash-functions/hash-collision-attack)\n   and can be hacked or compromised. using this technique, attackers can make your program __unexpectedly slow__ by\n   feeding the cached function with certain cleverly designed inputs. however, in ```memoization```, caching is always\n   typed, which means ```f(3)``` and ```f(3.0)``` will be treated as different calls and cached separately. also,\n   you can build your own cache key with a unique hashing strategy. these measures __prevents the attack__ from\n   happening (or at least makes it a lot harder).\n\n```python\n>>> hash((1,))\n3430019387558\n>>> hash(3430019387558.0)  # two different arguments with an identical hash value\n3430019387558\n```\n\n3. unlike `lru_cache`, `memoization` is designed to be highly extensible, which make it easy for developers to add and integrate\n__any caching algorithms__ (beyond fifo, lru and lfu) into this library. see [contributing guidance](https://github.com/lonelyenvoy/python-memoization/blob/master/contributing.md) for further detail.\n\n\n## installation\n\n```bash\npip install -u memoization\n```\n\n\n## 1-minute tutorial\n\n```python\nfrom memoization import cached\n\n@cached\ndef func(arg):\n    ...  # do something slow\n```\n\nsimple enough - the results of ```func()``` are cached. \nrepetitive calls to ```func()``` with the same arguments run ```func()``` only once, enhancing performance.\n\n>:warning:__warning:__ for functions with unhashable arguments, the default setting may not enable `memoization` to work properly. see [custom cache keys](https://github.com/lonelyenvoy/python-memoization#custom-cache-keys) section below for details.\n\n## 15-minute tutorial\n\nyou will learn about the advanced features in the following tutorial, which enable you to customize `memoization` .\n\nconfigurable options include `ttl`, `max_size`, `algorithm`, `thread_safe`, `order_independent` and `custom_key_maker`.\n\n### ttl (time-to-live)\n\n```python\n@cached(ttl=5)  # the cache expires after 5 seconds\ndef expensive_db_query(user_id):\n    ...\n```\n\nfor impure functions, ttl (in second) will be a solution. this will be useful when the function returns resources that is valid only for a short time, e.g. fetching something from databases.\n\n### limited cache capacity\n \n```python\n@cached(max_size=128)  # the cache holds no more than 128 items\ndef get_a_very_large_object(filename):\n    ...\n```\n\nby default, if you don't specify ```max_size```, the cache can hold unlimited number of items.\nwhen the cache is fully occupied, the former data will be overwritten by a certain algorithm described below.\n\n### choosing your caching algorithm\n\n```python\nfrom memoization import cached, cachingalgorithmflag\n\n@cached(max_size=128, algorithm=cachingalgorithmflag.lfu)  # the cache overwrites items using the lfu algorithm\ndef func(arg):\n    ...\n```\n\npossible values for ```algorithm``` are:\n\n- `cachingalgorithmflag.lru`: _least recently used_  (default)\n- `cachingalgorithmflag.lfu`: _least frequently used_ \n- `cachingalgorithmflag.fifo`: _first in first out_ \n\nthis option is valid only when a ```max_size``` is explicitly specified.\n\n### thread safe?\n\n```python\n@cached(thread_safe=false)\ndef func(arg):\n    ...\n```\n\n```thread_safe``` is ```true``` by default. setting it to ```false``` enhances performance.\n\n### order-independent cache key\n\nby default, the following function calls will be treated differently and cached twice, which means the cache misses at the second call.\n\n```python\nfunc(a=1, b=1)\nfunc(b=1, a=1)\n```\n\nyou can avoid this behavior by passing an `order_independent` argument to the decorator, although it will slow down the performance a little bit. \n\n```python\n@cached(order_independent=true)\ndef func(**kwargs):\n    ...\n```\n\n### custom cache keys\n\nprior to memorize your function inputs and outputs (i.e. putting them into a cache), `memoization` needs to\nbuild a __cache key__ using the inputs, so that the outputs can be retrieved later.\n\n> by default, `memoization` tries to combine all your function\narguments and calculate its hash value using `hash()`. if it turns out that parts of your arguments are\nunhashable, `memoization` will fall back to turning them into a string using `str()`. this behavior relies\non the assumption that the string exactly represents the internal state of the arguments, which is true for\nbuilt-in types.\n\nhowever, this is not true for all objects. __if you pass objects which are\ninstances of non-built-in classes, sometimes you will need to override the default key-making procedure__,\nbecause the `str()` function on these objects may not hold the correct information about their states.\n\nhere are some suggestions. __implementations of a valid key maker__:\n\n- must be a function with the same signature as the cached function.\n- must produce unique keys, which means two sets of different arguments always map to two different keys.\n- must produce hashable keys, and a key is comparable with another key (`memoization` only needs to check for their equality).\n- should compute keys efficiently and produce small objects as keys.\n\nexample:\n\n```python\ndef get_employee_id(employee):\n    return employee.id  # returns a string or a integer\n\n@cached(custom_key_maker=get_employee_id)\ndef calculate_performance(employee):\n    ...\n```\n\nnote that writing a robust key maker function can be challenging in some situations. if you find it difficult,\nfeel free to ask for help by submitting an [issue](https://github.com/lonelyenvoy/python-memoization/issues).\n\n\n### knowing how well the cache is behaving\n\n```python\n>>> @cached\n... def f(x): return x\n... \n>>> f.cache_info()\ncacheinfo(hits=0, misses=0, current_size=0, max_size=none, algorithm=<cachingalgorithmflag.lru: 2>, ttl=none, thread_safe=true, order_independent=false, use_custom_key=false)\n```\n\nwith ```cache_info```, you can retrieve the number of ```hits``` and ```misses``` of the cache, and other information indicating the caching status.\n\n- `hits`: the number of cache hits\n- `misses`: the number of cache misses\n- `current_size`: the number of items that were cached\n- `max_size`: the maximum number of items that can be cached (user-specified)\n- `algorithm`: caching algorithm (user-specified)\n- `ttl`: time-to-live value (user-specified)\n- `thread_safe`: whether the cache is thread safe (user-specified)\n- `order_independent`: whether the cache is kwarg-order-independent (user-specified)\n- `use_custom_key`: whether a custom key maker is used\n\n### other apis\n\n- access the original undecorated function `f` by `f.__wrapped__`.\n- clear the cache by `f.cache_clear()`.\n- check whether the cache is empty by `f.cache_is_empty()`.\n- check whether the cache is full by `f.cache_is_full()`.\n- disable `syntaxwarning` by `memoization.suppress_warnings()`.\n\n## advanced api references\n\n<details>\n<summary>details</summary>\n\n### checking whether the cache contains something\n\n#### cache_contains_argument(function_arguments, alive_only)\n\n```\nreturn true if the cache contains a cached item with the specified function call arguments\n\n:param function_arguments:  can be a list, a tuple or a dict.\n                            - full arguments: use a list to represent both positional arguments and keyword\n                              arguments. the list contains two elements, a tuple (positional arguments) and\n                              a dict (keyword arguments). for example,\n                                f(1, 2, 3, a=4, b=5, c=6)\n                              can be represented by:\n                                [(1, 2, 3), {'a': 4, 'b': 5, 'c': 6}]\n                            - positional arguments only: when the arguments does not include keyword arguments,\n                              a tuple can be used to represent positional arguments. for example,\n                                f(1, 2, 3)\n                              can be represented by:\n                                (1, 2, 3)\n                            - keyword arguments only: when the arguments does not include positional arguments,\n                              a dict can be used to represent keyword arguments. for example,\n                                f(a=4, b=5, c=6)\n                              can be represented by:\n                                {'a': 4, 'b': 5, 'c': 6}\n\n:param alive_only:          whether to check alive cache item only (default to true).\n\n:return:                    true if the desired cached item is present, false otherwise.\n```\n\n#### cache_contains_result(return_value, alive_only)\n\n```\nreturn true if the cache contains a cache item with the specified user function return value. o(n) time\ncomplexity.\n\n:param return_value:        a return value coming from the user function.\n\n:param alive_only:          whether to check alive cache item only (default to true).\n\n:return:                    true if the desired cached item is present, false otherwise.\n```\n\n### iterating through the cache\n\n#### cache_arguments()\n\n```\nget user function arguments of all alive cache elements\n\nsee also: cache_items()\n\nexample:\n   @cached\n   def f(a, b, c, d):\n       ...\n   f(1, 2, c=3, d=4)\n   for argument in f.cache_arguments():\n       print(argument)  # ((1, 2), {'c': 3, 'd': 4})\n\n:return: an iterable which iterates through a list of a tuple containing a tuple (positional arguments) and\n        a dict (keyword arguments)\n```\n\n#### cache_results()\n\n```\nget user function return values of all alive cache elements\n\nsee also: cache_items()\n\nexample:\n   @cached\n   def f(a):\n       return a\n   f('hello')\n   for result in f.cache_results():\n       print(result)  # 'hello'\n\n:return: an iterable which iterates through a list of user function result (of any type)\n```\n\n#### cache_items()\n\n```\nget cache items, i.e. entries of all alive cache elements, in the form of (argument, result).\n\nargument: a tuple containing a tuple (positional arguments) and a dict (keyword arguments).\nresult: a user function return value of any type.\n\nsee also: cache_arguments(), cache_results().\n\nexample:\n   @cached\n   def f(a, b, c, d):\n       return 'the answer is ' + str(a)\n   f(1, 2, c=3, d=4)\n   for argument, result in f.cache_items():\n       print(argument)  # ((1, 2), {'c': 3, 'd': 4})\n       print(result)    # 'the answer is 1'\n\n:return: an iterable which iterates through a list of (argument, result) entries\n```\n\n#### cache_for_each()\n\n```\nperform the given action for each cache element in an order determined by the algorithm until all\nelements have been processed or the action throws an error\n\n:param consumer:           an action function to process the cache elements. must have 3 arguments:\n                             def consumer(user_function_arguments, user_function_result, is_alive): ...\n                           user_function_arguments is a tuple holding arguments in the form of (args, kwargs).\n                             args is a tuple holding positional arguments.\n                             kwargs is a dict holding keyword arguments.\n                             for example, for a function: foo(a, b, c, d), calling it by: foo(1, 2, c=3, d=4)\n                             user_function_arguments == ((1, 2), {'c': 3, 'd': 4})\n                           user_function_result is a return value coming from the user function.\n                           is_alive is a boolean value indicating whether the cache is still alive\n                           (if a ttl is given).\n```\n\n### removing something from the cache\n\n#### cache_clear()\n\n```\nclear the cache and its statistics information\n```\n\n#### cache_remove_if(predicate)\n\n```\nremove all cache elements that satisfy the given predicate\n\n:param predicate:           a predicate function to judge whether the cache elements should be removed. must\n                            have 3 arguments, and returns true or false:\n                              def consumer(user_function_arguments, user_function_result, is_alive): ...\n                            user_function_arguments is a tuple holding arguments in the form of (args, kwargs).\n                              args is a tuple holding positional arguments.\n                              kwargs is a dict holding keyword arguments.\n                              for example, for a function: foo(a, b, c, d), calling it by: foo(1, 2, c=3, d=4)\n                              user_function_arguments == ((1, 2), {'c': 3, 'd': 4})\n                            user_function_result is a return value coming from the user function.\n                            is_alive is a boolean value indicating whether the cache is still alive\n                            (if a ttl is given).\n\n:return:                    true if at least one element is removed, false otherwise.\n```\n\n</details>\n\n## q&a\n\n1. **q: there are duplicated code in `memoization` and most of them can be eliminated by using another level of\nabstraction (e.g. classes and multiple inheritance). why not refactor?**\n\n   a: we would like to keep the code in a proper level of abstraction. however, these abstractions make it run slower.\nas this is a caching library focusing on speed, we have to give up some elegance for better performance. refactoring\nis our future work.\n\n\n2. **q: i have submitted an issue and not received a reply for a long time. anyone can help me?**\n\n   a: sorry! we are not working full-time, but working voluntarily on this project, so you might experience some delay.\nwe appreciate your patience.\n\n\n## contributing\n\nthis project welcomes contributions from anyone.\n- [read contributing guidance](https://github.com/lonelyenvoy/python-memoization/blob/master/contributing.md) first.\n- [submit bugs](https://github.com/lonelyenvoy/python-memoization/issues) and help us verify fixes.\n- [submit pull requests](https://github.com/lonelyenvoy/python-memoization/pulls) for bug fixes and features and discuss existing proposals. please make sure that your pr passes the tests in ```test.py```.\n- [see contributors](https://github.com/lonelyenvoy/python-memoization/blob/master/contributors.md) of this project.\n\n\n## license\n\n[the mit license](https://github.com/lonelyenvoy/python-memoization/blob/master/license)\n\n\n[pythonsvg]: https://img.shields.io/pypi/pyversions/memoization.svg\n[python]: https://www.python.org\n\n[travismaster]: https://travis-ci.com/lonelyenvoy/python-memoization.svg?branch=master\n[travis]: https://travis-ci.com/lonelyenvoy/python-memoization\n\n[coverallssvg]: https://coveralls.io/repos/github/lonelyenvoy/python-memoization/badge.svg?branch=master\n[coveralls]: https://coveralls.io/github/lonelyenvoy/python-memoization?branch=master\n\n[repositorysvg]: https://img.shields.io/pypi/v/memoization\n[repository]: https://pypi.org/project/memoization\n\n[downloadssvg]: https://img.shields.io/pypi/dm/memoization\n\n[prsvg]: https://img.shields.io/badge/pull_requests-welcome-blue.svg\n[pr]: https://github.com/lonelyenvoy/python-memoization#contributing\n\n[licensesvg]: https://img.shields.io/badge/license-mit-blue.svg\n[license]: https://github.com/lonelyenvoy/python-memoization/blob/master/license\n\n[codacysvg]: https://api.codacy.com/project/badge/grade/52c68fb9de6b4b149e77e8e173616db6\n[codacy]: https://www.codacy.com/manual/petrinchor/python-memoization?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=lonelyenvoy/python-memoization&amp;utm_campaign=badge_grade\n\n\n",
  "docs_url": null,
  "keywords": "memoization memorization remember decorator cache caching function callable functional ttl limited capacity fast high-performance optimization",
  "license": "mit",
  "name": "memoization",
  "package_url": "https://pypi.org/project/memoization/",
  "project_url": "https://pypi.org/project/memoization/",
  "project_urls": {
    "Homepage": "https://github.com/lonelyenvoy/python-memoization"
  },
  "release_url": "https://pypi.org/project/memoization/0.4.0/",
  "requires_dist": [],
  "requires_python": ">=3, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, <4",
  "summary": "a powerful caching library for python, with ttl support and multiple algorithm options. (https://github.com/lonelyenvoy/python-memoization)",
  "version": "0.4.0",
  "releases": [],
  "developers": [
    "lonelyenvoy",
    "petrinchor@gmail.com"
  ],
  "kwds": "lru_cache __cache cachingalgorithmflag cache_arguments cache",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_memoization",
  "homepage": "https://github.com/lonelyenvoy/python-memoization",
  "release_count": 10,
  "dependency_ids": []
}