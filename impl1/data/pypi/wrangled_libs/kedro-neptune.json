{
  "classifiers": [
    "development status :: 4 - beta",
    "environment :: console",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "natural language :: english",
    "operating system :: macos",
    "operating system :: microsoft :: windows",
    "operating system :: posix",
    "operating system :: unix",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.12",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "programming language :: python :: implementation :: cpython",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "# neptune + kedro integration\n\nkedro plugin for experiment tracking and metadata management. it lets you browse, filter and sort runs in a nice ui, visualize node/pipeline metadata, and compare pipelines.\n\n## what will you get with this integration?\n\n* **browse, filter, and sort** your model training runs\n* **compare nodes and pipelines** on metrics, visual node outputs, and more\n* **display all pipeline metadata** including learning curves for metrics, plots, and images, rich media like video and audio or interactive visualizations from plotly, altair, or bokeh\n* and do whatever else you would expect from a modern ml metadata store\n\n![image](https://user-images.githubusercontent.com/97611089/160640893-9b95aac1-095e-4869-88a1-99f2cba5a59f.png)\n*kedro pipeline metadata in custom dashboard in the neptune web app*\n\nnote: the kedro-neptune plugin supports distributed pipeline execution and works in kedro setups that use orchestrators like airflow or kubeflow.\n\n## resources\n\n* [documentation](https://docs.neptune.ai/integrations/kedro)\n* [code example on github](https://github.com/neptune-ai/examples/tree/main/integrations-and-supported-tools/kedro/scripts/kedro-neptune-quickstart)\n* [example run logged in the neptune app](https://app.neptune.ai/o/common/org/kedro-integration/e/ked-1563/dashboard/basic-pipeline-metadata-42874940-da74-4cdc-94a4-315a7cdfbfa8)\n* how to [compare kedro pipelines](https://docs.neptune.ai/integrations/kedro_comparing_pipelines/)\n* how to [compare results between kedro nodes](https://docs.neptune.ai/integrations/kedro_comparing_nodes/)\n* how to [display kedro node metadata and outputs](https://docs.neptune.ai/integrations/kedro_displaying_node_outputs/)\n\n## example\n\non the command line:\n\n```\npip install kedro neptune[kedro]\nkedro new --starter=pandas-iris\n```\n\nin your kedro project directory:\n\n```\nkedro neptune init\n```\n\nin a pipeline node, in `nodes.py`:\n\n```python\nimport neptune\n\n# add a neptune run handler to the report_accuracy() function\n\ndef report_accuracy(\n    y_pred: pd.series,\n    y_test: pd.series,\n    neptune_run: neptune.handler.handler,\n) -> none:\n    accuracy = (y_pred == y_test).sum() / len(y_test)\n    logger = logging.getlogger(__name__)\n    logger.info(\"model has accuracy of %.3f on test data.\", accuracy)\n\n    # log metrics to the neptune run\n    neptune_run[\"nodes/report/accuracy\"] = accuracy * 100\n\n# add the neptune run handler to the kedro pipeline\nnode(\n    func=report_accuracy,\n    inputs=[\"y_pred\", \"y_test\", \"neptune_run\"],\n    outputs=none,\n    name=\"report_accuracy\",\n)\n```\n\non the command line, run the kedro pipeline:\n\n```\nkedro run\n```\n\n## support\n\nif you got stuck or simply want to talk to us, here are your options:\n\n* check our [faq page](https://docs.neptune.ai/getting_help)\n* you can submit bug reports, feature requests, or contributions directly to the repository.\n* chat! when in the neptune application click on the blue message icon in the bottom-right corner and send a message. a real person will talk to you asap (typically very asap),\n* you can just shoot us an email at support@neptune.ai\n\n",
  "docs_url": null,
  "keywords": "mlops,ml experiment tracking,ml model registry,ml model store,ml metadata store",
  "license": "apache-2.0",
  "name": "kedro-neptune",
  "package_url": "https://pypi.org/project/kedro-neptune/",
  "project_url": "https://pypi.org/project/kedro-neptune/",
  "project_urls": {
    "Documentation": "https://docs.neptune.ai/integrations/kedro/",
    "Homepage": "https://neptune.ai/",
    "Repository": "https://github.com/neptune-ai/kedro-neptune",
    "Tracker": "https://github.com/neptune-ai/kedro-neptune/issues"
  },
  "release_url": "https://pypi.org/project/kedro-neptune/0.3.0/",
  "requires_dist": [
    "importlib-metadata ; python_version < \"3.8\"",
    "kedro (>=0.18.5)",
    "kedro-datasets (>=1.8.0)",
    "neptune (>=1.0.0) ; extra == \"dev\"",
    "pre-commit ; extra == \"dev\"",
    "pytest (>=5.0) ; extra == \"dev\"",
    "pytest-cov (==2.10.1) ; extra == \"dev\"",
    "ruamel.yaml (>=0.17.0,<0.18.0)"
  ],
  "requires_python": ">=3.7,<4.0",
  "summary": "neptune.ai integration with kedro",
  "version": "0.3.0",
  "releases": [],
  "developers": [
    "contact@neptune.ai",
    "neptune"
  ],
  "kwds": "kedro_comparing_pipelines metadata pipeline pipelines visualizations",
  "license_kwds": "apache-2.0",
  "libtype": "pypi",
  "id": "pypi_kedro_neptune",
  "homepage": "https://neptune.ai/",
  "release_count": 16,
  "dependency_ids": [
    "pypi_importlib_metadata",
    "pypi_kedro",
    "pypi_kedro_datasets",
    "pypi_neptune",
    "pypi_pre_commit",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_ruamel.yaml"
  ]
}