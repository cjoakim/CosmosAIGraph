{
  "classifiers": [
    "development status :: 3 - alpha",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: mozilla public license 2.0 (mpl 2.0)",
    "operating system :: posix :: linux",
    "programming language :: python",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.9",
    "topic :: multimedia",
    "topic :: multimedia :: sound/audio",
    "topic :: multimedia :: sound/audio :: speech",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: software development",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "\n## \ud83d\udc38coqui.ai news\n- \ud83d\udce3 \u24e7ttsv2 is here with 16 languages and better performance across the board.\n- \ud83d\udce3 \u24e7tts fine-tuning code is out. check the [example recipes](https://github.com/coqui-ai/tts/tree/dev/recipes/ljspeech).\n- \ud83d\udce3 \u24e7tts can now stream with <200ms latency.\n- \ud83d\udce3 \u24e7tts, our production tts model that can speak 13 languages, is released [blog post](https://coqui.ai/blog/tts/open_xtts), [demo](https://huggingface.co/spaces/coqui/xtts), [docs](https://tts.readthedocs.io/en/dev/models/xtts.html)\n- \ud83d\udce3 [\ud83d\udc36bark](https://github.com/suno-ai/bark) is now available for inference with unconstrained voice cloning. [docs](https://tts.readthedocs.io/en/dev/models/bark.html)\n- \ud83d\udce3 you can use [~1100 fairseq models](https://github.com/facebookresearch/fairseq/tree/main/examples/mms) with \ud83d\udc38tts.\n- \ud83d\udce3 \ud83d\udc38tts now supports \ud83d\udc22tortoise with faster inference. [docs](https://tts.readthedocs.io/en/dev/models/tortoise.html)\n- \ud83d\udce3 voice generation with prompts - **prompt to voice** - is live on [**coqui studio**](https://app.coqui.ai/auth/signin)!! - [blog post](https://coqui.ai/blog/tts/prompt-to-voice)\n- \ud83d\udce3 voice generation with fusion - **voice fusion** - is live on [**coqui studio**](https://app.coqui.ai/auth/signin).\n- \ud83d\udce3 voice cloning is live on [**coqui studio**](https://app.coqui.ai/auth/signin).\n\n<div align=\"center\">\n<img src=\"https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2\" />\n\n## <img src=\"https://raw.githubusercontent.com/coqui-ai/tts/main/images/coqui-log-green-tts.png\" height=\"56\"/>\n\n\n**\ud83d\udc38tts is a library for advanced text-to-speech generation.**\n\n\ud83d\ude80 pretrained models in +1100 languages.\n\n\ud83d\udee0\ufe0f tools for training new models and fine-tuning existing models in any language.\n\n\ud83d\udcda utilities for dataset analysis and curation.\n______________________________________________________________________\n\n[![discord](https://img.shields.io/discord/1037326658807533628?color=%239b59b6&label=chat%20on%20discord)](https://discord.gg/5exr5serrv)\n[![license](<https://img.shields.io/badge/license-mpl%202.0-brightgreen.svg>)](https://opensource.org/licenses/mpl-2.0)\n[![pypi version](https://badge.fury.io/py/tts.svg)](https://badge.fury.io/py/tts)\n[![covenant](https://camo.githubusercontent.com/7d620efaa3eac1c5b060ece5d6aacfcc8b81a74a04d05cd0398689c01c4463bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76322e3025323061646f707465642d6666363962342e737667)](https://github.com/coqui-ai/tts/blob/master/code_of_conduct.md)\n[![downloads](https://pepy.tech/badge/tts)](https://pepy.tech/project/tts)\n[![doi](https://zenodo.org/badge/265612440.svg)](https://zenodo.org/badge/latestdoi/265612440)\n\n![githubactions](https://github.com/coqui-ai/tts/actions/workflows/aux_tests.yml/badge.svg)\n![githubactions](https://github.com/coqui-ai/tts/actions/workflows/data_tests.yml/badge.svg)\n![githubactions](https://github.com/coqui-ai/tts/actions/workflows/docker.yaml/badge.svg)\n![githubactions](https://github.com/coqui-ai/tts/actions/workflows/inference_tests.yml/badge.svg)\n![githubactions](https://github.com/coqui-ai/tts/actions/workflows/style_check.yml/badge.svg)\n![githubactions](https://github.com/coqui-ai/tts/actions/workflows/text_tests.yml/badge.svg)\n![githubactions](https://github.com/coqui-ai/tts/actions/workflows/tts_tests.yml/badge.svg)\n![githubactions](https://github.com/coqui-ai/tts/actions/workflows/vocoder_tests.yml/badge.svg)\n![githubactions](https://github.com/coqui-ai/tts/actions/workflows/zoo_tests0.yml/badge.svg)\n![githubactions](https://github.com/coqui-ai/tts/actions/workflows/zoo_tests1.yml/badge.svg)\n![githubactions](https://github.com/coqui-ai/tts/actions/workflows/zoo_tests2.yml/badge.svg)\n[![docs](<https://readthedocs.org/projects/tts/badge/?version=latest&style=plastic>)](https://tts.readthedocs.io/en/latest/)\n\n</div>\n\n______________________________________________________________________\n\n## \ud83d\udcac where to ask questions\nplease use our dedicated channels for questions and discussion. help is much more valuable if it's shared publicly so that more people can benefit from it.\n\n| type                            | platforms                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udea8 **bug reports**              | [github issue tracker]                  |\n| \ud83c\udf81 **feature requests & ideas** | [github issue tracker]                  |\n| \ud83d\udc69\u200d\ud83d\udcbb **usage questions**          | [github discussions]                    |\n| \ud83d\uddef **general discussion**       | [github discussions] or [discord]   |\n\n[github issue tracker]: https://github.com/coqui-ai/tts/issues\n[github discussions]: https://github.com/coqui-ai/tts/discussions\n[discord]: https://discord.gg/5exr5serrv\n[tutorials and examples]: https://github.com/coqui-ai/tts/wiki/tts-notebooks-and-tutorials\n\n\n## \ud83d\udd17 links and resources\n| type                            | links                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udcbc **documentation**              | [readthedocs](https://tts.readthedocs.io/en/latest/)\n| \ud83d\udcbe **installation**               | [tts/readme.md](https://github.com/coqui-ai/tts/tree/dev#installation)|\n| \ud83d\udc69\u200d\ud83d\udcbb **contributing**               | [contributing.md](https://github.com/coqui-ai/tts/blob/main/contributing.md)|\n| \ud83d\udccc **road map**                   | [main development plans](https://github.com/coqui-ai/tts/issues/378)\n| \ud83d\ude80 **released models**            | [tts releases](https://github.com/coqui-ai/tts/releases) and [experimental models](https://github.com/coqui-ai/tts/wiki/experimental-released-models)|\n| \ud83d\udcf0 **papers**                    | [tts papers](https://github.com/erogol/tts-papers)|\n\n\n## \ud83e\udd47 tts performance\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/coqui-ai/tts/main/images/tts-performance.png\" width=\"800\" /></p>\n\nunderlined \"tts*\" and \"judy*\" are **internal** \ud83d\udc38tts models that are not released open-source. they are here to show the potential. models prefixed with a dot (.jofish .abe and .janice) are real human voices.\n\n## features\n- high-performance deep learning models for text2speech tasks.\n    - text2spec models (tacotron, tacotron2, glow-tts, speedyspeech).\n    - speaker encoder to compute speaker embeddings efficiently.\n    - vocoder models (melgan, multiband-melgan, gan-tts, parallelwavegan, wavegrad, wavernn)\n- fast and efficient model training.\n- detailed training logs on the terminal and tensorboard.\n- support for multi-speaker tts.\n- efficient, flexible, lightweight but feature complete `trainer api`.\n- released and ready-to-use models.\n- tools to curate text2speech datasets under```dataset_analysis```.\n- utilities to use and test your models.\n- modular (but not too much) code base enabling easy implementation of new ideas.\n\n## model implementations\n### spectrogram models\n- tacotron: [paper](https://arxiv.org/abs/1703.10135)\n- tacotron2: [paper](https://arxiv.org/abs/1712.05884)\n- glow-tts: [paper](https://arxiv.org/abs/2005.11129)\n- speedy-speech: [paper](https://arxiv.org/abs/2008.03802)\n- align-tts: [paper](https://arxiv.org/abs/2003.01950)\n- fastpitch: [paper](https://arxiv.org/pdf/2006.06873.pdf)\n- fastspeech: [paper](https://arxiv.org/abs/1905.09263)\n- fastspeech2: [paper](https://arxiv.org/abs/2006.04558)\n- sc-glowtts: [paper](https://arxiv.org/abs/2104.05557)\n- capacitron: [paper](https://arxiv.org/abs/1906.03402)\n- overflow: [paper](https://arxiv.org/abs/2211.06892)\n- neural hmm tts: [paper](https://arxiv.org/abs/2108.13320)\n- delightful tts: [paper](https://arxiv.org/abs/2110.12612)\n\n### end-to-end models\n- \u24e7tts: [blog](https://coqui.ai/blog/tts/open_xtts)\n- vits: [paper](https://arxiv.org/pdf/2106.06103)\n- \ud83d\udc38 yourtts: [paper](https://arxiv.org/abs/2112.02418)\n- \ud83d\udc22 tortoise: [orig. repo](https://github.com/neonbjb/tortoise-tts)\n- \ud83d\udc36 bark: [orig. repo](https://github.com/suno-ai/bark)\n\n### attention methods\n- guided attention: [paper](https://arxiv.org/abs/1710.08969)\n- forward backward decoding: [paper](https://arxiv.org/abs/1907.09006)\n- graves attention: [paper](https://arxiv.org/abs/1910.10288)\n- double decoder consistency: [blog](https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/)\n- dynamic convolutional attention: [paper](https://arxiv.org/pdf/1910.10288.pdf)\n- alignment network: [paper](https://arxiv.org/abs/2108.10447)\n\n### speaker encoder\n- ge2e: [paper](https://arxiv.org/abs/1710.10467)\n- angular loss: [paper](https://arxiv.org/pdf/2003.11982.pdf)\n\n### vocoders\n- melgan: [paper](https://arxiv.org/abs/1910.06711)\n- multibandmelgan: [paper](https://arxiv.org/abs/2005.05106)\n- parallelwavegan: [paper](https://arxiv.org/abs/1910.11480)\n- gan-tts discriminators: [paper](https://arxiv.org/abs/1909.11646)\n- wavernn: [origin](https://github.com/fatchord/wavernn/)\n- wavegrad: [paper](https://arxiv.org/abs/2009.00713)\n- hifigan: [paper](https://arxiv.org/abs/2010.05646)\n- univnet: [paper](https://arxiv.org/abs/2106.07889)\n\n### voice conversion\n- freevc: [paper](https://arxiv.org/abs/2210.15418)\n\nyou can also help us implement more models.\n\n## installation\n\ud83d\udc38tts is tested on ubuntu 18.04 with **python >= 3.9, < 3.12.**.\n\nif you are only interested in [synthesizing speech](https://tts.readthedocs.io/en/latest/inference.html) with the released \ud83d\udc38tts models, installing from pypi is the easiest option.\n\n```bash\npip install tts\n```\n\nif you plan to code or train models, clone \ud83d\udc38tts and install it locally.\n\n```bash\ngit clone https://github.com/coqui-ai/tts\npip install -e .[all,dev,notebooks]  # select the relevant extras\n```\n\nif you are on ubuntu (debian), you can also run following commands for installation.\n\n```bash\n$ make system-deps  # intended to be used on ubuntu (debian). let us know if you have a different os.\n$ make install\n```\n\nif you are on windows, \ud83d\udc51@guypaddock wrote installation instructions [here](https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system).\n\n\n## docker image\nyou can also try tts without install with the docker image.\nsimply run the following command and you will be able to run tts without installing it.\n\n```bash\ndocker run --rm -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu\npython3 tts/server/server.py --list_models #to get the list of available models\npython3 tts/server/server.py --model_name tts_models/en/vctk/vits # to start a server\n```\n\nyou can then enjoy the tts server [here](http://[::1]:5002/)\nmore details about the docker images (like gpu support) can be found [here](https://tts.readthedocs.io/en/latest/docker_images.html)\n\n\n## synthesizing speech by \ud83d\udc38tts\n\n### \ud83d\udc0d python api\n\n#### running a multi-speaker and multi-lingual model\n\n```python\nimport torch\nfrom tts.api import tts\n\n# get device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# list available \ud83d\udc38tts models\nprint(tts().list_models())\n\n# init tts\ntts = tts(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n\n# run tts\n# \u2757 since this model is multi-lingual voice cloning model, we must set the target speaker_wav and language\n# text to speech list of amplitude values as output\nwav = tts.tts(text=\"hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\")\n# text to speech to a file\ntts.tts_to_file(text=\"hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\n```\n\n#### running a single speaker model\n\n```python\n# init tts with the target model name\ntts = tts(model_name=\"tts_models/de/thorsten/tacotron2-ddc\", progress_bar=false).to(device)\n\n# run tts\ntts.tts_to_file(text=\"ich bin eine testnachricht.\", file_path=output_path)\n\n# example voice cloning with yourtts in english, french and portuguese\ntts = tts(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=false).to(device)\ntts.tts_to_file(\"this is voice cloning.\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\ntts.tts_to_file(\"c'est le clonage de la voix.\", speaker_wav=\"my/cloning/audio.wav\", language=\"fr-fr\", file_path=\"output.wav\")\ntts.tts_to_file(\"isso \u00e9 clonagem de voz.\", speaker_wav=\"my/cloning/audio.wav\", language=\"pt-br\", file_path=\"output.wav\")\n```\n\n#### example voice conversion\n\nconverting the voice in `source_wav` to the voice of `target_wav`\n\n```python\ntts = tts(model_name=\"voice_conversion_models/multilingual/vctk/freevc24\", progress_bar=false).to(\"cuda\")\ntts.voice_conversion_to_file(source_wav=\"my/source.wav\", target_wav=\"my/target.wav\", file_path=\"output.wav\")\n```\n\n#### example voice cloning together with the voice conversion model.\nthis way, you can clone voices by using any model in \ud83d\udc38tts.\n\n```python\n\ntts = tts(\"tts_models/de/thorsten/tacotron2-ddc\")\ntts.tts_with_vc_to_file(\n    \"wie sage ich auf italienisch, dass ich dich liebe?\",\n    speaker_wav=\"target/speaker.wav\",\n    file_path=\"output.wav\"\n)\n```\n\n#### example text to speech using **fairseq models in ~1100 languages** \ud83e\udd2f.\nfor fairseq models, use the following name format: `tts_models/<lang-iso_code>/fairseq/vits`.\nyou can find the language iso codes [here](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)\nand learn about the fairseq models [here](https://github.com/facebookresearch/fairseq/tree/main/examples/mms).\n\n```python\n# tts with on the fly voice conversion\napi = tts(\"tts_models/deu/fairseq/vits\")\napi.tts_with_vc_to_file(\n    \"wie sage ich auf italienisch, dass ich dich liebe?\",\n    speaker_wav=\"target/speaker.wav\",\n    file_path=\"output.wav\"\n)\n```\n\n### command-line `tts`\n\n<!-- begin-tts-readme -->\n\nsynthesize speech on command line.\n\nyou can either use your trained model or choose a model from the provided list.\n\nif you don't specify any models, then it uses ljspeech based english model.\n\n#### single speaker models\n\n- list provided models:\n\n  ```\n  $ tts --list_models\n  ```\n\n- get model info (for both tts_models and vocoder_models):\n\n  - query by type/name:\n    the model_info_by_name uses the name as it from the --list_models.\n    ```\n    $ tts --model_info_by_name \"<model_type>/<language>/<dataset>/<model_name>\"\n    ```\n    for example:\n    ```\n    $ tts --model_info_by_name tts_models/tr/common-voice/glow-tts\n    $ tts --model_info_by_name vocoder_models/en/ljspeech/hifigan_v2\n    ```\n  - query by type/idx:\n    the model_query_idx uses the corresponding idx from --list_models.\n\n    ```\n    $ tts --model_info_by_idx \"<model_type>/<model_query_idx>\"\n    ```\n\n    for example:\n\n    ```\n    $ tts --model_info_by_idx tts_models/3\n    ```\n\n  - query info for model info by full name:\n    ```\n    $ tts --model_info_by_name \"<model_type>/<language>/<dataset>/<model_name>\"\n    ```\n\n- run tts with default models:\n\n  ```\n  $ tts --text \"text for tts\" --out_path output/path/speech.wav\n  ```\n\n- run tts and pipe out the generated tts wav file data:\n\n  ```\n  $ tts --text \"text for tts\" --pipe_out --out_path output/path/speech.wav | aplay\n  ```\n\n- run a tts model with its default vocoder model:\n\n  ```\n  $ tts --text \"text for tts\" --model_name \"<model_type>/<language>/<dataset>/<model_name>\" --out_path output/path/speech.wav\n  ```\n\n  for example:\n\n  ```\n  $ tts --text \"text for tts\" --model_name \"tts_models/en/ljspeech/glow-tts\" --out_path output/path/speech.wav\n  ```\n\n- run with specific tts and vocoder models from the list:\n\n  ```\n  $ tts --text \"text for tts\" --model_name \"<model_type>/<language>/<dataset>/<model_name>\" --vocoder_name \"<model_type>/<language>/<dataset>/<model_name>\" --out_path output/path/speech.wav\n  ```\n\n  for example:\n\n  ```\n  $ tts --text \"text for tts\" --model_name \"tts_models/en/ljspeech/glow-tts\" --vocoder_name \"vocoder_models/en/ljspeech/univnet\" --out_path output/path/speech.wav\n  ```\n\n- run your own tts model (using griffin-lim vocoder):\n\n  ```\n  $ tts --text \"text for tts\" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n  ```\n\n- run your own tts and vocoder models:\n\n  ```\n  $ tts --text \"text for tts\" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n      --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json\n  ```\n\n#### multi-speaker models\n\n- list the available speakers and choose a <speaker_id> among them:\n\n  ```\n  $ tts --model_name \"<language>/<dataset>/<model_name>\"  --list_speaker_idxs\n  ```\n\n- run the multi-speaker tts model with the target speaker id:\n\n  ```\n  $ tts --text \"text for tts.\" --out_path output/path/speech.wav --model_name \"<language>/<dataset>/<model_name>\"  --speaker_idx <speaker_id>\n  ```\n\n- run your own multi-speaker tts model:\n\n  ```\n  $ tts --text \"text for tts\" --out_path output/path/speech.wav --model_path path/to/model.pth --config_path path/to/config.json --speakers_file_path path/to/speaker.json --speaker_idx <speaker_id>\n  ```\n\n### voice conversion models\n\n```\n$ tts --out_path output/path/speech.wav --model_name \"<language>/<dataset>/<model_name>\" --source_wav <path/to/speaker/wav> --target_wav <path/to/reference/wav>\n```\n\n<!-- end-tts-readme -->\n\n## directory structure\n```\n|- notebooks/       (jupyter notebooks for model evaluation, parameter selection and data analysis.)\n|- utils/           (common utilities.)\n|- tts\n    |- bin/             (folder for all the executables.)\n      |- train*.py                  (train your target model.)\n      |- ...\n    |- tts/             (text to speech models)\n        |- layers/          (model layer definitions)\n        |- models/          (model definitions)\n        |- utils/           (model specific utilities.)\n    |- speaker_encoder/ (speaker encoder models.)\n        |- (same)\n    |- vocoder/         (vocoder models.)\n        |- (same)\n```\n",
  "docs_url": null,
  "keywords": "",
  "license": "mpl-2.0",
  "name": "tts",
  "package_url": "https://pypi.org/project/TTS/",
  "project_url": "https://pypi.org/project/TTS/",
  "project_urls": {
    "Discussions": "https://github.com/coqui-ai/TTS/discussions",
    "Documentation": "https://github.com/coqui-ai/TTS/wiki",
    "Homepage": "https://github.com/coqui-ai/TTS",
    "Repository": "https://github.com/coqui-ai/TTS",
    "Tracker": "https://github.com/coqui-ai/TTS/issues"
  },
  "release_url": "https://pypi.org/project/TTS/0.22.0/",
  "requires_dist": [
    "cython >=0.29.30",
    "scipy >=1.11.2",
    "torch >=2.1",
    "torchaudio",
    "soundfile >=0.12.0",
    "librosa >=0.10.0",
    "scikit-learn >=1.3.0",
    "inflect >=5.6.0",
    "tqdm >=4.64.1",
    "anyascii >=0.3.0",
    "pyyaml >=6.0",
    "fsspec >=2023.6.0",
    "aiohttp >=3.8.1",
    "packaging >=23.1",
    "flask >=2.0.1",
    "pysbd >=0.3.4",
    "umap-learn >=0.5.1",
    "pandas <2.0,>=1.4",
    "matplotlib >=3.7.0",
    "trainer >=0.0.32",
    "coqpit >=0.0.16",
    "jieba",
    "pypinyin",
    "hangul-romanize",
    "gruut[de,es,fr] ==2.2.3",
    "jamo",
    "nltk",
    "g2pkk >=0.1.1",
    "bangla",
    "bnnumerizer",
    "bnunicodenormalizer",
    "einops >=0.6.0",
    "transformers >=4.33.0",
    "encodec >=0.1.1",
    "unidecode >=1.3.2",
    "num2words",
    "spacy[ja] >=3",
    "numba ==0.55.1 ; python_version < \"3.9\"",
    "numpy ==1.22.0 ; python_version <= \"3.10\"",
    "numpy >=1.24.3 ; python_version > \"3.10\"",
    "numba >=0.57.0 ; python_version >= \"3.9\"",
    "black ; extra == 'all'",
    "coverage ; extra == 'all'",
    "isort ; extra == 'all'",
    "nose2 ; extra == 'all'",
    "pylint ==2.10.2 ; extra == 'all'",
    "bokeh ==1.4.0 ; extra == 'all'",
    "mecab-python3 ==1.0.6 ; extra == 'all'",
    "unidic-lite ==1.0.8 ; extra == 'all'",
    "cutlet ; extra == 'all'",
    "black ; extra == 'dev'",
    "coverage ; extra == 'dev'",
    "isort ; extra == 'dev'",
    "nose2 ; extra == 'dev'",
    "pylint ==2.10.2 ; extra == 'dev'",
    "mecab-python3 ==1.0.6 ; extra == 'ja'",
    "unidic-lite ==1.0.8 ; extra == 'ja'",
    "cutlet ; extra == 'ja'",
    "bokeh ==1.4.0 ; extra == 'notebooks'"
  ],
  "requires_python": ">=3.9.0, <3.12",
  "summary": "deep learning for text to speech by coqui.",
  "version": "0.22.0",
  "releases": [],
  "developers": [
    "egolge@coqui.ai"
  ],
  "kwds": "xtts_v2 tts_models xtts open_xtts voice_conversion_models",
  "license_kwds": "mpl-2.0",
  "libtype": "pypi",
  "id": "pypi_tts",
  "homepage": "https://github.com/coqui-ai/tts",
  "release_count": 86,
  "dependency_ids": [
    "pypi_aiohttp",
    "pypi_anyascii",
    "pypi_bangla",
    "pypi_black",
    "pypi_bnnumerizer",
    "pypi_bnunicodenormalizer",
    "pypi_bokeh",
    "pypi_coqpit",
    "pypi_coverage",
    "pypi_cutlet",
    "pypi_cython",
    "pypi_einops",
    "pypi_encodec",
    "pypi_flask",
    "pypi_fsspec",
    "pypi_g2pkk",
    "pypi_gruut",
    "pypi_hangul_romanize",
    "pypi_inflect",
    "pypi_isort",
    "pypi_jamo",
    "pypi_jieba",
    "pypi_librosa",
    "pypi_matplotlib",
    "pypi_mecab_python3",
    "pypi_nltk",
    "pypi_nose2",
    "pypi_num2words",
    "pypi_numba",
    "pypi_numpy",
    "pypi_packaging",
    "pypi_pandas",
    "pypi_pylint",
    "pypi_pypinyin",
    "pypi_pysbd",
    "pypi_pyyaml",
    "pypi_scikit_learn",
    "pypi_scipy",
    "pypi_soundfile",
    "pypi_spacy",
    "pypi_torch",
    "pypi_torchaudio",
    "pypi_tqdm",
    "pypi_trainer",
    "pypi_transformers",
    "pypi_umap_learn",
    "pypi_unidecode",
    "pypi_unidic_lite"
  ]
}