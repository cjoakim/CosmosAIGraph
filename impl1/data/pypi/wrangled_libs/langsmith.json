{
  "classifiers": [
    "license :: osi approved :: mit license",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.9"
  ],
  "description": "# langsmith client sdk\n\nthis package contains the python client for interacting with the [langsmith platform](https://smith.langchain.com/).\n\nto install:\n\n```bash\npip install langsmith\n```\n\nlangsmith helps you and your team develop and evaluate language models and intelligent agents. it is compatible with any llm application and provides seamless integration with [langchain](https://github.com/hwchase17/langchain), a widely recognized open-source framework that simplifies the process for developers to create powerful language model applications.\n\n> **note**: you can enjoy the benefits of langsmith without using the langchain open-source packages! to get started with your own proprietary framework, set up your account and then skip to [logging traces outside langchain](#logging-traces-outside-langchain).\n\n> **cookbook:** for tutorials on how to get more value out of langsmith, check out the [langsmith cookbook](https://github.com/langchain-ai/langsmith-cookbook/tree/main) repo.\n\na typical workflow looks like:\n\n1. set up an account with langsmith.\n2. log traces.\n3. debug, create datasets, and evaluate runs.\n\nwe'll walk through these steps in more detail below.\n\n## 1. connect to langsmith\n\nsign up for [langsmith](https://smith.langchain.com/) using your github, discord accounts, or an email address and password. if you sign up with an email, make sure to verify your email address before logging in.\n\nthen, create a unique api key on the [settings page](https://smith.langchain.com/settings), which is found in the menu at the top right corner of the page.\n\nnote: save the api key in a secure location. it will not be shown again.\n\n## 2. log traces\n\nyou can log traces natively in your langchain application or using a langsmith runtree.\n\n### logging traces with langchain\n\nlangsmith seamlessly integrates with the python langchain library to record traces from your llm applications.\n\n1. **copy the environment variables from the settings page and add them to your application.**\n\ntracing can be activated by setting the following environment variables or by manually specifying the langchaintracer.\n\n```python\nimport os\nos.environ[\"langchain_tracing_v2\"] = \"true\"\nos.environ[\"langchain_endpoint\"] = \"https://api.smith.langchain.com\"\nos.environ[\"langchain_api_key\"] = \"<your-langsmith-api-key>\"\n# os.environ[\"langchain_project\"] = \"my project name\" # optional: \"default\" is used if not set\n```\n\n> **tip:** projects are groups of traces. all runs are logged to a project. if not specified, the project is set to `default`.\n\n2. **run an agent, chain, or language model in langchain**\n\nif the environment variables are correctly set, your application will automatically connect to the langsmith platform.\n\n```python\nfrom langchain.chat_models import chatopenai\n\nchat = chatopenai()\nresponse = chat.predict(\n    \"translate this sentence from english to french. i love programming.\"\n)\nprint(response)\n```\n\n### logging traces outside langchain\n\n_note: this api is experimental and may change in the future_\n\nyou can still use the langsmith development platform without depending on any\nlangchain code. you can connect either by setting the appropriate environment variables,\nor by directly specifying the connection information in the runtree.\n\n1. **copy the environment variables from the settings page and add them to your application.**\n\n```python\nimport os\nos.environ[\"langchain_endpoint\"] = \"https://api.smith.langchain.com\"\nos.environ[\"langchain_api_key\"] = \"<your-langsmith-api-key>\"\n# os.environ[\"langchain_project\"] = \"my project name\" # optional: \"default\" is used if not set\n```\n\n2. **log traces**\n\nthe easiest way to log traces using the sdk is via the `@traceable` decorator. below is an example. \n\n```python\nfrom datetime import datetime\nfrom typing import list, optional, tuple\n\nimport openai\nfrom langsmith import traceable\n\n\n@traceable(run_type=\"llm\")\ndef call_openai(data: list[dict], model: str = \"gpt-3.5-turbo\", temperature: float = 0.0):\n    return openai.chatcompletion.create(\n        model=model,\n        messages=data,\n        temperature=temperature,\n    )\n\n\n@traceable(run_type=\"chain\")\ndef argument_generator(query: str, additional_description: str = \"\") -> str:\n    return call_openai(\n        [\n            {\"role\": \"system\", \"content\": f\"you are a debater making an argument on a topic.\"\n             f\"{additional_description}\"\n             f\" the current time is {datetime.now()}\"},\n            {\"role\": \"user\", \"content\": f\"the discussion topic is {query}\"}\n        ]\n    ).choices[0].message.content\n\n    \n\n@traceable(run_type=\"chain\")      \ndef argument_chain(query: str, additional_description: str = \"\") -> str:\n    argument = argument_generator(query, additional_description)\n    # ... do other processing or call other functions... \n    return argument\n\nargument_chain(\"why is blue better than orange?\")\n```\n\nalternatively, you can manually log events using the `client` directly or using a `runtree`, which is what the traceable decorator is meant to manage for you!\n\na runtree tracks your application. each runtree object is required to have a `name` and `run_type`. these and other important attributes are as follows:\n\n- `name`: `str` - used to identify the component's purpose\n- `run_type`: `str` - currently one of \"llm\", \"chain\" or \"tool\"; more options will be added in the future\n- `inputs`: `dict` - the inputs to the component\n- `outputs`: `optional[dict]` - the (optional) returned values from the component\n- `error`: `optional[str]` - any error messages that may have arisen during the call\n\n```python\nfrom langsmith.run_trees import runtree\n\nparent_run = runtree(\n    name=\"my chat bot\",\n    run_type=\"chain\",\n    inputs={\"text\": \"summarize this morning's meetings.\"},\n    serialized={},  # serialized representation of this chain\n    # project_name= \"defaults to the langchain_project env var\"\n    # api_url= \"defaults to the langchain_endpoint env var\"\n    # api_key= \"defaults to the langchain_api_key env var\"\n)\nparent_run.post()\n# .. my chat bot calls an llm\nchild_llm_run = parent_run.create_child(\n    name=\"my proprietary llm\",\n    run_type=\"llm\",\n    inputs={\n        \"prompts\": [\n            \"you are an ai assistant. the time is xyz.\"\n            \" summarize this morning's meetings.\"\n        ]\n    },\n)\nchild_llm_run.post()\nchild_llm_run.end(\n    outputs={\n        \"generations\": [\n            \"i should use the transcript_loader tool\"\n            \" to fetch meeting_transcripts from xyz\"\n        ]\n    }\n)\nchild_llm_run.patch()\n# ..  my chat bot takes the llm output and calls\n# a tool / function for fetching transcripts ..\nchild_tool_run = parent_run.create_child(\n    name=\"transcript_loader\",\n    run_type=\"tool\",\n    inputs={\"date\": \"xyz\", \"content_type\": \"meeting_transcripts\"},\n)\nchild_tool_run.post()\n# the tool returns meeting notes to the chat bot\nchild_tool_run.end(outputs={\"meetings\": [\"meeting1 notes..\"]})\nchild_tool_run.patch()\n\nchild_chain_run = parent_run.create_child(\n    name=\"unreliable component\",\n    run_type=\"tool\",\n    inputs={\"input\": \"summarize these notes...\"},\n)\nchild_chain_run.post()\n\ntry:\n    # .... the component does work\n    raise valueerror(\"something went wrong\")\n    child_chain_run.end(outputs={\"output\": \"foo\"}\n    child_chain_run.patch()\nexcept exception as e:\n    child_chain_run.end(error=f\"i errored again {e}\")\n    child_chain_run.patch()\n    pass\n# .. the chat agent recovers\n\nparent_run.end(outputs={\"output\": [\"the meeting notes are as follows:...\"]})\nres = parent_run.patch()\nres.result()\n```\n\n## create a dataset from existing runs\n\nonce your runs are stored in langsmith, you can convert them into a dataset.\nfor this example, we will do so using the client, but you can also do this using\nthe web interface, as explained in the [langsmith docs](https://docs.smith.langchain.com/docs/).\n\n```python\nfrom langsmith import client\n\nclient = client()\ndataset_name = \"example dataset\"\n# we will only use examples from the top level agentexecutor run here,\n# and exclude runs that errored.\nruns = client.list_runs(\n    project_name=\"my_project\",\n    execution_order=1,\n    error=false,\n)\n\ndataset = client.create_dataset(dataset_name, description=\"an example dataset\")\nfor run in runs:\n    client.create_example(\n        inputs=run.inputs,\n        outputs=run.outputs,\n        dataset_id=dataset.id,\n    )\n```\n\n## evaluating runs\n\ncheck out the [langsmith testing & evaluation dos](https://docs.smith.langchain.com/docs/evaluation/) for up-to-date workflows.\n\nfor generating automated feedback on individual runs, you can run evaluations directly using the langsmith client.\n\n```python\nfrom typing import optional\nfrom langsmith.evaluation import stringevaluator\n\n\ndef jaccard_chars(output: str, answer: str) -> float:\n    \"\"\"naive jaccard similarity between two strings.\"\"\"\n    prediction_chars = set(output.strip().lower())\n    answer_chars = set(answer.strip().lower())\n    intersection = prediction_chars.intersection(answer_chars)\n    union = prediction_chars.union(answer_chars)\n    return len(intersection) / len(union)\n\n\ndef grader(run_input: str, run_output: str, answer: optional[str]) -> dict:\n    \"\"\"compute the score and/or label for this run.\"\"\"\n    if answer is none:\n        value = \"ambiguous\"\n        score = 0.5\n    else:\n        score = jaccard_chars(run_output, answer)\n        value = \"correct\" if score > 0.9 else \"incorrect\"\n    return dict(score=score, value=value)\n\nevaluator = stringevaluator(evaluation_name=\"jaccard\", grading_function=grader)\n\nruns = client.list_runs(\n    project_name=\"my_project\",\n    execution_order=1,\n    error=false,\n)\nfor run in runs:\n    client.evaluate_run(run, evaluator)\n```\n\n## additional documentation\n\nto learn more about the langsmith platform, check out the [docs](https://docs.smith.langchain.com/docs/).\n\n",
  "docs_url": null,
  "keywords": "langsmith,langchain,llm,nlp,language,translation,evaluation,tracing,platform",
  "license": "mit",
  "name": "langsmith",
  "package_url": "https://pypi.org/project/langsmith/",
  "project_url": "https://pypi.org/project/langsmith/",
  "project_urls": {
    "Documentation": "https://docs.smith.langchain.com/",
    "Homepage": "https://smith.langchain.com/",
    "Repository": "https://github.com/langchain-ai/langsmith-sdk"
  },
  "release_url": "https://pypi.org/project/langsmith/0.0.75/",
  "requires_dist": [
    "pydantic (>=1,<3)",
    "requests (>=2,<3)"
  ],
  "requires_python": ">=3.8.1,<4.0",
  "summary": "client library to connect to the langsmith llm tracing and evaluation platform.",
  "version": "0.0.75",
  "releases": [],
  "developers": [
    "langchain",
    "support@langchain.dev"
  ],
  "kwds": "langchaintracer langchain_project langchain python langchain_endpoint",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_langsmith",
  "homepage": "https://smith.langchain.com/",
  "release_count": 75,
  "dependency_ids": [
    "pypi_pydantic",
    "pypi_requests"
  ]
}