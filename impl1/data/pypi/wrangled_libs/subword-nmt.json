{
  "classifiers": [
    "intended audience :: developers",
    "license :: osi approved :: mit license",
    "programming language :: python :: 2",
    "programming language :: python :: 3",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: text processing"
  ],
  "description": "subword neural machine translation\n==================================\n\nthis repository contains preprocessing scripts to segment text into subword\nunits. the primary purpose is to facilitate the reproduction of our experiments\non neural machine translation with subword units (see below for reference).\n\ninstallation\n------------\n\ninstall via pip (from pypi):\n\n    pip install subword-nmt\n\ninstall via pip (from github):\n\n    pip install https://github.com/rsennrich/subword-nmt/archive/master.zip\n\nalternatively, clone this repository; the scripts are executable stand-alone.\n\n\nusage instructions\n------------------\n\ncheck the individual files for usage instructions.\n\nto apply byte pair encoding to word segmentation, invoke these commands:\n\n    subword-nmt learn-bpe -s {num_operations} < {train_file} > {codes_file}\n    subword-nmt apply-bpe -c {codes_file} < {test_file} > {out_file}\n\nto segment rare words into character n-grams, do the following:\n\n    subword-nmt get-vocab --train_file {train_file} --vocab_file {vocab_file}\n    subword-nmt segment-char-ngrams --vocab {vocab_file} -n {order} --shortlist {size} < {test_file} > {out_file}\n\nthe original segmentation can be restored with a simple replacement:\n\n    sed -r 's/(@@ )|(@@ ?$)//g'\n\nif you cloned the repository and did not install a package, you can also run the individual commands as scripts:\n\n    ./subword_nmt/learn_bpe.py -s {num_operations} < {train_file} > {codes_file}\n\nbest practice advice for byte pair encoding in nmt\n--------------------------------------------------\n\nwe found that for languages that share an alphabet, learning bpe on the\nconcatenation of the (two or more) involved languages increases the consistency\nof segmentation, and reduces the problem of inserting/deleting characters when\ncopying/transliterating names.\n\nhowever, this introduces undesirable edge cases in that a word may be segmented\nin a way that has only been observed in the other language, and is thus unknown\nat test time. to prevent this, `apply_bpe.py` accepts a `--vocabulary` and a\n`--vocabulary-threshold` option so that the script will only produce symbols\nwhich also appear in the vocabulary (with at least some frequency).\n\nto use this functionality, we recommend the following recipe (assuming l1 and l2\nare the two languages):\n\nlearn byte pair encoding on the concatenation of the training text, and get resulting vocabulary for each:\n\n    cat {train_file}.l1 {train_file}.l2 | subword-nmt learn-bpe -s {num_operations} -o {codes_file}\n    subword-nmt apply-bpe -c {codes_file} < {train_file}.l1 | subword-nmt get-vocab > {vocab_file}.l1\n    subword-nmt apply-bpe -c {codes_file} < {train_file}.l2 | subword-nmt get-vocab > {vocab_file}.l2\n\nmore conventiently, you can do the same with with this command:\n\n    subword-nmt learn-joint-bpe-and-vocab --input {train_file}.l1 {train_file}.l2 -s {num_operations} -o {codes_file} --write-vocabulary {vocab_file}.l1 {vocab_file}.l2\n\nre-apply byte pair encoding with vocabulary filter:\n\n    subword-nmt apply-bpe -c {codes_file} --vocabulary {vocab_file}.l1 --vocabulary-threshold 50 < {train_file}.l1 > {train_file}.bpe.l1\n    subword-nmt apply-bpe -c {codes_file} --vocabulary {vocab_file}.l2 --vocabulary-threshold 50 < {train_file}.l2 > {train_file}.bpe.l2\n\nas a last step, extract the vocabulary to be used by the neural network. example with nematus:\n\n    nematus/data/build_dictionary.py {train_file}.bpe.l1 {train_file}.bpe.l2\n\n[you may want to take the union of all vocabularies to support multilingual systems]\n\nfor test/dev data, re-use the same options for consistency:\n\n    subword-nmt apply-bpe -c {codes_file} --vocabulary {vocab_file}.l1 --vocabulary-threshold 50 < {test_file}.l1 > {test_file}.bpe.l1\n\nadvanced features\n-----------------\n\non top of the basic bpe implementation, this repository supports:\n\n- bpe dropout (provilkov, emelianenko and voita, 2019): https://arxiv.org/abs/1910.13267\n  use the argument `--dropout 0.1` for `subword-nmt apply-bpe` to randomly drop out possible merges.\n  doing this on the training corpus can improve quality of the final system; at test time, use bpe without dropout.\n  in order to obtain reproducible results, argument `--seed` can be used to set the random seed.\n  \n  **note:** in the original paper, the authors used bpe-dropout on each new batch separately. you can copy the training corpus several times to get similar behavior to obtain multiple segmentations for the same sentence.\n\n- support for glossaries:\n  use the argument `--glossaries` for `subword-nmt apply-bpe` to provide a list of words and/or regular expressions\n  that should always be passed to the output without subword segmentation\n\npublications\n------------\n\nthe segmentation methods are described in:\n\nrico sennrich, barry haddow and alexandra birch (2016):\n    neural machine translation of rare words with subword units\n    proceedings of the 54th annual meeting of the association for computational linguistics (acl 2016). berlin, germany.\n\nhow implementation differs from sennrich et al. (2016)\n------------------------------------------------------\n\nthis repository implements the subword segmentation as described in sennrich et al. (2016),\nbut since version 0.2, there is one core difference related to end-of-word tokens.\n\nin sennrich et al. (2016), the end-of-word token `</w>` is initially represented as a separate token, which can be merged with other subwords over time:\n\n```\nu n d </w>\nf u n d </w>\n```\n\nsince 0.2, end-of-word tokens are initially concatenated with the word-final character:\n\n```\nu n d</w>\nf u n d</w>\n```\n\nthe new representation ensures that when bpe codes are learned from the above examples and then applied to new text, it is clear that a subword unit `und` is unambiguously word-final, and `un` is unambiguously word-internal, preventing the production of up to two different subword units from each bpe merge operation.\n\n`apply_bpe.py` is backward-compatible and continues to accept old-style bpe files. new-style bpe files are identified by having the following first line: `#version: 0.2`\n\nacknowledgments\n---------------\nthis project has received funding from samsung electronics polska sp. z o.o. - samsung r&d institute poland, and from the european union\u2019s horizon 2020 research and innovation programme under grant agreement 645452 (qt21).\n\n\nchangelog\n---------\n\nv0.3.8:\n  - multiprocessing support (get_vocab and apply_bpe)\n  - progress bar for learn_bpe\n  - seed parameter for deterministic bpe dropout\n  - ignore some unicode line separators which would crash subword-nmt\n\nv0.3.7:\n  - bpe dropout (provilkov et al., 2019)\n  - more efficient glossaries (https://github.com/rsennrich/subword-nmt/pull/69)\n\nv0.3.6:\n  - fix to subword-bpe command encoding\n\nv0.3.5:\n  - fix to subword-bpe command under python 2\n  - wider support of --total-symbols argument\n\nv0.3.4:\n  - segment_tokens method to improve library usability (https://github.com/rsennrich/subword-nmt/pull/52)\n  - support regex glossaries (https://github.com/rsennrich/subword-nmt/pull/56)\n  - allow unicode separators (https://github.com/rsennrich/subword-nmt/pull/57)\n  - new option --total-symbols in learn-bpe (commit 61ad8)\n  - fix documentation (best practices) (https://github.com/rsennrich/subword-nmt/pull/60)\n\nv0.3:\n - library is now installable via pip\n - fix occasional problems with utf-8 whitespace and new lines in learn_bpe and apply_bpe.\n   - do not silently convert utf-8 newline characters into \"\\n\"\n   - do not silently convert utf-8 whitespace characters into \" \"\n   - utf-8 whitespace and newline characters are now considered part of a word, and segmented by bpe\n\nv0.2:\n - different, more consistent handling of end-of-word token (commit a749a7) (https://github.com/rsennrich/subword-nmt/issues/19)\n - allow passing of vocabulary and frequency threshold to apply_bpe.py, preventing the production of oov (or rare) subword units (commit a00db)\n - made learn_bpe.py deterministic (commit 4c54e)\n - various changes to make handling of utf more consistent between python versions\n - new command line arguments for apply_bpe.py:\n   - '--glossaries' to prevent given strings from being affected by bpe\n   - '--merges' to apply a subset of learned bpe operations\n - new command line arguments for learn_bpe.py:\n   - '--dict-input': rather than raw text file, interpret input as a frequency dictionary (as created by get_vocab.py).\n\n\nv0.1:\n - consistent cross-version unicode handling\n - all scripts are now deterministic\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "subword-nmt",
  "package_url": "https://pypi.org/project/subword-nmt/",
  "project_url": "https://pypi.org/project/subword-nmt/",
  "project_urls": {
    "Homepage": "https://github.com/rsennrich/subword-nmt"
  },
  "release_url": "https://pypi.org/project/subword-nmt/0.3.8/",
  "requires_dist": [
    "mock",
    "tqdm"
  ],
  "requires_python": "",
  "summary": "unsupervised word segmentation for neural machine translation and text generation",
  "version": "0.3.8",
  "releases": [],
  "developers": [
    "rico_sennrich"
  ],
  "kwds": "subword_nmt subwords subword nmt corpus",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_subword_nmt",
  "homepage": "https://github.com/rsennrich/subword-nmt",
  "release_count": 8,
  "dependency_ids": [
    "pypi_mock",
    "pypi_tqdm"
  ]
}