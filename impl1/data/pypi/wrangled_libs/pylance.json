{
  "classifiers": [
    "development status :: 3 - alpha",
    "environment :: console",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "operating system :: os independent",
    "programming language :: python",
    "programming language :: python :: 3",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "programming language :: rust",
    "topic :: scientific/engineering"
  ],
  "description": "# python bindings for lance data format\n\n> :warning: **under heavy development**\n\n<div align=\"center\">\n<p align=\"center\">\n\n<img width=\"257\" alt=\"lance logo\" src=\"https://user-images.githubusercontent.com/917119/199353423-d3e202f7-0269-411d-8ff2-e747e419e492.png\">\n\nlance is a new columnar data format for data science and machine learning\n</p></div>\n\nwhy you should use lance\n1. is order of magnitude faster than parquet for point queries and nested data structures common to ds/ml\n2. comes with a fast vector index that delivers sub-millisecond nearest neighbors search performance\n3. is automatically versioned and supports lineage and time-travel for full reproducibility\n4. integrated with duckdb/pandas/polars already. easily convert from/to parquet in 2 lines of code\n\n\n## quick start\n\n**installation**\n\n```shell\npip install pylance\n```\n\nmake sure you have a recent version of pandas (1.5+), pyarrow (10.0+), and duckdb (0.7.0+)\n\n**converting to lance**\n```python\nimport lance\n\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.dataset\n\ndf = pd.dataframe({\"a\": [5], \"b\": [10]})\nuri = \"/tmp/test.parquet\"\ntbl = pa.table.from_pandas(df)\npa.dataset.write_dataset(tbl, uri, format='parquet')\n\nparquet = pa.dataset.dataset(uri, format='parquet')\nlance.write_dataset(parquet, \"/tmp/test.lance\")\n```\n\n**reading lance data**\n```python\ndataset = lance.dataset(\"/tmp/test.lance\")\nassert isinstance(dataset, pa.dataset.dataset)\n```\n\n**pandas**\n```python\ndf = dataset.to_table().to_pandas()\n```\n\n**duckdb**\n```python\nimport duckdb\n\n# if this segfaults, make sure you have duckdb v0.7+ installed\nduckdb.query(\"select * from dataset limit 10\").to_df()\n```\n\n**vector search**\n\ndownload the sift1m subset\n\n```shell\nwget ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz\ntar -xzf sift.tar.gz\n```\n\nconvert it to lance\n\n```python\nimport lance\nfrom lance.vector import vec_to_table\nimport numpy as np\nimport struct\n\nnvecs = 1000000\nndims = 128\nwith open(\"sift/sift_base.fvecs\", mode=\"rb\") as fobj:\n    buf = fobj.read()\n    data = np.array(struct.unpack(\"<128000000f\", buf[4 : 4 + 4 * nvecs * ndims])).reshape((nvecs, ndims))\n    dd = dict(zip(range(nvecs), data))\n\ntable = vec_to_table(dd)\nuri = \"vec_data.lance\"\nsift1m = lance.write_dataset(table, uri, max_rows_per_group=8192, max_rows_per_file=1024*1024)\n```\n\nbuild the index\n\n```python\nsift1m.create_index(\"vector\",\n                    index_type=\"ivf_pq\", \n                    num_partitions=256,  # ivf\n                    num_sub_vectors=16)  # pq\n```\n\nsearch the dataset\n\n```python\n# get top 10 similar vectors\nimport duckdb\n\ndataset = lance.dataset(uri)\n\n# sample 100 query vectors. if this segfaults, make sure you have duckdb v0.7+ installed\nsample = duckdb.query(\"select vector from dataset using sample 100\").to_df()\nquery_vectors = np.array([np.array(x) for x in sample.vector])\n\n# get nearest neighbors for all of them\nrs = [dataset.to_table(nearest={\"column\": \"vector\", \"k\": 10, \"q\": q})      \n      for q in query_vectors]\n```\n\n*more distance metrics, hnsw, and distributed support is on the roadmap\n\n\n## python package details\n\ninstall from pypi: `pip install pylance`  # >=0.3.0 is the new rust-based implementation\ninstall from source: `maturin develop` (under the `/python` directory)\nrun unit tests: `make test`\nrun integration tests: `make integtest`\n\nimport via: `import lance`\n\nthe python integration is done via pyo3 + custom python code:\n\n1. we make wrapper classes in rust for dataset/scanner/recordbatchreader that's exposed to python.\n2. these are then used by lancedataset / lancescanner implementations that extend pyarrow dataset/scanner for duckdb compat.\n3. data is delivered via the arrow c data interface\n\n## motivation\n\nwhy do we *need* a new format for data science and machine learning?\n\n### 1. reproducibility is a must-have\n\nversioning and experimentation support should be built into the dataset instead of requiring multiple tools.<br/>\nit should also be efficient and not require expensive copying everytime you want to create a new version.<br/>\nwe call this \"zero copy versioning\" in lance. it makes versioning data easy without increasing storage costs.\n\n### 2. cloud storage is now the default\n\nremote object storage is the default now for data science and machine learning and the performance characteristics of cloud are fundamentally different.<br/>\nlance format is optimized to be cloud native. common operations like filter-then-take can be order of magnitude faster\nusing lance than parquet, especially for ml data.\n\n### 3. vectors must be a first class citizen, not a separate thing\n\nthe majority of reasonable scale workflows should not require the added complexity and cost of a\nspecialized database just to compute vector similarity. lance integrates optimized vector indices\ninto a columnar format so no additional infrastructure is required to get low latency top-k similarity search.\n\n### 4. open standards is a requirement\n\nthe ds/ml ecosystem is incredibly rich and data *must be* easily accessible across different languages, tools, and environments.\nlance makes apache arrow integration its primary interface, which means conversions to/from is 2 lines of code, your\ncode does not need to change after conversion, and nothing is locked-up to force you to pay for vendor compute.\nwe need open-source not fauxpen-source.\n\n\n",
  "docs_url": null,
  "keywords": "data-format,data-science,machine-learning,arrow,data-analytics",
  "license": "",
  "name": "pylance",
  "package_url": "https://pypi.org/project/pylance/",
  "project_url": "https://pypi.org/project/pylance/",
  "project_urls": null,
  "release_url": "https://pypi.org/project/pylance/0.9.1/",
  "requires_dist": [
    "pyarrow>=12",
    "numpy>=1.22",
    "pandas>=1.4,<2.1; extra == 'tests'",
    "pytest; extra == 'tests'",
    "duckdb; extra == 'tests'",
    "ml_dtypes; extra == 'tests'",
    "polars[pyarrow,pandas]; extra == 'tests'",
    "semver; extra == 'tests'",
    "tensorflow; extra == 'tests'",
    "tqdm; extra == 'tests'",
    "pytest-benchmark; extra == 'benchmarks'",
    "torch; extra == 'torch'"
  ],
  "requires_python": ">=3.8",
  "summary": "python wrapper for lance columnar format",
  "version": "0.9.1",
  "releases": [],
  "developers": [
    "dev@lancedb.com"
  ],
  "kwds": "lance parquet columnar lancescanner lancedataset",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_pylance",
  "homepage": "",
  "release_count": 110,
  "dependency_ids": [
    "pypi_duckdb",
    "pypi_ml_dtypes",
    "pypi_numpy",
    "pypi_pandas",
    "pypi_polars",
    "pypi_pyarrow",
    "pypi_pytest",
    "pypi_pytest_benchmark",
    "pypi_semver",
    "pypi_tensorflow",
    "pypi_torch",
    "pypi_tqdm"
  ]
}