{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: gnu general public license (gpl)",
    "operating system :: os independent",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: education",
    "topic :: scientific/engineering",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: scientific/engineering :: image recognition",
    "topic :: software development :: libraries",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "<h1 align=\"center\">\n  packaged ultralytics/yolov5\n</h1>\n\n<h4 align=\"center\">\n  pip install yolov5\n</h4>\n\n<div align=\"center\">\n  <a href=\"https://pepy.tech/project/yolov5\"><img src=\"https://pepy.tech/badge/yolov5\" alt=\"total downloads\"></a>\n  <a href=\"https://pepy.tech/project/yolov5\"><img src=\"https://pepy.tech/badge/yolov5/month\" alt=\"monthly downloads\"></a>\n  <a href=\"https://twitter.com/fcakyon\"><img src=\"https://img.shields.io/badge/twitter-fcakyon_-blue?logo=twitter&style=flat\" alt=\"fcakyon twitter\"></a>\n  <br>\n  <a href=\"https://badge.fury.io/py/yolov5\"><img src=\"https://badge.fury.io/py/yolov5.svg?kill_cache=1\" alt=\"pypi version\"></a>\n  <a href=\"https://github.com/fcakyon/yolov5-pip/actions/workflows/ci.yml\"><img src=\"https://github.com/fcakyon/yolov5-pip/actions/workflows/ci.yml/badge.svg\" alt=\"ci testing\"></a>\n  <a href=\"https://github.com/fcakyon/yolov5-pip/actions/workflows/package_testing.yml\"><img src=\"https://github.com/fcakyon/yolov5-pip/actions/workflows/package_testing.yml/badge.svg\" alt=\"package testing\"></a>\n</div>\n\n## <div align=\"center\">overview</div>\n\n<div align=\"center\">\nyou can finally install <a href=\"https://github.com/ultralytics/yolov5\">yolov5 object detector</a> using <a href=\"https://pypi.org/project/yolov5/\">pip</a> and integrate into your project easily.\n\n<img src=\"https://user-images.githubusercontent.com/26833433/136901921-abcfcd9d-f978-4942-9b97-0e3f202907df.png\" width=\"1000\">\n</div>\n\n<br>\nthis yolov5 package contains everything from ultralytics/yolov5 <a href=\"https://github.com/ultralytics/yolov5/tree/5deff1471dede726f6399be43e7073ee7ed3a7d4\">at this commit</a> plus:\n<br>\n1. easy installation via pip: <b>pip install yolov5</b>\n<br>\n2. full cli integration with <a href=\"https://github.com/google/python-fire\">fire</a> package\n<br>\n3. coco dataset format support (for training)\n<br>\n4. full <a href=\"https://huggingface.co/models?other=yolov5\">\ud83e\udd17 hub</a> integration\n<br>\n5. <a href=\"https://aws.amazon.com/s3/\">s3</a> support (model and dataset upload)\n<br>\n6. <a href=\"https://neptune.ai/\">neptuneai</a> logger support (metric, model and dataset logging)\n<br>\n7. classwise ap logging during experiments\n\n\n\n## <div align=\"center\">install</div>\n\ninstall yolov5 using pip (for python >=3.7)\n\n```console\npip install yolov5\n```\n\n## <div align=\"center\">model zoo</div>\n\n\n\n<div align=\"center\">\n\neffortlessly explore and use finetuned yolov5 models with one line of code: <a href=\"https://github.com/keremberke/awesome-yolov5-models\">awesome-yolov5-models</a>\n\n<a href=\"https://github.com/keremberke/awesome-yolov5-models\"><img src=\"https://user-images.githubusercontent.com/34196005/210134158-108b24f4-2b8e-43ea-95c8-44731625cde2.gif\" width=\"640\"></a>\n</div>\n\n## <div align=\"center\">use from python</div>\n\n```python\nimport yolov5\n\n# load pretrained model\nmodel = yolov5.load('yolov5s.pt')\n\n# or load custom model\nmodel = yolov5.load('train/best.pt')\n  \n# set model parameters\nmodel.conf = 0.25  # nms confidence threshold\nmodel.iou = 0.45  # nms iou threshold\nmodel.agnostic = false  # nms class-agnostic\nmodel.multi_label = false  # nms multiple labels per box\nmodel.max_det = 1000  # maximum number of detections per image\n\n# set image\nimg = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n\n# perform inference\nresults = model(img)\n\n# inference with larger input size\nresults = model(img, size=1280)\n\n# inference with test time augmentation\nresults = model(img, augment=true)\n\n# parse results\npredictions = results.pred[0]\nboxes = predictions[:, :4] # x1, y1, x2, y2\nscores = predictions[:, 4]\ncategories = predictions[:, 5]\n\n# show detection bounding boxes on image\nresults.show()\n\n# save results into \"results/\" folder\nresults.save(save_dir='results/')\n\n```\n\n<details closed>\n<summary>train/detect/test/export</summary>\n\n- you can directly use these functions by importing them:\n\n```python\nfrom yolov5 import train, val, detect, export\n# from yolov5.classify import train, val, predict\n# from yolov5.segment import train, val, predict\n\ntrain.run(imgsz=640, data='coco128.yaml')\nval.run(imgsz=640, data='coco128.yaml', weights='yolov5s.pt')\ndetect.run(imgsz=640)\nexport.run(imgsz=640, weights='yolov5s.pt')\n```\n\n- you can pass any argument as input:\n\n```python\nfrom yolov5 import detect\n\nimg_url = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n\ndetect.run(source=img_url, weights=\"yolov5s6.pt\", conf_thres=0.25, imgsz=640)\n\n```\n\n</details>\n\n## <div align=\"center\">use from cli</div>\n\nyou can call `yolov5 train`, `yolov5 detect`, `yolov5 val` and `yolov5 export` commands after installing the package via `pip`:\n\n<details open>\n<summary>training</summary>\n\n- finetune one of the pretrained yolov5 models using your custom `data.yaml`:\n\n```bash\n$ yolov5 train --data data.yaml --weights yolov5s.pt --batch-size 16 --img 640\n                                          yolov5m.pt              8\n                                          yolov5l.pt              4\n                                          yolov5x.pt              2\n```\n\n- start a training using a coco formatted dataset:\n\n```yaml\n# data.yml\ntrain_json_path: \"train.json\"\ntrain_image_dir: \"train_image_dir/\"\nval_json_path: \"val.json\"\nval_image_dir: \"val_image_dir/\"\n```\n\n```bash\n$ yolov5 train --data data.yaml --weights yolov5s.pt\n```\n\n- train your model using [roboflow universe](https://universe.roboflow.com/) datasets (roboflow>=0.2.29 required):\n\n```bash\n$ yolov5 train --data dataset_universe_url --weights yolov5s.pt --roboflow_token your_roboflow_token\n```\n\nwhere `dataset_universe_url` must be in `https://universe.roboflow.com/workspace_name/project_name/project_version` format.\n\n- visualize your experiments via [neptune.ai](https://neptune.ai/) (neptune-client>=0.10.10 required):\n\n```bash\n$ yolov5 train --data data.yaml --weights yolov5s.pt --neptune_project namespace/project_name --neptune_token your_neptune_token\n```\n\n- automatically upload weights to [huggingface hub](https://huggingface.co/models?other=yolov5):\n\n```bash\n$ yolov5 train --data data.yaml --weights yolov5s.pt --hf_model_id username/modelname --hf_token your-hf-write-token\n```\n\n- automatically upload weights and datasets to aws s3 (with neptune.ai artifact tracking integration):\n\n```bash\nexport aws_access_key_id=your_key\nexport aws_secret_access_key=your_key\n```\n\n```bash\n$ yolov5 train --data data.yaml --weights yolov5s.pt --s3_upload_dir your_s3_folder_directory --upload_dataset\n```\n\n- add `yolo_s3_data_dir` into `data.yaml` to match neptune dataset with a present dataset in s3.\n\n```yaml\n# data.yml\ntrain_json_path: \"train.json\"\ntrain_image_dir: \"train_image_dir/\"\nval_json_path: \"val.json\"\nval_image_dir: \"val_image_dir/\"\nyolo_s3_data_dir: s3://bucket_name/data_dir/\n```\n\n</details>\n\n<details open>\n<summary>inference</summary>\n\nyolov5 detect command runs inference on a variety of sources, downloading models automatically from the [latest yolov5 release](https://github.com/ultralytics/yolov5/releases) and saving results to `runs/detect`.\n\n```bash\n$ yolov5 detect --source 0  # webcam\n                         file.jpg  # image\n                         file.mp4  # video\n                         path/  # directory\n                         path/*.jpg  # glob\n                         rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa  # rtsp stream\n                         rtmp://192.168.1.105/live/test  # rtmp stream\n                         http://112.50.243.8/pltv/88888888/224/3221225900/1.m3u8  # http stream\n```\n\n</details>\n\n<details open>\n<summary>export</summary>\n\nyou can export your fine-tuned yolov5 weights to any format such as `torchscript`, `onnx`, `coreml`, `pb`, `tflite`, `tfjs`:\n\n```bash\n$ yolov5 export --weights yolov5s.pt --include torchscript,onnx,coreml,pb,tfjs\n```\n\n</details>\n\n<details open>\n<summary>classify</summary>\n\ntrain/val/predict with yolov5 image classifier:\n\n```bash\n$ yolov5 classify train --img 640 --data mnist2560 --weights yolov5s-cls.pt --epochs 1\n```\n\n```bash\n$ yolov5 classify predict --img 640 --weights yolov5s-cls.pt --source images/\n```\n\n</details>\n\n<details open>\n<summary>segment</summary>\n\ntrain/val/predict with yolov5 instance segmentation model:\n\n```bash\n$ yolov5 segment train --img 640 --weights yolov5s-seg.pt --epochs 1\n```\n\n```bash\n$ yolov5 segment predict --img 640 --weights yolov5s-seg.pt --source images/\n```\n\n</details>\n",
  "docs_url": null,
  "keywords": "machine-learning,deep-learning,ml,pytorch,yolo,object-detection,vision,yolov5,yolov7",
  "license": "gpl",
  "name": "yolov5",
  "package_url": "https://pypi.org/project/yolov5/",
  "project_url": "https://pypi.org/project/yolov5/",
  "project_urls": {
    "Homepage": "https://github.com/fcakyon/yolov5-pip"
  },
  "release_url": "https://pypi.org/project/yolov5/7.0.13/",
  "requires_dist": [
    "gitpython >=3.1.30",
    "matplotlib >=3.3",
    "numpy >=1.18.5",
    "opencv-python >=4.1.1",
    "Pillow >=7.1.2",
    "psutil",
    "PyYAML >=5.3.1",
    "requests >=2.23.0",
    "scipy >=1.4.1",
    "thop >=0.1.1",
    "torch >=1.7.0",
    "torchvision >=0.8.1",
    "tqdm >=4.64.0",
    "ultralytics >=8.0.100",
    "tensorboard >=2.4.1",
    "pandas >=1.1.4",
    "seaborn >=0.11.0",
    "setuptools >=65.5.1",
    "fire",
    "boto3 >=1.19.1",
    "sahi >=0.11.10",
    "huggingface-hub >=0.12.0",
    "roboflow >=0.2.29"
  ],
  "requires_python": ">=3.7",
  "summary": "packaged version of the yolov5 object detector",
  "version": "7.0.13",
  "releases": [],
  "developers": [],
  "kwds": "yolov5 yolov5s yolov5s6 yolov7 yolov5l",
  "license_kwds": "gpl",
  "libtype": "pypi",
  "id": "pypi_yolov5",
  "homepage": "https://github.com/fcakyon/yolov5-pip",
  "release_count": 53,
  "dependency_ids": [
    "pypi_boto3",
    "pypi_fire",
    "pypi_gitpython",
    "pypi_huggingface_hub",
    "pypi_matplotlib",
    "pypi_numpy",
    "pypi_opencv_python",
    "pypi_pandas",
    "pypi_pillow",
    "pypi_psutil",
    "pypi_pyyaml",
    "pypi_requests",
    "pypi_roboflow",
    "pypi_sahi",
    "pypi_scipy",
    "pypi_seaborn",
    "pypi_setuptools",
    "pypi_tensorboard",
    "pypi_thop",
    "pypi_torch",
    "pypi_torchvision",
    "pypi_tqdm",
    "pypi_ultralytics"
  ]
}