{
  "classifiers": [
    "development status :: 5 - production/stable",
    "environment :: plugins",
    "intended audience :: developers",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "# airflow clickhouse plugin\n\n![pypi - downloads](https://img.shields.io/pypi/dm/airflow-clickhouse-plugin)\n![github workflow status](https://img.shields.io/github/actions/workflow/status/bryzgaloff/airflow-clickhouse-plugin/tests.yml?branch=master)\n![github contributors](https://img.shields.io/github/contributors/bryzgaloff/airflow-clickhouse-plugin?color=blue)\n\n\ud83d\udd1d the most popular [apache airflow][airflow] plugin for clickhouse, ranked in the top 1% of downloads [on pypi](https://pypi.org/project/airflow-clickhouse-plugin/). based on awesome [mymarilyn/clickhouse-driver][ch-driver].\n\nthis plugin provides two families of operators: richer [`clickhouse_driver.client.execute`-based](#clickhouse-driver-family) and standardized [compatible with python db api 2.0](#python-db-api-20-family).\n\nboth operators' families are fully supported and covered with tests for different versions of airflow and python.\n\n## `clickhouse-driver` family\n\n- `clickhouseoperator`\n- `clickhousehook`\n- `clickhousesensor`\n\nthese operators are based on [mymarilyn/clickhouse-driver][ch-driver]'s `client.execute` method and arguments. they offer a full functionality of `clickhouse-driver` and are recommended if you are starting fresh with clickhouse in airflow.\n\n### features\n\n- **sql templating**: sql queries and other parameters are templated.\n- **multiple sql queries**: execute run multiple sql queries within a single `clickhouseoperator`. the result of the last query is pushed to xcom (configurable by `do_xcom_push`).\n- **logging**: executed queries are logged in a visually pleasing format, making it easier to track and debug.\n- **efficient native clickhouse protocol**: utilizes efficient _native_ clickhouse tcp protocol, thanks to [clickhouse-driver][ch-driver-docs]. **does not support http protocol.**\n- **custom connection parameters**: supports additional clickhouse [connection parameters][ch-driver-connection], such as various timeouts, `compression`, `secure`, through the airflow [connection.extra][airflow-conn-extra] property.\n\nsee reference and examples [below](#usage).\n\n### installation and dependencies\n\n`pip install -u airflow-clickhouse-plugin`\n\ndependencies: only `apache-airflow` and `clickhouse-driver`.\n\n## python db api 2.0 family\n\n- operators:\n  - `clickhousesqlexecutequeryoperator`\n  - `clickhousesqlcolumncheckoperator`\n  - `clickhousesqltablecheckoperator`\n  - `clickhousesqlcheckoperator`\n  - `clickhousesqlvaluecheckoperator`\n  - `clickhousesqlintervalcheckoperator`\n  - `clickhousesqlthresholdcheckoperator`\n  - `clickhousebranchsqloperator`\n- `clickhousedbapihook`\n- `clickhousesqlsensor`\n\nthese operators combine [`clickhouse_driver.dbapi`][ch-driver-db-api] with [apache-airflow-providers-common-sql]. while they have limited functionality compared to `client.execute` (not all arguments are supported), they provide a standardized interface. this is useful when porting airflow pipelines to clickhouse from another sql provider backed by `common.sql` airflow package, such as mysql, postgres, bigquery, and others.\n\nthe feature set of this version is fully based on `common.sql` airflow provider: refer to its [reference][common-sql-reference] and [examples][common-sql-examples] for details.\n\nan example is also available [below](#db-api-20-clickhousesqlsensor-and-clickhousesqlexecutequeryoperator-example).\n\n### installation and dependencies\n\nadd `common.sql` extra when installing the plugin: `pip install -u airflow-clickhouse-plugin[common.sql]` \u2014 to enable db api 2.0 operators.\n\ndependencies: `apache-airflow-providers-common-sql` (usually pre-packed with airflow) in addition to `apache-airflow` and `clickhouse-driver`.\n\n## python and airflow versions support\n\ndifferent versions of the plugin support different combinations of python and\n    airflow versions. we _primarily_ support **airflow 2.0+ and python 3.8+**.\n    if you need to use the plugin with older python-airflow combinations, pick a\n    suitable plugin version:\n\n| airflow-clickhouse-plugin version | airflow version         | python version     |\n|-----------------------------------|-------------------------|--------------------|\n| 1.1.0                             | \\>=2.0.0,<2.8.0         | ~=3.8              |\n| 1.0.0                             | \\>=2.0.0,<2.7.0         | ~=3.8              |\n| 0.11.0                            | ~=2.0.0,\\>=2.2.0,<2.7.0 | ~=3.7              |\n| 0.10.0,0.10.1                     | ~=2.0.0,\\>=2.2.0,<2.6.0 | ~=3.7              |\n| 0.9.0,0.9.1                       | ~=2.0.0,\\>=2.2.0,<2.5.0 | ~=3.7              |\n| 0.8.2                             | \\>=2.0.0,<2.4.0         | ~=3.7              |\n| 0.8.0,0.8.1                       | \\>=2.0.0,<2.3.0         | ~=3.6              |\n| 0.7.0                             | \\>=2.0.0,<2.2.0         | ~=3.6              |\n| 0.6.0                             | ~=2.0.1                 | ~=3.6              |\n| \\>=0.5.4,<0.6.0                   | ~=1.10.6                | \\>=2.7 or >=3.5.\\* |\n| \\>=0.5.0,<0.5.4                   | ==1.10.6                | \\>=2.7 or >=3.5.\\* |\n\n`~=` means compatible release, see [pep 440][pep-440-compatible-releases] for an\n    explanation.\n\nprevious versions of the plugin might require `pandas` extra: `pip install airflow-clickhouse-plugin[pandas]==0.11.0`. check out earlier versions of `readme.md` for details.\n\n# usage\n\nto see examples [scroll down](#examples). to run them, [create an airflow connection to clickhouse](#how-to-create-an-airflow-connection-to-clickhouse).\n\n## clickhouseoperator reference\n\nto import `clickhouseoperator` use `from airflow_clickhouse_plugin.operators.clickhouse import clickhouseoperator`.\n\nsupported arguments:\n* `sql` (templated, required): query (if argument is a single `str`) or multiple queries (iterable of `str`). supports files with `.sql` extension.\n* `clickhouse_conn_id`: airflow connection id. connection schema is described [below](#clickhouse-connection-schema). default connection id is `clickhouse_default`.\n* arguments of [`clickhouse_driver.client.execute` method][ch-driver-execute-summary]:\n  * `parameters` (templated): passed `params` of the `execute` method. (renamed to avoid name conflict with airflow tasks' `params` argument.)\n    * `dict` for `select` queries.\n    * `list`/`tuple`/generator for `insert` queries.\n    * if multiple queries are provided via `sql` then the `parameters` are passed to _all_ of them.\n  * `with_column_types` (not templated).\n  * `external_tables` (templated).\n  * `query_id` (templated).\n  * `settings` (templated).\n  * `types_check` (not templated).\n  * `columnar` (not templated).\n  * for the documentation of these arguments, refer to [`clickhouse_driver.client.execute` api reference][ch-driver-execute-reference].\n* `database` (templated): if present, overrides `schema` of airflow connection.\n* other arguments (including a required `task_id`) are inherited from airflow [baseoperator][airflow-base-op].\n\nresult of the _last_ query is pushed to xcom (disable using `do_xcom_push=false` argument).\n\nin other words, the operator simply wraps [`clickhousehook.execute` method](#clickhousehook-reference).\n\nsee [example](#clickhouseoperator-example) below.\n\n## clickhousehook reference\n\nto import `clickhousehook` use `from airflow_clickhouse_plugin.hooks.clickhouse import clickhousehook`.\n\nsupported kwargs of constructor (`__init__` method):\n* `clickhouse_conn_id`: airflow connection id. connection schema is described [below](#clickhouse-connection-schema). default connection id is `clickhouse_default`.\n* `database`: if present, overrides `schema` of airflow connection.\n\ndefines `clickhousehook.execute` method which simply wraps [`clickhouse_driver.client.execute`][ch-driver-execute-reference]. it has all the same arguments, except of:\n* `sql` (instead of `execute`'s `query`): query (if argument is a single `str`) or multiple queries (iterable of `str`).\n\n`clickhousehook.execute` returns a result of the _last_ query.\n\nalso, the hook defines `get_conn()` method which returns an underlying [clickhouse_driver.client][ch-driver-client] instance.\n\nsee [example](#clickhousehook-example) below.\n\n## clickhousesensor reference\n\nto import `clickhousesensor` use `from airflow_clickhouse_plugin.sensors.clickhouse import clickhousesensor`.\n\nthis class wraps [`clickhousehook.execute` method](#clickhousehook-reference) into an [airflow sensor][airflow-sensor]. supports all the arguments of [`clickhouseoperator`](#clickhouseoperator-reference) and additionally:\n* `is_success`: a callable which accepts a single argument \u2014 a return value of `clickhousehook.execute`. if a return value of `is_success` is truthy, the sensor succeeds. by default, the callable is `bool`: i.e. if the return value of `clickhousehook.execute` is truthy, the sensor succeeds. usually, `execute` is a list of records returned by query: thus, by default it is falsy if no records are returned.\n* `is_failure`: a callable which accepts a single argument \u2014 a return value of `clickhousehook.execute`. if a return value of `is_failure` is truthy, the sensor raises `airflowexception`. by default, `is_failure` is `none` and no failure check is performed.\n\nsee [example](#clickhousesensor-example) below.\n\n## how to create an airflow connection to clickhouse\n\nas a `type` of a new connection, choose **sqlite** or any other sql database. there is **no** special clickhouse connection type yet, so we use any sql as the closest one.\n\nall the connection attributes are optional: default host is `localhost` and other credentials [have defaults](#default-values) defined by `clickhouse-driver`. if you use non-default values, set them according to the [connection schema](#clickhouse-connection-schema).\n\nif you use a secure connection to clickhouse (this requires additional configurations on clickhouse side), set `extra` to `{\"secure\":true}`. all `extra` connection parameters are passed to [`clickhouse_driver.client`][ch-driver-client] as-is.\n\n### clickhouse connection schema\n\n[`clickhouse_driver.client`][ch-driver-client] is initialized with attributes stored in airflow [connection attributes][airflow-connection-howto]:\n  \n| airflow connection attribute | `client.__init__` argument |\n|------------------------------|----------------------------|\n| `host`                       | `host`                     |\n| `port` (`int`)               | `port`                     |\n| `schema`                     | `database`                 |\n| `login`                      | `user`                     |\n| `password`                   | `password`                 |\n| `extra`                      | `**kwargs`                 |\n\n`database` argument of `clickhouseoperator`, `clickhousehook`, `clickhousesensor`, and others overrides `schema` attribute of the airflow connection.\n\n### extra arguments\n\nyou may set non-standard arguments of [`clickhouse_driver.client`][ch-driver-client], such as timeouts, `compression`, `secure`, etc. using airflow's [`connection.extra`][airflow-conn-extra] attribute. the attribute should contain a json object which will be [deserialized][airflow-conn-dejson] and all of its properties will be passed as-is to the `client`.\n\nfor example, if airflow connection contains `extra='{\"secure\": true}'` then the `client.__init__` will receive `secure=true` keyword argument in addition to other connection attributes.\n\n#### compression\n\nyou should install specific packages to support compression. for example, for lz4:\n\n```bash\npip3 install clickhouse-cityhash lz4\n```\n\nthen you should include `compression` parameter in airflow connection uri: `extra='{\"compression\":\"lz4\"}'`. you can get additional information about extra options from [official documentation of clickhouse-driver][ch-driver-pypi-install].\n\nconnection uri with compression will look like `clickhouse://login:password@host:port/?compression=lz4`.\n\nsee [official documentation][airflow-connection-howto] to learn more about connections management in airflow.\n\n### default values\n\nif some airflow connection attribute is not set, it is not passed to `clickhouse_driver.client`. in such cases, the plugin uses a default value from the corresponding [`clickhouse_driver.connection`][ch-driver-connection] argument. for instance, `user` defaults to `'default'`.\n\nthis means that the plugin itself does not define any default values for the clickhouse connection. you may fully rely on default values of the [clickhouse-driver][ch-driver] version you use.\n\nthe only exception is `host`: if the attribute of airflow connection is not set then `'localhost'` is used.\n\n### default connection\n\nby default, the plugin uses airflow connection with id `'clickhouse_default'`.\n\n## examples\n\n### clickhouseoperator example\n\n```python\nfrom airflow import dag\nfrom airflow_clickhouse_plugin.operators.clickhouse import clickhouseoperator\nfrom airflow.operators.python import pythonoperator\nfrom airflow.utils.dates import days_ago\n\nwith dag(\n        dag_id='update_income_aggregate',\n        start_date=days_ago(2),\n) as dag:\n    clickhouseoperator(\n        task_id='update_income_aggregate',\n        database='default',\n        sql=(\n            '''\n                insert into aggregate\n                select eventdt, sum(price * qty) as income from sales\n                where eventdt = '{{ ds }}' group by eventdt\n            ''', '''\n                optimize table aggregate on cluster {{ var.value.cluster_name }}\n                partition todate('{{ execution_date.format('%y-%m-01') }}')\n            ''', '''\n                select sum(income) from aggregate\n                where eventdt between\n                    '{{ execution_date.start_of('month').to_date_string() }}'\n                    and '{{ execution_date.end_of('month').to_date_string() }}'\n            ''',\n            # result of the last query is pushed to xcom\n        ),\n        # query_id is templated and allows to quickly identify query in clickhouse logs\n        query_id='{{ ti.dag_id }}-{{ ti.task_id }}-{{ ti.run_id }}-{{ ti.try_number }}',\n        clickhouse_conn_id='clickhouse_test',\n    ) >> pythonoperator(\n        task_id='print_month_income',\n        python_callable=lambda task_instance:\n            # pulling xcom value and printing it\n            print(task_instance.xcom_pull(task_ids='update_income_aggregate')),\n    )\n```\n\n### clickhousehook example\n\n```python\nfrom airflow import dag\nfrom airflow_clickhouse_plugin.hooks.clickhouse import clickhousehook\nfrom airflow.providers.sqlite.hooks.sqlite import sqlitehook\nfrom airflow.operators.python import pythonoperator\nfrom airflow.utils.dates import days_ago\n\n\ndef sqlite_to_clickhouse():\n    sqlite_hook = sqlitehook()\n    ch_hook = clickhousehook()\n    records = sqlite_hook.get_records('select * from some_sqlite_table')\n    ch_hook.execute('insert into some_ch_table values', records)\n\n\nwith dag(\n        dag_id='sqlite_to_clickhouse',\n        start_date=days_ago(2),\n) as dag:\n    dag >> pythonoperator(\n        task_id='sqlite_to_clickhouse',\n        python_callable=sqlite_to_clickhouse,\n    )\n```\n\nimportant note: don't try to insert values using `ch_hook.execute('insert into some_ch_table values (1)')` literal form. [`clickhouse-driver` requires][ch-driver-insert] values for `insert` query to be provided via `parameters` due to specifics of the native clickhouse protocol.\n\n### clickhousesensor example\n\n```python\nfrom airflow import dag\nfrom airflow_clickhouse_plugin.sensors.clickhouse import clickhousesensor\nfrom airflow_clickhouse_plugin.operators.clickhouse import clickhouseoperator\nfrom airflow.utils.dates import days_ago\n\n\nwith dag(\n        dag_id='listen_warnings',\n        start_date=days_ago(2),\n) as dag:\n    dag >> clickhousesensor(\n        task_id='poke_events_count',\n        database='monitor',\n        sql=\"select count() from warnings where eventdate = '{{ ds }}'\",\n        is_success=lambda cnt: cnt > 10000,\n    ) >> clickhouseoperator(\n        task_id='create_alert',\n        database='alerts',\n        sql='''\n            insert into events select eventdate, count()\n            from monitor.warnings where eventdate = '{{ ds }}'\n        ''',\n    )\n```\n\n### db api 2.0: clickhousesqlsensor and clickhousesqlexecutequeryoperator example\n\n```python\nfrom airflow import dag\nfrom airflow_clickhouse_plugin.sensors.clickhouse_dbapi import clickhousesqlsensor\nfrom airflow_clickhouse_plugin.operators.clickhouse_dbapi import clickhousesqlexecutequeryoperator\nfrom airflow.utils.dates import days_ago\n\n\nwith dag(\n        dag_id='listen_warnings',\n        start_date=days_ago(2),\n) as dag:\n    dag >> clickhousesqlsensor(\n        task_id='poke_events_count',\n        hook_params=dict(schema='monitor'),\n        sql=\"select count() from warnings where eventdate = '{{ ds }}'\",\n        success=lambda cnt: cnt > 10000,\n        conn_id=none,  # required by common.sql sqlsensor; use none for default\n    ) >> clickhousesqlexecutequeryoperator(\n        task_id='create_alert',\n        database='alerts',\n        sql='''\n            insert into events select eventdate, count()\n            from monitor.warnings where eventdate = '{{ ds }}'\n        ''',\n    )\n```\n\n# how to run tests\n\nunit tests: `python3 -m unittest discover -t tests -s unit`\n\nintegration tests require access to a clickhouse server. here is how to set up a local test environment using docker:\n* run clickhouse server in a local docker container: `docker run -p 9000:9000 --ulimit nofile=262144:262144 -it clickhouse/clickhouse-server`\n* run tests with airflow connection details set [via environment variable][airflow-conn-env]: `pythonpath=src airflow_conn_clickhouse_default=clickhouse://localhost python3 -m unittest discover -t tests -s integration`\n* stop the container after running the tests to deallocate its resources.\n\nrun all (unit&integration) tests with clickhouse connection defined: `pythonpath=src airflow_conn_clickhouse_default=clickhouse://localhost python3 -m unittest discover -s tests`\n\n## github actions\n\n[github action][github-action-src] is configured for this project.\n\n## run all tests inside docker\n\nstart a clickhouse server inside docker: `docker exec -it $(docker run --rm -d clickhouse/clickhouse-server) bash`\n\nthe above command will open `bash` inside the container.\n\ninstall dependencies into container and run tests (execute inside container):\n\n```bash\napt-get update\napt-get install -y python3 python3-pip git make\ngit clone https://github.com/whisklabs/airflow-clickhouse-plugin.git\ncd airflow-clickhouse-plugin\npython3 -m pip install -r requirements.txt\npythonpath=src airflow_conn_clickhouse_default=clickhouse://localhost python3 -m unittest discover -s tests\n```\n\nstop the container.\n\n# contributors\n\n* created by anton bryzgalov, [@bryzgaloff](https://github.com/bryzgaloff), originally at [whisk, samsung](https://github.com/whisklabs)\n* inspired by viktor taranenko, [@viktortnk](https://github.com/viktortnk) (whisk, samsung)\n\ncommunity contributors:\n\n* danila ganchar, [@d-ganchar](https://github.com/d-ganchar)\n* mikhail, [@glader](https://github.com/glader)\n* alexander chashnikov, [@ne1r0n](https://github.com/ne1r0n)\n* simone brundu, [@saimon46](https://github.com/saimon46)\n* [@gkarg](https://github.com/gkarg)\n* stanislav morozov, [@r3b-fish](https://github.com/r3b-fish)\n* sergey bychkov, [@sergeybychkov](https://github.com/sergeybychkov)\n* [@was-av](https://github.com/was-av)\n* maxim tarasov, [@maximtar](https://github.com/maximtar)\n* [@dvnrvn](https://github.com/dvnrvn)\n* giovanni corsetti, [@corsettis](https://github.com/corsettis)\n* dmytro zhyzniev, [@1ng4lipt](https://github.com/1ng4lipt)\n* anton bezdenezhnykh, [@gameram](https://github.com/gameram)\n\n\n[airflow]: https://airflow.apache.org/\n[ch-driver]: https://github.com/mymarilyn/clickhouse-driver\n[ch-driver-docs]: https://clickhouse-driver.readthedocs.io/en/latest/\n[ch-driver-execute-summary]: https://clickhouse-driver.readthedocs.io/en/latest/quickstart.html#selecting-data\n[ch-driver-execute-reference]: https://clickhouse-driver.readthedocs.io/en/latest/api.html#clickhouse_driver.client.execute\n[airflow-base-op]: https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/baseoperator/index.html\n[ch-driver-insert]: https://clickhouse-driver.readthedocs.io/en/latest/quickstart.html#inserting-data\n[ch-driver-client]: https://clickhouse-driver.readthedocs.io/en/latest/api.html#client\n[ch-driver-connection]: https://clickhouse-driver.readthedocs.io/en/latest/api.html#connection\n[airflow-conn-extra]: https://airflow.apache.org/docs/2.1.0/_api/airflow/models/connection/index.html#airflow.models.connection.connection.extra\n[airflow-connection-howto]: https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html\n[airflow-conn-dejson]: https://airflow.apache.org/docs/apache-airflow/2.1.0/_api/airflow/models/index.html?highlight=connection#airflow.models.connection.extra_dejson\n[airflow-conn-env]: https://airflow.apache.org/docs/apache-airflow/2.1.0/howto/connection.html#storing-a-connection-in-environment-variables\n[github-action-src]: https://github.com/whisklabs/airflow-clickhouse-plugin/tree/master/.github/workflows\n[pep-440-compatible-releases]: https://peps.python.org/pep-0440/#compatible-release\n[apache-airflow-providers-common-sql]: https://airflow.apache.org/docs/apache-airflow-providers-common-sql/stable/index.html\n[db-api-pep]: https://peps.python.org/pep-0249/\n[airflow-sensor]: https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/sensors.html\n[ch-driver-pypi-install]: https://clickhouse-driver.readthedocs.io/en/latest/installation.html#installation-pypi\n[common-sql-reference]: https://airflow.apache.org/docs/apache-airflow-providers-common-sql/stable/_api/airflow/providers/common/sql/index.html\n[common-sql-examples]: https://airflow.apache.org/docs/apache-airflow-providers-common-sql/stable/operators.html\n[ch-driver-db-api]: https://clickhouse-driver.readthedocs.io/en/latest/dbapi.html\n",
  "docs_url": null,
  "keywords": "clickhouse,airflow",
  "license": "mit license",
  "name": "airflow-clickhouse-plugin",
  "package_url": "https://pypi.org/project/airflow-clickhouse-plugin/",
  "project_url": "https://pypi.org/project/airflow-clickhouse-plugin/",
  "project_urls": {
    "Changelog": "https://github.com/bryzgaloff/airflow-clickhouse-plugin/releases",
    "Documentation": "https://github.com/bryzgaloff/airflow-clickhouse-plugin#airflow-clickhouse-plugin",
    "GitHub": "https://github.com/bryzgaloff/airflow-clickhouse-plugin",
    "Issues": "https://github.com/bryzgaloff/airflow-clickhouse-plugin/issues"
  },
  "release_url": "https://pypi.org/project/airflow-clickhouse-plugin/1.1.0/",
  "requires_dist": [
    "clickhouse-driver ~=0.2.0",
    "apache-airflow <2.8.0,>=2.0.0",
    "apache-airflow[common.sql] <2.8.0,>=2.2.0 ; extra == 'common.sql'",
    "clickhouse-driver >=0.2.1 ; extra == 'common.sql'"
  ],
  "requires_python": ">=3.8",
  "summary": "airflow-clickhouse-plugin \u2014 airflow plugin to execute clickhouse commands and queries",
  "version": "1.1.0",
  "releases": [],
  "developers": [
    "tony.bryzgaloff@gmail.com"
  ],
  "kwds": "airflow_clickhouse_plugin clickhouse_driver airflow_conn_clickhouse_default clickhouseoperator clickhousesqltablecheckoperator",
  "license_kwds": "mit license",
  "libtype": "pypi",
  "id": "pypi_airflow_clickhouse_plugin",
  "homepage": "",
  "release_count": 23,
  "dependency_ids": [
    "pypi_apache_airflow",
    "pypi_clickhouse_driver"
  ]
}