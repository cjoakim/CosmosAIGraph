{
  "classifiers": [
    "environment :: gpu :: nvidia cuda :: 11.8",
    "environment :: gpu :: nvidia cuda :: 12",
    "license :: osi approved :: mit license",
    "natural language :: english",
    "programming language :: c++",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "# autoawq\n\n<p align=\"center\">\n| <a href=\"https://github.com/casper-hansen/autoawq/issues/32\"><b>roadmap</b></a> | <a href=\"https://github.com/casper-hansen/autoawq/tree/main/examples\"><b>examples</b></a> | <a href=\"https://github.com/casper-hansen/autoawq/issues?q=is%3aopen+is%3aissue+label%3a%22help+wanted%22\"><b>issues: help wanted</b></a> |\n\n</p>\n<p align=\"center\">\n    <a href=\"https://huggingface.co/models?search=awq\">\n        <img alt=\"huggingface - models\" src=\"https://img.shields.io/badge/\ud83e\udd17_1000+_models_available-8a2be2\">\n    </a>\n    <a href=\"https://github.com/casper-hansen/autoawq/releases\">\n        <img alt=\"github - releases\" src=\"https://img.shields.io/github/release/casper-hansen/autoawq.svg\">\n    </a>\n    <a href=\"https://pypi.org/project/autoawq/\">\n        <img alt=\"pypi - downloads\" src=\"https://static.pepy.tech/badge/autoawq/month\">\n    </a>\n</p>\n\nautoawq is an easy-to-use package for 4-bit quantized models. autoawq speeds up models by 3x and reduces memory requirements by 3x compared to fp16. autoawq implements the activation-aware weight quantization (awq) algorithm for quantizing llms.  autoawq was created and improved upon from the [original work](https://github.com/mit-han-lab/llm-awq) from mit.\n\n*latest news* \ud83d\udd25\n- [2023/12] mixtral, llava, qwen, baichuan model support.\n- [2023/11] autoawq inference has been integrated into \ud83e\udd17 transformers. now includes cuda 12.1 wheels.\n- [2023/10] mistral (fused modules), bigcode, turing support, memory bug fix (saves 2gb vram)\n- [2023/09] 1.6x-2.5x speed boost on fused models (now including mpt and falcon).\n- [2023/09] multi-gpu support, bug fixes, and better benchmark scripts available\n- [2023/08] pypi package released and automodel class available\n\n## install\n\n### prerequisites\n\n- your gpu(s) must be of compute capability 7.5. turing and later architectures are supported.\n- your cuda version must be cuda 11.8 or later.\n\n### install from pypi\n\nto install the newest autoawq from pypi, you need cuda 12.1 installed.\n\n```\npip install autoawq\n```\n\nif you cannot use cuda 12.1, you can still use cuda 11.8 and install the wheel from the [latest release](https://github.com/casper-hansen/autoawq/releases).\n\n```\npip install https://github.com/casper-hansen/autoawq/releases/download/v0.1.6/autoawq-0.1.6+cu118-cp310-cp310-linux_x86_64.whl\n```\n\n### build from source\n\nbuild time can take 10-20 minutes. download your model while you install autoawq.\n\n```\ngit clone https://github.com/casper-hansen/autoawq\ncd autoawq\npip install -e .\n```\n\n## supported models\n\nthe detailed support list:\n\n| models   | sizes                       |\n| ---------| ----------------------------|\n| llama-2  | 7b/13b/70b                  |\n| llama    | 7b/13b/30b/65b              |\n| mistral  | 7b                          |\n| vicuna   | 7b/13b                      |\n| mpt      | 7b/30b                      |\n| falcon   | 7b/40b                      |\n| opt      | 125m/1.3b/2.7b/6.7b/13b/30b |\n| bloom    | 560m/3b/7b/                 |\n| gptj     | 6.7b                        |\n| aquila   | 7b                          |\n| aquila2  | 7b/34b                      |\n| yi       | 6b/34b                      |\n| qwen     | 1.8b/7b/14b/72b             |\n| bigcode  | 1b/7b/15b                   |\n| gpt neox | 20b                         |\n| gpt-j    | 6b                          |\n| llava    | 7b/13b                      |\n| mixtral  | 8x7b                        |\n| baichuan | 7b/13b                      |\n| qwen     | 1.8b/7b/14/72b              |\n\n## usage\n\nunder examples, you can find examples of how to quantize, run inference, and benchmark autoawq models.\n\n### int4 gemm vs int4 gemv vs fp16\n\nthere are two versions of awq: gemm and gemv. both names relate to how matrix multiplication runs under the hood. we suggest the following:\n\n- gemv (quantized): 20% faster than gemm, only batch size 1 (not good for large context).\n- gemm (quantized): much faster than fp16 at batch sizes below 8 (good with large contexts).\n- fp16 (non-quantized): recommended for highest throughput: [vllm](https://github.com/vllm-project/vllm).\n\n#### compute-bound vs memory-bound\n\nat small batch sizes with small 7b models, we are memory-bound. this means we are bound by the bandwidth our gpu has to push around the weights in memory, and this is essentially what limits how many tokens per second we can generate. being memory-bound is what makes quantized models faster because your weights are 3x smaller and can therefore be pushed around in memory much faster. this is different from being compute-bound where the main time spent during generation is doing matrix multiplication. \n\nin the scenario of being compute-bound, which happens at higher batch sizes, you will not gain a speed-up using a w4a16 quantized model because the overhead of dequantization will slow down the overall generation. this happens because awq quantized models only store the weights in int4 but perform fp16 operations during inference, so we are essentially converting int4 -> fp16 during inference.\n\n### fused modules\n\nfused modules are a large part of the speedup you get from autoawq. the idea is to combine multiple layers into a single operation, thus becoming more efficient. fused modules represent a set of custom modules that work separately from huggingface models. they are compatible with `model.generate()` and other huggingface methods, which comes with some inflexibility in how you can use your model if you activate fused modules:\n\n- fused modules are activated when you use `fuse_layers=true`.\n- a custom cache is implemented. it preallocates based on batch size and sequence length.\n    - you cannot change the sequence length after you have created your model.\n    - reference: `autoawqforcausallm.from_quantized(max_new_tokens=seq_len, batch_size=batch_size)`\n- the main accelerator in the fused modules comes from fastertransformer, which is only compatible with linux.\n- the `past_key_values` from `model.generate()` are only dummy values, so they cannot be used after generation.\n\n### examples\n\nmore examples can be found in the [examples directory](examples).\n\n<details>\n\n<summary>quantization</summary>\n\nexpect this to take 10-15 minutes on smaller 7b models, and around 1 hour for 70b models.\n\n```python\nfrom awq import autoawqforcausallm\nfrom transformers import autotokenizer\n\nmodel_path = 'lmsys/vicuna-7b-v1.5'\nquant_path = 'vicuna-7b-v1.5-awq'\nquant_config = { \"zero_point\": true, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"gemm\" }\n\n# load model\nmodel = autoawqforcausallm.from_pretrained(model_path)\ntokenizer = autotokenizer.from_pretrained(model_path, trust_remote_code=true)\n\n# quantize\nmodel.quantize(tokenizer, quant_config=quant_config)\n\n# save quantized model\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\n```\n\n</details>\n\n<details>\n\n<summary>inference</summary>\n\n```python\nfrom awq import autoawqforcausallm\nfrom transformers import autotokenizer, textstreamer\n\nquant_path = \"thebloke/zephyr-7b-beta-awq\"\n\n# load model\nmodel = autoawqforcausallm.from_quantized(quant_path, fuse_layers=true)\ntokenizer = autotokenizer.from_pretrained(quant_path, trust_remote_code=true)\nstreamer = textstreamer(tokenizer, skip_prompt=true, skip_special_tokens=true)\n\n# convert prompt to tokens\nprompt_template = \"\"\"\\\n<|system|>\n</s>\n<|user|>\n{prompt}</s>\n<|assistant|>\"\"\"\n\nprompt = \"you're standing on the surface of the earth. \"\\\n        \"you walk one mile south, one mile west and one mile north. \"\\\n        \"you end up exactly where you started. where are you?\"\n\ntokens = tokenizer(\n    prompt_template.format(prompt=prompt), \n    return_tensors='pt'\n).input_ids.cuda()\n\n# generate output\ngeneration_output = model.generate(\n    tokens, \n    streamer=streamer,\n    max_new_tokens=512\n)\n```\n\n</details>\n\n## benchmarks\n\nthese benchmarks showcase the speed and memory usage of processing context (prefill) and generating tokens (decoding). the results include speed at various batch sizes and different versions of awq kernels. we have aimed to test models fairly using the same benchmarking tool that you can use to reproduce the results. do note that speed may vary not only between gpus but also between cpus. what matters most is a gpu with high memory bandwidth and a cpu with high single core clock speed.\n\n- tested with autoawq version 0.1.6\n- gpu: rtx 4090 (amd ryzen 9 7950x)\n- command: `python examples/benchmark.py --model_path <hf_model> --batch_size 1`\n- \ud83d\udfe2 for gemv, \ud83d\udd35 for gemm, \ud83d\udd34 for avoid using\n\n| model name |  size    | version          | batch size | prefill length | decode length | prefill tokens/s | decode tokens/s | memory (vram)    |\n|------------|----------|------------------|------------|----------------|---------------|------------------|-----------------|------------------|\n| vicuna     |   7b     | \ud83d\udfe2gemv           | 1          | 64             | 64            | 639.65           | 198.848         | 4.50 gb (19.05%) |\n| vicuna     |   7b     | \ud83d\udfe2gemv           | 1          | 2048           | 2048          | 1123.63          | 133.191         | 6.15 gb (26.02%) |\n| ...        |   ...    | ...              | ...        | ...            | ...           | ...              | ...             | ...              |\n| mistral    |   7b     | \ud83d\udd35gemm           | 1          | 64             | 64            | 1093.35          | 156.317         | 4.35 gb (18.41%) |\n| mistral    |   7b     | \ud83d\udd35gemm           | 1          | 2048           | 2048          | 3897.02          | 114.355         | 5.55 gb (23.48%) |\n| mistral    |   7b     | \ud83d\udd35gemm           | 8          | 64             | 64            | 4199.18          | 1185.25         | 4.35 gb (18.41%) |\n| mistral    |   7b     | \ud83d\udd35gemm           | 8          | 2048           | 2048          | 3661.46          | 829.754         | 16.82 gb (71.12%)|\n| ...        |   ...    | ...              | ...        | ...            | ...           | ...              | ...             | ...              |\n| mistral    |   7b     | \ud83d\udfe2gemv           | 1          | 64             | 64            | 531.99           | 188.29          | 4.28 gb (18.08%) |\n| mistral    |   7b     | \ud83d\udfe2gemv           | 1          | 2048           | 2048          | 903.83           | 130.66          | 5.55 gb (23.48%) |\n| mistral    |   7b     | \ud83d\udd34gemv           | 8          | 64             | 64            | 897.87           | 486.46          | 4.33 gb (18.31%) |\n| mistral    |   7b     | \ud83d\udd34gemv           | 8          | 2048           | 2048          | 884.22           | 411.893         | 16.82 gb (71.12%)|\n| ...        |   ...    | ...              | ...        | ...            | ...           | ...              | ...             | ...              |\n| tinyllama  |   1b     | \ud83d\udfe2gemv           | 1          | 64             | 64            | 1088.63          | 548.993         | 0.86 gb (3.62%)  |\n| tinyllama  |   1b     | \ud83d\udfe2gemv           | 1          | 2048           | 2048          | 5178.98          | 431.468         | 2.10 gb (8.89%)  |\n| ...        |   ...    | ...              | ...        | ...            | ...           | ...              | ...             | ...              |\n| llama 2    |   13b    | \ud83d\udd35gemm           | 1          | 64             | 64            | 820.34           | 96.74           | 8.47 gb (35.83%) |\n| llama 2    |   13b    | \ud83d\udd35gemm           | 1          | 2048           | 2048          | 2279.41          | 73.8213         | 10.28 gb (43.46%)|\n| llama 2    |   13b    | \ud83d\udd35gemm           | 3          | 64             | 64            | 1593.88          | 286.249         | 8.57 gb (36.24%) |\n| llama 2    |   13b    | \ud83d\udd35gemm           | 3          | 2048           | 2048          | 2226.7           | 189.573         | 16.90 gb (71.47%)|\n| ...        |   ...    | ...              | ...        | ...            | ...           | ...              | ...             | ...              |\n| mpt        |   7b     | \ud83d\udd35gemm           | 1          | 64             | 64            | 1079.06          | 161.344         | 3.67 gb (15.51%) |\n| mpt        |   7b     | \ud83d\udd35gemm           | 1          | 2048           | 2048          | 4069.78          | 114.982         | 5.87 gb (24.82%) |\n| ...        |   ...    | ...              | ...        | ...            | ...           | ...              | ...             | ...              |\n| falcon     |   7b     | \ud83d\udd35gemm           | 1          | 64             | 64            | 1139.93          | 133.585         | 4.47 gb (18.92%) |\n| falcon     |   7b     | \ud83d\udd35gemm           | 1          | 2048           | 2048          | 2850.97          | 115.73          | 6.83 gb (28.88%) |\n| ...        |   ...    | ...              | ...        | ...            | ...           | ...              | ...             | ...              |\n| codellama  |   34b    | \ud83d\udd35gemm           | 1          | 64             | 64            | 681.74           | 41.01           | 19.05 gb (80.57%)|\n| codellama  |   34b    | \ud83d\udd35gemm           | 1          | 2048           | 2048          | 1072.36          | 35.8316         | 20.26 gb (85.68%)|\n| ...        |  ...     | ...              | ...        | ...            | ...           | ...              | ...             | ...              |\n| deepseek   |   33b    | \ud83d\udd35gemm           | 1          | 64             | 64            | 1160.18          | 40.29           | 18.92 gb (80.00%)|\n| deepseek   |   33b    | \ud83d\udd35gemm           | 1          | 2048           | 2048          | 1012.1           | 34.0093         | 19.87 gb (84.02%)|\n\n## reference\n\nif you find awq useful or relevant to your research, you can cite their [paper](https://arxiv.org/abs/2306.00978):\n\n```\n@article{lin2023awq,\n  title={awq: activation-aware weight quantization for llm compression and acceleration},\n  author={lin, ji and tang, jiaming and tang, haotian and yang, shang and dang, xingyu and han, song},\n  journal={arxiv},\n  year={2023}\n}\n```\n",
  "docs_url": null,
  "keywords": "awq,autoawq,quantization,transformers",
  "license": "mit",
  "name": "autoawq",
  "package_url": "https://pypi.org/project/autoawq/",
  "project_url": "https://pypi.org/project/autoawq/",
  "project_urls": {
    "Homepage": "https://github.com/casper-hansen/AutoAWQ"
  },
  "release_url": "https://pypi.org/project/autoawq/0.1.8/",
  "requires_dist": [
    "transformers >=4.35.0",
    "torch >=2.0.1",
    "tokenizers >=0.12.1",
    "accelerate",
    "sentencepiece",
    "lm-eval",
    "texttable",
    "toml",
    "attributedict",
    "protobuf",
    "torchvision",
    "tabulate"
  ],
  "requires_python": ">=3.8.0",
  "summary": "autoawq implements the awq algorithm for 4-bit quantization with a 2x speedup during inference.",
  "version": "0.1.8",
  "releases": [],
  "developers": [
    "casper_hansen"
  ],
  "kwds": "autoawq autoawqforcausallm awq automodel quant_config",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_autoawq",
  "homepage": "https://github.com/casper-hansen/autoawq",
  "release_count": 11,
  "dependency_ids": [
    "pypi_accelerate",
    "pypi_attributedict",
    "pypi_lm_eval",
    "pypi_protobuf",
    "pypi_sentencepiece",
    "pypi_tabulate",
    "pypi_texttable",
    "pypi_tokenizers",
    "pypi_toml",
    "pypi_torch",
    "pypi_torchvision",
    "pypi_transformers"
  ]
}