{
  "classifiers": [
    "development status :: 4 - beta",
    "intended audience :: developers",
    "license :: osi approved :: apache software license",
    "natural language :: english",
    "operating system :: os independent",
    "programming language :: python",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "programming language :: python :: implementation :: cpython",
    "programming language :: python :: implementation :: pypy",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "# dbnd airflow operator\n\nthis plugin was written to provide an explicit way of declaratively passing messages between two airflow operators.\n\nthis plugin was inspired by [aip-31](https://cwiki.apache.org/confluence/display/airflow/aip-31%3a+airflow+functional+dag+definition).\nessentially, this plugin connects between dbnd's implementation of tasks and pipelines to airflow operators.\n\nthis implementation uses xcom communication and xcom templates to transfer said messages.\nthis plugin is fully functional, however as soon as aip-31 is implemented it will support all edge-cases.\n\nfully tested on airflow 1.10.x.\n\n# code example\n\nhere is an example of how we achieve our goal:\n\n```python\nimport logging\nfrom typing import tuple\nfrom datetime import timedelta, datetime\nfrom airflow import dag\nfrom airflow.utils.dates import days_ago\nfrom airflow.operators.python_operator import pythonoperator\nfrom dbnd import task\n\n# define arguments that we will pass to our dag\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"depends_on_past\": false,\n    \"start_date\": days_ago(2),\n    \"retries\": 1,\n    \"retry_delay\": timedelta(seconds=10),\n}\n@task\ndef my_task(p_int=3, p_str=\"check\", p_int_with_default=0) -> str:\n    logging.info(\"i am running\")\n    return \"success\"\n\n\n@task\ndef my_multiple_outputs(p_str=\"some_string\") -> tuple[int, str]:\n    return (1, p_str + \"_extra_postfix\")\n\n\ndef some_python_function(input_path, output_path):\n    logging.error(\"i am running\")\n    input_value = open(input_path, \"r\").read()\n    with open(output_path, \"w\") as output_file:\n        output_file.write(input_value)\n        output_file.write(\"\\n\\n\")\n        output_file.write(str(datetime.now().strftime(\"%y-%m-%dt%h:%m:%s\")))\n    return \"success\"\n\n# define dag context\nwith dag(dag_id=\"dbnd_operators\", default_args=default_args) as dag_operators:\n    # t1, t2 and t3 are examples of tasks created by instantiating operators\n    # all tasks and operators created under this dag context will be collected as a part of this dag\n    t1 = my_task(2)\n    t2, t3 = my_multiple_outputs(t1)\n    python_op = pythonoperator(\n        task_id=\"some_python_function\",\n        python_callable=some_python_function,\n        op_kwargs={\"input_path\": t3, \"output_path\": \"/tmp/output.txt\"},\n    )\n    \"\"\"\n    t3.op describes the operator used to execute my_multiple_outputs\n    this call defines the some_python_function task's operator as dependent upon t3's operator\n    \"\"\"\n    python_op.set_upstream(t3.op)\n```\n\nas you can see, messages are passed explicitly between all three tasks:\n\n-   t1, the result of the first task is passed to the next task my_multiple_outputs\n-   t2 and t3 represent the results of my_multiple_outputs\n-   some_python_function is wrapped with an operator\n-   the new python operator is defined as dependent upon t3's execution (downstream) - explicitly.\n\n> note: if you run a function marked with the `@task` decorator without a dag context, and without using the dbnd\n> library to run it - it will execute absolutely normally!\n\nusing this method to pass arguments between tasks not only improves developer user-experience, but also allows\nfor pipeline execution support for many use-cases. it does not break currently existing dags.\n\n# using dbnd_config\n\nlet's look at the example again, but change the default_args defined at the very top:\n\n```python\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"depends_on_past\": false,\n    \"start_date\": days_ago(2),\n    \"retries\": 1,\n    \"retry_delay\": timedelta(minutes=5),\n    'dbnd_config': {\n        \"my_task.p_int_with_default\": 4\n    }\n}\n```\n\nadded a new key-value pair to the arguments called `dbnd_config`\n\n`dbnd_config` is expected to define a dictionary of configuration settings that you can pass to your tasks. for example,\nthe `dbnd_config` in this code section defines that the int parameter `p_int_with_default` passed to my_task will be\noverridden and changed to `4` from the default value `0`.\n\nto see further possibilities of changing configuration settings, see our [documentation](https://dbnd.readme.io/)\n",
  "docs_url": null,
  "keywords": "orchestration,data,machinelearning",
  "license": "",
  "name": "dbnd-airflow",
  "package_url": "https://pypi.org/project/dbnd-airflow/",
  "project_url": "https://pypi.org/project/dbnd-airflow/",
  "project_urls": {
    "Bug-Tracker": "https://github.com/databand-ai/dbnd/issues",
    "Documentation": "https://dbnd.readme.io/",
    "Homepage": "https://github.com/databand-ai/dbnd",
    "Source-Code": "https://github.com/databand-ai/dbnd"
  },
  "release_url": "https://pypi.org/project/dbnd-airflow/1.0.20.3/",
  "requires_dist": [
    "dbnd (==1.0.20.3)",
    "packaging",
    "pytest (==6.2.5) ; extra == 'tests'",
    "coverage (==7.0.1) ; extra == 'tests'",
    "pytest-cov (==3.0.0) ; extra == 'tests'",
    "boto3 ; extra == 'tests'",
    "mock ; extra == 'tests'",
    "sh ; extra == 'tests'"
  ],
  "requires_python": "",
  "summary": "machine learning orchestration",
  "version": "1.0.20.3",
  "releases": [],
  "developers": [
    "evgeny.shulman@databand.ai",
    "evgeny_shulman"
  ],
  "kwds": "airflow dbnd_operators dag_operators pythonoperator pipeline",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_dbnd_airflow",
  "homepage": "https://github.com/databand-ai/dbnd",
  "release_count": 555,
  "dependency_ids": [
    "pypi_boto3",
    "pypi_coverage",
    "pypi_dbnd",
    "pypi_mock",
    "pypi_packaging",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_sh"
  ]
}