{
  "classifiers": [
    "license :: osi approved :: mit license",
    "natural language :: english",
    "programming language :: python :: 2",
    "programming language :: python :: 2.7",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.5",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "programming language :: python :: implementation :: cpython"
  ],
  "description": "databricks-api\n==============\n\n**please switch to the official databricks sdk for python (https://github.com/databricks/databricks-sdk-py) by running the following command:**\n\n.. code-block:: bash\n\n    pip install databricks-sdk\n\n\n|pypi| |pyversions|\n\n.. |pypi| image:: https://img.shields.io/pypi/v/databricks-api.svg\n    :target: https://pypi.python.org/pypi/databricks-api\n\n.. |pyversions| image:: https://img.shields.io/pypi/pyversions/databricks-api.svg\n    :target: https://pypi.python.org/pypi/databricks-api\n\n*[this documentation is auto-generated]*\n\nthis package provides a simplified interface for the databricks rest api.\nthe interface is autogenerated on instantiation using the underlying client\nlibrary used in the official ``databricks-cli`` python package.\n\ninstall using\n\n.. code-block:: bash\n\n    pip install databricks-api\n    \n\nthe docs here describe the interface for version **0.17.0** of\nthe ``databricks-cli`` package for api version **2.0**.\n\nthe ``databricks-api`` package contains a ``databricksapi`` class which provides\ninstance attributes for the ``databricks-cli`` ``apiclient``, as well as each of\nthe available service instances. the attributes of a ``databricksapi`` instance are:\n\n* databricksapi.client *<databricks_cli.sdk.api_client.apiclient>*\n* databricksapi.jobs *<databricks_cli.sdk.service.jobsservice>*\n* databricksapi.cluster *<databricks_cli.sdk.service.clusterservice>*\n* databricksapi.policy *<databricks_cli.sdk.service.policyservice>*\n* databricksapi.managed_library *<databricks_cli.sdk.service.managedlibraryservice>*\n* databricksapi.dbfs *<databricks_cli.sdk.service.dbfsservice>*\n* databricksapi.workspace *<databricks_cli.sdk.service.workspaceservice>*\n* databricksapi.secret *<databricks_cli.sdk.service.secretservice>*\n* databricksapi.groups *<databricks_cli.sdk.service.groupsservice>*\n* databricksapi.token *<databricks_cli.sdk.service.tokenservice>*\n* databricksapi.instance_pool *<databricks_cli.sdk.service.instancepoolservice>*\n* databricksapi.delta_pipelines *<databricks_cli.sdk.service.deltapipelinesservice>*\n* databricksapi.repos *<databricks_cli.sdk.service.reposservice>*\n\nto instantiate the client, provide the databricks host and either a token or\nuser and password. also shown is the full signature of the\nunderlying ``apiclient.__init__``\n\n.. code-block:: python\n\n    from databricks_api import databricksapi\n\n    # provide a host and token\n    db = databricksapi(\n        host=\"example.cloud.databricks.com\",\n        token=\"dpapi123...\"\n    )\n\n    # or a host and user and password\n    db = databricksapi(\n        host=\"example.cloud.databricks.com\",\n        user=\"me@example.com\",\n        password=\"password\"\n    )\n\n    # full __init__ signature\n    db = databricksapi(\n        user=none,\n        password=none,\n        host=none,\n        token=none,\n        api_version='2.0',\n        default_headers={},\n        verify=true,\n        command_name='',\n        jobs_api_version=none\n    )\n\nrefer to the `official documentation <https://docs.databricks.com/api/index.html>`_\non the functionality and required arguments of each method below.\n\neach of the service instance attributes provides the following public methods:\n\ndatabricksapi.jobs\n------------------\n\n.. code-block:: python\n\n    db.jobs.cancel_run(\n        run_id,\n        headers=none,\n        version=none,\n    )\n\n    db.jobs.create_job(\n        name=none,\n        existing_cluster_id=none,\n        new_cluster=none,\n        libraries=none,\n        email_notifications=none,\n        timeout_seconds=none,\n        max_retries=none,\n        min_retry_interval_millis=none,\n        retry_on_timeout=none,\n        schedule=none,\n        notebook_task=none,\n        spark_jar_task=none,\n        spark_python_task=none,\n        spark_submit_task=none,\n        max_concurrent_runs=none,\n        tasks=none,\n        headers=none,\n        version=none,\n    )\n\n    db.jobs.delete_job(\n        job_id,\n        headers=none,\n        version=none,\n    )\n\n    db.jobs.delete_run(\n        run_id=none,\n        headers=none,\n        version=none,\n    )\n\n    db.jobs.export_run(\n        run_id,\n        views_to_export=none,\n        headers=none,\n        version=none,\n    )\n\n    db.jobs.get_job(\n        job_id,\n        headers=none,\n        version=none,\n    )\n\n    db.jobs.get_run(\n        run_id=none,\n        headers=none,\n        version=none,\n    )\n\n    db.jobs.get_run_output(\n        run_id,\n        headers=none,\n        version=none,\n    )\n\n    db.jobs.list_jobs(\n        job_type=none,\n        expand_tasks=none,\n        limit=none,\n        offset=none,\n        headers=none,\n        version=none,\n    )\n\n    db.jobs.list_runs(\n        job_id=none,\n        active_only=none,\n        completed_only=none,\n        offset=none,\n        limit=none,\n        headers=none,\n        version=none,\n    )\n\n    db.jobs.reset_job(\n        job_id,\n        new_settings,\n        headers=none,\n        version=none,\n    )\n\n    db.jobs.run_now(\n        job_id=none,\n        jar_params=none,\n        notebook_params=none,\n        python_params=none,\n        spark_submit_params=none,\n        python_named_params=none,\n        idempotency_token=none,\n        headers=none,\n        version=none,\n    )\n\n    db.jobs.submit_run(\n        run_name=none,\n        existing_cluster_id=none,\n        new_cluster=none,\n        libraries=none,\n        notebook_task=none,\n        spark_jar_task=none,\n        spark_python_task=none,\n        spark_submit_task=none,\n        timeout_seconds=none,\n        tasks=none,\n        headers=none,\n        version=none,\n    )\n\n\ndatabricksapi.cluster\n---------------------\n\n.. code-block:: python\n\n    db.cluster.create_cluster(\n        num_workers=none,\n        autoscale=none,\n        cluster_name=none,\n        spark_version=none,\n        spark_conf=none,\n        aws_attributes=none,\n        node_type_id=none,\n        driver_node_type_id=none,\n        ssh_public_keys=none,\n        custom_tags=none,\n        cluster_log_conf=none,\n        spark_env_vars=none,\n        autotermination_minutes=none,\n        enable_elastic_disk=none,\n        cluster_source=none,\n        instance_pool_id=none,\n        headers=none,\n    )\n\n    db.cluster.delete_cluster(\n        cluster_id,\n        headers=none,\n    )\n\n    db.cluster.edit_cluster(\n        cluster_id,\n        num_workers=none,\n        autoscale=none,\n        cluster_name=none,\n        spark_version=none,\n        spark_conf=none,\n        aws_attributes=none,\n        node_type_id=none,\n        driver_node_type_id=none,\n        ssh_public_keys=none,\n        custom_tags=none,\n        cluster_log_conf=none,\n        spark_env_vars=none,\n        autotermination_minutes=none,\n        enable_elastic_disk=none,\n        cluster_source=none,\n        instance_pool_id=none,\n        headers=none,\n    )\n\n    db.cluster.get_cluster(\n        cluster_id,\n        headers=none,\n    )\n\n    db.cluster.get_events(\n        cluster_id,\n        start_time=none,\n        end_time=none,\n        order=none,\n        event_types=none,\n        offset=none,\n        limit=none,\n        headers=none,\n    )\n\n    db.cluster.list_available_zones(headers=none)\n\n    db.cluster.list_clusters(headers=none)\n\n    db.cluster.list_node_types(headers=none)\n\n    db.cluster.list_spark_versions(headers=none)\n\n    db.cluster.permanent_delete_cluster(\n        cluster_id,\n        headers=none,\n    )\n\n    db.cluster.pin_cluster(\n        cluster_id,\n        headers=none,\n    )\n\n    db.cluster.resize_cluster(\n        cluster_id,\n        num_workers=none,\n        autoscale=none,\n        headers=none,\n    )\n\n    db.cluster.restart_cluster(\n        cluster_id,\n        headers=none,\n    )\n\n    db.cluster.start_cluster(\n        cluster_id,\n        headers=none,\n    )\n\n    db.cluster.unpin_cluster(\n        cluster_id,\n        headers=none,\n    )\n\n\ndatabricksapi.policy\n--------------------\n\n.. code-block:: python\n\n    db.policy.create_policy(\n        policy_name,\n        definition,\n        headers=none,\n    )\n\n    db.policy.delete_policy(\n        policy_id,\n        headers=none,\n    )\n\n    db.policy.edit_policy(\n        policy_id,\n        policy_name,\n        definition,\n        headers=none,\n    )\n\n    db.policy.get_policy(\n        policy_id,\n        headers=none,\n    )\n\n    db.policy.list_policies(headers=none)\n\n\ndatabricksapi.managed_library\n-----------------------------\n\n.. code-block:: python\n\n    db.managed_library.all_cluster_statuses(headers=none)\n\n    db.managed_library.cluster_status(\n        cluster_id,\n        headers=none,\n    )\n\n    db.managed_library.install_libraries(\n        cluster_id,\n        libraries=none,\n        headers=none,\n    )\n\n    db.managed_library.uninstall_libraries(\n        cluster_id,\n        libraries=none,\n        headers=none,\n    )\n\n\ndatabricksapi.dbfs\n------------------\n\n.. code-block:: python\n\n    db.dbfs.add_block(\n        handle,\n        data,\n        headers=none,\n    )\n\n    db.dbfs.add_block_test(\n        handle,\n        data,\n        headers=none,\n    )\n\n    db.dbfs.close(\n        handle,\n        headers=none,\n    )\n\n    db.dbfs.close_test(\n        handle,\n        headers=none,\n    )\n\n    db.dbfs.create(\n        path,\n        overwrite=none,\n        headers=none,\n    )\n\n    db.dbfs.create_test(\n        path,\n        overwrite=none,\n        headers=none,\n    )\n\n    db.dbfs.delete(\n        path,\n        recursive=none,\n        headers=none,\n    )\n\n    db.dbfs.delete_test(\n        path,\n        recursive=none,\n        headers=none,\n    )\n\n    db.dbfs.get_status(\n        path,\n        headers=none,\n    )\n\n    db.dbfs.get_status_test(\n        path,\n        headers=none,\n    )\n\n    db.dbfs.list(\n        path,\n        headers=none,\n    )\n\n    db.dbfs.list_test(\n        path,\n        headers=none,\n    )\n\n    db.dbfs.mkdirs(\n        path,\n        headers=none,\n    )\n\n    db.dbfs.mkdirs_test(\n        path,\n        headers=none,\n    )\n\n    db.dbfs.move(\n        source_path,\n        destination_path,\n        headers=none,\n    )\n\n    db.dbfs.move_test(\n        source_path,\n        destination_path,\n        headers=none,\n    )\n\n    db.dbfs.put(\n        path,\n        contents=none,\n        overwrite=none,\n        headers=none,\n        src_path=none,\n    )\n\n    db.dbfs.put_test(\n        path,\n        contents=none,\n        overwrite=none,\n        headers=none,\n        src_path=none,\n    )\n\n    db.dbfs.read(\n        path,\n        offset=none,\n        length=none,\n        headers=none,\n    )\n\n    db.dbfs.read_test(\n        path,\n        offset=none,\n        length=none,\n        headers=none,\n    )\n\n\ndatabricksapi.workspace\n-----------------------\n\n.. code-block:: python\n\n    db.workspace.delete(\n        path,\n        recursive=none,\n        headers=none,\n    )\n\n    db.workspace.export_workspace(\n        path,\n        format=none,\n        direct_download=none,\n        headers=none,\n    )\n\n    db.workspace.get_status(\n        path,\n        headers=none,\n    )\n\n    db.workspace.import_workspace(\n        path,\n        format=none,\n        language=none,\n        content=none,\n        overwrite=none,\n        headers=none,\n    )\n\n    db.workspace.list(\n        path,\n        headers=none,\n    )\n\n    db.workspace.mkdirs(\n        path,\n        headers=none,\n    )\n\n\ndatabricksapi.secret\n--------------------\n\n.. code-block:: python\n\n    db.secret.create_scope(\n        scope,\n        initial_manage_principal=none,\n        scope_backend_type=none,\n        backend_azure_keyvault=none,\n        headers=none,\n    )\n\n    db.secret.delete_acl(\n        scope,\n        principal,\n        headers=none,\n    )\n\n    db.secret.delete_scope(\n        scope,\n        headers=none,\n    )\n\n    db.secret.delete_secret(\n        scope,\n        key,\n        headers=none,\n    )\n\n    db.secret.get_acl(\n        scope,\n        principal,\n        headers=none,\n    )\n\n    db.secret.list_acls(\n        scope,\n        headers=none,\n    )\n\n    db.secret.list_scopes(headers=none)\n\n    db.secret.list_secrets(\n        scope,\n        headers=none,\n    )\n\n    db.secret.put_acl(\n        scope,\n        principal,\n        permission,\n        headers=none,\n    )\n\n    db.secret.put_secret(\n        scope,\n        key,\n        string_value=none,\n        bytes_value=none,\n        headers=none,\n    )\n\n\ndatabricksapi.groups\n--------------------\n\n.. code-block:: python\n\n    db.groups.add_to_group(\n        parent_name,\n        user_name=none,\n        group_name=none,\n        headers=none,\n    )\n\n    db.groups.create_group(\n        group_name,\n        headers=none,\n    )\n\n    db.groups.get_group_members(\n        group_name,\n        headers=none,\n    )\n\n    db.groups.get_groups(headers=none)\n\n    db.groups.get_groups_for_principal(\n        user_name=none,\n        group_name=none,\n        headers=none,\n    )\n\n    db.groups.remove_from_group(\n        parent_name,\n        user_name=none,\n        group_name=none,\n        headers=none,\n    )\n\n    db.groups.remove_group(\n        group_name,\n        headers=none,\n    )\n\n\ndatabricksapi.token\n-------------------\n\n.. code-block:: python\n\n    db.token.create_token(\n        lifetime_seconds=none,\n        comment=none,\n        headers=none,\n    )\n\n    db.token.list_tokens(headers=none)\n\n    db.token.revoke_token(\n        token_id,\n        headers=none,\n    )\n\n\ndatabricksapi.instance_pool\n---------------------------\n\n.. code-block:: python\n\n    db.instance_pool.create_instance_pool(\n        instance_pool_name=none,\n        min_idle_instances=none,\n        max_capacity=none,\n        aws_attributes=none,\n        node_type_id=none,\n        custom_tags=none,\n        idle_instance_autotermination_minutes=none,\n        enable_elastic_disk=none,\n        disk_spec=none,\n        preloaded_spark_versions=none,\n        headers=none,\n    )\n\n    db.instance_pool.delete_instance_pool(\n        instance_pool_id=none,\n        headers=none,\n    )\n\n    db.instance_pool.edit_instance_pool(\n        instance_pool_id,\n        instance_pool_name=none,\n        min_idle_instances=none,\n        max_capacity=none,\n        aws_attributes=none,\n        node_type_id=none,\n        custom_tags=none,\n        idle_instance_autotermination_minutes=none,\n        enable_elastic_disk=none,\n        disk_spec=none,\n        preloaded_spark_versions=none,\n        headers=none,\n    )\n\n    db.instance_pool.get_instance_pool(\n        instance_pool_id=none,\n        headers=none,\n    )\n\n    db.instance_pool.list_instance_pools(headers=none)\n\n\ndatabricksapi.delta_pipelines\n-----------------------------\n\n.. code-block:: python\n\n    db.delta_pipelines.create(\n        id=none,\n        name=none,\n        storage=none,\n        configuration=none,\n        clusters=none,\n        libraries=none,\n        trigger=none,\n        filters=none,\n        allow_duplicate_names=none,\n        headers=none,\n    )\n\n    db.delta_pipelines.delete(\n        pipeline_id=none,\n        headers=none,\n    )\n\n    db.delta_pipelines.deploy(\n        pipeline_id=none,\n        id=none,\n        name=none,\n        storage=none,\n        configuration=none,\n        clusters=none,\n        libraries=none,\n        trigger=none,\n        filters=none,\n        allow_duplicate_names=none,\n        headers=none,\n    )\n\n    db.delta_pipelines.get(\n        pipeline_id=none,\n        headers=none,\n    )\n\n    db.delta_pipelines.list(\n        pagination=none,\n        headers=none,\n    )\n\n    db.delta_pipelines.reset(\n        pipeline_id=none,\n        headers=none,\n    )\n\n    db.delta_pipelines.run(\n        pipeline_id=none,\n        headers=none,\n    )\n\n    db.delta_pipelines.start_update(\n        pipeline_id=none,\n        full_refresh=none,\n        headers=none,\n    )\n\n    db.delta_pipelines.stop(\n        pipeline_id=none,\n        headers=none,\n    )\n\n\ndatabricksapi.repos\n-------------------\n\n.. code-block:: python\n\n    db.repos.create_repo(\n        url,\n        provider,\n        path=none,\n        headers=none,\n    )\n\n    db.repos.delete_repo(\n        id,\n        headers=none,\n    )\n\n    db.repos.get_repo(\n        id,\n        headers=none,\n    )\n\n    db.repos.list_repos(\n        path_prefix=none,\n        next_page_token=none,\n        headers=none,\n    )\n\n    db.repos.update_repo(\n        id,\n        branch=none,\n        tag=none,\n        headers=none,\n    )\n\n\n",
  "docs_url": null,
  "keywords": "databricks,api,client",
  "license": "mit",
  "name": "databricks-api",
  "package_url": "https://pypi.org/project/databricks-api/",
  "project_url": "https://pypi.org/project/databricks-api/",
  "project_urls": {
    "Homepage": "https://github.com/crflynn/databricks-api",
    "Repository": "https://github.com/crflynn/databricks-api"
  },
  "release_url": "https://pypi.org/project/databricks-api/0.9.0/",
  "requires_dist": [
    "databricks-cli"
  ],
  "requires_python": ">=3.6,<4.0",
  "summary": "databricks api client auto-generated from the official databricks-cli package",
  "version": "0.9.0",
  "releases": [],
  "developers": [
    "christopher_flynn",
    "crf204@gmail.com"
  ],
  "kwds": "databricks_api databricks_cli databricks databricksapi spark_python_task",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_databricks_api",
  "homepage": "https://github.com/crflynn/databricks-api",
  "release_count": 10,
  "dependency_ids": [
    "pypi_databricks_cli"
  ]
}