{
  "classifiers": [
    "development status :: 5 - production/stable",
    "environment :: console",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: mit license",
    "operating system :: macos :: macos x",
    "programming language :: cython",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.12",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering"
  ],
  "description": "<a href=\"https://explosion.ai\"><img src=\"https://explosion.ai/assets/img/logo.svg\" width=\"125\" height=\"125\" align=\"right\" /></a>\n\n# thinc-apple-ops\n\nmake [spacy](https://spacy.io) and [thinc](https://thinc.ai) **up to 8 &times; faster**\non macos by calling into apple's native libraries.\n\n## \u23f3 install\n\nmake sure you have [xcode](https://developer.apple.com/xcode/) installed and\nthen install with `pip`:\n\n```bash\npip install thinc-apple-ops\n```\n\n## \ud83c\udfeb motivation\n\nmatrix multiplication is one of the primary operations in machine learning.\nsince matrix multiplication is computationally expensive, using a fast matrix\nmultiplication implementation can speed up training and prediction\nsignificantly.\n\nmost linear algebra libraries provide matrix multiplication in the form of the\nstandardized\n[blas](https://en.wikipedia.org/wiki/basic_linear_algebra_subprograms) `gemm`\nfunctions. the work behind scences is done by a set of matrix multiplication\nkernels that are meticulously tuned for specific architectures. matrix\nmultiplication kernels use architecture-specific\n[simd](https://en.wikipedia.org/wiki/simd) instructions for data-level parallism\nand can take factors such as cache sizes and intstruction latency into account.\n[thinc](https://github.com/explosion/thinc) uses the\n[blis](https://github.com/flame/blis) linear algebra library, which provides\noptimized matrix multiplication kernels for most x86_64 and some arm cpus.\n\nrecent [apple silicon](https://en.wikipedia.org/wiki/apple_silicon) cpus, such\nas the [m-series](https://en.wikipedia.org/wiki/apple_silicon#m_series) used in\nmacs, differ from traditional x86_64 and arm cpus in that they have a separate\nmatrix co-processor(s) called amx. since amx is not well-documented, it is\nunclear how many amx units apple m cpus have. it is certain that the (single)\nperformance cluster of the m1 has an amx unit and there is [empirical\nevidence](https://twitter.com/danieldekok/status/1454383754512945155?s=20) that\nboth performance clusters of the m1 pro/max have an amx unit.\n\n\neven though amx units use a set of [undocumented\ninstructions](https://gist.github.com/dougallj/7a75a3be1ec69ca550e7c36dc75e0d6f),\nthe units can be used through apple's\n[accelerate](https://developer.apple.com/documentation/accelerate) linear\nalgebra library. since accelerate implements the blas interface, it can be used\nas a replacement of the blis library that is used by thinc. this is where the\n`thinc-apple-ops` package comes in. `thinc-apple-ops` extends the default thinc\nops, so that `gemm` matrix multiplication from accelerate is used in place of\nthe blis implementation of `gemm`. as a result, matrix multiplication in thinc\nis performed on the fast amx unit(s).\n\n## \u23f1 benchmarks\n\nusing `thinc-apple-ops` leads to large speedups in prediction and training on\napple silicon macs, as shown by the benchmarks below.\n\n### prediction\n\nthis first benchmark compares prediction speed of the `de_core_news_lg` spacy\nmodel between the m1 with and without `thinc-apple-ops`. results for an intel\nmac mini and amd ryzen 5900x are also provided for comparison. results are in\nwords per second. in this prediction benchmark, using `thinc-apple-ops` improves\nperformance by **4.3** times.\n\n| *cpu*                      | *blis* | *thinc-apple-ops* | *package power (watt)* |\n| -------------------------- | -----: | ----------------: | ---------------------: |\n| mac mini (m1)              |   6492 |             27676 |                      5 |\n| macbook air core i5 2020   |   9790 |             10983 |                      9 |\n| mac mini core i7 late 2018 |  16364 |             14858 |                     31 |\n| amd ryzen 5900x            |  22568 |               n/a |                     52 |\n\n### training\n\nin the second benchmark, we compare the training speed of the `de_core_news_lg`\nspacy model (without ner). the results are in training iterations per second.\nusing `thinc-apple-ops` improves training time by **3.0** times.\n\n| *cpu*                      | *blis* | *thinc-apple-ops* | *package power (watt)* |\n| -------------------------- | -----: | ----------------: | ---------------------: |\n| mac mini m1 2020           |   3.34 |             10.07 |                      5 |\n| macbook air core i5 2020   |   3.10 |              3.27 |                     10 |\n| mac mini core i7 late 2018 |   4.71 |              4.93 |                     32 |\n| amd ryzen 5900x            |   6.53 |               n/a |                     53 |\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "thinc-apple-ops",
  "package_url": "https://pypi.org/project/thinc-apple-ops/",
  "project_url": "https://pypi.org/project/thinc-apple-ops/",
  "project_urls": {
    "Homepage": "https://github.com/explosion/thinc-apple-ops"
  },
  "release_url": "https://pypi.org/project/thinc-apple-ops/0.1.4/",
  "requires_dist": [
    "numpy >=1.21.0",
    "thinc <9.1.0,>=8.1.0"
  ],
  "requires_python": ">=3.7",
  "summary": "improve thinc's performance on apple devices with native libraries",
  "version": "0.1.4",
  "releases": [],
  "developers": [
    "contact@explosion.ai",
    "explosion"
  ],
  "kwds": "basic_linear_algebra_subprograms xcode apple_silicon matrix macs",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_thinc_apple_ops",
  "homepage": "https://github.com/explosion/thinc-apple-ops",
  "release_count": 13,
  "dependency_ids": [
    "pypi_numpy",
    "pypi_thinc"
  ]
}