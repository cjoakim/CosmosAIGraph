{
  "classifiers": [
    "development status :: 1 - planning",
    "intended audience :: developers",
    "intended audience :: education",
    "intended audience :: science/research",
    "license :: osi approved :: mit license",
    "programming language :: python :: 3",
    "topic :: scientific/engineering",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: scientific/engineering :: mathematics",
    "topic :: software development",
    "topic :: software development :: libraries",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "# [ctransformers](https://github.com/marella/ctransformers) [![pypi](https://img.shields.io/pypi/v/ctransformers)](https://pypi.org/project/ctransformers/) [![tests](https://github.com/marella/ctransformers/actions/workflows/tests.yml/badge.svg)](https://github.com/marella/ctransformers/actions/workflows/tests.yml) [![build](https://github.com/marella/ctransformers/actions/workflows/build.yml/badge.svg)](https://github.com/marella/ctransformers/actions/workflows/build.yml)\n\npython bindings for the transformer models implemented in c/c++ using [ggml](https://github.com/ggerganov/ggml) library.\n\n> also see [chatdocs](https://github.com/marella/chatdocs)\n\n- [supported models](#supported-models)\n- [installation](#installation)\n- [usage](#usage)\n  - [\ud83e\udd17 transformers](#transformers)\n  - [langchain](#langchain)\n  - [gpu](#gpu)\n  - [gptq](#gptq)\n- [documentation](#documentation)\n- [license](#license)\n\n## supported models\n\n| models              | model type    | cuda | metal |\n| :------------------ | ------------- | :--: | :---: |\n| gpt-2               | `gpt2`        |      |       |\n| gpt-j, gpt4all-j    | `gptj`        |      |       |\n| gpt-neox, stablelm  | `gpt_neox`    |      |       |\n| falcon              | `falcon`      |  \u2705  |       |\n| llama, llama 2      | `llama`       |  \u2705  |  \u2705   |\n| mpt                 | `mpt`         |  \u2705  |       |\n| starcoder, starchat | `gpt_bigcode` |  \u2705  |       |\n| dolly v2            | `dolly-v2`    |      |       |\n| replit              | `replit`      |      |       |\n\n## installation\n\n```sh\npip install ctransformers\n```\n\n## usage\n\nit provides a unified interface for all models:\n\n```py\nfrom ctransformers import automodelforcausallm\n\nllm = automodelforcausallm.from_pretrained(\"/path/to/ggml-model.bin\", model_type=\"gpt2\")\n\nprint(llm(\"ai is going to\"))\n```\n\n[run in google colab](https://colab.research.google.com/drive/1gmhymuav_tyzkpfvui1nirm8-9mcxqyl)\n\nto stream the output, set `stream=true`:\n\n```py\nfor text in llm(\"ai is going to\", stream=true):\n    print(text, end=\"\", flush=true)\n```\n\nyou can load models from hugging face hub directly:\n\n```py\nllm = automodelforcausallm.from_pretrained(\"marella/gpt-2-ggml\")\n```\n\nif a model repo has multiple model files (`.bin` or `.gguf` files), specify a model file using:\n\n```py\nllm = automodelforcausallm.from_pretrained(\"marella/gpt-2-ggml\", model_file=\"ggml-model.bin\")\n```\n\n<a id=\"transformers\"></a>\n\n### \ud83e\udd17 transformers\n\n> **note:** this is an experimental feature and may change in the future.\n\nto use it with \ud83e\udd17 transformers, create model and tokenizer using:\n\n```py\nfrom ctransformers import automodelforcausallm, autotokenizer\n\nmodel = automodelforcausallm.from_pretrained(\"marella/gpt-2-ggml\", hf=true)\ntokenizer = autotokenizer.from_pretrained(model)\n```\n\n[run in google colab](https://colab.research.google.com/drive/1fvslftj2ibbq1ou2rqz0mkpjbab_5got)\n\nyou can use \ud83e\udd17 transformers text generation pipeline:\n\n```py\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nprint(pipe(\"ai is going to\", max_new_tokens=256))\n```\n\nyou can use \ud83e\udd17 transformers generation [parameters](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.generationconfig):\n\n```py\npipe(\"ai is going to\", max_new_tokens=256, do_sample=true, temperature=0.8, repetition_penalty=1.1)\n```\n\nyou can use \ud83e\udd17 transformers tokenizers:\n\n```py\nfrom ctransformers import automodelforcausallm\nfrom transformers import autotokenizer\n\nmodel = automodelforcausallm.from_pretrained(\"marella/gpt-2-ggml\", hf=true)  # load model from ggml model repo.\ntokenizer = autotokenizer.from_pretrained(\"gpt2\")  # load tokenizer from original model repo.\n```\n\n### langchain\n\nit is integrated into langchain. see [langchain docs](https://python.langchain.com/docs/ecosystem/integrations/ctransformers).\n\n### gpu\n\nto run some of the model layers on gpu, set the `gpu_layers` parameter:\n\n```py\nllm = automodelforcausallm.from_pretrained(\"thebloke/llama-2-7b-ggml\", gpu_layers=50)\n```\n\n[run in google colab](https://colab.research.google.com/drive/1ihn7ipcyiqltotpkqa1tohuipjbrj1tp)\n\n#### cuda\n\ninstall cuda libraries using:\n\n```sh\npip install ctransformers[cuda]\n```\n\n#### rocm\n\nto enable rocm support, install the `ctransformers` package using:\n\n```sh\nct_hipblas=1 pip install ctransformers --no-binary ctransformers\n```\n\n#### metal\n\nto enable metal support, install the `ctransformers` package using:\n\n```sh\nct_metal=1 pip install ctransformers --no-binary ctransformers\n```\n\n### gptq\n\n> **note:** this is an experimental feature and only llama models are supported using [exllama](https://github.com/turboderp/exllama).\n\ninstall additional dependencies using:\n\n```sh\npip install ctransformers[gptq]\n```\n\nload a gptq model using:\n\n```py\nllm = automodelforcausallm.from_pretrained(\"thebloke/llama-2-7b-gptq\")\n```\n\n[run in google colab](https://colab.research.google.com/drive/1szhslj4ciycmogrppqecj4vycwfnyrn0)\n\n> if model name or path doesn't contain the word `gptq` then specify `model_type=\"gptq\"`.\n\nit can also be used with langchain. low-level apis are not fully supported.\n\n## documentation\n\n<!-- api_docs -->\n\n### config\n\n| parameter            | type        | description                                                     | default |\n| :------------------- | :---------- | :-------------------------------------------------------------- | :------ |\n| `top_k`              | `int`       | the top-k value to use for sampling.                            | `40`    |\n| `top_p`              | `float`     | the top-p value to use for sampling.                            | `0.95`  |\n| `temperature`        | `float`     | the temperature to use for sampling.                            | `0.8`   |\n| `repetition_penalty` | `float`     | the repetition penalty to use for sampling.                     | `1.1`   |\n| `last_n_tokens`      | `int`       | the number of last tokens to use for repetition penalty.        | `64`    |\n| `seed`               | `int`       | the seed value to use for sampling tokens.                      | `-1`    |\n| `max_new_tokens`     | `int`       | the maximum number of new tokens to generate.                   | `256`   |\n| `stop`               | `list[str]` | a list of sequences to stop generation when encountered.        | `none`  |\n| `stream`             | `bool`      | whether to stream the generated text.                           | `false` |\n| `reset`              | `bool`      | whether to reset the model state before generating text.        | `true`  |\n| `batch_size`         | `int`       | the batch size to use for evaluating tokens in a single prompt. | `8`     |\n| `threads`            | `int`       | the number of threads to use for evaluating tokens.             | `-1`    |\n| `context_length`     | `int`       | the maximum context length to use.                              | `-1`    |\n| `gpu_layers`         | `int`       | the number of layers to run on gpu.                             | `0`     |\n\n> **note:** currently only llama, mpt and falcon models support the `context_length` parameter.\n\n### <kbd>class</kbd> `automodelforcausallm`\n\n---\n\n#### <kbd>classmethod</kbd> `automodelforcausallm.from_pretrained`\n\n```python\nfrom_pretrained(\n    model_path_or_repo_id: str,\n    model_type: optional[str] = none,\n    model_file: optional[str] = none,\n    config: optional[ctransformers.hub.autoconfig] = none,\n    lib: optional[str] = none,\n    local_files_only: bool = false,\n    revision: optional[str] = none,\n    hf: bool = false,\n    **kwargs\n) \u2192 llm\n```\n\nloads the language model from a local file or remote repo.\n\n**args:**\n\n- <b>`model_path_or_repo_id`</b>: the path to a model file or directory or the name of a hugging face hub model repo.\n- <b>`model_type`</b>: the model type.\n- <b>`model_file`</b>: the name of the model file in repo or directory.\n- <b>`config`</b>: `autoconfig` object.\n- <b>`lib`</b>: the path to a shared library or one of `avx2`, `avx`, `basic`.\n- <b>`local_files_only`</b>: whether or not to only look at local files (i.e., do not try to download the model).\n- <b>`revision`</b>: the specific model version to use. it can be a branch name, a tag name, or a commit id.\n- <b>`hf`</b>: whether to create a hugging face transformers model.\n\n**returns:**\n`llm` object.\n\n### <kbd>class</kbd> `llm`\n\n### <kbd>method</kbd> `llm.__init__`\n\n```python\n__init__(\n    model_path: str,\n    model_type: optional[str] = none,\n    config: optional[ctransformers.llm.config] = none,\n    lib: optional[str] = none\n)\n```\n\nloads the language model from a local file.\n\n**args:**\n\n- <b>`model_path`</b>: the path to a model file.\n- <b>`model_type`</b>: the model type.\n- <b>`config`</b>: `config` object.\n- <b>`lib`</b>: the path to a shared library or one of `avx2`, `avx`, `basic`.\n\n---\n\n##### <kbd>property</kbd> llm.bos_token_id\n\nthe beginning-of-sequence token.\n\n---\n\n##### <kbd>property</kbd> llm.config\n\nthe config object.\n\n---\n\n##### <kbd>property</kbd> llm.context_length\n\nthe context length of model.\n\n---\n\n##### <kbd>property</kbd> llm.embeddings\n\nthe input embeddings.\n\n---\n\n##### <kbd>property</kbd> llm.eos_token_id\n\nthe end-of-sequence token.\n\n---\n\n##### <kbd>property</kbd> llm.logits\n\nthe unnormalized log probabilities.\n\n---\n\n##### <kbd>property</kbd> llm.model_path\n\nthe path to the model file.\n\n---\n\n##### <kbd>property</kbd> llm.model_type\n\nthe model type.\n\n---\n\n##### <kbd>property</kbd> llm.pad_token_id\n\nthe padding token.\n\n---\n\n##### <kbd>property</kbd> llm.vocab_size\n\nthe number of tokens in vocabulary.\n\n---\n\n#### <kbd>method</kbd> `llm.detokenize`\n\n```python\ndetokenize(tokens: sequence[int], decode: bool = true) \u2192 union[str, bytes]\n```\n\nconverts a list of tokens to text.\n\n**args:**\n\n- <b>`tokens`</b>: the list of tokens.\n- <b>`decode`</b>: whether to decode the text as utf-8 string.\n\n**returns:**\nthe combined text of all tokens.\n\n---\n\n#### <kbd>method</kbd> `llm.embed`\n\n```python\nembed(\n    input: union[str, sequence[int]],\n    batch_size: optional[int] = none,\n    threads: optional[int] = none\n) \u2192 list[float]\n```\n\ncomputes embeddings for a text or list of tokens.\n\n> **note:** currently only llama and falcon models support embeddings.\n\n**args:**\n\n- <b>`input`</b>: the input text or list of tokens to get embeddings for.\n- <b>`batch_size`</b>: the batch size to use for evaluating tokens in a single prompt. default: `8`\n- <b>`threads`</b>: the number of threads to use for evaluating tokens. default: `-1`\n\n**returns:**\nthe input embeddings.\n\n---\n\n#### <kbd>method</kbd> `llm.eval`\n\n```python\neval(\n    tokens: sequence[int],\n    batch_size: optional[int] = none,\n    threads: optional[int] = none\n) \u2192 none\n```\n\nevaluates a list of tokens.\n\n**args:**\n\n- <b>`tokens`</b>: the list of tokens to evaluate.\n- <b>`batch_size`</b>: the batch size to use for evaluating tokens in a single prompt. default: `8`\n- <b>`threads`</b>: the number of threads to use for evaluating tokens. default: `-1`\n\n---\n\n#### <kbd>method</kbd> `llm.generate`\n\n```python\ngenerate(\n    tokens: sequence[int],\n    top_k: optional[int] = none,\n    top_p: optional[float] = none,\n    temperature: optional[float] = none,\n    repetition_penalty: optional[float] = none,\n    last_n_tokens: optional[int] = none,\n    seed: optional[int] = none,\n    batch_size: optional[int] = none,\n    threads: optional[int] = none,\n    reset: optional[bool] = none\n) \u2192 generator[int, nonetype, nonetype]\n```\n\ngenerates new tokens from a list of tokens.\n\n**args:**\n\n- <b>`tokens`</b>: the list of tokens to generate tokens from.\n- <b>`top_k`</b>: the top-k value to use for sampling. default: `40`\n- <b>`top_p`</b>: the top-p value to use for sampling. default: `0.95`\n- <b>`temperature`</b>: the temperature to use for sampling. default: `0.8`\n- <b>`repetition_penalty`</b>: the repetition penalty to use for sampling. default: `1.1`\n- <b>`last_n_tokens`</b>: the number of last tokens to use for repetition penalty. default: `64`\n- <b>`seed`</b>: the seed value to use for sampling tokens. default: `-1`\n- <b>`batch_size`</b>: the batch size to use for evaluating tokens in a single prompt. default: `8`\n- <b>`threads`</b>: the number of threads to use for evaluating tokens. default: `-1`\n- <b>`reset`</b>: whether to reset the model state before generating text. default: `true`\n\n**returns:**\nthe generated tokens.\n\n---\n\n#### <kbd>method</kbd> `llm.is_eos_token`\n\n```python\nis_eos_token(token: int) \u2192 bool\n```\n\nchecks if a token is an end-of-sequence token.\n\n**args:**\n\n- <b>`token`</b>: the token to check.\n\n**returns:**\n`true` if the token is an end-of-sequence token else `false`.\n\n---\n\n#### <kbd>method</kbd> `llm.prepare_inputs_for_generation`\n\n```python\nprepare_inputs_for_generation(\n    tokens: sequence[int],\n    reset: optional[bool] = none\n) \u2192 sequence[int]\n```\n\nremoves input tokens that are evaluated in the past and updates the llm context.\n\n**args:**\n\n- <b>`tokens`</b>: the list of input tokens.\n- <b>`reset`</b>: whether to reset the model state before generating text. default: `true`\n\n**returns:**\nthe list of tokens to evaluate.\n\n---\n\n#### <kbd>method</kbd> `llm.reset`\n\n```python\nreset() \u2192 none\n```\n\ndeprecated since 0.2.27.\n\n---\n\n#### <kbd>method</kbd> `llm.sample`\n\n```python\nsample(\n    top_k: optional[int] = none,\n    top_p: optional[float] = none,\n    temperature: optional[float] = none,\n    repetition_penalty: optional[float] = none,\n    last_n_tokens: optional[int] = none,\n    seed: optional[int] = none\n) \u2192 int\n```\n\nsamples a token from the model.\n\n**args:**\n\n- <b>`top_k`</b>: the top-k value to use for sampling. default: `40`\n- <b>`top_p`</b>: the top-p value to use for sampling. default: `0.95`\n- <b>`temperature`</b>: the temperature to use for sampling. default: `0.8`\n- <b>`repetition_penalty`</b>: the repetition penalty to use for sampling. default: `1.1`\n- <b>`last_n_tokens`</b>: the number of last tokens to use for repetition penalty. default: `64`\n- <b>`seed`</b>: the seed value to use for sampling tokens. default: `-1`\n\n**returns:**\nthe sampled token.\n\n---\n\n#### <kbd>method</kbd> `llm.tokenize`\n\n```python\ntokenize(text: str, add_bos_token: optional[bool] = none) \u2192 list[int]\n```\n\nconverts a text into list of tokens.\n\n**args:**\n\n- <b>`text`</b>: the text to tokenize.\n- <b>`add_bos_token`</b>: whether to add the beginning-of-sequence token.\n\n**returns:**\nthe list of tokens.\n\n---\n\n#### <kbd>method</kbd> `llm.__call__`\n\n```python\n__call__(\n    prompt: str,\n    max_new_tokens: optional[int] = none,\n    top_k: optional[int] = none,\n    top_p: optional[float] = none,\n    temperature: optional[float] = none,\n    repetition_penalty: optional[float] = none,\n    last_n_tokens: optional[int] = none,\n    seed: optional[int] = none,\n    batch_size: optional[int] = none,\n    threads: optional[int] = none,\n    stop: optional[sequence[str]] = none,\n    stream: optional[bool] = none,\n    reset: optional[bool] = none\n) \u2192 union[str, generator[str, nonetype, nonetype]]\n```\n\ngenerates text from a prompt.\n\n**args:**\n\n- <b>`prompt`</b>: the prompt to generate text from.\n- <b>`max_new_tokens`</b>: the maximum number of new tokens to generate. default: `256`\n- <b>`top_k`</b>: the top-k value to use for sampling. default: `40`\n- <b>`top_p`</b>: the top-p value to use for sampling. default: `0.95`\n- <b>`temperature`</b>: the temperature to use for sampling. default: `0.8`\n- <b>`repetition_penalty`</b>: the repetition penalty to use for sampling. default: `1.1`\n- <b>`last_n_tokens`</b>: the number of last tokens to use for repetition penalty. default: `64`\n- <b>`seed`</b>: the seed value to use for sampling tokens. default: `-1`\n- <b>`batch_size`</b>: the batch size to use for evaluating tokens in a single prompt. default: `8`\n- <b>`threads`</b>: the number of threads to use for evaluating tokens. default: `-1`\n- <b>`stop`</b>: a list of sequences to stop generation when encountered. default: `none`\n- <b>`stream`</b>: whether to stream the generated text. default: `false`\n- <b>`reset`</b>: whether to reset the model state before generating text. default: `true`\n\n**returns:**\nthe generated text.\n\n<!-- api_docs -->\n\n## license\n\n[mit](https://github.com/marella/ctransformers/blob/main/license)\n\n\n",
  "docs_url": null,
  "keywords": "ctransformers transformers ai llm",
  "license": "mit",
  "name": "ctransformers",
  "package_url": "https://pypi.org/project/ctransformers/",
  "project_url": "https://pypi.org/project/ctransformers/",
  "project_urls": {
    "Homepage": "https://github.com/marella/ctransformers"
  },
  "release_url": "https://pypi.org/project/ctransformers/0.2.27/",
  "requires_dist": [
    "huggingface-hub",
    "py-cpuinfo (<10.0.0,>=9.0.0)",
    "nvidia-cublas-cu12 ; extra == 'cuda'",
    "nvidia-cuda-runtime-cu12 ; extra == 'cuda'",
    "exllama (==0.1.0) ; extra == 'gptq'",
    "pytest ; extra == 'tests'"
  ],
  "requires_python": "",
  "summary": "python bindings for the transformer models implemented in c/c++ using ggml library.",
  "version": "0.2.27",
  "releases": [],
  "developers": [
    "mv.ravindra007@gmail.com",
    "ravindra_marella"
  ],
  "kwds": "ctransformers transformer transformers yml badge",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_ctransformers",
  "homepage": "https://github.com/marella/ctransformers",
  "release_count": 30,
  "dependency_ids": [
    "pypi_exllama",
    "pypi_huggingface_hub",
    "pypi_nvidia_cublas_cu12",
    "pypi_nvidia_cuda_runtime_cu12",
    "pypi_py_cpuinfo",
    "pypi_pytest"
  ]
}