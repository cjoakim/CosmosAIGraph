{
  "classifiers": [
    "development status :: 4 - beta",
    "intended audience :: developers",
    "license :: osi approved :: apache software license",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: software development :: testing"
  ],
  "description": "# pytest-docker-tools\n\nyou have written a software application (in any language) and have packaged it as a docker image. now you want to smoke test the built image or do some integration testing with other containers before releasing it. you:\n\n* want to reason about your environment in a similar way to a docker-compose.yml`\n* want the environment to be automatically created and destroyed as tests run\n* want the option to reuse previously created resources (e.g. containers) when executing tests in high frequency\n* don't want to have to write loads of boilerplate code for creating the test environment\n* want to be able to run the tests in parallel\n* want the tests to be reliable\n\n`pytest-docker-tools` is a set of opinionated helpers for creating `py.test` fixtures for your smoke testing and integration testing. it strives to keep your environment definition declarative, like a docker-compose.yml. it embraces py.test fixture overloading. ~~it tries not to be too magical~~. it ended up kind of magical, but no more so that `py.test` itself.\n\nthe main interface provided by this library is a set of 'fixture factories'. it provides a 'best in class' implementation of a fixture, and then allows you to treat it as a template - injecting your own configuration declaratively. you can define your fixtures in your `conftest.py` and access them from all your tests, and you can override them as needed in individual test modules.\n\nthe api is straightforward and implicitly captures the dependencies between fixtures in the specification. for example, here is how it might look if you were building out a microservice and wanted to point its dns and a mock dns server:\n\n```python\n# conftest.py\n\nfrom http.client import httpconnection\n\nimport pytest\nfrom pytest_docker_tools import build, container\n\nfakedns_image = build(\n    path='examples/resolver-service/dns',\n)\n\nfakedns = container(\n    image='{fakedns_image.id}',\n    environment={\n        'dns_example_com__a': '127.0.0.1',\n    }\n)\n\napiserver_image = build(\n    path='examples/resolver-service/api',\n)\n\napiserver = container(\n    image='{apiserver_image.id}',\n    ports={\n        '8080/tcp': none,\n    },\n    dns=['{fakedns.ips.primary}']\n)\n\n\n@pytest.fixture\ndef apiclient(apiserver):\n    port = apiserver.ports['8080/tcp'][0]\n    return httpconnection(f'localhost:{port}')\n```\n\nyou can now create a test that exercises your microservice:\n\n```python\n# test_smoketest.py\n\nimport socket\n\ndef test_my_frobulator(apiserver):\n    sock = socket.socket()\n    sock.connect(('127.0.0.1', apiserver.ports['8080/tcp'][0]))\n\n\ndef test_my_frobulator_works_after_restart(apiserver):\n    apiserver.restart()\n\n    sock = socket.socket()\n    sock.connect(('127.0.0.1', apiserver.ports['8080/tcp'][0]))\n```\n\nin this example all the dependencies will be resolved in order and once per session:\n\n* the latest `redis:latest` will be fetched\n* a container image will be build from the `dockerfile` in the `db` folder.\n\nthen once per test:\n\n* a new volume will be created\n* a new 'backend' container will be created from `redis:latest`. it will be attached to the new volume.\n* a new 'frontend' container will be created from the freshly built container. it will be given the ip if the backend via an environment variable. port 3679 in the container will be exposed as an ephemeral port on the host.\n\nthe test can then run and access the container via its ephemeral high port. at the end of the test the environment will be thrown away.\n\nif the test fails the `docker logs` output from each container will be captured and added to the test output.\n\nin the example you'll notice we defined an `apiclient` fixture. of course if you use that it will implicitly pull in both of the server fixtures and 'just work':\n\n```python\n# test_smoketest.py\n\nimport json\n\n\ndef test_api_server(apiclient):\n    apiclient.request('get', '/')\n    response = apiclient.getresponse()\n    assert response.status == 200\n    assert json.loads(response.read()) == {'result': '127.0.0.1'}\n```\n\n## scope\n\nall of the fixture factories take the `scope` keyword. fixtures created with these factories will behave like any py.test fixture with that scope.\n\nin this example we create a memcache that is `session` scoped and another that is `module` scoped.\n\n```python\n# conftest.py\n\nfrom pytest_docker_tools import container, fetch\n\nmemcache_image = fetch(repository='memcached:latest')\n\nmemcache_session = container(\n    image='{memcache_image.id}',\n    scope='session',\n    ports={\n        '11211/tcp': none,\n    },\n)\n\nmemcache_module = container(\n    image='{memcache_image.id}',\n    scope='module',\n    ports={\n        '11211/tcp': none,\n    },\n)\n```\n\nwhen `test_scope_1.py` runs neither container is running so a new instance of each is started. their scope is longer than a single `function` so they are kept alive for the next test that needs them.\n\n```python\n# test_scope_1.py\n\nimport socket\n\ndef test_session_1(memcache_session):\n    sock = socket.socket()\n    sock.connect(('127.0.0.1', memcache_session.ports['11211/tcp'][0]))\n    sock.sendall(b'set mykey 0 600 4\\r\\ndata\\r\\n')\n    sock.sendall(b'get mykey\\r\\n')\n    assert sock.recv(1024) == b'stored\\r\\nvalue mykey 0 4\\r\\ndata\\r\\nend\\r\\n'\n    sock.close()\n\ndef test_session_2(memcache_session):\n    sock = socket.socket()\n    sock.connect(('127.0.0.1', memcache_session.ports['11211/tcp'][0]))\n    sock.sendall(b'set mykey 0 600 4\\r\\ndata\\r\\n')\n    sock.sendall(b'get mykey\\r\\n')\n    assert sock.recv(1024) == b'stored\\r\\nvalue mykey 0 4\\r\\ndata\\r\\nend\\r\\n'\n    sock.close()\n\ndef test_module_1(memcache_module):\n    sock = socket.socket()\n    sock.connect(('127.0.0.1', memcache_module.ports['11211/tcp'][0]))\n    sock.sendall(b'set mykey 0 600 4\\r\\ndata\\r\\n')\n    sock.sendall(b'get mykey\\r\\n')\n    assert sock.recv(1024) == b'stored\\r\\nvalue mykey 0 4\\r\\ndata\\r\\nend\\r\\n'\n    sock.close()\n\ndef test_module_2(memcache_module):\n    sock = socket.socket()\n    sock.connect(('127.0.0.1', memcache_module.ports['11211/tcp'][0]))\n    sock.sendall(b'set mykey 0 600 4\\r\\ndata\\r\\n')\n    sock.sendall(b'get mykey\\r\\n')\n    assert sock.recv(1024) == b'stored\\r\\nvalue mykey 0 4\\r\\ndata\\r\\nend\\r\\n'\n    sock.close()\n```\n\nwhen `test_scope_2.py` runs the `session` scoped container is still running, so it will be reused. but we are now in a new module now so the `module` scoped container will have been destroyed. a new empty instance will be created.\n\n```python\n# test_scope_2.py\n\nimport socket\n\ndef test_session_3(memcache_session):\n    sock = socket.socket()\n    sock.connect(('127.0.0.1', memcache_session.ports['11211/tcp'][0]))\n    sock.sendall(b'get mykey\\r\\n')\n    assert sock.recv(1024).endswith(b'end\\r\\n')\n    sock.close()\n\ndef test_module_3(memcache_module):\n    sock = socket.socket()\n    sock.connect(('127.0.0.1', memcache_module.ports['11211/tcp'][0]))\n    sock.sendall(b'get mykey\\r\\n')\n    assert sock.recv(1024) == b'end\\r\\n'\n    sock.close()\n```\n\n## parallelism\n\nintegration and smoke tests are often slow, but a lot of time is spent waiting. so running tests in parallel is a great way to speed them up. `pytest-docker-tools` avoids creating resource names that could collide. it also makes it easy to not care what port your service is bound to. this means its a great fit for use with `pytest-xdist`.\n\nhere is a bare minimum example that just tests creating and destroying 100 instances of a redis fixture that runs under xdist. create a `test_xdist.py` plugin:\n\n```python\n\nimport pytest\nfrom pytest_docker_tools import container, fetch\n\nmy_redis_image = fetch(repository='redis:latest')\n\nmy_redis = container(\n    image='{my_redis_image.id}',\n)\n\n\n@pytest.mark.parametrize(\"i\", list(range(100)))\ndef test_xdist(i, my_redis):\n    assert my_redis.status == \"running\"\n```\n\nand invoke it with:\n\n```bash\npytest test_xdist.py -n auto\n```\n\nit will create a worker per core and run the tests in parallel:\n\n```\n===================================== test session starts ======================================\nplatform darwin -- python 3.6.5, pytest-3.6.3, py-1.5.4, pluggy-0.6.0\nrootdir: ~/pytest-docker-tools, inifile:\nplugins: xdist-1.22.2, forked-0.2, docker-tools-0.0.2\ngw0 [100] / gw1 [100] / gw2 [100] / gw3 [100] / gw4 [100] / gw5 [100] / gw6 [100] / gw7 [100]\nscheduling tests via loadscheduling\n......................................................................................... [ 82%]\n...........                                                                              [100%]\n================================= 100 passed in 70.08 seconds ==================================\n```\n\n## factories reference\n\n### containers\n\nto create a container in your tests use the `container` fixture factory.\n\n```python\nfrom pytest_docker_tools import container\n\nmy_microservice_backend = container(image='redis:latest')\n```\n\nthe default scope for this factory is `function`. this means a new container will be created for each test.\n\nthe `container` fixture factory supports all parameters that can be passed to the docker-py `run` method. see [here](https://docker-py.readthedocs.io/en/stable/containers.html#docker.models.containers.containercollection.run) for them all.\n\nany string variables are interpolated against other defined fixtures. this means that a fixture can depend on other fixtures, and they will be built and run in order.\n\nfor example:\n\n```python\nfrom pytest_docker_tools import container, fetch\n\nredis_image = fetch(repository='redis:latest')\nredis = container(image='{redis_image.id}')\n\n\ndef test_container_starts(redis):\n    assert redis.status == \"running\"\n```\n\nthis will fetch the latest `redis:latest` first, and then run a container from the exact image that was pulled. note that if you don't use `build` or `fetch` to prepare a docker image then the tag or hash that you specify must already exist on the host where you are running the tests. there is no implicit fetching of docker images.\n\nthe container will be ready when the test is started, and will be automatically deleted after the test has finished.\n\nif for some reason it's not ready in the timeout period (30 seconds by default) the test will fail.\n\n`timeout` can be passed to the `container` factory:\n\n```python\nfrom pytest_docker_tools import container, fetch\n\nredis_image = fetch(repository='redis:latest')\nredis = container(image='{redis_image.id}', timeout=30)\n\ndef test_container_starts(redis):\n    assert redis.status == \"running\"\n```\n\nto create a container defining its `dockerfile` in code:\n\n```python\nimport io\n\nfrom pytest_docker_tools import build, container\n\ndockerfile = io.bytesio(b\"\"\"\nfrom alpine:3.12\nrun apk --no-cache add python3\ncmd [\"tail\", \"-f\", \"/dev/null\"]\n\"\"\")\n\nimage = build(fileobj=dockerfile)\ncontainer = container(image='{image.id}')\n\ndef test_container_starts(container):\n    assert container.status == \"running\"\n```\n\n#### ip addresses\n\nif your container is only attached to a single network you can get its ip address through a helper property. given this environment:\n\n```python\n# conftest.py\n\nfrom pytest_docker_tools import container, fetch, network\n\nredis_image = fetch(repository='redis:latest')\nbackend_network = network()\n\nredis = container(\n  image='{redis_image.id}',\n  network='{backend_network.name}',\n)\n```\n\nyou can access the ip via the container helper:\n\n```python\nimport ipaddress\n\ndef test_get_service_ip(redis):\n    # this will raise a valueerror if not a valid ip\n    ipaddress.ip_address(redis.ips.primary)\n```\n\nif you want to look up its ip address by network you can also access it more specifically:\n\n```python\nimport ipaddress\n\ndef test_get_service_ip(backend_network, redis):\n    ipaddress.ip_address(redis.ips[backend_network])\n```\n\n#### ports\n\nthe factory takes the same port arguments as the official python docker api. we recommend using the ephemeral high ports syntax:\n\n```python\n# conftest.py\n\nfrom pytest_docker_tools import container\n\napiserver = container(\n  image='{apiserver_image.id}',\n  ports={'8080/tcp': none}\n)\n```\n\ndocker will map port 8080 in the container to a random port on your host. in order to access it from your tests you can get the bound port from the container instance:\n\n```python\ndef test_connect_my_service(apiserver):\n    assert apiserver.ports['8080/tcp'][0] != 8080\n```\n\n#### logs\n\nyou can inspect the logs of your container with the logs method:\n\n```python\nfrom pytest_docker_tools import container, fetch\n\n\nredis_image = fetch(repository='redis:latest')\nredis = container(\n    image='{redis_image.id}',\n    ports={'6379/tcp': none},\n)\n\ndef test_logs(redis):\n    assert 'oo0ooo0ooo0oo redis is starting oo0ooo0ooo0oo' in redis.logs()\n```\n\n### images\n\nto pull an image from your default repository use the `fetch` fixture factory. to build an image from local source use the `build` fixture factory. if you are smoke testing an artifact already built locally you can use the `image` fixture factory to reference it.\n\n```python\nfrom pytest_docker_tools import build, image, fetch\n\nmy_image = fetch(repository='redis:latest')\n\nmy_image_2 = build(\n  path='db'\n)\n```\n\nthe `build` fixture factory supports all parameters that can be passed to the docker-py `build` method. see [here](https://docker-py.readthedocs.io/en/stable/images.html#docker.models.images.imagecollection.build) for them all. the `fetch` fixture factory supports all parameters that can be passed to the docker-py `pull` method. see [here](https://docker-py.readthedocs.io/en/stable/images.html#docker.models.images.imagecollection.pull) for them all.\n\nthe default scope for this factory is `session`. this means the fixture will only build or fetch once per py.test invocation. the fixture will not be triggered until a test (or other fixture) tries to use it. this means you won't waste time building an image if you aren't running the test that uses it.\n\n#### caching\n\nby default images are kept between invocations. this speeds things up a lot. but when doing incremental development of an image it can leave you with lots of orphaned layers. running `docker image prune` will throw away these layers, but as that is all untagged images it will include ones that are still valid for you current project. for large images this can really slow down your test cycle.\n\nto avoid this you need to tag your images as `docker image prune` won't throw away tagged images by default. but for this to be effective for multi-stage images, you need to tag your stages as well. to support this, pytest-docker-tools takes a `stages` parameter. for example:\n\n```python\nfrom pytest_docker_tools import build, image, fetch\n\nmy_image = build(\n  path='db',\n  tag='localhost/myproject:latest',\n  stages={\n      'builder': 'localhost/myproject:builder'\n  }\n)\n```\n\nunder the hood this will make pytest-docker-tools first build (and tag) the `builder` stage. this is like running:\n\n```bash\ndocker build --target builder --tag localhost/myproject:builder .\n```\n\nthen when that is tagged it will run the default target as before, which is like running:\n\n```bash\ndocker build --tag localhost/myproject:latest .\n```\n\nthis will reuse the layers generated in the previous build (where applicable).\n\nnow when you run `docker image prune` both the latest image build and the latest versions of the stages it depends on are left alone.\n\n### networks\n\nby default any containers you create with the `container()` fixture factory will run on your default docker network. you can create a dedicated network for your test with the `network()` fixture factory.\n\n```python\nfrom pytest_docker_tools import container, fetch, network\n\nfrontend_network = network()\n\nredis_image = fetch(repository='redis:latest')\nredis = container(\n    image='{redis_image.id}',\n    network='{frontend_network.name}',\n)\n```\n\nthe `network` fixture factory supports all parameters that can be passed to the docker-py network `create` method. see [here](https://docker-py.readthedocs.io/en/stable/networks.html#docker.models.networks.networkcollection.create) for them all.\n\nthe default scope for this factory is `function`. this means a new network will be created for each test that is executed.\n\nthe network will be removed after the test using it has finished.\n\n### volumes\n\nin the ideal case a docker container instance is read only. no data inside the container is written to, if it is its to a volume. if you are testing that your service can run read only you might want to mount a rw volume. you can use the `volume()` fixture factory to create a docker volume with a lifecycle tied to your tests.\n\n```python\nfrom pytest_docker_tools import volume\n\nbackend_storage = volume()\n```\n\nthe `volume` fixture factory supports all parameters that can be passed to the docker-py volume `create` method. see [here](https://docker-py.readthedocs.io/en/stable/volumes.html#docker.models.volumes.volumecollection.create) for them all.\n\nin addition you can specify a `initial_content` dictionary. this allows you to seed a volume with a small set of initial state. in the following example we'll preseed a minio service with 2 buckets and 1 object in 1 of those buckets.\n\n```python\nfrom pytest_docker_tools import container, fetch, volume\n\n\nminio_image = fetch(repository='minio/minio:latest')\n\nminio_volume = volume(\n    initial_content={\n        'bucket-1': none,\n        'bucket-2/example.txt': b'test file 1',\n    }\n)\n\nminio = container(\n    image='{minio_image.id}',\n    command=['server', '/data'],\n    volumes={\n        '{minio_volume.name}': {'bind': '/data'},\n    },\n    environment={\n        'minio_access_key': 'minio',\n        'minio_secret_key': 'minio123',\n    },\n)\n\ndef test_volume_is_seeded(minio):\n    files = minio.get_files('/data')\n    assert files['data/bucket-2/example.txt'] == b'test file 1'\n    assert files['data/bucket-1'] == none\n```\n\nthe `minio_volume` container will be created with an empty folder (`bucket-1`) and a text file called `example.txt` in a seperate folder called `bucket-2`.\n\nthe default scope for this factory is `function`. this means a new volume will be created for each test that is executed. the volume will be removed after the test using it has finished.\n\n## fixtures\n\n### docker_client\n\nthe `docker_client` fixture returns an instance of the official docker client.\n\n```python\ndef test_container_created(docker_client, fakedns):\n    for c in docker_client.containers.list(ignore_removed=true):\n        if c.id == fakedns.id:\n            # looks like we managed to start one!\n            break\n    else:\n        assert false, 'looks like we failed to start a container'\n```\n\ntake care when using the `docker_client` directly:\n\n* obviously resources created imperatively via the api won't be removed at the end of the test automatically\n* it's easy to break xdist compatibility\n  * always use `ignore_removed` with `docker_client.containers.list()` - it is racy without\n  * it's easy to find other instances of the resources you are working with (created in other workers). be mindful of this!\n* don't take destructive action - someone could be running tests on a machine with other (non-test) containers running, collateral damage is easy and should be avoided.\n\nthis is the fixture used by our fixture factories. this means if you define a `docker_client` fixture of your own then the tests will use that instead.\n\n## tips and tricks\n\n### testing build artifacts\n\nwe often find ourselves using a set of tests against a container we've built at test time (with `build()`) but then wanting to use the same tests with an artifact generated on our ci platform (with `image()`). this ended up looking like this:\n\n```\nif not os.environ.get('image_id', ''):\n    image = build(path='examples/resolver-service/dns')\nelse:\n    image = image(name=os.environ['image_id'])\n```\n\nbut now you can just do:\n\n```python\nfrom pytest_docker_tools import image_or_build\n\nimage = image_or_build(\n    environ_key='image_id',\n    path='examples/resolver-service/dns',\n)\n\ndef test_image(image):\n    assert image.attrs['os'] == 'linux'\n```\n\n### network differences between dev env and ci\n\nanother common difference between your dev environment and your ci environment might be that your tests end up running in docker on your ci. if you bind-mount your `docker.sock` then your tests might end up running on the same container network as the containers you are testing, and unable to access any port you are mapping to the host box. in otherwords:\n\n* on your dev machine your tests might access locahost:8000 to access your test instance (ports mapped to host)\n* on your ci machine they might need to access 172.16.0.5:8000 to access your test instance\n\nthe container object has a `get_addr` helper which will return the right thing depending on the environment it is in.\n\n```python\nfrom pytest_docker_tools import container\n\napiserver = container(\n  image='{apiserver_image.id}',\n  ports={'8080/tcp': none}\n)\n\ndef test_connect_my_service(apiserver):\n    ip, port = apiserver.get_addr('8080/tcp')\n    # ... connect to ip:port ...\n```\n\n### dynamic scope\n\nthe pytest fixture decorator now lets you specify a callback to determine the scope of a fixture this is called [dynamic scope](https://docs.pytest.org/en/stable/fixture.html#dynamic-scope). you can use this to make it a runtime option whether to have a container instance per test or per test run. for example:\n\n```python\n# conftest.py\nfrom pytest_docker_tools import container, fetch\n\ndef determine_scope(fixture_name, config):\n    if config.getoption(\"--keep-containers\", none):\n        return \"session\"\n    return \"function\"\n\nmemcache_image = fetch(repository='memcached:latest')\n\nmemcache = container(\n    image='{memcache_image.id}',\n    scope=determine_scope,\n    ports={\n        '11211/tcp': none,\n    },\n)\n```\n\nyour tests can look exactly the same as before:\n\n```python\ndef test_connect_my_service(memcache):\n    ip, port = memcache.get_addr('11211/tcp')\n    # ... connect to ip:port ...\n```\n\n### client fixtures\n\nyou will probably want to create an api client for the service you are testing. although we've already done this in the readme, its worth calling it out. you can define a client fixture, have it depend on your docker containers, and then only have to reference the client from your tests.\n\n```python\n# conftest.py\n\nfrom http.client import httpconnection\n\nimport pytest\nfrom pytest_docker_tools import build, container\n\nfakedns_image = build(\n    path='examples/resolver-service/dns',\n)\n\nfakedns = container(\n    image='{fakedns_image.id}',\n    environment={\n        'dns_example_com__a': '127.0.0.1',\n    }\n)\n\napiserver_image = build(\n    path='examples/resolver-service/api',\n)\n\napiserver = container(\n    image='{apiserver_image.id}',\n    ports={\n        '8080/tcp': none,\n    },\n    dns=['{fakedns.ips.primary}']\n)\n\n\n@pytest.fixture\ndef apiclient(apiserver):\n    port = apiserver.ports['8080/tcp'][0]\n    return httpconnection(f'localhost:{port}')\n```\n\nand then reference it from your tests:\n\n```python\n# test_the_test_client.py\n\nimport json\n\n\ndef test_api_server(apiclient):\n    apiclient.request('get', '/')\n    response = apiclient.getresponse()\n    assert response.status == 200\n    result = json.loads(response.read())\n    assert result['result'] == '127.0.0.1'\n```\n\nin this example, any test that uses the `hpfeeds_client` fixture will get a properly configure client connected to a broker running in a docker container on an ephemeral high port. when the test finishes the client will cleanly disconnect, and the docker container will be thrown away.\n\n### fixture overloading\n\ncomplicated environments can be defined with fixture factories. they form a directed acyclic graph. by using fixture overloading it is possible to (in the context of a single test module) replace a node in that dependency graph without having to redefine the entire environment.\n\n#### replacing a container fixture without having to redefine its dependents\n\nyou can define a fixture in your `conftest.py`:\n\n```python\n# conftest.py\n\nfrom http.client import httpconnection\n\nimport pytest\nfrom pytest_docker_tools import build, container\n\nfakedns_image = build(\n    path='examples/resolver-service/dns',\n)\n\nfakedns = container(\n    image='{fakedns_image.id}',\n    environment={\n        'dns_example_com__a': '127.0.0.1',\n    }\n)\n\napiserver_image = build(\n    path='examples/resolver-service/api',\n)\n\napiserver = container(\n    image='{apiserver_image.id}',\n    ports={\n        '8080/tcp': none,\n    },\n    dns=['{fakedns.ips.primary}']\n)\n\n\n@pytest.fixture\ndef apiclient(apiserver):\n    port = apiserver.ports['8080/tcp'][0]\n    return httpconnection(f'localhost:{port}')\n```\n\nyou can then overload these fixtures in your test modules. for example, if redis had a magic replication feature and you want to test for an edge case with your api you could in your `test_smoketest_alternate.py`:\n\n```python\n# test_smoketest_alternate.py\n\nimport json\n\nfrom pytest_docker_tools import container\n\nfakedns = container(\n    image='{fakedns_image.id}',\n    environment={\n        'dns_example_com__a': '192.168.192.168',\n    }\n)\n\ndef test_api_server(apiclient):\n    apiclient.request('get', '/')\n    response = apiclient.getresponse()\n    assert response.status == 200\n    result = json.loads(response.read())\n    assert result['result'] == '192.168.192.168'\n```\n\nhere we have redefined the fakedns container locally in `test_smoketest_alternate`. it is able to use the `fakedns_image` fixture we defined in `conftest.py`. more crucially though, in `test_smoketest_alternate.py` when we use the core `apiclient` fixture it actually pulls in the local definition of `fakedns` and not the one from `conftest.py`! you don't have to redefine anything else. it just works.\n\n#### injecting fixture configuration through fixtures\n\nyou can pull in normal py.test fixtures from your fixture factory too. this means we can use fixture overloading and pass in config. in your `conftest.py`:\n\n```python\n# conftest.py\n\nfrom http.client import httpconnection\n\nimport pytest\nfrom pytest_docker_tools import build, container\n\nfakedns_image = build(\n    path='examples/resolver-service/dns',\n)\n\nfakedns = container(\n    image='{fakedns_image.id}',\n    environment={\n        'dns_example_com__a': '{example_com_a}',\n    }\n)\n\napiserver_image = build(\n    path='examples/resolver-service/api',\n)\n\napiserver = container(\n    image='{apiserver_image.id}',\n    ports={\n        '8080/tcp': none,\n    },\n    dns=['{fakedns.ips.primary}']\n)\n\n\n@pytest.fixture\ndef apiclient(apiserver):\n    port = apiserver.ports['8080/tcp'][0]\n    return httpconnection(f'localhost:{port}')\n\n\n@pytest.fixture\ndef example_com_a():\n    return '127.0.0.1'\n\n```\n\nwhen a test uses the apiclient fixture now they will get the fakedns container configured as normal. however you can redefine the fixture in your test module - and the other fixtures will still respect it. for example:\n\n```python\n# test_smoketest_alternate.py\n\nimport json\n\nimport pytest\n\n\n@pytest.fixture\ndef example_com_a():\n    return '192.168.192.168'\n\n\ndef test_api_server(apiclient):\n    apiclient.request('get', '/')\n    response = apiclient.getresponse()\n    assert response.status == 200\n    result = json.loads(response.read())\n    assert result['result'] == '192.168.192.168'\n```\n\nyour `api_server` container (and its `redis` backend) will be built as normal, only in this one test module it will use its sqlite backend.\n\n### fixture parameterisation\n\nyou can create parameterisation fixtures. perhaps you wan to run all your `api_server` tests against both of your authentication backends. perhaps you have a fake that you want to test multiple configurations of.\n\nin your `conftest.py`:\n\n```python\n# conftest.py\n\nfrom http.client import httpconnection\n\nimport pytest\nfrom pytest_docker_tools import build, container\n\nfakedns_image = build(\n    path='examples/resolver-service/dns',\n)\n\nfakedns_localhost = container(\n    image='{fakedns_image.id}',\n    environment={\n        'dns_example_com__a': '127.0.0.1',\n    }\n)\n\nfakedns_alternate = container(\n    image='{fakedns_image.id}',\n    environment={\n        'dns_example_com__a': '192.168.192.168',\n    }\n)\n\n@pytest.fixture(scope='function', params=['fakedns_localhost', 'fakedns_alternate'])\ndef fakedns(request):\n      return request.getfixturevalue(request.param)\n\napiserver_image = build(\n    path='examples/resolver-service/api',\n)\n\napiserver = container(\n    image='{apiserver_image.id}',\n    ports={\n        '8080/tcp': none,\n    },\n    dns=['{fakedns.ips.primary}']\n)\n\n\n@pytest.fixture\ndef apiclient(apiserver):\n    port = apiserver.ports['8080/tcp'][0]\n    return httpconnection(f'localhost:{port}')\n```\n\nthe test is the same as the first example, only now it will be tested against 2 different fake configurations.\n\n```python\n# test_smoketest.py\n\nimport ipaddress\nimport json\n\n\ndef test_api_server(apiclient):\n    apiclient.request('get', '/')\n    response = apiclient.getresponse()\n    assert response.status == 200\n    result = json.loads(response.read())\n    ipaddress.ip_address(result['result'])\n```\n\nthis test will be invoked twice - once against the memory backend, and once against the sqlite backend.\n\n### fixture wrappers\n\nyou can wrap your fixtures with a `wrapper_class`. this allows you to add helper methods to fixtures for use in your tests. in the case of the `container` fixture factory you can also implement `ready()` to add additional container readyness checks.\n\nin previous tests we've created an entire test client fixture. with `wrapper_class` we could hang this convenience method off the fixture itself instead:\n\n```python\n# test_fixture_wrappers.py\n\nimport ipaddress\nimport json\nimport random\n\nfrom http.client import httpconnection\n\nimport pytest\nfrom pytest_docker_tools import build, container\nfrom pytest_docker_tools import wrappers\n\n\nclass container(wrappers.container):\n\n    def ready(self):\n        # this is called until it returns true - its a great hook for e.g.\n        # waiting until a log message appears or a pid file is created etc\n        if super().ready():\n            return random.choice([true, false])\n        return false\n\n    def client(self):\n        port = self.ports['8080/tcp'][0]\n        return httpconnection(f'localhost:{port}')\n\n\nfakedns_image = build(\n    path='examples/resolver-service/dns',\n)\n\nfakedns = container(\n    image='{fakedns_image.id}',\n    environment={\n        'dns_example_com__a': '127.0.0.1',\n    }\n)\n\napiserver_image = build(\n    path='examples/resolver-service/api',\n)\n\napiserver = container(\n    image='{apiserver_image.id}',\n    ports={\n        '8080/tcp': none,\n    },\n    dns=['{fakedns.ips.primary}'],\n    wrapper_class=container,\n)\n\n\ndef test_container_wrapper_class(apiserver):\n    client = apiserver.client()\n    client.request('get', '/')\n    response = client.getresponse()\n    assert response.status == 200\n    result = json.loads(response.read())\n    ipaddress.ip_address(result['result'])\n\n```\n\n### referencing non-string returning fixtures\n\nyou can define resources by calling the provided factories with parameters:\n\n```python\nfrom pytest_docker_tools import container\n\ncache = container(\n    name='my_cache_container',\n    image='1838567e84867e8498695403067879',\n    environment={\n        'foo': 'bar',\n        'mode': 'prod'\n        }\n    )\n```\n\nas given in previous examples it's possible to resolve factory arguments dynamically by referencing fixtures in a string templates like manner  ('{<fixture_name>}'):\n\n```python\nfrom pytest_docker_tools import container, fetch\n\ncache_image = fetch(repository='memcached:latest')\n\ncache = container(\n    name='my_cache_container',\n    image='{cache_image.id}',\n    environment={\n        'foo': 'bar',\n        'mode': 'prod'\n        }\n    )\n```\n\nin this example the image id will be obtained from the image wrapper object that is provided by `fetch()`. however this only allows to retrieve values that are string like. e.g. it's not possible to dynamically obtain a dictionary object for the `envinronment` argument by using the string template like syntax. doing so would only result in a stringified dictionary.\n\nto obtain non string return value from a fixture there are two options. first you can define another fixture in the same file or import the fixture. afterwards you need to reference it as follows:\n\n```python\nimport pytest\nfrom pytest_docker_tools import container, fetch, fxtr\n\ncache_image = fetch(repository='memcached:latest')\n\n@pytest.fixture()\ndef memcached_env():\n    yield {'foo': 'bar',\n           'mode': 'prod'}\n\ncache = container(\n    name='my_cache_container',\n    image='{cache_image.id}',\n    environment=memcached_env\n    )\n```\n\nhowever normally working with fixtures in pytest does not require importing them in the first place. this is where the `fxtr` helper function can be used:\n\n```python\nimport pytest\nfrom pytest_docker_tools import container, fetch, fxtr\n\ncache_image = fetch(repository='memcached:latest')\n\n@pytest.fixture()\ndef memcached_env():\n    yield {'foo': 'bar',\n           'mode': 'prod'}\n\ncache = container(\n    name='my_cache_container',\n    image='{cache_image.id}',\n    environment=fxtr('memcached_env')\n    )\n```\n\nin both examples a proper dict object is handed over to the container function. for container resources this is useful to dynamically set environments or volumes based on fixtures.\n\n### reusable containers\n\nby default, the container fixture factory of pytest-docker-tools will create every defined container when pytest is invoked, and clean them up before the session ends. this ensures that your test environment is clean and your tests aren't passing because of some tate left in the containers previously.\n\nsometimes this behavior might not be what you want. when you are developing iteratively and running the tests over and over again the speed of your \"test cycle\" (how long it takes to fix a typo and re-run the tests) becomes important. when using the `--reuse-containers` command line argument pytest-docker-tools **won't** automatically remove containers it has created. it will try to reuse them between pytest invocations. pytest-docker-tools will also keep track of if a container, volume or network has become stale (for example, if you change an image version) and automatically replace it.\n\n**attention**: when using `--reuse-containers` you must set the `name` attribute on all your pytest-docker-tools fixtures. if you don't use `--reuse-containers` setting the `name` attribute is not required.\n\n#### notes on using reusable containers\n\n* resources created using the `--reuse-containers` argument (containers, networks, volumes) will not have a finalizer, so scopes will may not behave like they normally would. it is up to the test author to make sure there are no collisions where 2 different fixtures share a name.\n* when reusing resources you are responsible to clean them up (e.g. databases, volume data) as data written during tests will not be deleted when they are finished.\n* each resource container created by pytest-docker-tools will get the following label: `creator: pytest-docker-tools`. when required, this can be used to search for left over\n  resources. for example containers can be manually cleaned up by executing `docker ps -aq --filter \"label=creator=pytest-docker-tools\" | xargs docker rm -f`\n\n## hacking\n\nthis project usings poetry. you need a working poetry and python 3 environment before setting up a development environment for `pytest-docker-tools`. when you do you can just:\n\n```\npoetry install\n```\n\nto run all the linters and tests run `./scripts/test.sh` within poetry:\n\n```\npoetry run ./scripts/tests.sh\n```\n\nthis will run `pyupgrade`, `isort` and `black` which will modify your changes in place to match the code style we use.\n",
  "docs_url": null,
  "keywords": "devops,docker,pytest",
  "license": "apache-2.0",
  "name": "pytest-docker-tools",
  "package_url": "https://pypi.org/project/pytest-docker-tools/",
  "project_url": "https://pypi.org/project/pytest-docker-tools/",
  "project_urls": {
    "Homepage": "https://github.com/Jc2k/pytest-docker-tools"
  },
  "release_url": "https://pypi.org/project/pytest-docker-tools/3.1.3/",
  "requires_dist": [
    "pytest (>=6.0.1)",
    "docker (>=4.3.1)"
  ],
  "requires_python": ">=3.7.0,<4.0.0",
  "summary": "docker integration tests for pytest",
  "version": "3.1.3",
  "releases": [],
  "developers": [
    "john.carr@unrouted.co.uk",
    "john_carr"
  ],
  "kwds": "pytest_docker_tools pytest test_container_starts test_container_created docker",
  "license_kwds": "apache-2.0",
  "libtype": "pypi",
  "id": "pypi_pytest_docker_tools",
  "homepage": "https://github.com/jc2k/pytest-docker-tools",
  "release_count": 28,
  "dependency_ids": [
    "pypi_docker",
    "pypi_pytest"
  ]
}