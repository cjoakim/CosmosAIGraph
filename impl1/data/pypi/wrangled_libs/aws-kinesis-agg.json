{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: developers",
    "license :: osi approved :: apache software license",
    "natural language :: english",
    "operating system :: os independent",
    "programming language :: python",
    "programming language :: python :: 2.7",
    "programming language :: python :: 3.6",
    "topic :: software development :: libraries :: python modules",
    "topic :: utilities"
  ],
  "description": "# python kinesis aggregation & deaggregation modules\n\nthe kinesis aggregation/deaggregation modules for python provide the ability to do in-memory aggregation and deaggregation of standard kinesis user records using the [kinesis aggregated record format](https://github.com/awslabs/amazon-kinesis-producer/blob/master/aggregation-format.md) to allow for more efficient transmission of records.\n\n## installation\n\nthe python record aggregation/deaggregation modules are available on the python package index (pypi) as [aws_kinesis_agg](https://pypi.python.org/pypi/aws_kinesis_agg).  you can install it via the `pip` command line tool:\n\n```\npip install aws_kinesis_agg\n```\n\nalternately, you can simply copy the aws_kinesis_agg module from this repository and use it directly with the caveat that the [google protobuf module](https://pypi.python.org/pypi/protobuf) must also be available (if you install via `pip`, this dependency will be handled for you).\n\n## record aggregation module (aggregator.py)\n\nthe [aggregator.py](aggregator.py) module contains python classes that allow you to aggregate records using the [kinesis aggregated record format](https://github.com/awslabs/amazon-kinesis-producer/blob/master/aggregation-format.md).  using record aggregation improves throughput and reduces costs when writing producer applications that publish data to amazon kinesis.\n\n### caution - this module is only suitable for low-value messages which are processed in aggregate. do not use kinesis aggregation for data which is sensitive or where every message must be delivered, and where the kcl (including with aws lambda) is used for processing. [data loss can occur.](../potential_data_loss.md)\n\n### usage\n\nthe record aggregation module provides a simple interface for creating protocol buffers encoded data in a producer application. the `aws_kinesis_agg` module provides methods for efficiently packing individual records into larger aggregated records, and deaggregating large records into a set of 'real' user records.\n\nwhen using aggregation, you create a managing class which helps you to target the correct kinesis shard, and then provide a partition key, raw data and (optionally) an explicit hash key for each record.  you can choose to either provide a callback function that will be invoked when a fully-packed aggregated record is available or you can add records and check byte sizes or number of records until the aggregated record is suitably full.  you're guaranteed that any aggregated record returned from the recordaggregator object will fit within a single putrecord request to kinesis. as you produce records in your producer application, you will aggregate them using a base `recordaggregator` object, which provides methods to do both iterative aggregation and callback-based aggregation.\n\nthere are two ways to create aggregated user records. the first is to use a raw `recordaggregator`, which can aggregate messages *which are targeted for a single shard*, or use the `aggregationmanager` to aggregate messages which may span shards. from version `1.2.0`, we __highly__ recommend the use of `aggregationmanager` to limit any exposure to data loss.\n\n### raw aggregation\n\nyou can construct a raw `recordaggregator` class with:\n\n```\nimport aws_kinesis_agg as agg\nkinesis_aggregator = agg.recordaggregator()\n```\n\n#### iterative aggregation\n\nthe iterative aggregation method involves adding records one at a time to the recordaggregator and checking the response to determine when a full aggregated record is available.  the `add_user_record` method returns `none` when there is room for more records in the existing aggregated record and returns an `aggrecord` object when a full aggregated record is available for transmission.\n\n```\nfor rec in records:\n    result = kinesis_aggregator.add_user_record(rec.partitionkey, rec.data, rec.explicithashkey)\n    if result:\n        #send the result to kinesis    \n```\n\n#### callback-based aggregation\n\nto use callback-based aggregation, you must register a callback via the `on_record_complete` method.  as you add individual records to the `recordaggregator` object, you will receive a callback (on a separate thread) whenever a new fully-packed aggregated record is available.\n\n```\ndef my_callback(agg_record):\n    #send the record to kinesis\n   \n...\n\nkinesis_aggregator.on_record_complete(my_callback)\nfor rec in records:\n    kinesis_aggregator.add_user_record(rec.partitionkey, rec.data, rec.explicithashkey)\n```\n\n### examples\n\nthis repository includes an example script that uses the record aggregation module [aggregator.py](aggregator.py) to aggregate records and transmit them to amazon kinesis using callback-based aggregation. you can find this example functionality in the file [kinesis_publisher.py](src/kinesis_publisher.py), which you can use as a template for your own applications to to easily build and transmit encoded data.\n\n#### callback-based aggregation and transmission example\n\nthe example below assumes you are running python version 2.7.x and also requires you to install and configure the `boto3` module.  you can install `boto3` via `pip install boto3` or any other normal python install mechanism.  to configure the example to be able to publish to your kinesis stream, make sure you follow the instructions in the [boto3 configuration guide](https://boto3.readthedocs.org/en/latest/guide/configuration.html).  the example below has been stripped down for brevity, but you can still find the full working version at [kinesis_publisher.py](src/kinesis_publisher.py). the abridged example is:\n\n```\nimport boto3\nimport aws_kinesis_agg.aggregator\n    \nkinesis_client = none\n    \ndef send_record(agg_record):\n    global kinesis_client\n    pk, ehk, data = agg_record.get_contents()\n    kinesis_client.put_record(streamname='mykinesisstreamname',\n                                  data=data,\n                                  partitionkey=pk,\n                                  explicithashkey=ehk)\n    \nif __name__ == '__main__':\n    kinesis_client = boto3.client('kinesis', region_name='us-west-2')\n     \n    kinesis_agg = aws_kinesis_agg.aggregator.recordaggregator()\n    kinesis_agg.on_record_complete(send_record)\n    \n    for i in range(0,1024):\n        pk, ehk, data = get_record(...)\n        kinesis_agg.add_user_record(pk, data, ehk)\n    \n    #clear out any remaining records that didn't trigger a callback yet\n    send_record(kinesis_agg.clear_and_get()) \n```\n\n\n## record deaggregation module (deaggregator.py)\n\nthe [deaggregator.py](deaggregator.py) module contains python classes that allow you to deaggregate records that were transmitted using the [kinesis aggregated record format](https://github.com/awslabs/amazon-kinesis-producer/blob/master/aggregation-format.md), including those transmitted by the kinesis producer library.  this library will allow you to deaggregate aggregated records in any python environment, including aws lambda.\n\n### usage\n\nthe record deaggregation module provides a simple interface for working with kinesis aggregated message data in a consumer application. the `aws_kinesis_agg` module provides methods for both bulk and generator-based processing. \n\nwhen using deaggregation, you provide an aggregated kinesis record and get back multiple kinesis user records. if a kinesis record that is provided is not an aggregated kinesis record, that's perfectly fine - you'll just get a single record output from the single record input. a kinesis user record which is returned from deaggregation looks like:\n\n```\n{\n    'eventversion' : string - the version number of the kinesis event used\n    'eventid' : string - the unique id of this kinesis event\n    'kinesis' :\n    {\n        'partitionkey' : string - the partition key provided when the record was submitted\n        'explicithashkey' : string - the hash value used to explicitly determine the shard the data record is assigned to by overriding the partition key hash (or none if absent) \n        'data' : string - the original data transmitted by the producer (base64 encoded)\n        'kinesisschemaversion' : string - the version number of the kinesis message schema used,\n        'sequencenumber' : bigint - the sequence number assigned to the record on submission to kinesis\n        'subsequencenumber' : int - the sub-sequence number for the user record in the aggregated record, if aggregation was in use by the producer\n        'aggregated' : boolean - always true for a user record extracted from a kinesis aggregated record\n    },\n    'invokeidentityarn' : string - the arn of the iam user used to invoke this lambda function\n    'eventname' : string - always \"aws:kinesis:record\" for a kinesis record\n    'eventsourcearn' : string - the arn of the source kinesis stream\n    'eventsource' : string - always \"aws:kinesis\" for a kinesis record\n    'awsregion' : string - the name of the source region for the event (e.g. \"us-east-1\")\n}\n```\n\nto get started, import the `aws_kinesis_agg` module:\n\n`import aws_kinesis_agg`\n\nnext, when you receive a kinesis record in your consumer application, you will extract the user records using the deaggregation methods available in the `aws_kinesis_agg` module.\n\n**important**: the deaggregation methods available in the `aws_kinesis_agg` module expect input records in the same dictionary-based format that they are normally received in from aws lambda. see the [programming model for authoring lambda functions in python](https://docs.aws.amazon.com/lambda/latest/dg/python-programming-model.html) section of the aws documentation for more details.\n\n#### bulk conversion\n\nthe bulk conversion method of deaggregation takes in a list of kinesis records, extracts all the aggregated user records and accumulates them into a list.  any records that are passed in to this method that are not kinesis aggregated records will be returned unchanged.  the method returns a list of kinesis user records in the same format as they are normally delivered by lambda's kinesis event handler.\n\n```\nuser_records = deaggregate_records(raw_kinesis_records)\n```\n\n#### generator-based conversion\n\nthe generator-based conversion method of deaggregation uses a python [generator function](https://wiki.python.org/moin/generators) to extract user records from a raw kinesis record one at a time in an iterative fashion.  any records that are passed in to this method that are not kinesis aggregated records will be returned unchanged.  for example, you could use this code to iterate through each deaggregated record:\n\n```\nfor record in iter_deaggregate_records(raw_kinesis_records):        \n        \n    #process each record\n    pass \n```\n\n### examples\n\nthis module includes two example aws lambda function in the file [lambda_function.py](src/lambda_function.py) that give you the ability to easily build new functions to process kinesis aggregated data via aws lambda.\n\n#### bulk conversion example\n\n```\nfrom __future__ import print_function\n\nfrom aws_kinesis_agg.deaggregator import deaggregate_records\nimport base64\n\ndef lambda_bulk_handler(event, context):\n    \n    raw_kinesis_records = event['records']\n    \n    #deaggregate all records in one call\n    user_records = deaggregate_records(raw_kinesis_records)\n    \n    #iterate through deaggregated records\n    for record in user_records:        \n        \n        # kinesis data in python lambdas is base64 encoded\n        payload = base64.b64decode(record['kinesis']['data'])\n        \n        #todo: process each record\n    \n    return 'successfully processed {} records.'.format(len(user_records))\n```\n\n#### generator-based conversion example\n\n```\nfrom __future__ import print_function\n\nfrom aws_kinesis_agg.deaggregator import iter_deaggregate_records\nimport base64\n\ndef lambda_generator_handler(event, context):\n    \n    raw_kinesis_records = event['records']\n    record_count = 0\n    \n    #deaggregate all records using a generator function\n    for record in iter_deaggregate_records(raw_kinesis_records):   \n             \n        # kinesis data in python lambdas is base64 encoded\n        payload = base64.b64decode(record['kinesis']['data'])\n       \n        #todo: process each record\n       \n        record_count += 1\n        \n    return 'successfully processed {} records.'.format(record_count)\n```\n\n### build & deploy a lambda function to process kinesis records\n\none easy way to get started processing kinesis data is to use aws lambda.  by building on top of the existing [lambda_function.py](lambda_function.py) module in this repository, you can take advantage of kinesis message deaggregation features without having to write boilerplate code.\n\nwhen you're ready to make a build and upload to aws lambda, you have two choices:\n\n* follow the existing instructions at [creating a deployment package (python)](https://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html)\n\nor \n\n* at the root of this python project, you can find a sample build file called [make_lambda_build.py](make_lambda_build.py).  this file is a platform-agnostic build script that will take the existing python project in this demo and package it in a single build file called `python_lambda_build.zip` that you can upload directly to aws lambda.\n\nin order to use the build script, make sure that the python `pip` tool is available on your command line.  if you have other `pip` dependencies, make sure to add them to the `pip_dependencies` list at the top of the [make_lambda_build.py](make_lambda_build.py).  then run this command:\n\n```\npython make_lambda_build.py --output_dir <mydir>\n```\n\nthe build script will create a new folder called `build`, copy all the python source files, download any necessary dependencies via `pip` and create the file `<mydir>/python_lambda_build.zip` that you can deploy to aws lambda.\n\n#### important build note for aws lambda users\n\nif you choose to make your own python zip file to deploy to aws lambda, be aware that the google [protobuf](https://pypi.python.org/pypi/protobuf) module normally relies on using a python `pth` setting to make the root `google` module importable.  if you see an error in your aws lambda logs such as:\n\n```\n\"unable to import module 'lambda_function': no module named google.protobuf\"\n```\n\nyou can go into the `google` module folder (the same folder containing the `protobuf` folder) and make an empty file called `__init__.py`.  once you rezip everything and redeploy, this should fix the error above.\n\n**note**: if you used the provided [`make_lambda_build.py`](make_lambda_build.py) script, this issue is already handled for you.\n \n----\n\ncopyright amazon.com, inc. or its affiliates. all rights reserved.\n\nlicensed under the apache license, version 2.0 (the \"license\");\nyou may not use this file except in compliance with the license.\nyou may obtain a copy of the license at\n\n   http://www.apache.org/licenses/license-2.0\n\nunless required by applicable law or agreed to in writing, software\ndistributed under the license is distributed on an \"as is\" basis,\nwithout warranties or conditions of any kind, either express or implied.\nsee the license for the specific language governing permissions and\nlimitations under the license.\n",
  "docs_url": null,
  "keywords": "aws,kinesis,aggregation,deaggregation,kpl",
  "license": "apache-2.0",
  "name": "aws-kinesis-agg",
  "package_url": "https://pypi.org/project/aws-kinesis-agg/",
  "project_url": "https://pypi.org/project/aws-kinesis-agg/",
  "project_urls": {
    "Homepage": "http://github.com/awslabs/kinesis-aggregation"
  },
  "release_url": "https://pypi.org/project/aws-kinesis-agg/1.2.3/",
  "requires_dist": [
    "protobuf"
  ],
  "requires_python": "",
  "summary": "python module to assist in taking advantage of the kinesis message aggregation format for both aggregation and deaggregation.",
  "version": "1.2.3",
  "releases": [],
  "developers": [
    "ian_meyers",
    "meyersi@amazon.com"
  ],
  "kwds": "aws_kinesis_agg kinesis_aggregator raw_kinesis_records kinesis_agg kinesis",
  "license_kwds": "apache-2.0",
  "libtype": "pypi",
  "id": "pypi_aws_kinesis_agg",
  "homepage": "http://github.com/awslabs/kinesis-aggregation",
  "release_count": 14,
  "dependency_ids": [
    "pypi_protobuf"
  ]
}