{
  "classifiers": [
    "development status :: 5 - production/stable",
    "license :: osi approved :: apache software license",
    "operating system :: macos :: macos x",
    "operating system :: microsoft :: windows",
    "operating system :: posix :: linux",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.12",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "<!-- markdownlint-disable-next-line md041 -->\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/dbt-athena/dbt-athena/main/static/images/dbt-athena-long.png\" />\n    <a href=\"https://pypi.org/project/dbt-athena-community/\">\n      <img src=\"https://badge.fury.io/py/dbt-athena-community.svg\" />\n    </a>\n    <a target=\"_blank\" href=\"https://pypi.org/project/dbt-athena-community/\" style=\"background:none\">\n      <img src=\"https://img.shields.io/pypi/pyversions/dbt-athena-community\">\n    </a>\n    <a href=\"https://pycqa.github.io/isort/\">\n      <img src=\"https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelcolor=ef8336\" />\n    </a>\n    <a href=\"https://github.com/psf/black\"><img src=\"https://img.shields.io/badge/code%20style-black-000000.svg\" /></a>\n    <a href=\"https://github.com/python/mypy\"><img src=\"https://www.mypy-lang.org/static/mypy_badge.svg\" /></a>\n    <a href=\"https://pepy.tech/project/dbt-athena-community\">\n      <img src=\"https://static.pepy.tech/badge/dbt-athena-community/month\" />\n    </a>\n</p>\n<!-- toc -->\n\n- [features](#features)\n  - [quick start](#quick-start)\n    - [installation](#installation)\n    - [prerequisites](#prerequisites)\n    - [credentials](#credentials)\n    - [configuring your profile](#configuring-your-profile)\n    - [additional information](#additional-information)\n  - [models](#models)\n    - [table configuration](#table-configuration)\n    - [table location](#table-location)\n    - [incremental models](#incremental-models)\n    - [on schema change](#on-schema-change)\n    - [iceberg](#iceberg)\n    - [highly available table (ha)](#highly-available-table-ha)\n      - [ha known issues](#ha-known-issues)\n  - [snapshots](#snapshots)\n    - [timestamp strategy](#timestamp-strategy)\n    - [check strategy](#check-strategy)\n    - [hard-deletes](#hard-deletes)\n    - [aws lakeformation integration](#aws-lakeformation-integration)\n    - [working example](#working-example)\n    - [snapshots known issues](#snapshots-known-issues)\n  - [contracts](#contracts)\n  - [contributing](#contributing)\n  - [contributors \u2728](#contributors-)\n<!-- toc -->\n\n# features\n\n- supports dbt version `1.6.*`\n- support for python\n- supports [seeds][seeds]\n- correctly detects views and their columns\n- supports [table materialization][table]\n  - [iceberg tables][athena-iceberg] are supported **only with athena engine v3** and **a unique table location**\n    (see table location section below)\n  - hive tables are supported by both athena engines.\n- supports [incremental models][incremental]\n  - on iceberg tables :\n    - supports the use of `unique_key` only with the `merge` strategy\n    - supports the `append` strategy\n  - on hive tables :\n    - supports two incremental update strategies: `insert_overwrite` and `append`\n    - does **not** support the use of `unique_key`\n- supports [snapshots][snapshots]\n- does not support [python models][python-models]\n\n[seeds]: https://docs.getdbt.com/docs/building-a-dbt-project/seeds\n\n[incremental]: https://docs.getdbt.com/docs/build/incremental-models\n\n[table]: https://docs.getdbt.com/docs/build/materializations#table\n\n[python-models]: https://docs.getdbt.com/docs/build/python-models#configuring-python-models\n\n[athena-iceberg]: https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg.html\n\n[snapshots]: https://docs.getdbt.com/docs/build/snapshots\n\n## quick start\n\n### installation\n\n- `pip install dbt-athena-community`\n- or `pip install git+https://github.com/dbt-athena/dbt-athena.git`\n\n### prerequisites\n\nto start, you will need an s3 bucket, for instance `my-bucket` and an athena database:\n\n```sql\ncreate database if not exists analytics_dev\ncomment 'analytics models generated by dbt (development)'\nlocation 's3://my-bucket/'\nwith dbproperties ('creator'='foo bar', 'email'='foo@bar.com');\n```\n\nnotes:\n\n- take note of your aws region code (e.g. `us-west-2` or `eu-west-2`, etc.).\n- you can also use [aws glue](https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html) to create and manage athena\n  databases.\n\n### credentials\n\ncredentials can be passed directly to the adapter, or they can\nbe [determined automatically](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) based\non `aws cli`/`boto3` conventions.\nyou can either:\n\n- configure `aws_access_key_id` and `aws_secret_access_key`\n- configure `aws_profile_name` to match a profile defined in your aws credentials file\n  checkout dbt profile configuration below for details.\n\n### configuring your profile\n\na dbt profile can be configured to run against aws athena using the following configuration:\n\n| option                | description                                                                              | required? | example                                    |\n| --------------------- | ---------------------------------------------------------------------------------------- | --------- | ------------------------------------------ |\n| s3_staging_dir        | s3 location to store athena query results and metadata                                   | required  | `s3://bucket/dbt/`                         |\n| s3_data_dir           | prefix for storing tables, if different from the connection's `s3_staging_dir`           | optional  | `s3://bucket2/dbt/`                        |\n| s3_data_naming        | how to generate table paths in `s3_data_dir`                                             | optional  | `schema_table_unique`                      |\n| s3_tmp_table_dir      | prefix for storing temporary tables, if different from the connection's `s3_data_dir`    | optional  | `s3://bucket3/dbt/`                        |\n| region_name           | aws region of your athena instance                                                       | required  | `eu-west-1`                                |\n| schema                | specify the schema (athena database) to build models into (lowercase **only**)           | required  | `dbt`                                      |\n| database              | specify the database (data catalog) to build models into (lowercase **only**)            | required  | `awsdatacatalog`                           |\n| poll_interval         | interval in seconds to use for polling the status of query results in athena             | optional  | `5`                                        |\n| debug_query_state     | flag if debug message with athena query state is needed                                  | optional  | `false`                                    |\n| aws_access_key_id     | access key id of the user performing requests.                                           | optional  | `akiaiosfodnn7example`                     |\n| aws_secret_access_key | secret access key of the user performing requests                                        | optional  | `wjalrxutnfemi/k7mdeng/bpxrficyexamplekey` |\n| aws_profile_name      | profile to use from your aws shared credentials file.                                    | optional  | `my-profile`                               |\n| work_group            | identifier of athena workgroup                                                           | optional  | `my-custom-workgroup`                      |\n| num_retries           | number of times to retry a failing query                                                 | optional  | `3`                                        |\n| num_boto3_retries     | number of times to retry boto3 requests (e.g. deleting s3 files for materialized tables) | optional  | `5`                                        |\n| seed_s3_upload_args   | dictionary containing boto3 extraargs when uploading to s3                               | optional  | `{\"acl\": \"bucket-owner-full-control\"}`     |\n| lf_tags_database      | default lf tags for new database if it's created by dbt                                  | optional  | `tag_key: tag_value`                       |\n\n**example profiles.yml entry:**\n\n```yaml\nathena:\n  target: dev\n  outputs:\n    dev:\n      type: athena\n      s3_staging_dir: s3://athena-query-results/dbt/\n      s3_data_dir: s3://your_s3_bucket/dbt/\n      s3_data_naming: schema_table\n      s3_tmp_table_dir: s3://your_s3_bucket/temp/\n      region_name: eu-west-1\n      schema: dbt\n      database: awsdatacatalog\n      aws_profile_name: my-profile\n      work_group: my-workgroup\n      seed_s3_upload_args:\n        acl: bucket-owner-full-control\n```\n\n### additional information\n\n- `threads` is supported\n- `database` and `catalog` can be used interchangeably\n\n## models\n\n### table configuration\n\n- `external_location` (`default=none`)\n  - if set, the full s3 path in which the table will be saved.\n  - does not work with iceberg table or hive table with `ha` set to true.\n- `partitioned_by` (`default=none`)\n  - an array list of columns by which the table will be partitioned\n  - limited to creation of 100 partitions (*currently*)\n- `bucketed_by` (`default=none`)\n  - an array list of columns to bucket data, ignored if using iceberg\n- `bucket_count` (`default=none`)\n  - the number of buckets for bucketing your data, ignored if using iceberg\n- `table_type` (`default='hive'`)\n  - the type of table\n  - supports `hive` or `iceberg`\n- `ha` (`default=false`)\n  - if the table should be built using the high-availability method. this option is only available for hive tables\n    since it is by default for iceberg tables (see the section [below](#highly-available-table-ha))\n- `format` (`default='parquet'`)\n  - the data format for the table\n  - supports `orc`, `parquet`, `avro`, `json`, `textfile`\n- `write_compression` (`default=none`)\n  - the compression type to use for any storage format that allows compression to be specified. to see which options are\n    available, check out [create table as][create-table-as]\n- `field_delimiter` (`default=none`)\n  - custom field delimiter, for when format is set to `textfile`\n- `table_properties`: table properties to add to the table, valid for iceberg only\n- `native_drop`: relation drop operations will be performed with sql, not direct glue api calls. no s3 calls will be\n  made to manage data in s3. data in s3 will only be cleared up for iceberg\n  tables [see aws docs](https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-managing-tables.html). note that\n  iceberg drop table operations may timeout if they take longer than 60 seconds.\n- `seed_by_insert` (`default=false`)\n  - default behaviour uploads seed data to s3. this flag will create seeds using an sql insert statement\n  - large seed files cannot use `seed_by_insert`, as the sql insert statement would\n    exceed [the athena limit of 262144 bytes](https://docs.aws.amazon.com/athena/latest/ug/service-limits.html)\n- `force_batch` (`default=false`)\n  - skip creating the table as ctas and run the operation directly in batch insert mode.\n  - this is particularly useful when the standard table creation process fails due to partition limitations,\n  allowing you to work with temporary tables and persist the dataset more efficiently.\n- `lf_tags_config` (`default=none`)\n  - [aws lakeformation](#aws-lakeformation-integration) tags to associate with the table and columns\n  - `enabled` (`default=false`) whether lf tags management is enabled for a model\n  - `tags` dictionary with tags and their values to assign for the model\n  - `tags_columns` dictionary with a tag key, value and list of columns they must be assigned to\n  - `lf_inherited_tags` (`default=none`)\n    - list of lake formation tag keys that are intended to be inherited from the database level and thus shouldn't be\n      removed during association of those defined in `lf_tags_config`\n      - i.e., the default behavior of `lf_tags_config` is to be exhaustive and first remove any pre-existing tags from\n        tables and columns before associating the ones currently defined for a given model\n      - this breaks tag inheritance as inherited tags appear on tables and columns like those associated directly\n\n```sql\n{{\n  config(\n    materialized='incremental',\n    incremental_strategy='append',\n    on_schema_change='append_new_columns',\n    table_type='iceberg',\n    schema='test_schema',\n    lf_tags_config={\n          'enabled': true,\n          'tags': {\n            'tag1': 'value1',\n            'tag2': 'value2'\n          },\n          'tags_columns': {\n            'tag1': {\n              'value1': ['column1', 'column2'],\n              'value2': ['column3', 'column4']\n            }\n          },\n          'inherited_tags': ['tag1', 'tag2']\n    }\n  )\n}}\n```\n\n- format for `dbt_project.yml`:\n\n```yaml\n  +lf_tags_config:\n    enabled: true\n    tags:\n      tag1: value1\n      tag2: value2\n    tags_columns:\n      tag1:\n        value1: [ column1, column2 ]\n    inherited_tags: [ tag1, tag2 ]\n```\n\n- `lf_grants` (`default=none`)\n  - lakeformation grants config for data_cell filters\n  - format:\n\n  ```python\n  lf_grants={\n          'data_cell_filters': {\n              'enabled': true | false,\n              'filters': {\n                  'filter_name': {\n                      'row_filter': '<filter_condition>',\n                      'principals': ['principal_arn1', 'principal_arn2']\n                  }\n              }\n          }\n      }\n  ```\n\n> notes:  \n>\n> - `lf_tags` and `lf_tags_columns` configs support only attaching lf tags to corresponding resources.\n> we recommend managing lf tags permissions somewhere outside dbt. for example, you may use\n> [terraform](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/lakeformation_permissions) or\n> [aws cdk](https://docs.aws.amazon.com/cdk/api/v1/docs/aws-lakeformation-readme.html) for such purpose.\n> - `data_cell_filters` management can't be automated outside dbt because the filter can't be attached to the table\n> which doesn't exist. once you `enable` this config, dbt will set all filters and their permissions during every\n> dbt run. such approach keeps the actual state of row level security configuration actual after every dbt run and\n> apply changes if they occur: drop, create, update filters and their permissions.\n> - any tags listed in `lf_inherited_tags` should be strictly inherited from the database level and never overridden at\n    the table and column level\n>   - currently `dbt-athena` does not differentiate between an inherited tag association and an override of same it made\n>     previously\n>   - e.g. if an inherited tag is overridden by an `lf_tags_config` value in one dbt run, and that override is removed\n      prior to a subsequent run, the prior override will linger and no longer be encoded anywhere (in e.g. terraform\n      where the inherited value is configured nor in the dbt project where the override previously existed but now is\n      gone)\n\n[create-table-as]: https://docs.aws.amazon.com/athena/latest/ug/create-table-as.html#ctas-table-properties\n\n### table location\n\nthe location in which a table is saved is determined by:\n\n1. if `external_location` is defined, that value is used.\n2. if `s3_data_dir` is defined, the path is determined by that and `s3_data_naming`\n3. if `s3_data_dir` is not defined, data is stored under `s3_staging_dir/tables/`\n\nhere all the options available for `s3_data_naming`:\n\n- `unique`: `{s3_data_dir}/{uuid4()}/`\n- `table`: `{s3_data_dir}/{table}/`\n- `table_unique`: `{s3_data_dir}/{table}/{uuid4()}/`\n- `schema_table`: `{s3_data_dir}/{schema}/{table}/`\n- `s3_data_naming=schema_table_unique`: `{s3_data_dir}/{schema}/{table}/{uuid4()}/`\n\nit's possible to set the `s3_data_naming` globally in the target profile, or overwrite the value in the table config,\nor setting up the value for groups of model in dbt_project.yml.\n\n> note: when using a workgroup with a default output location configured, `s3_data_naming` and any configured buckets\n> are ignored and the location configured in the workgroup is used.\n\n### incremental models\n\nsupport for [incremental models](https://docs.getdbt.com/docs/build/incremental-models).\n\nthese strategies are supported:\n\n- `insert_overwrite` (default): the insert overwrite strategy deletes the overlapping partitions from the destination\n  table, and then inserts the new records from the source. this strategy depends on the `partitioned_by` keyword! if no\n  partitions are defined, dbt will fall back to the `append` strategy.\n- `append`: insert new records without updating, deleting or overwriting any existing data. there might be duplicate\n  data (e.g. great for log or historical data).\n- `merge`: conditionally updates, deletes, or inserts rows into an iceberg table. used in combination with `unique_key`.\n  only available when using iceberg.\n\n### on schema change\n\n`on_schema_change` is an option to reflect changes of schema in incremental models.\nthe following options are supported:\n\n- `ignore` (default)\n- `fail`\n- `append_new_columns`\n- `sync_all_columns`\n\nfor details, please refer\nto [dbt docs](https://docs.getdbt.com/docs/build/incremental-models#what-if-the-columns-of-my-incremental-model-change).\n\n### iceberg\n\nthe adapter supports table materialization for iceberg.\n\nto get started just add this as your model:\n\n```sql\n{{ config(\n    materialized='table',\n    table_type='iceberg',\n    format='parquet',\n    partitioned_by=['bucket(user_id, 5)'],\n    table_properties={\n     'optimize_rewrite_delete_file_threshold': '2'\n     }\n) }}\n\nselect 'a'          as user_id,\n       'pi'         as name,\n       'active'     as status,\n       17.89        as cost,\n       1            as quantity,\n       100000000    as quantity_big,\n       current_date as my_date\n```\n\niceberg supports bucketing as hidden partitions, therefore use the `partitioned_by` config to add specific bucketing\nconditions.\n\niceberg supports several table formats for data : `parquet`, `avro` and `orc`.\n\nit is possible to use iceberg in an incremental fashion, specifically two strategies are supported:\n\n- `append`: new records are appended to the table, this can lead to duplicates.\n- `merge`: performs an upsert (and optional delete), where new records are added and existing records are updated. only\n  available with athena engine version 3.\n  - `unique_key` **(required)**: columns that define a unique record in the source and target tables.\n  - `incremental_predicates` (optional): sql conditions that enable custom join clauses in the merge statement. this can\n    be useful for improving performance via predicate pushdown on the target table.\n  - `delete_condition` (optional): sql condition used to identify records that should be deleted.\n  - `update_condition` (optional): sql condition used to identify records that should be updated.\n  - `insert_condition` (optional): sql condition used to identify records that should be inserted.\n    - `incremental_predicates`, `delete_condition`, `update_condition` and `insert_condition` can include any column of\n      the incremental table (`src`) or the final table (`target`).\n      column names must be prefixed by either `src` or `target` to prevent a `column is ambiguous` error.\n\n`delete_condition` example:\n\n```sql\n{{ config(\n    materialized='incremental',\n    table_type='iceberg',\n    incremental_strategy='merge',\n    unique_key='user_id',\n    incremental_predicates=[\"src.quantity > 1\", \"target.my_date >= now() - interval '4' year\"],\n    delete_condition=\"src.status != 'active' and target.my_date < now() - interval '2' year\",\n    format='parquet'\n) }}\n\nselect 'a' as user_id,\n       'pi' as name,\n       'active' as status,\n       17.89 as cost,\n       1 as quantity,\n       100000000 as quantity_big,\n       current_date as my_date\n```\n\n`update_condition` example:\n\n```sql\n{{ config(\n        materialized='incremental',\n        incremental_strategy='merge',\n        unique_key=['id'],\n        update_condition='target.id > 1',\n        schema='sandbox'\n    )\n}}\n\n{% if is_incremental() %}\n\nselect * from (\n    values\n    (1, 'v1-updated')\n    , (2, 'v2-updated')\n) as t (id, value)\n\n{% else %}\n\nselect * from (\n    values\n    (-1, 'v-1')\n    , (0, 'v0')\n    , (1, 'v1')\n    , (2, 'v2')\n) as t (id, value)\n\n{% endif %}\n```\n\n`insert_condition` example:\n\n```sql\n{{ config(\n        materialized='incremental',\n        incremental_strategy='merge',\n        unique_key=['id'],\n        insert_condition='target.status != 0',\n        schema='sandbox'\n    )\n}}\n\nselect * from (\n    values\n    (1, 0)\n    , (2, 1)\n) as t (id, status)\n\n```\n\n### highly available table (ha)\n\nthe current implementation of the table materialization can lead to downtime, as target table is dropped and re-created.\nto have the less destructive behavior it's possible to use the `ha` config on your `table` materialized models.\nit leverages the table versions feature of glue catalog, creating a tmp table and swapping the target table to the\nlocation of the tmp table. this materialization is only available for `table_type=hive` and requires using unique\nlocations. for iceberg, high availability is by default.\n\n```sql\n{{ config(\n    materialized='table',\n    ha=true,\n    format='parquet',\n    table_type='hive',\n    partitioned_by=['status'],\n    s3_data_naming='table_unique'\n) }}\n\nselect 'a'      as user_id,\n       'pi'     as user_name,\n       'active' as status\nunion all\nselect 'b'        as user_id,\n       'sh'       as user_name,\n       'disabled' as status\n```\n\nby default, the materialization keeps the last 4 table versions, you can change it by setting `versions_to_keep`.\n\n#### ha known issues\n\n- when swapping from a table with partitions to a table without (and the other way around), there could be a little\n  downtime.\n  in case high performances are needed consider bucketing instead of partitions\n- by default, glue \"duplicates\" the versions internally, so the last two versions of a table point to the same location\n- it's recommended to have `versions_to_keep` >= 4, as this will avoid having the older location removed\n- the macro `athena__end_of_time` needs to be overwritten by the user if using athena engine v3 since it requires a\n  precision parameter for timestamps\n\n## snapshots\n\nthe adapter supports snapshot materialization. it supports both timestamp and check strategy. to create a snapshot\ncreate a snapshot file in the snapshots directory. if the directory does not exist create one.\n\n### timestamp strategy\n\nto use the timestamp strategy refer to\nthe [dbt docs](https://docs.getdbt.com/docs/build/snapshots#timestamp-strategy-recommended)\n\n### check strategy\n\nto use the check strategy refer to the [dbt docs](https://docs.getdbt.com/docs/build/snapshots#check-strategy)\n\n### hard-deletes\n\nthe materialization also supports invalidating hard deletes. check\nthe [docs](https://docs.getdbt.com/docs/build/snapshots#hard-deletes-opt-in) to understand usage.\n\n### aws lakeformation integration\n\nthe adapter implements aws lakeformation tags management in the following way:\n\n- you can enable or disable lf-tags management via [config](#table-configuration) (disabled by default)\n- once you enable the feature, lf-tags will be updated on every dbt run\n- first, all lf-tags for columns are removed to avoid inheritance issues\n- then all redundant lf-tags are removed from table and actual tags from config are applied\n- finally, lf-tags for columns are applied\n\nit's important to understand the following points:\n\n- dbt does not manage lf-tags for database\n- dbt does not manage lakeformation permissions\n\nthat's why you should handle this by yourself manually or using some automation tools like terraform, aws cdk etc.  \nyou may find the following links useful to manage that:\n\n<!-- markdownlint-disable -->\n* [terraform aws_lakeformation_permissions](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/lakeformation_permissions)\n* [terraform aws_lakeformation_resource_lf_tags](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/lakeformation_resource_lf_tags)\n<!-- markdownlint-restore -->\n\n### working example\n\nseed file - employent_indicators_november_2022_csv_tables.csv\n\n```csv\nseries_reference,period,data_value,suppressed\nmeim.s1wa,1999.04,80267,\nmeim.s1wa,1999.05,70803,\nmeim.s1wa,1999.06,65792,\nmeim.s1wa,1999.07,66194,\nmeim.s1wa,1999.08,67259,\nmeim.s1wa,1999.09,69691,\nmeim.s1wa,1999.1,72475,\nmeim.s1wa,1999.11,79263,\nmeim.s1wa,1999.12,86540,\nmeim.s1wa,2000.01,82552,\nmeim.s1wa,2000.02,81709,\nmeim.s1wa,2000.03,84126,\nmeim.s1wa,2000.04,77089,\nmeim.s1wa,2000.05,73811,\nmeim.s1wa,2000.06,70070,\nmeim.s1wa,2000.07,69873,\nmeim.s1wa,2000.08,71468,\nmeim.s1wa,2000.09,72462,\nmeim.s1wa,2000.1,74897,\n```\n\nmodel.sql\n\n```sql\n{{ config(\n    materialized='table'\n) }}\n\nselect row_number() over() as id\n       , *\n       , cast(from_unixtime(to_unixtime(now())) as timestamp(6)) as refresh_timestamp\nfrom {{ ref('employment_indicators_november_2022_csv_tables') }}\n```\n\ntimestamp strategy - model_snapshot_1\n\n```sql\n{% snapshot model_snapshot_1 %}\n\n{{\n    config(\n      strategy='timestamp',\n      updated_at='refresh_timestamp',\n      unique_key='id'\n    )\n}}\n\nselect *\nfrom {{ ref('model') }} {% endsnapshot %}\n```\n\ninvalidate hard deletes - model_snapshot_2\n\n```sql\n{% snapshot model_snapshot_2 %}\n\n{{\n    config\n    (\n        unique_key='id',\n        strategy='timestamp',\n        updated_at='refresh_timestamp',\n        invalidate_hard_deletes=true,\n    )\n}}\nselect *\nfrom {{ ref('model') }} {% endsnapshot %}\n```\n\ncheck strategy - model_snapshot_3\n\n```sql\n{% snapshot model_snapshot_3 %}\n\n{{\n    config\n    (\n        unique_key='id',\n        strategy='check',\n        check_cols=['series_reference','data_value']\n    )\n}}\nselect *\nfrom {{ ref('model') }} {% endsnapshot %}\n```\n\n### snapshots known issues\n\n- incremental iceberg models - sync all columns on schema change can't remove columns used as partitioning.\n  the only way, from a dbt perspective, is to do a full-refresh of the incremental model.\n\n- tables, schemas and database should only be lowercase\n\n- in order to avoid potential conflicts, make sure [`dbt-athena-adapter`](https://github.com/tomme/dbt-athena) is not\n  installed in the target environment.\n  see <https://github.com/dbt-athena/dbt-athena/issues/103> for more details.\n\n- snapshot does not support dropping columns from the source table. if you drop a column make sure to drop the column\n  from the snapshot as well. another workaround is to null the column in the snapshot definition to preserve history\n\n## contracts\n\nthe adapter partly supports contract definition.\n\n- concerning the `data_type`, it is supported but needs to be adjusted for complex types. they must be specified\n  entirely (for instance `array<int>`) even though they won't be checked. indeed, as dbt recommends, we only compare\n  the broader type (array, map, int, varchar). the complete definition is used in order to check that the data types\n  defined in athena are ok (pre-flight check).\n- the adapter does not support the constraints since no constraints don't exist in athena.\n\n## contributing\n\nsee [contributing](contributing.md) for more information on how to contribute to this project.\n\n## contributors \u2728\n\nthanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n<!-- all-contributors-list:start - do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tbody>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nicor88\"><img src=\"https://avatars.githubusercontent.com/u/6278547?v=4?s=100\" width=\"100px;\" alt=\"nicor88\"/><br /><sub><b>nicor88</b></sub></a><br /><a href=\"https://github.com/dbt-athena/dbt-athena/commits?author=nicor88\" title=\"code\">\ud83d\udcbb</a> <a href=\"#maintenance-nicor88\" title=\"maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/dbt-athena/dbt-athena/issues?q=author%3anicor88\" title=\"bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://jessedobbelae.re\"><img src=\"https://avatars.githubusercontent.com/u/1352979?v=4?s=100\" width=\"100px;\" alt=\"jesse dobbelaere\"/><br /><sub><b>jesse dobbelaere</b></sub></a><br /><a href=\"https://github.com/dbt-athena/dbt-athena/issues?q=author%3ajessedobbelaere\" title=\"bug reports\">\ud83d\udc1b</a> <a href=\"#maintenance-jessedobbelaere\" title=\"maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/lemiffe\"><img src=\"https://avatars.githubusercontent.com/u/7487772?v=4?s=100\" width=\"100px;\" alt=\"lemiffe\"/><br /><sub><b>lemiffe</b></sub></a><br /><a href=\"#design-lemiffe\" title=\"design\">\ud83c\udfa8</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jrmyy\"><img src=\"https://avatars.githubusercontent.com/u/9251353?v=4?s=100\" width=\"100px;\" alt=\"j\u00e9r\u00e9my guiselin\"/><br /><sub><b>j\u00e9r\u00e9my guiselin</b></sub></a><br /><a href=\"#maintenance-jrmyy\" title=\"maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/dbt-athena/dbt-athena/commits?author=jrmyy\" title=\"code\">\ud83d\udcbb</a> <a href=\"https://github.com/dbt-athena/dbt-athena/issues?q=author%3ajrmyy\" title=\"bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/tomme\"><img src=\"https://avatars.githubusercontent.com/u/932895?v=4?s=100\" width=\"100px;\" alt=\"tom\"/><br /><sub><b>tom</b></sub></a><br /><a href=\"#maintenance-tomme\" title=\"maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/dbt-athena/dbt-athena/commits?author=tomme\" title=\"code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mattiamatrix\"><img src=\"https://avatars.githubusercontent.com/u/5013654?v=4?s=100\" width=\"100px;\" alt=\"mattia\"/><br /><sub><b>mattia</b></sub></a><br /><a href=\"#maintenance-mattiamatrix\" title=\"maintenance\">\ud83d\udea7</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/gatsby-lee\"><img src=\"https://avatars.githubusercontent.com/u/22950880?v=4?s=100\" width=\"100px;\" alt=\"gatsby lee\"/><br /><sub><b>gatsby lee</b></sub></a><br /><a href=\"https://github.com/dbt-athena/dbt-athena/issues?q=author%3agatsby-lee\" title=\"bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/brechtdevlieger\"><img src=\"https://avatars.githubusercontent.com/u/12074972?v=4?s=100\" width=\"100px;\" alt=\"brechtdevlieger\"/><br /><sub><b>brechtdevlieger</b></sub></a><br /><a href=\"https://github.com/dbt-athena/dbt-athena/issues?q=author%3abrechtdevlieger\" title=\"bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/aartaria\"><img src=\"https://avatars.githubusercontent.com/u/10273710?v=4?s=100\" width=\"100px;\" alt=\"andrea artaria\"/><br /><sub><b>andrea artaria</b></sub></a><br /><a href=\"https://github.com/dbt-athena/dbt-athena/issues?q=author%3aaartaria\" title=\"bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/maiarareinaldo\"><img src=\"https://avatars.githubusercontent.com/u/72740386?v=4?s=100\" width=\"100px;\" alt=\"maiara reinaldo\"/><br /><sub><b>maiara reinaldo</b></sub></a><br /><a href=\"https://github.com/dbt-athena/dbt-athena/issues?q=author%3amaiarareinaldo\" title=\"bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/henriblancke\"><img src=\"https://avatars.githubusercontent.com/u/1708162?v=4?s=100\" width=\"100px;\" alt=\"henri blancke\"/><br /><sub><b>henri blancke</b></sub></a><br /><a href=\"https://github.com/dbt-athena/dbt-athena/commits?author=henriblancke\" title=\"code\">\ud83d\udcbb</a> <a href=\"https://github.com/dbt-athena/dbt-athena/issues?q=author%3ahenriblancke\" title=\"bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/svdimchenko\"><img src=\"https://avatars.githubusercontent.com/u/39801237?v=4?s=100\" width=\"100px;\" alt=\"serhii dimchenko\"/><br /><sub><b>serhii dimchenko</b></sub></a><br /><a href=\"https://github.com/dbt-athena/dbt-athena/commits?author=svdimchenko\" title=\"code\">\ud83d\udcbb</a> <a href=\"https://github.com/dbt-athena/dbt-athena/issues?q=author%3asvdimchenko\" title=\"bug reports\">\ud83d\udc1b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/chrischin478\"><img src=\"https://avatars.githubusercontent.com/u/47199426?v=4?s=100\" width=\"100px;\" alt=\"chrischin478\"/><br /><sub><b>chrischin478</b></sub></a><br /><a href=\"https://github.com/dbt-athena/dbt-athena/commits?author=chrischin478\" title=\"code\">\ud83d\udcbb</a> <a href=\"https://github.com/dbt-athena/dbt-athena/issues?q=author%3achrischin478\" title=\"bug reports\">\ud83d\udc1b</a></td>\n    </tr>\n  </tbody>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- all-contributors-list:end -->\n\nthis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification.\ncontributions of any kind welcome!\n",
  "docs_url": null,
  "keywords": "",
  "license": "apache license 2.0",
  "name": "dbt-athena-community",
  "package_url": "https://pypi.org/project/dbt-athena-community/",
  "project_url": "https://pypi.org/project/dbt-athena-community/",
  "project_urls": {
    "Homepage": "https://github.com/dbt-athena/dbt-athena"
  },
  "release_url": "https://pypi.org/project/dbt-athena-community/1.7.0/",
  "requires_dist": [
    "boto3 ~=1.26",
    "boto3-stubs[athena,glue,lakeformation,sts] ~=1.26",
    "dbt-core ~=1.7.0",
    "pyathena <4.0,>=2.25",
    "pydantic <3.0,>=1.10",
    "tenacity ~=8.2"
  ],
  "requires_python": ">=3.8",
  "summary": "the athena adapter plugin for dbt (data build tool)",
  "version": "1.7.0",
  "releases": [],
  "developers": [],
  "kwds": "athena athena__end_of_time dbt_project markdownlint dbt",
  "license_kwds": "apache license 2.0",
  "libtype": "pypi",
  "id": "pypi_dbt_athena_community",
  "homepage": "https://github.com/dbt-athena/dbt-athena",
  "release_count": 24,
  "dependency_ids": [
    "pypi_boto3",
    "pypi_boto3_stubs",
    "pypi_dbt_core",
    "pypi_pyathena",
    "pypi_pydantic",
    "pypi_tenacity"
  ]
}