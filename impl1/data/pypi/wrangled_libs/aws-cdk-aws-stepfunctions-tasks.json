{
  "classifiers": [
    "development status :: 7 - inactive",
    "framework :: aws cdk",
    "framework :: aws cdk :: 1",
    "intended audience :: developers",
    "license :: osi approved",
    "operating system :: os independent",
    "programming language :: javascript",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "typing :: typed"
  ],
  "description": "# tasks for aws step functions\n\n<!--begin stability banner-->---\n\n\n![end-of-support](https://img.shields.io/badge/end--of--support-critical.svg?style=for-the-badge)\n\n> aws cdk v1 has reached end-of-support on 2023-06-01.\n> this package is no longer being updated, and users should migrate to aws cdk v2.\n>\n> for more information on how to migrate, see the [*migrating to aws cdk v2* guide](https://docs.aws.amazon.com/cdk/v2/guide/migrating-v2.html).\n\n---\n<!--end stability banner-->\n\n[aws step functions](https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html) is a web service that enables you to coordinate the\ncomponents of distributed applications and microservices using visual workflows.\nyou build applications from individual components that each perform a discrete\nfunction, or task, allowing you to scale and change applications quickly.\n\na [task](https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-task-state.html) state represents a single unit of work performed by a state machine.\nall work in your state machine is performed by tasks.\n\nthis module is part of the [aws cloud development kit](https://github.com/aws/aws-cdk) project.\n\n## table of contents\n\n* [tasks for aws step functions](#tasks-for-aws-step-functions)\n\n  * [table of contents](#table-of-contents)\n  * [task](#task)\n  * [paths](#paths)\n\n    * [inputpath](#inputpath)\n    * [outputpath](#outputpath)\n    * [resultpath](#resultpath)\n  * [task parameters from the state json](#task-parameters-from-the-state-json)\n  * [evaluate expression](#evaluate-expression)\n  * [api gateway](#api-gateway)\n\n    * [call rest api endpoint](#call-rest-api-endpoint)\n    * [call http api endpoint](#call-http-api-endpoint)\n  * [aws sdk](#aws-sdk)\n  * [athena](#athena)\n\n    * [startqueryexecution](#startqueryexecution)\n    * [getqueryexecution](#getqueryexecution)\n    * [getqueryresults](#getqueryresults)\n    * [stopqueryexecution](#stopqueryexecution)\n  * [batch](#batch)\n\n    * [submitjob](#submitjob)\n  * [codebuild](#codebuild)\n\n    * [startbuild](#startbuild)\n  * [dynamodb](#dynamodb)\n\n    * [getitem](#getitem)\n    * [putitem](#putitem)\n    * [deleteitem](#deleteitem)\n    * [updateitem](#updateitem)\n  * [ecs](#ecs)\n\n    * [runtask](#runtask)\n\n      * [ec2](#ec2)\n      * [fargate](#fargate)\n  * [emr](#emr)\n\n    * [create cluster](#create-cluster)\n    * [termination protection](#termination-protection)\n    * [terminate cluster](#terminate-cluster)\n    * [add step](#add-step)\n    * [cancel step](#cancel-step)\n    * [modify instance fleet](#modify-instance-fleet)\n    * [modify instance group](#modify-instance-group)\n  * [emr on eks](#emr-on-eks)\n\n    * [create virtual cluster](#create-virtual-cluster)\n    * [delete virtual cluster](#delete-virtual-cluster)\n    * [start job run](#start-job-run)\n  * [eks](#eks)\n\n    * [call](#call)\n  * [eventbridge](#eventbridge)\n\n    * [put events](#put-events)\n  * [glue](#glue)\n  * [glue databrew](#glue-databrew)\n  * [lambda](#lambda)\n  * [sagemaker](#sagemaker)\n\n    * [create training job](#create-training-job)\n    * [create transform job](#create-transform-job)\n    * [create endpoint](#create-endpoint)\n    * [create endpoint config](#create-endpoint-config)\n    * [create model](#create-model)\n    * [update endpoint](#update-endpoint)\n  * [sns](#sns)\n  * [step functions](#step-functions)\n\n    * [start execution](#start-execution)\n    * [invoke activity](#invoke-activity)\n  * [sqs](#sqs)\n\n## task\n\na task state represents a single unit of work performed by a state machine. in the\ncdk, the exact work to be done is determined by a class that implements `istepfunctionstask`.\n\naws step functions [integrates](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-service-integrations.html) with some aws services so that you can call api\nactions, and coordinate executions directly from the amazon states language in\nstep functions. you can directly call and pass parameters to the apis of those\nservices.\n\n## paths\n\nin the amazon states language, a [path](https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-paths.html) is a string beginning with `$` that you\ncan use to identify components within json text.\n\nlearn more about input and output processing in step functions [here](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-input-output-filtering.html)\n\n### inputpath\n\nboth `inputpath` and `parameters` fields provide a way to manipulate json as it\nmoves through your workflow. aws step functions applies the `inputpath` field first,\nand then the `parameters` field. you can first filter your raw input to a selection\nyou want using inputpath, and then apply parameters to manipulate that input\nfurther, or add new values. if you don't specify an `inputpath`, a default value\nof `$` will be used.\n\nthe following example provides the field named `input` as the input to the `task`\nstate that runs a lambda function.\n\n```python\n# fn: lambda.function\n\nsubmit_job = tasks.lambdainvoke(self, \"invoke handler\",\n    lambda_function=fn,\n    input_path=\"$.input\"\n)\n```\n\n### outputpath\n\ntasks also allow you to select a portion of the state output to pass to the next\nstate. this enables you to filter out unwanted information, and pass only the\nportion of the json that you care about. if you don't specify an `outputpath`,\na default value of `$` will be used. this passes the entire json node to the next\nstate.\n\nthe [response](https://docs.aws.amazon.com/lambda/latest/dg/api_invoke.html#api_invoke_responsesyntax) from a lambda function includes the response from the function\nas well as other metadata.\n\nthe following example assigns the output from the task to a field named `result`\n\n```python\n# fn: lambda.function\n\nsubmit_job = tasks.lambdainvoke(self, \"invoke handler\",\n    lambda_function=fn,\n    output_path=\"$.payload.result\"\n)\n```\n\n### resultselector\n\nyou can use [`resultselector`](https://docs.aws.amazon.com/step-functions/latest/dg/input-output-inputpath-params.html#input-output-resultselector)\nto manipulate the raw result of a task, map or parallel state before it is\npassed to [`resultpath`](###resultpath). for service integrations, the raw\nresult contains metadata in addition to the response payload. you can use\nresultselector to construct a json payload that becomes the effective result\nusing static values or references to the raw result or context object.\n\nthe following example extracts the output payload of a lambda function task and combines\nit with some static values and the state name from the context object.\n\n```python\n# fn: lambda.function\n\ntasks.lambdainvoke(self, \"invoke handler\",\n    lambda_function=fn,\n    result_selector={\n        \"lambda_output\": sfn.jsonpath.string_at(\"$.payload\"),\n        \"invoke_request_id\": sfn.jsonpath.string_at(\"$.sdkresponsemetadata.requestid\"),\n        \"static_value\": {\n            \"foo\": \"bar\"\n        },\n        \"state_name\": sfn.jsonpath.string_at(\"$.state.name\")\n    }\n)\n```\n\n### resultpath\n\nthe output of a state can be a copy of its input, the result it produces (for\nexample, output from a task state\u2019s lambda function), or a combination of its\ninput and result. use [`resultpath`](https://docs.aws.amazon.com/step-functions/latest/dg/input-output-resultpath.html) to control which combination of these is\npassed to the state output. if you don't specify an `resultpath`, a default\nvalue of `$` will be used.\n\nthe following example adds the item from calling dynamodb's `getitem` api to the state\ninput and passes it to the next state.\n\n```python\n# my_table: dynamodb.table\n\ntasks.dynamoputitem(self, \"putitem\",\n    item={\n        \"messageid\": tasks.dynamoattributevalue.from_string(\"message-id\")\n    },\n    table=my_table,\n    result_path=\"$.item\"\n)\n```\n\n\u26a0\ufe0f the `outputpath` is computed after applying `resultpath`. all service integrations\nreturn metadata as part of their response. when using `resultpath`, it's not possible to\nmerge a subset of the task output to the input.\n\n## task parameters from the state json\n\nmost tasks take parameters. parameter values can either be static, supplied directly\nin the workflow definition (by specifying their values), or a value available at runtime\nin the state machine's execution (either as its input or an output of a prior state).\nparameter values available at runtime can be specified via the `jsonpath` class,\nusing methods such as `jsonpath.stringat()`.\n\nthe following example provides the field named `input` as the input to the lambda function\nand invokes it asynchronously.\n\n```python\n# fn: lambda.function\n\n\nsubmit_job = tasks.lambdainvoke(self, \"invoke handler\",\n    lambda_function=fn,\n    payload=sfn.taskinput.from_json_path_at(\"$.input\"),\n    invocation_type=tasks.lambdainvocationtype.event\n)\n```\n\nyou can also use [intrinsic functions](https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-intrinsic-functions.html) available on `jsonpath`, for example `jsonpath.format()`.\nhere is an example of starting an athena query that is dynamically created using the task input:\n\n```python\nstart_query_execution_job = tasks.athenastartqueryexecution(self, \"athena start query\",\n    query_string=sfn.jsonpath.format(\"select contacts where year={};\", sfn.jsonpath.string_at(\"$.year\")),\n    query_execution_context=tasks.queryexecutioncontext(\n        database_name=\"interactions\"\n    ),\n    result_configuration=tasks.resultconfiguration(\n        encryption_configuration=tasks.encryptionconfiguration(\n            encryption_option=tasks.encryptionoption.s3_managed\n        ),\n        output_location=s3.location(\n            bucket_name=\"mybucket\",\n            object_key=\"myprefix\"\n        )\n    ),\n    integration_pattern=sfn.integrationpattern.run_job\n)\n```\n\neach service integration has its own set of parameters that can be supplied.\n\n## evaluate expression\n\nuse the `evaluateexpression` to perform simple operations referencing state paths. the\n`expression` referenced in the task will be evaluated in a lambda function\n(`eval()`). this allows you to not have to write lambda code for simple operations.\n\nexample: convert a wait time from milliseconds to seconds, concat this in a message and wait:\n\n```python\nconvert_to_seconds = tasks.evaluateexpression(self, \"convert to seconds\",\n    expression=\"$.waitmilliseconds / 1000\",\n    result_path=\"$.waitseconds\"\n)\n\ncreate_message = tasks.evaluateexpression(self, \"create message\",\n    # note: this is a string inside a string.\n    expression=\"`now waiting ${$.waitseconds} seconds...`\",\n    runtime=lambda_.runtime.nodejs_14_x,\n    result_path=\"$.message\"\n)\n\npublish_message = tasks.snspublish(self, \"publish message\",\n    topic=sns.topic(self, \"cool-topic\"),\n    message=sfn.taskinput.from_json_path_at(\"$.message\"),\n    result_path=\"$.sns\"\n)\n\nwait = sfn.wait(self, \"wait\",\n    time=sfn.waittime.seconds_path(\"$.waitseconds\")\n)\n\nsfn.statemachine(self, \"statemachine\",\n    definition=convert_to_seconds.next(create_message).next(publish_message).next(wait)\n)\n```\n\nthe `evaluateexpression` supports a `runtime` prop to specify the lambda\nruntime to use to evaluate the expression. currently, only runtimes\nof the node.js family are supported.\n\n## api gateway\n\nstep functions supports [api gateway](https://docs.aws.amazon.com/step-functions/latest/dg/connect-api-gateway.html) through the service integration pattern.\n\nhttp apis are designed for low-latency, cost-effective integrations with aws services, including aws lambda, and http endpoints.\nhttp apis support oidc and oauth 2.0 authorization, and come with built-in support for cors and automatic deployments.\nprevious-generation rest apis currently offer more features. more details can be found [here](https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html).\n\n### call rest api endpoint\n\nthe `callapigatewayrestapiendpoint` calls the rest api endpoint.\n\n```python\nimport aws_cdk.aws_apigateway as apigateway\n\nrest_api = apigateway.restapi(self, \"myrestapi\")\n\ninvoke_task = tasks.callapigatewayrestapiendpoint(self, \"call rest api\",\n    api=rest_api,\n    stage_name=\"prod\",\n    method=tasks.httpmethod.get\n)\n```\n\nbe aware that the header values must be arrays. when passing the task token\nin the headers field `wait_for_task_token` integration, use\n`jsonpath.array()` to wrap the token in an array:\n\n```python\nimport aws_cdk.aws_apigateway as apigateway\n# api: apigateway.restapi\n\n\ntasks.callapigatewayrestapiendpoint(self, \"endpoint\",\n    api=api,\n    stage_name=\"stage\",\n    method=tasks.httpmethod.put,\n    integration_pattern=sfn.integrationpattern.wait_for_task_token,\n    headers=sfn.taskinput.from_object({\n        \"tasktoken\": sfn.jsonpath.array(sfn.jsonpath.task_token)\n    })\n)\n```\n\n### call http api endpoint\n\nthe `callapigatewayhttpapiendpoint` calls the http api endpoint.\n\n```python\nimport aws_cdk.aws_apigatewayv2 as apigatewayv2\n\nhttp_api = apigatewayv2.httpapi(self, \"myhttpapi\")\n\ninvoke_task = tasks.callapigatewayhttpapiendpoint(self, \"call http api\",\n    api_id=http_api.api_id,\n    api_stack=stack.of(http_api),\n    method=tasks.httpmethod.get\n)\n```\n\n### aws sdk\n\nstep functions supports calling [aws service's api actions](https://docs.aws.amazon.com/step-functions/latest/dg/supported-services-awssdk.html)\nthrough the service integration pattern.\n\nyou can use step functions' aws sdk integrations to call any of the over two hundred aws services\ndirectly from your state machine, giving you access to over nine thousand api actions.\n\n```python\n# my_bucket: s3.bucket\n\nget_object = tasks.callawsservice(self, \"getobject\",\n    service=\"s3\",\n    action=\"getobject\",\n    parameters={\n        \"bucket\": my_bucket.bucket_name,\n        \"key\": sfn.jsonpath.string_at(\"$.key\")\n    },\n    iam_resources=[my_bucket.arn_for_objects(\"*\")]\n)\n```\n\nuse camelcase for actions and pascalcase for parameter names.\n\nthe task automatically adds an iam statement to the state machine role's policy based on the\nservice and action called. the resources for this statement must be specified in `iamresources`.\n\nuse the `iamaction` prop to manually specify the iam action name in the case where the iam\naction name does not match with the api service/action name:\n\n```python\nlist_buckets = tasks.callawsservice(self, \"listbuckets\",\n    service=\"s3\",\n    action=\"listbuckets\",\n    iam_resources=[\"*\"],\n    iam_action=\"s3:listallmybuckets\"\n)\n```\n\n## athena\n\nstep functions supports [athena](https://docs.aws.amazon.com/step-functions/latest/dg/connect-athena.html) through the service integration pattern.\n\n### startqueryexecution\n\nthe [startqueryexecution](https://docs.aws.amazon.com/athena/latest/apireference/api_startqueryexecution.html) api runs the sql query statement.\n\n```python\nstart_query_execution_job = tasks.athenastartqueryexecution(self, \"start athena query\",\n    query_string=sfn.jsonpath.string_at(\"$.querystring\"),\n    query_execution_context=tasks.queryexecutioncontext(\n        database_name=\"mydatabase\"\n    ),\n    result_configuration=tasks.resultconfiguration(\n        encryption_configuration=tasks.encryptionconfiguration(\n            encryption_option=tasks.encryptionoption.s3_managed\n        ),\n        output_location=s3.location(\n            bucket_name=\"query-results-bucket\",\n            object_key=\"folder\"\n        )\n    )\n)\n```\n\n### getqueryexecution\n\nthe [getqueryexecution](https://docs.aws.amazon.com/athena/latest/apireference/api_getqueryexecution.html) api gets information about a single execution of a query.\n\n```python\nget_query_execution_job = tasks.athenagetqueryexecution(self, \"get query execution\",\n    query_execution_id=sfn.jsonpath.string_at(\"$.queryexecutionid\")\n)\n```\n\n### getqueryresults\n\nthe [getqueryresults](https://docs.aws.amazon.com/athena/latest/apireference/api_getqueryresults.html) api that streams the results of a single query execution specified by queryexecutionid from s3.\n\n```python\nget_query_results_job = tasks.athenagetqueryresults(self, \"get query results\",\n    query_execution_id=sfn.jsonpath.string_at(\"$.queryexecutionid\")\n)\n```\n\n### stopqueryexecution\n\nthe [stopqueryexecution](https://docs.aws.amazon.com/athena/latest/apireference/api_stopqueryexecution.html) api that stops a query execution.\n\n```python\nstop_query_execution_job = tasks.athenastopqueryexecution(self, \"stop query execution\",\n    query_execution_id=sfn.jsonpath.string_at(\"$.queryexecutionid\")\n)\n```\n\n## batch\n\nstep functions supports [batch](https://docs.aws.amazon.com/step-functions/latest/dg/connect-batch.html) through the service integration pattern.\n\n### submitjob\n\nthe [submitjob](https://docs.aws.amazon.com/batch/latest/apireference/api_submitjob.html) api submits an aws batch job from a job definition.\n\n```python\nimport aws_cdk.aws_batch as batch\n# batch_job_definition: batch.jobdefinition\n# batch_queue: batch.jobqueue\n\n\ntask = tasks.batchsubmitjob(self, \"submit job\",\n    job_definition_arn=batch_job_definition.job_definition_arn,\n    job_name=\"myjob\",\n    job_queue_arn=batch_queue.job_queue_arn\n)\n```\n\n## codebuild\n\nstep functions supports [codebuild](https://docs.aws.amazon.com/step-functions/latest/dg/connect-codebuild.html) through the service integration pattern.\n\n### startbuild\n\n[startbuild](https://docs.aws.amazon.com/codebuild/latest/apireference/api_startbuild.html) starts a codebuild project by project name.\n\n```python\nimport aws_cdk.aws_codebuild as codebuild\n\n\ncodebuild_project = codebuild.project(self, \"project\",\n    project_name=\"mytestproject\",\n    build_spec=codebuild.buildspec.from_object({\n        \"version\": \"0.2\",\n        \"phases\": {\n            \"build\": {\n                \"commands\": [\"echo \\\"hello, codebuild!\\\"\"\n                ]\n            }\n        }\n    })\n)\n\ntask = tasks.codebuildstartbuild(self, \"task\",\n    project=codebuild_project,\n    integration_pattern=sfn.integrationpattern.run_job,\n    environment_variables_override={\n        \"zone\": codebuild.buildenvironmentvariable(\n            type=codebuild.buildenvironmentvariabletype.plaintext,\n            value=sfn.jsonpath.string_at(\"$.envvariables.zone\")\n        )\n    }\n)\n```\n\n## dynamodb\n\nyou can call dynamodb apis from a `task` state.\nread more about calling dynamodb apis [here](https://docs.aws.amazon.com/step-functions/latest/dg/connect-ddb.html)\n\n### getitem\n\nthe [getitem](https://docs.aws.amazon.com/amazondynamodb/latest/apireference/api_getitem.html) operation returns a set of attributes for the item with the given primary key.\n\n```python\n# my_table: dynamodb.table\n\ntasks.dynamogetitem(self, \"get item\",\n    key={\"message_id\": tasks.dynamoattributevalue.from_string(\"message-007\")},\n    table=my_table\n)\n```\n\n### putitem\n\nthe [putitem](https://docs.aws.amazon.com/amazondynamodb/latest/apireference/api_putitem.html) operation creates a new item, or replaces an old item with a new item.\n\n```python\n# my_table: dynamodb.table\n\ntasks.dynamoputitem(self, \"putitem\",\n    item={\n        \"messageid\": tasks.dynamoattributevalue.from_string(\"message-007\"),\n        \"text\": tasks.dynamoattributevalue.from_string(sfn.jsonpath.string_at(\"$.bar\")),\n        \"totalcount\": tasks.dynamoattributevalue.from_number(10)\n    },\n    table=my_table\n)\n```\n\n### deleteitem\n\nthe [deleteitem](https://docs.aws.amazon.com/amazondynamodb/latest/apireference/api_deleteitem.html) operation deletes a single item in a table by primary key.\n\n```python\n# my_table: dynamodb.table\n\ntasks.dynamodeleteitem(self, \"deleteitem\",\n    key={\"messageid\": tasks.dynamoattributevalue.from_string(\"message-007\")},\n    table=my_table,\n    result_path=sfn.jsonpath.discard\n)\n```\n\n### updateitem\n\nthe [updateitem](https://docs.aws.amazon.com/amazondynamodb/latest/apireference/api_updateitem.html) operation edits an existing item's attributes, or adds a new item\nto the table if it does not already exist.\n\n```python\n# my_table: dynamodb.table\n\ntasks.dynamoupdateitem(self, \"updateitem\",\n    key={\n        \"messageid\": tasks.dynamoattributevalue.from_string(\"message-007\")\n    },\n    table=my_table,\n    expression_attribute_values={\n        \":val\": tasks.dynamoattributevalue.number_from_string(sfn.jsonpath.string_at(\"$.item.totalcount.n\")),\n        \":rand\": tasks.dynamoattributevalue.from_number(20)\n    },\n    update_expression=\"set totalcount = :val + :rand\"\n)\n```\n\n## ecs\n\nstep functions supports [ecs/fargate](https://docs.aws.amazon.com/step-functions/latest/dg/connect-ecs.html) through the service integration pattern.\n\n### runtask\n\n[runtask](https://docs.aws.amazon.com/step-functions/latest/dg/connect-ecs.html) starts a new task using the specified task definition.\n\n#### ec2\n\nthe ec2 launch type allows you to run your containerized applications on a cluster\nof amazon ec2 instances that you manage.\n\nwhen a task that uses the ec2 launch type is launched, amazon ecs must determine where\nto place the task based on the requirements specified in the task definition, such as\ncpu and memory. similarly, when you scale down the task count, amazon ecs must determine\nwhich tasks to terminate. you can apply task placement strategies and constraints to\ncustomize how amazon ecs places and terminates tasks. learn more about [task placement](https://docs.aws.amazon.com/amazonecs/latest/developerguide/task-placement.html)\n\nthe latest active revision of the passed task definition is used for running the task.\n\nthe following example runs a job from a task definition on ec2\n\n```python\nvpc = ec2.vpc.from_lookup(self, \"vpc\",\n    is_default=true\n)\n\ncluster = ecs.cluster(self, \"ec2cluster\", vpc=vpc)\ncluster.add_capacity(\"defaultautoscalinggroup\",\n    instance_type=ec2.instancetype(\"t2.micro\"),\n    vpc_subnets=ec2.subnetselection(subnet_type=ec2.subnettype.public)\n)\n\ntask_definition = ecs.taskdefinition(self, \"td\",\n    compatibility=ecs.compatibility.ec2\n)\n\ntask_definition.add_container(\"thecontainer\",\n    image=ecs.containerimage.from_registry(\"foo/bar\"),\n    memory_limit_mi_b=256\n)\n\nrun_task = tasks.ecsruntask(self, \"run\",\n    integration_pattern=sfn.integrationpattern.run_job,\n    cluster=cluster,\n    task_definition=task_definition,\n    launch_target=tasks.ecsec2launchtarget(\n        placement_strategies=[\n            ecs.placementstrategy.spread_across_instances(),\n            ecs.placementstrategy.packed_by_cpu(),\n            ecs.placementstrategy.randomly()\n        ],\n        placement_constraints=[\n            ecs.placementconstraint.member_of(\"blieptuut\")\n        ]\n    )\n)\n```\n\n#### fargate\n\naws fargate is a serverless compute engine for containers that works with amazon\nelastic container service (ecs). fargate makes it easy for you to focus on building\nyour applications. fargate removes the need to provision and manage servers, lets you\nspecify and pay for resources per application, and improves security through application\nisolation by design. learn more about [fargate](https://aws.amazon.com/fargate/)\n\nthe fargate launch type allows you to run your containerized applications without the need\nto provision and manage the backend infrastructure. just register your task definition and\nfargate launches the container for you. the latest active revision of the passed\ntask definition is used for running the task. learn more about\n[fargate versioning](https://docs.aws.amazon.com/amazonecs/latest/apireference/api_describetaskdefinition.html)\n\nthe following example runs a job from a task definition on fargate\n\n```python\nvpc = ec2.vpc.from_lookup(self, \"vpc\",\n    is_default=true\n)\n\ncluster = ecs.cluster(self, \"fargatecluster\", vpc=vpc)\n\ntask_definition = ecs.taskdefinition(self, \"td\",\n    memory_mi_b=\"512\",\n    cpu=\"256\",\n    compatibility=ecs.compatibility.fargate\n)\n\ncontainer_definition = task_definition.add_container(\"thecontainer\",\n    image=ecs.containerimage.from_registry(\"foo/bar\"),\n    memory_limit_mi_b=256\n)\n\nrun_task = tasks.ecsruntask(self, \"runfargate\",\n    integration_pattern=sfn.integrationpattern.run_job,\n    cluster=cluster,\n    task_definition=task_definition,\n    assign_public_ip=true,\n    container_overrides=[tasks.containeroverride(\n        container_definition=container_definition,\n        environment=[tasks.taskenvironmentvariable(name=\"some_key\", value=sfn.jsonpath.string_at(\"$.somekey\"))]\n    )],\n    launch_target=tasks.ecsfargatelaunchtarget()\n)\n```\n\n## emr\n\nstep functions supports amazon emr through the service integration pattern.\nthe service integration apis correspond to amazon emr apis but differ in the\nparameters that are used.\n\n[read more](https://docs.aws.amazon.com/step-functions/latest/dg/connect-emr.html) about the differences when using these service integrations.\n\n### create cluster\n\ncreates and starts running a cluster (job flow).\ncorresponds to the [`runjobflow`](https://docs.aws.amazon.com/emr/latest/apireference/api_runjobflow.html) api in emr.\n\n```python\ncluster_role = iam.role(self, \"clusterrole\",\n    assumed_by=iam.serviceprincipal(\"ec2.amazonaws.com\")\n)\n\nservice_role = iam.role(self, \"servicerole\",\n    assumed_by=iam.serviceprincipal(\"elasticmapreduce.amazonaws.com\")\n)\n\nauto_scaling_role = iam.role(self, \"autoscalingrole\",\n    assumed_by=iam.serviceprincipal(\"elasticmapreduce.amazonaws.com\")\n)\n\nauto_scaling_role.assume_role_policy.add_statements(\n    iam.policystatement(\n        effect=iam.effect.allow,\n        principals=[\n            iam.serviceprincipal(\"application-autoscaling.amazonaws.com\")\n        ],\n        actions=[\"sts:assumerole\"\n        ]\n    ))\n\ntasks.emrcreatecluster(self, \"create cluster\",\n    instances=tasks.emrcreatecluster.instancesconfigproperty(),\n    cluster_role=cluster_role,\n    name=sfn.taskinput.from_json_path_at(\"$.clustername\").value,\n    service_role=service_role,\n    auto_scaling_role=auto_scaling_role\n)\n```\n\nif you want to run multiple steps in [parallel](https://docs.aws.amazon.com/emr/latest/managementguide/emr-concurrent-steps.html),\nyou can specify the `stepconcurrencylevel` property. the concurrency range is between 1\nand 256 inclusive, where the default concurrency of 1 means no step concurrency is allowed.\n`stepconcurrencylevel` requires the emr release label to be 5.28.0 or above.\n\n```python\ntasks.emrcreatecluster(self, \"create cluster\",\n    instances=tasks.emrcreatecluster.instancesconfigproperty(),\n    name=sfn.taskinput.from_json_path_at(\"$.clustername\").value,\n    step_concurrency_level=10\n)\n```\n\n### termination protection\n\nlocks a cluster (job flow) so the ec2 instances in the cluster cannot be\nterminated by user intervention, an api call, or a job-flow error.\n\ncorresponds to the [`setterminationprotection`](https://docs.aws.amazon.com/step-functions/latest/dg/connect-emr.html) api in emr.\n\n```python\ntasks.emrsetclusterterminationprotection(self, \"task\",\n    cluster_id=\"clusterid\",\n    termination_protected=false\n)\n```\n\n### terminate cluster\n\nshuts down a cluster (job flow).\ncorresponds to the [`terminatejobflows`](https://docs.aws.amazon.com/emr/latest/apireference/api_terminatejobflows.html) api in emr.\n\n```python\ntasks.emrterminatecluster(self, \"task\",\n    cluster_id=\"clusterid\"\n)\n```\n\n### add step\n\nadds a new step to a running cluster.\ncorresponds to the [`addjobflowsteps`](https://docs.aws.amazon.com/emr/latest/apireference/api_addjobflowsteps.html) api in emr.\n\n```python\ntasks.emraddstep(self, \"task\",\n    cluster_id=\"clusterid\",\n    name=\"stepname\",\n    jar=\"jar\",\n    action_on_failure=tasks.actiononfailure.continue\n)\n```\n\n### cancel step\n\ncancels a pending step in a running cluster.\ncorresponds to the [`cancelsteps`](https://docs.aws.amazon.com/emr/latest/apireference/api_cancelsteps.html) api in emr.\n\n```python\ntasks.emrcancelstep(self, \"task\",\n    cluster_id=\"clusterid\",\n    step_id=\"stepid\"\n)\n```\n\n### modify instance fleet\n\nmodifies the target on-demand and target spot capacities for the instance\nfleet with the specified instancefleetname.\n\ncorresponds to the [`modifyinstancefleet`](https://docs.aws.amazon.com/emr/latest/apireference/api_modifyinstancefleet.html) api in emr.\n\n```python\ntasks.emrmodifyinstancefleetbyname(self, \"task\",\n    cluster_id=\"clusterid\",\n    instance_fleet_name=\"instancefleetname\",\n    target_on_demand_capacity=2,\n    target_spot_capacity=0\n)\n```\n\n### modify instance group\n\nmodifies the number of nodes and configuration settings of an instance group.\n\ncorresponds to the [`modifyinstancegroups`](https://docs.aws.amazon.com/emr/latest/apireference/api_modifyinstancegroups.html) api in emr.\n\n```python\ntasks.emrmodifyinstancegroupbyname(self, \"task\",\n    cluster_id=\"clusterid\",\n    instance_group_name=sfn.jsonpath.string_at(\"$.instancegroupname\"),\n    instance_group=tasks.emrmodifyinstancegroupbyname.instancegroupmodifyconfigproperty(\n        instance_count=1\n    )\n)\n```\n\n## emr on eks\n\nstep functions supports amazon emr on eks through the service integration pattern.\nthe service integration apis correspond to amazon emr on eks apis, but differ in the parameters that are used.\n\n[read more](https://docs.aws.amazon.com/step-functions/latest/dg/connect-emr-eks.html) about the differences when using these service integrations.\n\n[setting up](https://docs.aws.amazon.com/emr/latest/emr-on-eks-developmentguide/setting-up.html) the eks cluster is required.\n\n### create virtual cluster\n\nthe [createvirtualcluster](https://docs.aws.amazon.com/emr-on-eks/latest/apireference/api_createvirtualcluster.html) api creates a single virtual cluster that's mapped to a single kubernetes namespace.\n\nthe eks cluster containing the kubernetes namespace where the virtual cluster will be mapped can be passed in from the task input.\n\n```python\ntasks.emrcontainerscreatevirtualcluster(self, \"create a virtual cluster\",\n    eks_cluster=tasks.eksclusterinput.from_task_input(sfn.taskinput.from_text(\"clusterid\"))\n)\n```\n\nthe eks cluster can also be passed in directly.\n\n```python\nimport aws_cdk.aws_eks as eks\n\n# eks_cluster: eks.cluster\n\n\ntasks.emrcontainerscreatevirtualcluster(self, \"create a virtual cluster\",\n    eks_cluster=tasks.eksclusterinput.from_cluster(eks_cluster)\n)\n```\n\nby default, the kubernetes namespace that a virtual cluster maps to is \"default\", but a specific namespace within an eks cluster can be selected.\n\n```python\ntasks.emrcontainerscreatevirtualcluster(self, \"create a virtual cluster\",\n    eks_cluster=tasks.eksclusterinput.from_task_input(sfn.taskinput.from_text(\"clusterid\")),\n    eks_namespace=\"specified-namespace\"\n)\n```\n\n### delete virtual cluster\n\nthe [deletevirtualcluster](https://docs.aws.amazon.com/emr-on-eks/latest/apireference/api_deletevirtualcluster.html) api deletes a virtual cluster.\n\n```python\ntasks.emrcontainersdeletevirtualcluster(self, \"delete a virtual cluster\",\n    virtual_cluster_id=sfn.taskinput.from_json_path_at(\"$.virtualcluster\")\n)\n```\n\n### start job run\n\nthe [startjobrun](https://docs.aws.amazon.com/emr-on-eks/latest/apireference/api_startjobrun.html) api starts a job run. a job is a unit of work that you submit to amazon emr on eks for execution. the work performed by the job can be defined by a spark jar, pyspark script, or sparksql query. a job run is an execution of the job on the virtual cluster.\n\nrequired setup:\n\n* if not done already, follow the [steps](https://docs.aws.amazon.com/emr/latest/emr-on-eks-developmentguide/setting-up.html) to setup emr on eks and [create an eks cluster](https://docs.aws.amazon.com/cdk/api/latest/docs/aws-eks-readme.html#quick-start).\n* enable [cluster access](https://docs.aws.amazon.com/emr/latest/emr-on-eks-developmentguide/setting-up-cluster-access.html)\n* enable [iam role access](https://docs.aws.amazon.com/emr/latest/emr-on-eks-developmentguide/setting-up-enable-iam.html)\n\nthe following actions must be performed if the virtual cluster id is supplied from the task input. otherwise, if it is supplied statically in the state machine definition, these actions will be done automatically.\n\n* create an [iam role](https://docs.aws.amazon.com/cdk/api/latest/docs/@aws-cdk_aws-iam.role.html)\n* update the [role trust policy](https://docs.aws.amazon.com/emr/latest/emr-on-eks-developmentguide/setting-up-trust-policy.html) of the job execution role.\n\nthe job can be configured with spark submit parameters:\n\n```python\ntasks.emrcontainersstartjobrun(self, \"emr containers start job run\",\n    virtual_cluster=tasks.virtualclusterinput.from_virtual_cluster_id(\"de92jdei2910fwedz\"),\n    release_label=tasks.releaselabel.emr_6_2_0,\n    job_driver=tasks.jobdriver(\n        spark_submit_job_driver=tasks.sparksubmitjobdriver(\n            entry_point=sfn.taskinput.from_text(\"local:///usr/lib/spark/examples/src/main/python/pi.py\"),\n            spark_submit_parameters=\"--conf spark.executor.instances=2 --conf spark.executor.memory=2g --conf spark.executor.cores=2 --conf spark.driver.cores=1\"\n        )\n    )\n)\n```\n\nconfiguring the job can also be done via application configuration:\n\n```python\ntasks.emrcontainersstartjobrun(self, \"emr containers start job run\",\n    virtual_cluster=tasks.virtualclusterinput.from_virtual_cluster_id(\"de92jdei2910fwedz\"),\n    release_label=tasks.releaselabel.emr_6_2_0,\n    job_name=\"emr-containers-job\",\n    job_driver=tasks.jobdriver(\n        spark_submit_job_driver=tasks.sparksubmitjobdriver(\n            entry_point=sfn.taskinput.from_text(\"local:///usr/lib/spark/examples/src/main/python/pi.py\")\n        )\n    ),\n    application_config=[tasks.applicationconfiguration(\n        classification=tasks.classification.spark_defaults,\n        properties={\n            \"spark.executor.instances\": \"1\",\n            \"spark.executor.memory\": \"512m\"\n        }\n    )]\n)\n```\n\njob monitoring can be enabled if `monitoring.logging` is set true. this automatically generates an s3 bucket and cloudwatch logs.\n\n```python\ntasks.emrcontainersstartjobrun(self, \"emr containers start job run\",\n    virtual_cluster=tasks.virtualclusterinput.from_virtual_cluster_id(\"de92jdei2910fwedz\"),\n    release_label=tasks.releaselabel.emr_6_2_0,\n    job_driver=tasks.jobdriver(\n        spark_submit_job_driver=tasks.sparksubmitjobdriver(\n            entry_point=sfn.taskinput.from_text(\"local:///usr/lib/spark/examples/src/main/python/pi.py\"),\n            spark_submit_parameters=\"--conf spark.executor.instances=2 --conf spark.executor.memory=2g --conf spark.executor.cores=2 --conf spark.driver.cores=1\"\n        )\n    ),\n    monitoring=tasks.monitoring(\n        logging=true\n    )\n)\n```\n\notherwise, providing monitoring for jobs with existing log groups and log buckets is also available.\n\n```python\nimport aws_cdk.aws_logs as logs\n\n\nlog_group = logs.loggroup(self, \"log group\")\nlog_bucket = s3.bucket(self, \"s3 bucket\")\n\ntasks.emrcontainersstartjobrun(self, \"emr containers start job run\",\n    virtual_cluster=tasks.virtualclusterinput.from_virtual_cluster_id(\"de92jdei2910fwedz\"),\n    release_label=tasks.releaselabel.emr_6_2_0,\n    job_driver=tasks.jobdriver(\n        spark_submit_job_driver=tasks.sparksubmitjobdriver(\n            entry_point=sfn.taskinput.from_text(\"local:///usr/lib/spark/examples/src/main/python/pi.py\"),\n            spark_submit_parameters=\"--conf spark.executor.instances=2 --conf spark.executor.memory=2g --conf spark.executor.cores=2 --conf spark.driver.cores=1\"\n        )\n    ),\n    monitoring=tasks.monitoring(\n        log_group=log_group,\n        log_bucket=log_bucket\n    )\n)\n```\n\nusers can provide their own existing job execution role.\n\n```python\ntasks.emrcontainersstartjobrun(self, \"emr containers start job run\",\n    virtual_cluster=tasks.virtualclusterinput.from_task_input(sfn.taskinput.from_json_path_at(\"$.virtualclusterid\")),\n    release_label=tasks.releaselabel.emr_6_2_0,\n    job_name=\"emr-containers-job\",\n    execution_role=iam.role.from_role_arn(self, \"job-execution-role\", \"arn:aws:iam::xxxxxxxxxxxx:role/jobexecutionrole\"),\n    job_driver=tasks.jobdriver(\n        spark_submit_job_driver=tasks.sparksubmitjobdriver(\n            entry_point=sfn.taskinput.from_text(\"local:///usr/lib/spark/examples/src/main/python/pi.py\"),\n            spark_submit_parameters=\"--conf spark.executor.instances=2 --conf spark.executor.memory=2g --conf spark.executor.cores=2 --conf spark.driver.cores=1\"\n        )\n    )\n)\n```\n\n## eks\n\nstep functions supports amazon eks through the service integration pattern.\nthe service integration apis correspond to amazon eks apis.\n\n[read more](https://docs.aws.amazon.com/step-functions/latest/dg/connect-eks.html) about the differences when using these service integrations.\n\n### call\n\nread and write kubernetes resource objects via a kubernetes api endpoint.\ncorresponds to the [`call`](https://docs.aws.amazon.com/step-functions/latest/dg/connect-eks.html) api in step functions connector.\n\nthe following code snippet includes a task state that uses eks:call to list the pods.\n\n```python\nimport aws_cdk.aws_eks as eks\n\n\nmy_eks_cluster = eks.cluster(self, \"my sample cluster\",\n    version=eks.kubernetesversion.v1_18,\n    cluster_name=\"myekscluster\"\n)\n\ntasks.ekscall(self, \"call a eks endpoint\",\n    cluster=my_eks_cluster,\n    http_method=tasks.httpmethods.get,\n    http_path=\"/api/v1/namespaces/default/pods\"\n)\n```\n\n## eventbridge\n\nstep functions supports amazon eventbridge through the service integration pattern.\nthe service integration apis correspond to amazon eventbridge apis.\n\n[read more](https://docs.aws.amazon.com/step-functions/latest/dg/connect-eventbridge.html) about the differences when using these service integrations.\n\n### put events\n\nsend events to an eventbridge bus.\ncorresponds to the [`put-events`](https://docs.aws.amazon.com/step-functions/latest/dg/connect-eventbridge.html) api in step functions connector.\n\nthe following code snippet includes a task state that uses events:putevents to send an event to the default bus.\n\n```python\nimport aws_cdk.aws_events as events\n\n\nmy_event_bus = events.eventbus(self, \"eventbus\",\n    event_bus_name=\"myeventbus1\"\n)\n\ntasks.eventbridgeputevents(self, \"send an event to eventbridge\",\n    entries=[tasks.eventbridgeputeventsentry(\n        detail=sfn.taskinput.from_object({\n            \"message\": \"hello from step functions!\"\n        }),\n        event_bus=my_event_bus,\n        detail_type=\"messagefromstepfunctions\",\n        source=\"step.functions\"\n    )]\n)\n```\n\n## glue\n\nstep functions supports [aws glue](https://docs.aws.amazon.com/step-functions/latest/dg/connect-glue.html) through the service integration pattern.\n\nyou can call the [`startjobrun`](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-jobs-runs.html#aws-glue-api-jobs-runs-startjobrun) api from a `task` state.\n\n```python\ntasks.gluestartjobrun(self, \"task\",\n    glue_job_name=\"my-glue-job\",\n    arguments=sfn.taskinput.from_object({\n        \"key\": \"value\"\n    }),\n    timeout=duration.minutes(30),\n    notify_delay_after=duration.minutes(5)\n)\n```\n\n## glue databrew\n\nstep functions supports [aws glue databrew](https://docs.aws.amazon.com/step-functions/latest/dg/connect-databrew.html) through the service integration pattern.\n\nyou can call the [`startjobrun`](https://docs.aws.amazon.com/databrew/latest/dg/api_startjobrun.html) api from a `task` state.\n\n```python\ntasks.gluedatabrewstartjobrun(self, \"task\",\n    name=\"databrew-job\"\n)\n```\n\n## lambda\n\n[invoke](https://docs.aws.amazon.com/lambda/latest/dg/api_invoke.html) a lambda function.\n\nyou can specify the input to your lambda function through the `payload` attribute.\nby default, step functions invokes lambda function with the state input (json path '$')\nas the input.\n\nthe following snippet invokes a lambda function with the state input as the payload\nby referencing the `$` path.\n\n```python\n# fn: lambda.function\n\ntasks.lambdainvoke(self, \"invoke with state input\",\n    lambda_function=fn\n)\n```\n\nwhen a function is invoked, the lambda service sends  [these response\nelements](https://docs.aws.amazon.com/lambda/latest/dg/api_invoke.html#api_invoke_responseelements)\nback.\n\n\u26a0\ufe0f the response from the lambda function is in an attribute called `payload`\n\nthe following snippet invokes a lambda function by referencing the `$.payload` path\nto reference the output of a lambda executed before it.\n\n```python\n# fn: lambda.function\n\ntasks.lambdainvoke(self, \"invoke with empty object as payload\",\n    lambda_function=fn,\n    payload=sfn.taskinput.from_object({})\n)\n\n# use the output of fn as input\ntasks.lambdainvoke(self, \"invoke with payload field in the state input\",\n    lambda_function=fn,\n    payload=sfn.taskinput.from_json_path_at(\"$.payload\")\n)\n```\n\nthe following snippet invokes a lambda and sets the task output to only include\nthe lambda function response.\n\n```python\n# fn: lambda.function\n\ntasks.lambdainvoke(self, \"invoke and set function response as task output\",\n    lambda_function=fn,\n    output_path=\"$.payload\"\n)\n```\n\nif you want to combine the input and the lambda function response you can use\nthe `payloadresponseonly` property and specify the `resultpath`. this will put the\nlambda function arn directly in the \"resource\" string, but it conflicts with the\nintegrationpattern, invocationtype, clientcontext, and qualifier properties.\n\n```python\n# fn: lambda.function\n\ntasks.lambdainvoke(self, \"invoke and combine function response with task input\",\n    lambda_function=fn,\n    payload_response_only=true,\n    result_path=\"$.fn\"\n)\n```\n\nyou can have step functions pause a task, and wait for an external process to\nreturn a task token. read more about the [callback pattern](https://docs.aws.amazon.com/step-functions/latest/dg/callback-task-sample-sqs.html#call-back-lambda-example)\n\nto use the callback pattern, set the `token` property on the task. call the step\nfunctions `sendtasksuccess` or `sendtaskfailure` apis with the token to\nindicate that the task has completed and the state machine should resume execution.\n\nthe following snippet invokes a lambda with the task token as part of the input\nto the lambda.\n\n```python\n# fn: lambda.function\n\ntasks.lambdainvoke(self, \"invoke with callback\",\n    lambda_function=fn,\n    integration_pattern=sfn.integrationpattern.wait_for_task_token,\n    payload=sfn.taskinput.from_object({\n        \"token\": sfn.jsonpath.task_token,\n        \"input\": sfn.jsonpath.string_at(\"$.somefield\")\n    })\n)\n```\n\n\u26a0\ufe0f the task will pause until it receives that task token back with a `sendtasksuccess` or `sendtaskfailure`\ncall. learn more about [callback with the task\ntoken](https://docs.aws.amazon.com/step-functions/latest/dg/connect-to-resource.html#connect-wait-token).\n\naws lambda can occasionally experience transient service errors. in this case, invoking lambda\nresults in a 500 error, such as `serviceexception`, `awslambdaexception`, or `sdkclientexception`.\nas a best practice, the `lambdainvoke` task will retry on those errors with an interval of 2 seconds,\na back-off rate of 2 and 6 maximum attempts. set the `retryonserviceexceptions` prop to `false` to\ndisable this behavior.\n\n## sagemaker\n\nstep functions supports [aws sagemaker](https://docs.aws.amazon.com/step-functions/latest/dg/connect-sagemaker.html) through the service integration pattern.\n\nif your training job or model uses resources from aws marketplace,\n[network isolation is required](https://docs.aws.amazon.com/sagemaker/latest/dg/mkt-algo-model-internet-free.html).\nto do so, set the `enablenetworkisolation` property to `true` for `sagemakercreatemodel` or `sagemakercreatetrainingjob`.\n\nto set environment variables for the docker container use the `environment` property.\n\n### create training job\n\nyou can call the [`createtrainingjob`](https://docs.aws.amazon.com/sagemaker/latest/dg/api_createtrainingjob.html) api from a `task` state.\n\n```python\ntasks.sagemakercreatetrainingjob(self, \"trainsagemaker\",\n    training_job_name=sfn.jsonpath.string_at(\"$.jobname\"),\n    algorithm_specification=tasks.algorithmspecification(\n        algorithm_name=\"blazingtext\",\n        training_input_mode=tasks.inputmode.file\n    ),\n    input_data_config=[tasks.channel(\n        channel_name=\"train\",\n        data_source=tasks.datasource(\n            s3_data_source=tasks.s3datasource(\n                s3_data_type=tasks.s3datatype.s3_prefix,\n                s3_location=tasks.s3location.from_json_expression(\"$.s3bucket\")\n            )\n        )\n    )],\n    output_data_config=tasks.outputdataconfig(\n        s3_output_location=tasks.s3location.from_bucket(s3.bucket.from_bucket_name(self, \"bucket\", \"mybucket\"), \"myoutputpath\")\n    ),\n    resource_config=tasks.resourceconfig(\n        instance_count=1,\n        instance_type=ec2.instancetype(sfn.jsonpath.string_at(\"$.instancetype\")),\n        volume_size=size.gibibytes(50)\n    ),  # optional: default is 1 instance of ec2 `m4.xlarge` with `10gb` volume\n    stopping_condition=tasks.stoppingcondition(\n        max_runtime=duration.hours(2)\n    )\n)\n```\n\n### create transform job\n\nyou can call the [`createtransformjob`](https://docs.aws.amazon.com/sagemaker/latest/dg/api_createtransformjob.html) api from a `task` state.\n\n```python\ntasks.sagemakercreatetransformjob(self, \"batch inference\",\n    transform_job_name=\"mytransformjob\",\n    model_name=\"mymodelname\",\n    model_client_options=tasks.modelclientoptions(\n        invocations_max_retries=3,  # default is 0\n        invocations_timeout=duration.minutes(5)\n    ),\n    transform_input=tasks.transforminput(\n        transform_data_source=tasks.transformdatasource(\n            s3_data_source=tasks.transforms3datasource(\n                s3_uri=\"s3://inputbucket/train\",\n                s3_data_type=tasks.s3datatype.s3_prefix\n            )\n        )\n    ),\n    transform_output=tasks.transformoutput(\n        s3_output_path=\"s3://outputbucket/transformjoboutputpath\"\n    ),\n    transform_resources=tasks.transformresources(\n        instance_count=1,\n        instance_type=ec2.instancetype.of(ec2.instanceclass.m4, ec2.instancesize.xlarge)\n    )\n)\n```\n\n### create endpoint\n\nyou can call the [`createendpoint`](https://docs.aws.amazon.com/sagemaker/latest/apireference/api_createendpoint.html) api from a `task` state.\n\n```python\ntasks.sagemakercreateendpoint(self, \"sagemakerendpoint\",\n    endpoint_name=sfn.jsonpath.string_at(\"$.endpointname\"),\n    endpoint_config_name=sfn.jsonpath.string_at(\"$.endpointconfigname\")\n)\n```\n\n### create endpoint config\n\nyou can call the [`createendpointconfig`](https://docs.aws.amazon.com/sagemaker/latest/apireference/api_createendpointconfig.html) api from a `task` state.\n\n```python\ntasks.sagemakercreateendpointconfig(self, \"sagemakerendpointconfig\",\n    endpoint_config_name=\"myendpointconfig\",\n    production_variants=[tasks.productionvariant(\n        initial_instance_count=2,\n        instance_type=ec2.instancetype.of(ec2.instanceclass.m5, ec2.instancesize.xlarge),\n        model_name=\"mymodel\",\n        variant_name=\"awesome-variant\"\n    )]\n)\n```\n\n### create model\n\nyou can call the [`createmodel`](https://docs.aws.amazon.com/sagemaker/latest/apireference/api_createmodel.html) api from a `task` state.\n\n```python\ntasks.sagemakercreatemodel(self, \"sagemaker\",\n    model_name=\"mymodel\",\n    primary_container=tasks.containerdefinition(\n        image=tasks.dockerimage.from_json_expression(sfn.jsonpath.string_at(\"$.model.imagename\")),\n        mode=tasks.mode.single_model,\n        model_s3_location=tasks.s3location.from_json_expression(\"$.trainingjob.modelartifacts.s3modelartifacts\")\n    )\n)\n```\n\n### update endpoint\n\nyou can call the [`updateendpoint`](https://docs.aws.amazon.com/sagemaker/latest/apireference/api_updateendpoint.html) api from a `task` state.\n\n```python\ntasks.sagemakerupdateendpoint(self, \"sagemakerendpoint\",\n    endpoint_name=sfn.jsonpath.string_at(\"$.endpoint.name\"),\n    endpoint_config_name=sfn.jsonpath.string_at(\"$.endpoint.endpointconfig\")\n)\n```\n\n## sns\n\nstep functions supports [amazon sns](https://docs.aws.amazon.com/step-functions/latest/dg/connect-sns.html) through the service integration pattern.\n\nyou can call the [`publish`](https://docs.aws.amazon.com/sns/latest/api/api_publish.html) api from a `task` state to publish to an sns topic.\n\n```python\ntopic = sns.topic(self, \"topic\")\n\n# use a field from the execution data as message.\ntask1 = tasks.snspublish(self, \"publish1\",\n    topic=topic,\n    integration_pattern=sfn.integrationpattern.request_response,\n    message=sfn.taskinput.from_data_at(\"$.state.message\"),\n    message_attributes={\n        \"place\": tasks.messageattribute(\n            value=sfn.jsonpath.string_at(\"$.place\")\n        ),\n        \"pic\": tasks.messageattribute(\n            # binary must be explicitly set\n            data_type=tasks.messageattributedatatype.binary,\n            value=sfn.jsonpath.string_at(\"$.pic\")\n        ),\n        \"people\": tasks.messageattribute(\n            value=4\n        ),\n        \"handles\": tasks.messageattribute(\n            value=[\"@kslater\", \"@jjf\", null, \"@mfanning\"]\n        )\n    }\n)\n\n# combine a field from the execution data with\n# a literal object.\ntask2 = tasks.snspublish(self, \"publish2\",\n    topic=topic,\n    message=sfn.taskinput.from_object({\n        \"field1\": \"somedata\",\n        \"field2\": sfn.jsonpath.string_at(\"$.field2\")\n    })\n)\n```\n\n## step functions\n\n### start execution\n\nyou can manage [aws step functions](https://docs.aws.amazon.com/step-functions/latest/dg/connect-stepfunctions.html) executions.\n\naws step functions supports it's own [`startexecution`](https://docs.aws.amazon.com/step-functions/latest/apireference/api_startexecution.html) api as a service integration.\n\n```python\n# define a state machine with one pass state\nchild = sfn.statemachine(self, \"childstatemachine\",\n    definition=sfn.chain.start(sfn.pass(self, \"passstate\"))\n)\n\n# include the state machine in a task state with callback pattern\ntask = tasks.stepfunctionsstartexecution(self, \"childtask\",\n    state_machine=child,\n    integration_pattern=sfn.integrationpattern.wait_for_task_token,\n    input=sfn.taskinput.from_object({\n        \"token\": sfn.jsonpath.task_token,\n        \"foo\": \"bar\"\n    }),\n    name=\"myexecutionname\"\n)\n\n# define a second state machine with the task state above\nsfn.statemachine(self, \"parentstatemachine\",\n    definition=task\n)\n```\n\nyou can utilize [associate workflow executions](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-nested-workflows.html#nested-execution-startid)\nvia the `associatewithparent` property. this allows the step functions ui to link child\nexecutions from parent executions, making it easier to trace execution flow across state machines.\n\n```python\n# child: sfn.statemachine\n\ntask = tasks.stepfunctionsstartexecution(self, \"childtask\",\n    state_machine=child,\n    associate_with_parent=true\n)\n```\n\nthis will add the payload `aws_step_functions_started_by_execution_id.$: $$.execution.id` to the\n`input`property for you, which will pass the execution id from the context object to the\nexecution input. it requires `input` to be an object or not be set at all.\n\n### invoke activity\n\nyou can invoke a [step functions activity](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-activities.html) which enables you to have\na task in your state machine where the work is performed by a *worker* that can\nbe hosted on amazon ec2, amazon ecs, aws lambda, basically anywhere. activities\nare a way to associate code running somewhere (known as an activity worker) with\na specific task in a state machine.\n\nwhen step functions reaches an activity task state, the workflow waits for an\nactivity worker to poll for a task. an activity worker polls step functions by\nusing getactivitytask, and sending the arn for the related activity.\n\nafter the activity worker completes its work, it can provide a report of its\nsuccess or failure by using `sendtasksuccess` or `sendtaskfailure`. these two\ncalls use the tasktoken provided by getactivitytask to associate the result\nwith that task.\n\nthe following example creates an activity and creates a task that invokes the activity.\n\n```python\nsubmit_job_activity = sfn.activity(self, \"submitjob\")\n\ntasks.stepfunctionsinvokeactivity(self, \"submit job\",\n    activity=submit_job_activity\n)\n```\n\n## sqs\n\nstep functions supports [amazon sqs](https://docs.aws.amazon.com/step-functions/latest/dg/connect-sqs.html)\n\nyou can call the [`sendmessage`](https://docs.aws.amazon.com/awssimplequeueservice/latest/apireference/api_sendmessage.html) api from a `task` state\nto send a message to an sqs queue.\n\n```python\nqueue = sqs.queue(self, \"queue\")\n\n# use a field from the execution data as message.\ntask1 = tasks.sqssendmessage(self, \"send1\",\n    queue=queue,\n    message_body=sfn.taskinput.from_json_path_at(\"$.message\")\n)\n\n# combine a field from the execution data with\n# a literal object.\ntask2 = tasks.sqssendmessage(self, \"send2\",\n    queue=queue,\n    message_body=sfn.taskinput.from_object({\n        \"field1\": \"somedata\",\n        \"field2\": sfn.jsonpath.string_at(\"$.field2\")\n    })\n)\n```\n",
  "docs_url": null,
  "keywords": "",
  "license": "apache-2.0",
  "name": "aws-cdk.aws-stepfunctions-tasks",
  "package_url": "https://pypi.org/project/aws-cdk.aws-stepfunctions-tasks/",
  "project_url": "https://pypi.org/project/aws-cdk.aws-stepfunctions-tasks/",
  "project_urls": {
    "Homepage": "https://github.com/aws/aws-cdk",
    "Source": "https://github.com/aws/aws-cdk.git"
  },
  "release_url": "https://pypi.org/project/aws-cdk.aws-stepfunctions-tasks/1.204.0/",
  "requires_dist": [
    "aws-cdk.aws-apigateway (==1.204.0)",
    "aws-cdk.aws-cloudwatch (==1.204.0)",
    "aws-cdk.aws-codebuild (==1.204.0)",
    "aws-cdk.aws-dynamodb (==1.204.0)",
    "aws-cdk.aws-ec2 (==1.204.0)",
    "aws-cdk.aws-ecr-assets (==1.204.0)",
    "aws-cdk.aws-ecr (==1.204.0)",
    "aws-cdk.aws-ecs (==1.204.0)",
    "aws-cdk.aws-eks (==1.204.0)",
    "aws-cdk.aws-events (==1.204.0)",
    "aws-cdk.aws-iam (==1.204.0)",
    "aws-cdk.aws-kms (==1.204.0)",
    "aws-cdk.aws-lambda (==1.204.0)",
    "aws-cdk.aws-logs (==1.204.0)",
    "aws-cdk.aws-s3 (==1.204.0)",
    "aws-cdk.aws-sns (==1.204.0)",
    "aws-cdk.aws-sqs (==1.204.0)",
    "aws-cdk.aws-stepfunctions (==1.204.0)",
    "aws-cdk.core (==1.204.0)",
    "aws-cdk.custom-resources (==1.204.0)",
    "aws-cdk.lambda-layer-awscli (==1.204.0)",
    "constructs (<4.0.0,>=3.3.69)",
    "jsii (<2.0.0,>=1.84.0)",
    "publication (>=0.0.3)",
    "typeguard (~=2.13.3)"
  ],
  "requires_python": "~=3.7",
  "summary": "task integrations for aws stepfunctions",
  "version": "1.204.0",
  "releases": [],
  "developers": [
    "amazon_web_services"
  ],
  "kwds": "aws_cdk cdk_aws aws_codebuild aws_apigatewayv2 aws",
  "license_kwds": "apache-2.0",
  "libtype": "pypi",
  "id": "pypi_aws_cdk.aws_stepfunctions_tasks",
  "homepage": "https://github.com/aws/aws-cdk",
  "release_count": 253,
  "dependency_ids": [
    "pypi_aws_cdk.aws_apigateway",
    "pypi_aws_cdk.aws_cloudwatch",
    "pypi_aws_cdk.aws_codebuild",
    "pypi_aws_cdk.aws_dynamodb",
    "pypi_aws_cdk.aws_ec2",
    "pypi_aws_cdk.aws_ecr",
    "pypi_aws_cdk.aws_ecr_assets",
    "pypi_aws_cdk.aws_ecs",
    "pypi_aws_cdk.aws_eks",
    "pypi_aws_cdk.aws_events",
    "pypi_aws_cdk.aws_iam",
    "pypi_aws_cdk.aws_kms",
    "pypi_aws_cdk.aws_lambda",
    "pypi_aws_cdk.aws_logs",
    "pypi_aws_cdk.aws_s3",
    "pypi_aws_cdk.aws_sns",
    "pypi_aws_cdk.aws_sqs",
    "pypi_aws_cdk.aws_stepfunctions",
    "pypi_aws_cdk.core",
    "pypi_aws_cdk.custom_resources",
    "pypi_aws_cdk.lambda_layer_awscli",
    "pypi_constructs",
    "pypi_jsii",
    "pypi_publication",
    "pypi_typeguard"
  ]
}