{
  "classifiers": [
    "development status :: 2 - pre-alpha",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: mit license",
    "natural language :: english",
    "operating system :: macos",
    "operating system :: microsoft :: windows",
    "operating system :: posix",
    "operating system :: unix",
    "programming language :: python",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering",
    "topic :: software development"
  ],
  "description": "<p align=\"left\">\n<img width=\"15%\" src=\"https://dai.lids.mit.edu/wp-content/uploads/2018/06/logo_dai_highres.png\" alt=\"btb\" />\n<i>an open source project from data to ai lab at mit.</i>\n</p>\n\n![](https://raw.githubusercontent.com/mlbazaar/btb/master/docs/images/btb-icon-small.png)\n\na simple, extensible backend for developing auto-tuning systems.\n\n[![development status](https://img.shields.io/badge/development%20status-2%20--%20pre--alpha-yellow)](https://pypi.org/search/?c=development+status+%3a%3a+2+-+pre-alpha)\n[![pypi shield](https://img.shields.io/pypi/v/baytune.svg)](https://pypi.python.org/pypi/baytune)\n[![travis ci shield](https://travis-ci.com/mlbazaar/btb.svg?branch=master)](https://travis-ci.com/mlbazaar/btb)\n[![coverage status](https://codecov.io/gh/mlbazaar/btb/branch/master/graph/badge.svg)](https://codecov.io/gh/mlbazaar/btb)\n[![downloads](https://pepy.tech/badge/baytune)](https://pepy.tech/project/baytune)\n[![binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/mlbazaar/btb/master?filepath=tutorials)\n\n* license: [mit](https://github.com/mlbazaar/btb/blob/master/license)\n* development status: [pre-alpha](https://pypi.org/search/?c=development+status+%3a%3a+2+-+pre-alpha)\n* documentation: https://mlbazaar.github.io/btb\n* homepage: https://github.com/mlbazaar/btb\n\n# overview\n\nbtb (\"bayesian tuning and bandits\") is a simple, extensible backend for developing auto-tuning\nsystems such as automl systems. it provides an easy-to-use interface for *tuning* models and\n*selecting* between models.\n\nit is currently being used in several automl systems:\n\n- [atm](https://github.com/hdi-project/atm), a distributed, multi-tenant automl system for\nclassifier tuning\n- [mit's system](https://github.com/hdi-project/mit-d3m-ta2/) for the darpa\n[data-driven discovery of models](https://www.darpa.mil/program/data-driven-discovery-of-models) (d3m) program\n- [autobazaar](https://github.com/mlbazaar/autobazaar), a flexible, general-purpose\nautoml system\n\n## try it out now!\n\nif you want to quickly discover **btb**, simply click the button below and follow the tutorials!\n\n[![binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/mlbazaar/btb/master?filepath=tutorials)\n\n# install\n\n## requirements\n\n**btb** has been developed and tested on [python 3.6, 3.7 and 3.8](https://www.python.org/downloads/)\n\nalso, although it is not strictly required, the usage of a\n[virtualenv](https://virtualenv.pypa.io/en/latest/) is highly recommended in order to avoid\ninterfering with other software installed in the system where **btb** is run.\n\n## install with pip\n\nthe easiest and recommended way to install **btb** is using [pip](\nhttps://pip.pypa.io/en/stable/):\n\n```bash\npip install baytune\n```\n\nthis will pull and install the latest stable release from [pypi](https://pypi.org/).\n\nif you want to install from source or contribute to the project please read the\n[contributing guide](https://mlbazaar.github.io/btb/contributing.html#get-started).\n\n# quickstart\n\nin this short tutorial we will guide you through the necessary steps to get started using btb\nto `select` between models and `tune` a model to solve a machine learning problem.\n\nin particular, in this example we will be using ``btbsession`` to perform solve the [wine](\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data) classification problem\nby selecting between the `decisiontreeclassifier` and the `sgdclassifier` models from\n[scikit-learn](https://scikit-learn.org/) while also searching for their best `hyperparameter`\nconfiguration.\n\n## prepare a scoring function\n\nthe first step in order to use the `btbsession` class is to develop a `scoring` function.\n\nthis is a python function that, given a model name and a `hyperparameter` configuration,\nevaluates the performance of the model on your data and returns a score.\n\n```python3\nfrom sklearn.datasets import load_wine\nfrom sklearn.linear_model import sgdclassifier\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import decisiontreeclassifier\n\n\ndataset = load_wine()\nmodels = {\n    'dtc': decisiontreeclassifier,\n    'sgdc': sgdclassifier,\n}\n\ndef scoring_function(model_name, hyperparameter_values):\n    model_class = models[model_name]\n    model_instance = model_class(**hyperparameter_values)\n    scores = cross_val_score(\n        estimator=model_instance,\n        x=dataset.data,\n        y=dataset.target,\n        scoring=make_scorer(f1_score, average='macro')\n    )\n    return scores.mean()\n```\n\n## define the tunable hyperparameters\n\nthe second step is to define the `hyperparameters` that we want to `tune` for each model as\n`tunables`.\n\n```python3\nfrom btb.tuning import tunable\nfrom btb.tuning import hyperparams as hp\n\ntunables = {\n    'dtc': tunable({\n        'max_depth': hp.inthyperparam(min=3, max=200),\n        'min_samples_split': hp.floathyperparam(min=0.01, max=1)\n    }),\n    'sgdc': tunable({\n        'max_iter': hp.inthyperparam(min=1, max=5000, default=1000),\n        'tol': hp.floathyperparam(min=1e-3, max=1, default=1e-3),\n    })\n}\n```\n\n## start the searching process\n\nonce you have defined a `scoring` function and the tunable `hyperparameters` specification of your\nmodels, you can start the searching for the best model and `hyperparameter` configuration by using\nthe `btb.btbsession`.\n\nall you need to do is create an instance passing the tunable `hyperparameters` scpecification\nand the scoring function.\n\n```python3\nfrom btb import btbsession\n\nsession = btbsession(\n    tunables=tunables,\n    scorer=scoring_function\n)\n```\n\nand then call the `run` method indicating how many tunable iterations you want the `btbsession` to\nperform:\n\n\n```python3\nbest_proposal = session.run(20)\n```\n\nthe result will be a dictionary indicating the name of the best model that could be found\nand the `hyperparameter` configuration that was used:\n\n```\n{\n    'id': '826aedc2eff31635444e8104f0f3da43',\n    'name': 'dtc',\n    'config': {\n        'max_depth': 21,\n        'min_samples_split': 0.044010284821858835\n    },\n    'score': 0.907229308339589\n}\n ```\n\n# how does btb perform?\n\nwe have a comprehensive [benchmarking framework](https://github.com/mlbazaar/btb/tree/master/benchmark)\nthat we use to evaluate the performance of our `tuners`. for every release, we perform benchmarking\nagainst 100's of challenges, comparing tuners against each other in terms of number of wins.\nwe present the latest leaderboard from latest release below:\n\n## number of wins on latest version\n\n| tuner                   | with ties | without ties |\n|-------------------------|-----------|--------------|\n| `ax.optimize`           |    220    |           32 |\n| `btb.gcpeituner`        |    139    |            2 |\n| `btb.gcptuner`          |  **252**  |       **90** |\n| `btb.gpeituner`         |    208    |           16 |\n| `btb.gptuner`           |    213    |           24 |\n| `btb.uniformtuner`      |    177    |            1 |\n| `hyperopt.tpe`          |    186    |            6 |\n| `smac.hb4ac`            |    180    |            4 |\n| `smac.smac4hpo_ei`      |    220    |           31 |\n| `smac.smac4hpo_lcb`     |    205    |           16 |\n| `smac.smac4hpo_pi`      |    221    |           35 |\n\n- detailed results from which this summary emerged are available [here](https://docs.google.com/spreadsheets/d/15a-pav_t7ccdvqdyloymdvnfhikjfoj7bbgpmyipyts/edit?usp=sharing).\n- if you want to compare your own tuner, follow the steps in our benchmarking framework [here](https://github.com/mlbazaar/btb/tree/master/benchmark).\n- if you have a proposal for tuner that we should include in our benchmarking get in touch\nwith us at [dailabmit@gmail.com](mailto:dailabmit@gmail.com).\n\n# more tutorials\n\n1. to just `tune` `hyperparameters` - see our `tuning` tutorial [here](\nhttps://github.com/mlbazaar/btb/blob/master/tutorials/01_tuning.ipynb) and\n[documentation here](https://mlbazaar.github.io/btb/tutorials/01_tuning.html).\n2. to see the [types of `hyperparameters`](\nhttps://mlbazaar.github.io/btb/tutorials/01_tuning.html#what-is-a-hyperparameter?) we support\nsee our [documentation here](https://mlbazaar.github.io/btb/tutorials/01_tuning.html#what-is-a-hyperparameter?).\n3. you can read about [our benchmarking framework here](https://mlbazaar.github.io/btb/benchmark.html#).\n4. see our [tutorial on `selection` here](https://github.com/mlbazaar/btb/blob/master/tutorials/02_selection.ipynb)\nand [documentation here](https://mlbazaar.github.io/btb/tutorials/02_selection.html).\n\nfor more details about **btb** and all its possibilities and features, please check the\n[project documentation site](https://mlbazaar.github.io/btb/)!\n\nalso do not forget to have a look at the [notebook tutorials](tutorials).\n\n# citing btb\n\nif you use **btb**, please consider citing the following [paper](\nhttps://arxiv.org/pdf/1905.08942.pdf):\n\n```\n@article{smith2019mlbazaar,\n  author = {smith, micah j. and sala, carles and kanter, james max and veeramachaneni, kalyan},\n  title = {the machine learning bazaar: harnessing the ml ecosystem for effective system development},\n  journal = {arxiv e-prints},\n  year = {2019},\n  eid = {arxiv:1905.08942},\n  pages = {arxiv:1904.09535},\n  archiveprefix = {arxiv},\n  eprint = {1905.08942},\n}\n`````\n",
  "docs_url": null,
  "keywords": "data science,machine learning,hyperparameters,tuning,classification",
  "license": "mit license",
  "name": "baytune",
  "package_url": "https://pypi.org/project/baytune/",
  "project_url": "https://pypi.org/project/baytune/",
  "project_urls": {
    "Issue Tracker": "https://github.com/MLBazaar/BTB/issues",
    "Source Code": "https://github.com/MLBazaar/BTB/",
    "Twitter": "https://twitter.com/lab_dai"
  },
  "release_url": "https://pypi.org/project/baytune/0.5.0/",
  "requires_dist": [
    "copulas (>=0.3.2)",
    "numpy (>=1.20.0)",
    "scikit-learn (>=0.20.0)",
    "scipy (>=1.2)",
    "pandas (>=1)",
    "tqdm (>=4.36.1)",
    "ruff (>=0.0.260) ; extra == 'dev'",
    "black[jupyter] (>=22.12.0) ; extra == 'dev'",
    "pre-commit (==2.20.0) ; extra == 'dev'",
    "m2r (>=0.2.0) ; extra == 'docs'",
    "nbsphinx (>=0.5.0) ; extra == 'docs'",
    "Sphinx (>=1.7.1) ; extra == 'docs'",
    "sphinx-rtd-theme (>=0.2.4) ; extra == 'docs'",
    "pytest (>=3.4.2) ; extra == 'test'",
    "pytest-cov (>=2.6.0) ; extra == 'test'",
    "pytest-rerunfailures (>=9.1.1) ; extra == 'test'",
    "jupyter (>=1.0.0) ; extra == 'test'",
    "importlib-metadata (>=0.12) ; extra == 'test'"
  ],
  "requires_python": "<4,>=3.8",
  "summary": "bayesian tuning and bandits",
  "version": "0.5.0",
  "releases": [],
  "developers": [
    "dailabmit@mit.edu"
  ],
  "kwds": "classifier icon tutorial classification hyperparameters",
  "license_kwds": "mit license",
  "libtype": "pypi",
  "id": "pypi_baytune",
  "homepage": "",
  "release_count": 29,
  "dependency_ids": [
    "pypi_black",
    "pypi_copulas",
    "pypi_importlib_metadata",
    "pypi_jupyter",
    "pypi_m2r",
    "pypi_nbsphinx",
    "pypi_numpy",
    "pypi_pandas",
    "pypi_pre_commit",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_pytest_rerunfailures",
    "pypi_ruff",
    "pypi_scikit_learn",
    "pypi_scipy",
    "pypi_sphinx",
    "pypi_sphinx_rtd_theme",
    "pypi_tqdm"
  ]
}