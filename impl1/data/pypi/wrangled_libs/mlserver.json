{
  "classifiers": [
    "operating system :: macos",
    "operating system :: posix"
  ],
  "description": "# mlserver\n\nan open source inference server for your machine learning models.\n\n[![video_play_icon](https://user-images.githubusercontent.com/10466106/151803854-75d17c32-541c-4eee-b589-d45b07ea486d.png)](https://www.youtube.com/watch?v=azhe3z-8c_w)\n\n## overview\n\nmlserver aims to provide an easy way to start serving your machine learning\nmodels through a rest and grpc interface, fully compliant with [kfserving's v2\ndataplane](https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/v2-protocol.html)\nspec. watch a quick video introducing the project [here](https://www.youtube.com/watch?v=azhe3z-8c_w).\n\n- multi-model serving, letting users run multiple models within the same\n  process.\n- ability to run [inference in parallel for vertical\n  scaling](https://mlserver.readthedocs.io/en/latest/user-guide/parallel-inference.html)\n  across multiple models through a pool of inference workers.\n- support for [adaptive\n  batching](https://mlserver.readthedocs.io/en/latest/user-guide/adaptive-batching.html),\n  to group inference requests together on the fly.\n- scalability with deployment in kubernetes native frameworks, including\n  [seldon core](https://docs.seldon.io/projects/seldon-core/en/latest/graph/protocols.html#v2-kfserving-protocol) and\n  [kserve (formerly known as kfserving)](https://kserve.github.io/website/modelserving/v1beta1/sklearn/v2/), where\n  mlserver is the core python inference server used to serve machine learning\n  models.\n- support for the standard [v2 inference protocol](https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/v2-protocol.html) on\n  both the grpc and rest flavours, which has been standardised and adopted by\n  various model serving frameworks.\n\nyou can read more about the goals of this project on the [inital design\ndocument](https://docs.google.com/document/d/1c2uf4saatwltlbcciohvdikq2eay4u72vxad4bxe7iu/edit?usp=sharing).\n\n## usage\n\nyou can install the `mlserver` package running:\n\n```bash\npip install mlserver\n```\n\nnote that to use any of the optional [inference runtimes](#inference-runtimes),\nyou'll need to install the relevant package.\nfor example, to serve a `scikit-learn` model, you would need to install the\n`mlserver-sklearn` package:\n\n```bash\npip install mlserver-sklearn\n```\n\nfor further information on how to use mlserver, you can check any of the\n[available examples](#examples).\n\n## inference runtimes\n\ninference runtimes allow you to define how your model should be used within\nmlserver.\nyou can think of them as the **backend glue** between mlserver and your machine\nlearning framework of choice.\nyou can read more about [inference runtimes in their documentation\npage](./docs/runtimes/index.md).\n\nout of the box, mlserver comes with a set of pre-packaged runtimes which let\nyou interact with a subset of common frameworks.\nthis allows you to start serving models saved in these frameworks straight\naway.\nhowever, it's also possible to **[write custom\nruntimes](./docs/runtimes/custom.md)**.\n\nout of the box, mlserver provides support for:\n\n| framework     | supported | documentation                                                    |\n| ------------- | --------- | ---------------------------------------------------------------- |\n| scikit-learn  | \u2705        | [mlserver sklearn](./runtimes/sklearn)                           |\n| xgboost       | \u2705        | [mlserver xgboost](./runtimes/xgboost)                           |\n| spark mllib   | \u2705        | [mlserver mllib](./runtimes/mllib)                               |\n| lightgbm      | \u2705        | [mlserver lightgbm](./runtimes/lightgbm)                         |\n| tempo         | \u2705        | [`github.com/seldonio/tempo`](https://github.com/seldonio/tempo) |\n| mlflow        | \u2705        | [mlserver mlflow](./runtimes/mlflow)                             |\n| alibi-detect  | \u2705        | [mlserver alibi detect](./runtimes/alibi-detect)                 |\n| alibi-explain | \u2705        | [mlserver alibi explain](./runtimes/alibi-explain)               |\n| huggingface   | \u2705        | [mlserver huggingface](./runtimes/huggingface)                   |\n\n## examples\n\nto see mlserver in action, check out [our full list of\nexamples](./docs/examples/index.md).\nyou can find below a few selected examples showcasing how you can leverage\nmlserver to start serving your machine learning models.\n\n- [serving a `scikit-learn` model](./docs/examples/sklearn/readme.md)\n- [serving a `xgboost` model](./docs/examples/xgboost/readme.md)\n- [serving a `lightgbm` model](./docs/examples/lightgbm/readme.md)\n- [serving a `tempo` pipeline](./docs/examples/tempo/readme.md)\n- [serving a custom model](./docs/examples/custom/readme.md)\n- [serving an `alibi-detect` model](./docs/examples/alibi-detect/readme.md)\n- [serving a `huggingface` model](./docs/examples/huggingface/readme.md)\n- [multi-model serving with multiple frameworks](./docs/examples/mms/readme.md)\n- [loading / unloading models from a model repository](./docs/examples/model-repository/readme.md)\n\n## developer guide\n\n### versioning\n\nboth the main `mlserver` package and the [inference runtimes\npackages](./docs/runtimes/index.md) try to follow the same versioning schema.\nto bump the version across all of them, you can use the\n[`./hack/update-version.sh`](./hack/update-version.sh) script.\n\nfor example:\n\n```bash\n./hack/update-version.sh 0.2.0.dev1\n```\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "apache 2.0",
  "name": "mlserver",
  "package_url": "https://pypi.org/project/mlserver/",
  "project_url": "https://pypi.org/project/mlserver/",
  "project_urls": {
    "Homepage": "https://github.com/SeldonIO/MLServer.git"
  },
  "release_url": "https://pypi.org/project/mlserver/1.3.5/",
  "requires_dist": [
    "click",
    "fastapi (!=0.89.0,<=0.89.1,>=0.88.0)",
    "python-dotenv",
    "grpcio",
    "importlib-resources",
    "numpy",
    "pandas",
    "protobuf",
    "uvicorn",
    "starlette-exporter",
    "py-grpc-prometheus",
    "aiokafka",
    "tritonclient[http] (>=2.24)",
    "aiofiles",
    "orjson",
    "uvloop ; sys_platform != \"win32\" and (sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\")"
  ],
  "requires_python": "",
  "summary": "ml server",
  "version": "1.3.5",
  "releases": [],
  "developers": [
    "hello@seldon.io",
    "seldon_technologies_ltd"
  ],
  "kwds": "modelserving models mlflow grpc apis",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_mlserver",
  "homepage": "https://github.com/seldonio/mlserver.git",
  "release_count": 100,
  "dependency_ids": [
    "pypi_aiofiles",
    "pypi_aiokafka",
    "pypi_click",
    "pypi_fastapi",
    "pypi_grpcio",
    "pypi_importlib_resources",
    "pypi_numpy",
    "pypi_orjson",
    "pypi_pandas",
    "pypi_protobuf",
    "pypi_py_grpc_prometheus",
    "pypi_python_dotenv",
    "pypi_starlette_exporter",
    "pypi_tritonclient",
    "pypi_uvicorn",
    "pypi_uvloop"
  ]
}