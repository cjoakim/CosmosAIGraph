{
  "classifiers": [
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "# tokentrim\n\n![license: mit](https://img.shields.io/badge/license-mit-green.svg)\n\ntokentrim intelligently trims openai `messages` to fit within a model's token limit (shortening a message by removing characters from the middle), making it easy to avoid exceeding the maximum token count.\n\nit's best suited for use directly in openai api calls:\n\n```python\nimport tokentrim as tt\n\nmodel = \"gpt-4\"\n\nresponse = openai.chatcompletion.create(\n  model=model,\n  messages=tt.trim(messages, model) # trims old messages to fit under model's max token count\n)\n```\n\ntokentrim's behavior is based on openai's own [best practices.](https://github.com/openai/openai-cookbook/blob/main/examples/how_to_count_tokens_with_tiktoken.ipynb)\n\n## installation\n\nuse the package manager pip to install tokentrim:\n\n```bash\npip install tokentrim\n```\n\n## usage\n\nthe primary function in the tokentrim library is `trim()`. this function receives a list of messages and a model name, and it returns a trimmed list of messages that should be within the model's token limit.\n\n```python\nimport tokentrim as tt\n\n# force a system_message to be prepended to your messages list. this will not be trimmed.\nsystem_message = \"you are a helpful assistant.\"\n\nresponse = openai.chatcompletion.create(\n  model=model,\n  messages=tt.trim(messages, model, system_message=system_message)\n)\n```\n\n### parameters\n\n- `messages` : a list of message objects to be trimmed. each message is a dictionary with 'role' and 'content'.\n- `model` : the openai model being used (e.g., 'gpt-4', 'gpt-4-32k'). this determines the token limit.\n- `system_message` (optional): a system message to preserve at the start of the conversation.\n- `trim_ratio` (optional): target ratio of tokens to use after trimming. default is 0.75, meaning it will trim messages so they use about 75% of the model's token limit.\n- `return_response_tokens` (optional): if set to true, the function also returns the number of tokens left available for the response after trimming.\n\n### return value\n\nby default, `trim()` returns the trimmed list of messages. if `return_response_tokens` is set to true, it returns a tuple where the first element is the trimmed list of messages, and the second element is the number of tokens left available for the response.\n\n## license\n\nthis project is licensed under the terms of the mit license.\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "tokentrim",
  "package_url": "https://pypi.org/project/tokentrim/",
  "project_url": "https://pypi.org/project/tokentrim/",
  "project_urls": null,
  "release_url": "https://pypi.org/project/tokentrim/0.1.13/",
  "requires_dist": [
    "tiktoken (>=0.4.0)"
  ],
  "requires_python": ">=3.8,<4.0",
  "summary": "easily trim 'messages' arrays for use with gpts.",
  "version": "0.1.13",
  "releases": [],
  "developers": [
    "killian@openinterpreter.com",
    "killian_lucas"
  ],
  "kwds": "how_to_count_tokens_with_tiktoken openai tokentrim tokens token",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_tokentrim",
  "homepage": "",
  "release_count": 14,
  "dependency_ids": [
    "pypi_tiktoken"
  ]
}