{
  "classifiers": [
    "license :: osi approved :: mit license",
    "operating system :: os independent",
    "programming language :: python :: 3"
  ],
  "description": "<img align=\"right\" src=\"https://uber.github.io/img/h3logo-color.svg\" alt=\"h3 logo\" width=\"200\">\n\n# **h3-pyspark**: uber's h3 hexagonal hierarchical geospatial indexing system in pyspark\n\n[![pypi version](https://img.shields.io/pypi/v/h3-pyspark.svg)](https://pypi.org/project/h3-pyspark/)\n[![conda version](https://img.shields.io/conda/vn/conda-forge/h3-pyspark.svg)](https://anaconda.org/conda-forge/h3-pyspark)\n[![license](https://img.shields.io/badge/license-apache%202.0-blue.svg)](https://github.com/kevinschaich/h3-pyspark/blob/master/license)\n[![tests](https://github.com/kevinschaich/h3-pyspark/actions/workflows/tests.yml/badge.svg?branch=master)](https://github.com/kevinschaich/h3-pyspark/actions/workflows/tests.yml)\n\npyspark bindings for the [h3 core library](https://h3geo.org/).\n\nfor available functions, please see the vanilla python binding documentation at:\n\n- [uber.github.io/h3-py](https://uber.github.io/h3-py)\n\n## installation\n\nvia `pypi`:\n\n```bash\npip install h3-pyspark\n```\n\nvia `conda-forge`:\n\n```bash\nconda install -c conda-forge h3-pyspark\n```\n\n## usage\n\n```python\n>>> from pyspark.sql import sparksession, functions as f\n>>> import h3_pyspark\n>>>\n>>> spark = sparksession.builder.getorcreate()\n>>> df = spark.createdataframe([{\"lat\": 37.769377, \"lng\": -122.388903, 'resolution': 9}])\n>>>\n>>> df = df.withcolumn('h3_9', h3_pyspark.geo_to_h3('lat', 'lng', 'resolution'))\n>>> df.show()\n\n+---------+-----------+----------+---------------+\n|      lat|        lng|resolution|           h3_9|\n+---------+-----------+----------+---------------+\n|37.769377|-122.388903|         9|89283082e73ffff|\n+---------+-----------+----------+---------------+\n```\n\n## extension functions\n\nthere are also various extension functions available for geospatial common operations which are not available in the vanilla h3 library.\n\n### assumptions\n\n* you use geojson to represent geometries in your pyspark pipeline (as opposed to wkt)\n* geometries are stored in a geojson `string` within a column (such as `geometry`) in your pyspark dataset\n* individual h3 cells are stored as a `string` column (such as `h3_9`)\n* sets of h3 cells are stored in an `array(string)` column (such as `h3_9`)\n\n### indexing\n\n#### `index_shape(geometry: column, resolution: column)`\n\ngenerate an h3 spatial index for an input geojson geometry column.\n\nthis function accepts geojson `point`, `linestring`, `polygon`, `multipoint`, `multilinestring`, and `multipolygon`\ninput features, and returns the set of h3 cells at the specified resolution which completely cover them\n(could be more than one cell for a substantially large geometry and substantially granular resolution).\n\nthe schema of the output column will be `t.arraytype(t.stringtype())`, where each value in the array is an h3 cell.\n\nthis spatial index can then be used for bucketing, clustering, and joins in spark via an `explode()` operation.\n\n```python\n>>> from pyspark.sql import sparksession, functions as f\n>>> from h3_pyspark.indexing import index_shape\n>>> spark = sparksession.builder.getorcreate()\n>>>\n>>> df = spark.createdataframe([{\n        'geometry': '{ \"type\": \"multipolygon\", \"coordinates\": [ [ [ [ -80.79442262649536, 32.13522895845023 ], [ -80.79298496246338, 32.13522895845023 ], [ -80.79298496246338, 32.13602844594619 ], [ -80.79442262649536, 32.13602844594619 ], [ -80.79442262649536, 32.13522895845023 ] ] ], [ [ [ -80.7923412322998, 32.1330848437511 ], [ -80.79073190689087, 32.1330848437511 ], [ -80.79073190689087, 32.13375715632646 ], [ -80.7923412322998, 32.13375715632646 ], [ -80.7923412322998, 32.1330848437511 ] ] ] ] }',\n\n        'resolution': 9\n    }])\n>>>\n>>> df = df.withcolumn('h3_9', index_shape('geometry', 'resolution'))\n>>> df.show()\n+----------------------+----------+------------------------------------+\n|              geometry|resolution|                                h3_9|\n+----------------------+----------+------------------------------------+\n| { \"type\": \"multip... |         9| [8944d551077ffff, 8944d551073ffff] |\n+----------------------+----------+------------------------------------+\n```\n\noptionally, add another column `h3_9_geometry` for the geojson representation of each cell in the `h3_9` column [to easily map the result alongside your original input geometry](docs/spatial_index.geojson):\n\n```python\n>>> df = df.withcolumn('h3_9_geometry', h3_pyspark.h3_set_to_multi_polygon(f.col('h3_9'), f.lit(true)))\n```\n\n[view live map on github](docs/spatial_index.geojson)\n\n[![result](https://github.com/kevinschaich/h3-pyspark/raw/master/docs/spatial_index.png)](docs/spatial_index.geojson)\n\n### buffers\n\n#### `k_ring_distinct(cells: column, distance: column)`\n\ntakes in an array of input cells, perform a k-ring operation on each cell, and return the distinct set of output cells.\n\nthe schema of the output column will be `t.arraytype(t.stringtype())`, where each value in the array is an h3 cell.\n\nsince [we know the edge length & diameter (`2 * edge length`) of each h3 cell resolution](https://h3geo.org/docs/core-library/restable), we can use this to efficiently generate a \"buffered\" index of our input geometry (useful for operations such as distance joins):\n\n```python\n>>> from pyspark.sql import sparksession, functions as f\n>>> from h3_pyspark.indexing import index_shape\n>>> from h3_pyspark.traversal import k_ring_distinct\n>>> spark = sparksession.builder.getorcreate()\n>>>\n>>> df = spark.createdataframe([{\n        'geometry': '{ \"type\": \"multipolygon\", \"coordinates\": [ [ [ [ -80.79442262649536, 32.13522895845023 ], [ -80.79298496246338, 32.13522895845023 ], [ -80.79298496246338, 32.13602844594619 ], [ -80.79442262649536, 32.13602844594619 ], [ -80.79442262649536, 32.13522895845023 ] ] ], [ [ [ -80.7923412322998, 32.1330848437511 ], [ -80.79073190689087, 32.1330848437511 ], [ -80.79073190689087, 32.13375715632646 ], [ -80.7923412322998, 32.13375715632646 ], [ -80.7923412322998, 32.1330848437511 ] ] ] ] }',\n\n        'resolution': 9\n    }])\n>>>\n>>> df = df.withcolumn('h3_9', index_shape('geometry', 'resolution'))\n>>> df = df.withcolumn('h3_9_buffer', k_ring_distinct('h3_9', 1))\n>>> df.show()\n+--------------------+----------+--------------------+--------------------+\n|            geometry|resolution|                h3_9|         h3_9_buffer|\n+--------------------+----------+--------------------+--------------------+\n|{ \"type\": \"multip...|         9|[8944d551077ffff,...|[8944d551073ffff,...|\n+--------------------+----------+--------------------+--------------------+\n```\n\n[view live map on github](docs/buffer.geojson)\n\n[![result](https://github.com/kevinschaich/h3-pyspark/raw/master/docs/buffer.png)](docs/buffer.geojson)\n\n### spatial joins\n\nonce we have an indexed version of our geometries, we can easily join on the string column in h3 to get a set of pair candidates:\n\n```python\n>>> from pyspark.sql import sparksession, functions as f\n>>> from h3_pyspark.indexing import index_shape\n>>> spark = sparksession.builder.getorcreate()\n>>>\n>>> left = spark.createdataframe([{\n        'left_id': 'left_point',\n        'left_geometry': '{ \"type\": \"point\", \"coordinates\": [ -80.79527020454407, 32.132884966083935 ] }',\n    }])\n>>> right = spark.createdataframe([{\n        'right_id': 'right_polygon',\n        'right_geometry': '{ \"type\": \"polygon\", \"coordinates\": [ [ [ -80.80022692680359, 32.12864200501338 ], [ -80.79224467277527, 32.12864200501338 ], [ -80.79224467277527, 32.13378441213715 ], [ -80.80022692680359, 32.13378441213715 ], [ -80.80022692680359, 32.12864200501338 ] ] ] }',\n    }])\n>>>\n>>> left = left.withcolumn('h3_9', index_shape('left_geometry', f.lit(9)))\n>>> right = right.withcolumn('h3_9', index_shape('right_geometry', f.lit(9)))\n>>>\n>>> left = left.withcolumn('h3_9', f.explode('h3_9'))\n>>> right = right.withcolumn('h3_9', f.explode('h3_9'))\n>>>\n>>> joined = left.join(right, on='h3_9', how='inner')\n>>> joined.show()\n+---------------+--------------------+----------+--------------------+-------------+\n|           h3_9|       left_geometry|   left_id|      right_geometry|     right_id|\n+---------------+--------------------+----------+--------------------+-------------+\n|8944d55100fffff|{ \"type\": \"point\"...|left_point|{ \"type\": \"polygo...|right_polygon|\n+---------------+--------------------+----------+--------------------+-------------+\n```\n\nyou can combine this technique with a [buffer](#buffers) to do a **distance join**.\n\n<div style=\"color: red;\">\n\n> **\u26a0\ufe0f warning \u26a0\ufe0f:** the outputs of an h3 join are *approximate* \u2013 all resulting geometry pairs should be considered *intersection candidates* rather than *definitely intersecting*. pairing a join here with a subsequent `distance` calculation (`distance = 0` = intersecting) or `intersects` can make this calculation exact. [shapely](https://shapely.readthedocs.io) is a popular library with a well-documented [`distance`](https://shapely.readthedocs.io/en/stable/manual.html#object.distance) function which can be easily wrapped in a udf:\n\n</div>\n\n```python\nfrom pyspark.sql import functions as f, types as t\nfrom shapely import geometry\nimport json\n\n@f.udf(t.doubletype())\ndef distance(geometry1, geometry2):\n    geometry1 = json.loads(geometry1)\n    geometry1 = geometry.shape(geometry1)\n    geometry2 = json.loads(geometry2)\n    geometry2 = geometry.shape(geometry2)\n    return geometry1.distance(geometry2)\n```\n\nafter a spatial join (detailed above), you can filter to only directly intersecting geometries:\n\n```python\n>>> joined = joined.withcolumn('distance', distance(f.col('left_geometry'), f.col('right_geometry')))\n>>> joined = joined.filter(f.col('distance') == 0)\n>>> joined.show()\n+---------------+--------------------+----------+--------------------+-------------+--------+\n|           h3_9|       left_geometry|   left_id|      right_geometry|     right_id|distance|\n+---------------+--------------------+----------+--------------------+-------------+--------+\n|8944d55100fffff|{ \"type\": \"point\"...|left_point|{ \"type\": \"polygo...|right_polygon|     0.0|\n+---------------+--------------------+----------+--------------------+-------------+--------+\n```\n\n[view live map on github](docs/spatial_join.geojson)\n\n[![result](https://github.com/kevinschaich/h3-pyspark/raw/master/docs/spatial_join.png)](docs/spatial_join.geojson)\n\n## publishing new versions\n\n1. bump version in [`setup.cfg`](./setup.cfg)\n2. publish to `pypi`\n\n        git clean -fdx\n        python3 -m build\n        python3 -m twine upload --repository pypi dist/*\n\n3. create a new tag & release w/ version `x.x.x` and name `h3-pyspark-x.x.x` in github\n4. publish to `conda-forge`:\n    * bump version & new tag's `sha256` hash in [`meta.yml`](https://github.com/conda-forge/h3-pyspark-feedstock/blob/master/recipe/meta.yaml) in [`@conda-forge/h3-pyspark-feedstock`](https://github.com/conda-forge/h3-pyspark-feedstock)\n        openssl sha256 /path/to/h3-pyspark-x.x.x.tar.gz\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "h3-pyspark",
  "package_url": "https://pypi.org/project/h3-pyspark/",
  "project_url": "https://pypi.org/project/h3-pyspark/",
  "project_urls": {
    "Bug Tracker": "https://github.com/kevinschaich/h3-pyspark/issues",
    "Homepage": "https://github.com/kevinschaich/h3-pyspark"
  },
  "release_url": "https://pypi.org/project/h3-pyspark/1.2.6/",
  "requires_dist": [],
  "requires_python": ">=3.6",
  "summary": "pyspark bindings for h3, a hierarchical hexagonal geospatial indexing system",
  "version": "1.2.6",
  "releases": [],
  "developers": [
    "kevin_schaich",
    "schaich.kevin@gmail.com"
  ],
  "kwds": "h3_pyspark pyspark h3 geo_to_h3 h3logo",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_h3_pyspark",
  "homepage": "https://github.com/kevinschaich/h3-pyspark",
  "release_count": 9,
  "dependency_ids": []
}