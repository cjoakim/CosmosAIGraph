{
  "classifiers": [
    "license :: osi approved :: mit license",
    "operating system :: os independent",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "# rlcard: a toolkit for reinforcement learning in card games\n<img width=\"500\" src=\"https://dczha.com/files/rlcard/logo.jpg\" alt=\"logo\" />\n\n[![testing](https://github.com/datamllab/rlcard/actions/workflows/python-package.yml/badge.svg)](https://github.com/datamllab/rlcard/actions/workflows/python-package.yml)\n[![pypi version](https://badge.fury.io/py/rlcard.svg)](https://badge.fury.io/py/rlcard)\n[![coverage status](https://coveralls.io/repos/github/datamllab/rlcard/badge.svg)](https://coveralls.io/github/datamllab/rlcard?branch=master)\n[![downloads](https://pepy.tech/badge/rlcard)](https://pepy.tech/project/rlcard)\n[![downloads](https://pepy.tech/badge/rlcard/month)](https://pepy.tech/project/rlcard)\n[![license: mit](https://img.shields.io/badge/license-mit-yellow.svg)](https://opensource.org/licenses/mit)\n\n[\u4e2d\u6587\u6587\u6863](readme.zh-cn.md)\n\nrlcard is a toolkit for reinforcement learning (rl) in card games. it supports multiple card environments with easy-to-use interfaces for implementing various reinforcement learning and searching algorithms. the goal of rlcard is to bridge reinforcement learning and imperfect information games. rlcard is developed by [data lab](http://faculty.cs.tamu.edu/xiahu/) at rice and texas a&m university, and community contributors.\n\n*   official website: [https://www.rlcard.org](https://www.rlcard.org)\n*   tutorial in jupyter notebook: [https://github.com/datamllab/rlcard-tutorial](https://github.com/datamllab/rlcard-tutorial)\n*   paper: [https://arxiv.org/abs/1910.04376](https://arxiv.org/abs/1910.04376)\n*   video: [youtube](https://youtu.be/krk2jmsdkzc)\n*   gui: [rlcard-showdown](https://github.com/datamllab/rlcard-showdown)\n*   dou dizhu demo: [demo](https://douzero.org/)\n*   resources: [awesome-game-ai](https://github.com/datamllab/awesome-game-ai)\n*   related project: [douzero project](https://github.com/kwai/douzero)\n*   zhihu: https://zhuanlan.zhihu.com/p/526723604\n*   miscellaneous resources: have you heard of data-centric ai? please check out our [data-centric ai survey](https://arxiv.org/abs/2303.10158) and [awesome data-centric ai resources](https://github.com/daochenzha/data-centric-ai)!\n\n**community:**\n*  **slack**: discuss in our [#rlcard-project](https://join.slack.com/t/rlcard/shared_invite/zt-rkvktsaq-xkmwz8bfkupcm6zgho01xg) slack channel.\n*  **qq group**: join our qq group to discuss. password: rlcardqqgroup\n\t*  group 1: 665647450\n\t*  group 2: 117349516\n\n**news:**\n*   we have updated the tutorials in jupyter notebook to help you walk through rlcard! please check [rlcard tutorial](https://github.com/datamllab/rlcard-tutorial).\n*   all the algorithms can suppport [pettingzoo](https://github.com/pettingzoo-team/pettingzoo) now. please check [here](examples/pettingzoo). thanks the contribtuion from [yifei cheng](https://github.com/ycheng517).\n*   please follow [douzero](https://github.com/kwai/douzero), a strong dou dizhu ai and the [icml 2021 paper](https://arxiv.org/abs/2106.06135). an online demo is available [here](https://douzero.org/). the algorithm is also integrated in rlcard. see [training dmc on dou dizhu](docs/toy-examples.md#training-dmc-on-dou-dizhu).\n*   our package is used in [pettingzoo](https://github.com/pettingzoo-team/pettingzoo). please check it out!\n*   we have released rlcard-showdown, gui demo for rlcard. please check out [here](https://github.com/datamllab/rlcard-showdown)!\n*   jupyter notebook tutorial available! we add some examples in r to call python interfaces of rlcard with reticulate. see [here](docs/toy-examples-r.md)\n*   thanks for the contribution of [@clarit7](https://github.com/clarit7) for supporting different number of players in blackjack. we call for contributions for gradually making the games more configurable. see [here](contributing.md#making-configurable-environments) for more details.\n*   thanks for the contribution of [@clarit7](https://github.com/clarit7) for the blackjack and limit hold'em human interface.\n*   now rlcard supports environment local seeding and multiprocessing. thanks for the testing scripts provided by [@weepingwillowben](https://github.com/weepingwillowben).\n*   human interface of nolimit holdem available. the action space of nolimit holdem has been abstracted. thanks for the contribution of [@adrianp-](https://github.com/adrianp-).\n*   new game gin rummy and human gui available. thanks for the contribution of [@billh0420](https://github.com/billh0420).\n*   pytorch implementation available. thanks for the contribution of [@mjudell](https://github.com/mjudell).\n\n## contributors\nthe following games are mainly developed and maintained by community contributors. thank you!\n*   gin rummy: [@billh0420](https://github.com/billh0420)\n*   bridge: [@billh0420](https://github.com/billh0420)\n\nthank all the contributors!\n\n<a href=\"https://github.com/daochenzha\"><img src=\"https://github.com/daochenzha.png\" width=\"40px\" alt=\"daochenzha\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/hsywhu\"><img src=\"https://github.com/hsywhu.png\" width=\"40px\" alt=\"hsywhu\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/caoyuanpu\"><img src=\"https://github.com/caoyuanpu.png\" width=\"40px\" alt=\"caoyuanpu\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/billh0420\"><img src=\"https://github.com/billh0420.png\" width=\"40px\" alt=\"billh0420\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/ruzhwei\"><img src=\"https://github.com/ruzhwei.png\" width=\"40px\" alt=\"ruzhwei\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/adrianpgob\"><img src=\"https://github.com/adrianpgob.png\" width=\"40px\" alt=\"adrianpgob\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/zhigal\"><img src=\"https://github.com/zhigal.png\" width=\"40px\" alt=\"zhigal\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/aypee19\"><img src=\"https://github.com/aypee19.png\" width=\"40px\" alt=\"aypee19\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/clarit7\"><img src=\"https://github.com/clarit7.png\" width=\"40px\" alt=\"clarit7\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/lhenry15\"><img src=\"https://github.com/lhenry15.png\" width=\"40px\" alt=\"lhenry15\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/ismael-elatifi\"><img src=\"https://github.com/ismael-elatifi.png\" width=\"40px\" alt=\"ismael-elatifi\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/mjudell\"><img src=\"https://github.com/mjudell.png\" width=\"40px\" alt=\"mjudell\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/jkterry1\"><img src=\"https://github.com/jkterry1.png\" width=\"40px\" alt=\"jkterry1\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/kaanozdogru\"><img src=\"https://github.com/kaanozdogru.png\" width=\"40px\" alt=\"kaanozdogru\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/junyuguo\"><img src=\"https://github.com/junyuguo.png\" width=\"40px\" alt=\"junyuguo\" /></a>&nbsp;&nbsp;\n<br />\n<a href=\"https://github.com/xixo99\"><img src=\"https://github.com/xixo99.png\" width=\"40px\" alt=\"xixo99\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/rodrigodelazcano\"><img src=\"https://github.com/rodrigodelazcano.png\" width=\"40px\" alt=\"rodrigodelazcano\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/michael1015198808\"><img src=\"https://github.com/michael1015198808.png\" width=\"40px\" alt=\"michael1015198808\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/mia1996\"><img src=\"https://github.com/mia1996.png\" width=\"40px\" alt=\"mia1996\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/kaiks\"><img src=\"https://github.com/kaiks.png\" width=\"40px\" alt=\"kaiks\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/claude9493\"><img src=\"https://github.com/claude9493.png\" width=\"40px\" alt=\"claude9493\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/sonsang\"><img src=\"https://github.com/sonsang.png\" width=\"40px\" alt=\"sonsang\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/rishabhvarshney14\"><img src=\"https://github.com/rishabhvarshney14.png\" width=\"40px\" alt=\"rishabhvarshney14\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/aetheryang\"><img src=\"https://github.com/aetheryang.png\" width=\"40px\" alt=\"aetheryang\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/rxng8\"><img src=\"https://github.com/rxng8.png\" width=\"40px\" alt=\"rxng8\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/nondecidibile\"><img src=\"https://github.com/nondecidibile.png\" width=\"40px\" alt=\"nondecidibile\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/benblack769\"><img src=\"https://github.com/benblack769.png\" width=\"40px\" alt=\"benblack769\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/zhengsx\"><img src=\"https://github.com/zhengsx.png\" width=\"40px\" alt=\"zhengsx\" /></a>&nbsp;&nbsp;\n<a href=\"https://github.com/andrewnc\"><img src=\"https://github.com/andrewnc.png\" width=\"40px\" alt=\"andrewnc\" /></a>&nbsp;&nbsp;\n\n## cite this work\nif you find this repo useful, you may cite:\n\nzha, daochen, et al. \"rlcard: a platform for reinforcement learning in card games.\" ijcai. 2020.\n```bibtex\n@inproceedings{zha2020rlcard,\n  title={rlcard: a platform for reinforcement learning in card games},\n  author={zha, daochen and lai, kwei-herng and huang, songyi and cao, yuanpu and reddy, keerthana and vargas, juan and nguyen, alex and wei, ruzhe and guo, junyu and hu, xia},\n  booktitle={ijcai},\n  year={2020}\n}\n```\n\n## installation\nmake sure that you have **python 3.6+** and **pip** installed. we recommend installing the stable version of `rlcard` with `pip`:\n\n```\npip3 install rlcard\n```\nthe default installation will only include the card environments. to use pytorch implementation of the training algorithms, run\n```\npip3 install rlcard[torch]\n```\nif you are in china and the above command is too slow, you can use the mirror provided by tsinghua university:\n```\npip3 install rlcard -i https://pypi.tuna.tsinghua.edu.cn/simple\n```\nalternatively, you can clone the latest version with (if you are in china and github is slow, you can use the mirror in [gitee](https://gitee.com/daochenzha/rlcard)):\n```\ngit clone https://github.com/datamllab/rlcard.git\n```\nor only clone one branch to make it faster:\n```\ngit clone -b master --single-branch --depth=1 https://github.com/datamllab/rlcard.git\n```\nthen install with\n```\ncd rlcard\npip3 install -e .\npip3 install -e .[torch]\n```\n\nwe also provide [**conda** installation method](https://anaconda.org/toubun/rlcard):\n\n```\nconda install -c toubun rlcard\n```\n\nconda installation only provides the card environments, you need to manually install pytorch on your demands.\n\n## examples\na **short example** is as below.\n\n```python\nimport rlcard\nfrom rlcard.agents import randomagent\n\nenv = rlcard.make('blackjack')\nenv.set_agents([randomagent(num_actions=env.num_actions)])\n\nprint(env.num_actions) # 2\nprint(env.num_players) # 1\nprint(env.state_shape) # [[2]]\nprint(env.action_shape) # [none]\n\ntrajectories, payoffs = env.run()\n```\n\nrlcard can be flexibly connected to various algorithms. see the following examples:\n\n*   [playing with random agents](docs/toy-examples.md#playing-with-random-agents)\n*   [deep-q learning on blackjack](docs/toy-examples.md#deep-q-learning-on-blackjack)\n*   [training cfr (chance sampling) on leduc hold'em](docs/toy-examples.md#training-cfr-on-leduc-holdem)\n*   [having fun with pretrained leduc model](docs/toy-examples.md#having-fun-with-pretrained-leduc-model)\n*   [training dmc on dou dizhu](docs/toy-examples.md#training-dmc-on-dou-dizhu)\n*   [evaluating agents](docs/toy-examples.md#evaluating-agents)\n*   [training agents on pettingzoo](examples/pettingzoo)\n\n## demo\nrun `examples/human/leduc_holdem_human.py` to play with the pre-trained leduc hold'em model. leduc hold'em is a simplified version of texas hold'em. rules can be found [here](docs/games.md#leduc-holdem).\n\n```\n>> leduc hold'em pre-trained model\n\n>> start a new game!\n>> agent 1 chooses raise\n\n=============== community card ===============\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n===============   your hand    ===============\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502j        \u2502\n\u2502         \u2502\n\u2502         \u2502\n\u2502    \u2665    \u2502\n\u2502         \u2502\n\u2502         \u2502\n\u2502        j\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n===============     chips      ===============\nyours:   +\nagent 1: +++\n=========== actions you can choose ===========\n0: call, 1: raise, 2: fold\n\n>> you choose action (integer):\n```\nwe also provide a gui for easy debugging. please check [here](https://github.com/datamllab/rlcard-showdown/). some demos:\n\n![doudizhu-replay](https://github.com/datamllab/rlcard-showdown/blob/master/docs/imgs/doudizhu-replay.png?raw=true)\n![leduc-replay](https://github.com/datamllab/rlcard-showdown/blob/master/docs/imgs/leduc-replay.png?raw=true)\n\n## available environments\nwe provide a complexity estimation for the games on several aspects. **infoset number:** the number of information sets; **infoset size:** the average number of states in a single information set; **action size:** the size of the action space. **name:** the name that should be passed to `rlcard.make` to create the game environment. we also provide the link to the documentation and the random example.\n\n| game                                                                                                                                                                                           | infoset number  | infoset size      | action size | name            | usage                                                                                       |\n| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-------------: | :---------------: | :---------: | :-------------: | :-----------------------------------------------------------------------------------------: |\n| blackjack ([wiki](https://en.wikipedia.org/wiki/blackjack), [baike](https://baike.baidu.com/item/21%e7%82%b9/5481683?fr=aladdin))                                                              | 10^3            | 10^1              | 10^0        | blackjack       | [doc](docs/games.md#blackjack), [example](examples/run_random.py)                           |\n| leduc hold\u2019em ([paper](http://poker.cs.ualberta.ca/publications/uai05.pdf))                                                                                                                    | 10^2            | 10^2              | 10^0        | leduc-holdem    | [doc](docs/games.md#leduc-holdem), [example](examples/run_random.py)                        |\n| limit texas hold'em ([wiki](https://en.wikipedia.org/wiki/texas_hold_%27em), [baike](https://baike.baidu.com/item/%e5%be%b7%e5%85%8b%e8%90%a8%e6%96%af%e6%89%91%e5%85%8b/83440?fr=aladdin))    | 10^14           | 10^3              | 10^0        | limit-holdem    | [doc](docs/games.md#limit-texas-holdem), [example](examples/run_random.py)                  |\n| dou dizhu ([wiki](https://en.wikipedia.org/wiki/dou_dizhu), [baike](https://baike.baidu.com/item/%e6%96%97%e5%9c%b0%e4%b8%bb/177997?fr=aladdin))                                               | 10^53 ~ 10^83   | 10^23             | 10^4        | doudizhu        | [doc](docs/games.md#dou-dizhu), [example](examples/run_random.py)                           |\n| mahjong ([wiki](https://en.wikipedia.org/wiki/competition_mahjong_scoring_rules), [baike](https://baike.baidu.com/item/%e9%ba%bb%e5%b0%86/215))                                                | 10^121          | 10^48             | 10^2        | mahjong         | [doc](docs/games.md#mahjong), [example](examples/run_random.py)                             | \n| no-limit texas hold'em ([wiki](https://en.wikipedia.org/wiki/texas_hold_%27em), [baike](https://baike.baidu.com/item/%e5%be%b7%e5%85%8b%e8%90%a8%e6%96%af%e6%89%91%e5%85%8b/83440?fr=aladdin)) | 10^162          | 10^3              | 10^4        | no-limit-holdem | [doc](docs/games.md#no-limit-texas-holdem), [example](examples/run_random.py)               |\n| uno ([wiki](https://en.wikipedia.org/wiki/uno_\\(card_game\\)), [baike](https://baike.baidu.com/item/uno%e7%89%8c/2249587))                                                                      |  10^163         | 10^10             | 10^1        | uno             | [doc](docs/games.md#uno), [example](examples/run_random.py)                                 |\n| gin rummy ([wiki](https://en.wikipedia.org/wiki/gin_rummy), [baike](https://baike.baidu.com/item/%e9%87%91%e6%8b%89%e7%b1%b3/3471710))                                                         | 10^52           | -                 | -           | gin-rummy       | [doc](docs/games.md#gin-rummy), [example](examples/run_random.py)                           |\n| bridge ([wiki](https://en.wikipedia.org/wiki/bridge), [baike](https://baike.baidu.com/item/%e6%a1%a5%e7%89%8c/332030))                                                                         |                 | -                 | -           | bridge          | [doc](docs/games.md#bridge), [example](examples/run_random.py)                              |\n\n## supported algorithms\n| algorithm | example | reference |\n| :--------------------------------------: | :-----------------------------------------: | :------------------------------------------------------------------------------------------------------: |\n| deep monte-carlo (dmc)                   | [examples/run\\_dmc.py](examples/run_dmc.py) | [[paper]](https://arxiv.org/abs/2106.06135)                                                              |\n| deep q-learning (dqn)                    | [examples/run\\_rl.py](examples/run_rl.py)   | [[paper]](https://arxiv.org/abs/1312.5602)                                                               |\n| neural fictitious self-play (nfsp)       | [examples/run\\_rl.py](examples/run_rl.py)   | [[paper]](https://arxiv.org/abs/1603.01121)                                                              |\n| counterfactual regret minimization (cfr) | [examples/run\\_cfr.py](examples/run_cfr.py) | [[paper]](http://papers.nips.cc/paper/3306-regret-minimization-in-games-with-incomplete-information.pdf) |\n\n## pre-trained and rule-based models\nwe provide a [model zoo](rlcard/models) to serve as the baselines.\n\n| model                                    | explanation                                              |\n| :--------------------------------------: | :------------------------------------------------------: |\n| leduc-holdem-cfr                         | pre-trained cfr (chance sampling) model on leduc hold'em |\n| leduc-holdem-rule-v1                     | rule-based model for leduc hold'em, v1                   |\n| leduc-holdem-rule-v2                     | rule-based model for leduc hold'em, v2                   |\n| uno-rule-v1                              | rule-based model for uno, v1                             |\n| limit-holdem-rule-v1                     | rule-based model for limit texas hold'em, v1             |\n| doudizhu-rule-v1                         | rule-based model for dou dizhu, v1                       |\n| gin-rummy-novice-rule                    | gin rummy novice rule model                              |\n\n## api cheat sheet\n### how to create an environment\nyou can use the the following interface to make an environment. you may optionally specify some configurations with a dictionary.\n*   **env = rlcard.make(env_id, config={})**: make an environment. `env_id` is a string of a environment; `config` is a dictionary that specifies some environment configurations, which are as follows.\n\t*   `seed`: default `none`. set a environment local random seed for reproducing the results.\n\t*   `allow_step_back`: default `false`. `true` if allowing `step_back` function to traverse backward in the tree.\n\t*   game specific configurations: these fields start with `game_`. currently, we only support `game_num_players` in blackjack, .\n\nonce the environemnt is made, we can access some information of the game.\n*   **env.num_actions**: the number of actions.\n*   **env.num_players**: the number of players.\n*   **env.state_shape**: the shape of the state space of the observations.\n*   **env.action_shape**: the shape of the action features (dou dizhu's action can encoded as features)\n\n### what is state in rlcard\nstate is a python dictionary. it consists of observation `state['obs']`, legal actions `state['legal_actions']`, raw observation `state['raw_obs']` and raw legal actions `state['raw_legal_actions']`.\n\n### basic interfaces\nthe following interfaces provide a basic usage. it is easy to use but it has assumtions on the agent. the agent must follow [agent template](docs/developping-algorithms.md). \n*   **env.set_agents(agents)**: `agents` is a list of `agent` object. the length of the list should be equal to the number of the players in the game.\n*   **env.run(is_training=false)**: run a complete game and return trajectories and payoffs. the function can be used after the `set_agents` is called. if `is_training` is `true`, it will use `step` function in the agent to play the game. if `is_training` is `false`, `eval_step` will be called instead.\n\n### advanced interfaces\nfor advanced usage, the following interfaces allow flexible operations on the game tree. these interfaces do not make any assumtions on the agent.\n*   **env.reset()**: initialize a game. return the state and the first player id.\n*   **env.step(action, raw_action=false)**: take one step in the environment. `action` can be raw action or integer; `raw_action` should be `true` if the action is raw action (string).\n*   **env.step_back()**: available only when `allow_step_back` is `true`. take one step backward. this can be used for algorithms that operate on the game tree, such as cfr (chance sampling).\n*   **env.is_over()**: return `true` if the current game is over. otherewise, return `false`.\n*   **env.get_player_id()**: return the player id of the current player.\n*   **env.get_state(player_id)**: return the state that corresponds to `player_id`.\n*   **env.get_payoffs()**: in the end of the game, return a list of payoffs for all the players.\n*   **env.get_perfect_information()**: (currently only support some of the games) obtain the perfect information at the current state.\n\n## library structure\nthe purposes of the main modules are listed as below:\n\n*   [/examples](examples): examples of using rlcard.\n*   [/docs](docs): documentation of rlcard.\n*   [/tests](tests): testing scripts for rlcard.\n*   [/rlcard/agents](rlcard/agents): reinforcement learning algorithms and human agents.\n*   [/rlcard/envs](rlcard/envs): environment wrappers (state representation, action encoding etc.)\n*   [/rlcard/games](rlcard/games): various game engines.\n*   [/rlcard/models](rlcard/models): model zoo including pre-trained models and rule models.\n\n## more documents\nfor more documentation, please refer to the [documents](docs/readme.md) for general introductions. api documents are available at our [website](http://www.rlcard.org).\n\n## contributing\ncontribution to this project is greatly appreciated! please create an issue for feedbacks/bugs. if you want to contribute codes, please refer to [contributing guide](./contributing.md). if you have any questions, please contact [daochen zha](https://github.com/daochenzha) with [daochen.zha@rice.edu](mailto:daochen.zha@rice.edu).\n\n## acknowledgements\nwe would like to thank jj world network technology co.,ltd for the generous support and all the contributions from the community contributors.\n",
  "docs_url": null,
  "keywords": "reinforcement learning,game,rl,ai",
  "license": "",
  "name": "rlcard",
  "package_url": "https://pypi.org/project/rlcard/",
  "project_url": "https://pypi.org/project/rlcard/",
  "project_urls": {
    "Homepage": "https://github.com/datamllab/rlcard"
  },
  "release_url": "https://pypi.org/project/rlcard/1.2.0/",
  "requires_dist": [],
  "requires_python": "",
  "summary": "a toolkit for reinforcement learning in card games",
  "version": "1.2.0",
  "releases": [],
  "developers": [
    "daochen.zha@tamu.edu",
    "data_analytics_at_texas_a"
  ],
  "kwds": "reinforcement card_game rlcard card badge",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_rlcard",
  "homepage": "https://github.com/datamllab/rlcard",
  "release_count": 36,
  "dependency_ids": []
}