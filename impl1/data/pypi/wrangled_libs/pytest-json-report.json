{
  "classifiers": [
    "development status :: 4 - beta",
    "framework :: pytest",
    "programming language :: python",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "# pytest json report\n\n[![ci](https://github.com/numirias/pytest-json-report/actions/workflows/main.yml/badge.svg)](https://github.com/numirias/pytest-json-report/actions/workflows/main.yml)\n[![pypi version](https://img.shields.io/pypi/v/pytest-json-report.svg)](https://pypi.python.org/pypi/pytest-json-report)\n[![python versions](https://img.shields.io/pypi/pyversions/pytest-json-report.svg)](https://pypi.python.org/pypi/pytest-json-report)\n\nthis pytest plugin creates test reports as json. this makes it easy to process test results in other applications.\n\nit can report a summary, test details, captured output, logs, exception tracebacks and more. additionally, you can use the available fixtures and hooks to [add metadata](#metadata) and [customize](#modifying-the-report) the report as you like.\n\n## table of contents\n\n* [installation](#installation)\n* [options](#options)\n* [usage](#usage)\n   * [metadata](#metadata)\n   * [modifying the report](#modifying-the-report)\n   * [direct invocation](#direct-invocation)\n* [format](#format)\n   * [summary](#summary)\n   * [environment](#environment)\n   * [collectors](#collectors)\n   * [tests](#tests)\n   * [test stage](#test-stage)\n   * [log](#log)\n   * [warnings](#warnings)\n* [related tools](#related-tools)\n\n## installation\n\n```\npip install pytest-json-report --upgrade \n```\n\n## options\n\n| option | description |\n| --- | --- |\n| `--json-report` | create json report |\n| `--json-report-file=path` | target path to save json report (use \"none\" to not save the report) |\n| `--json-report-summary` | just create a summary without per-test details |\n| `--json-report-omit=field_list` | list of fields to omit in the report (choose from: `collectors`, `log`, `traceback`, `streams`, `warnings`, `keywords`) |\n| `--json-report-indent=level` | pretty-print json with specified indentation level |\n| `--json-report-verbosity=level` | set verbosity (default is value of `--verbosity`) |\n\n## usage\n\njust run pytest with `--json-report`. the report is saved in `.report.json` by default.\n\n```bash\n$ pytest --json-report -v tests/\n$ cat .report.json\n{\"created\": 1518371686.7981803, ... \"tests\":[{\"nodeid\": \"test_foo.py\", \"outcome\": \"passed\", ...}, ...]}\n```\n\nif you just need to know how many tests passed or failed and don't care about details, you can produce a summary only:\n\n```bash\n$ pytest --json-report --json-report-summary\n```\n\nmany fields can be omitted to keep the report size small. e.g., this will leave out keywords and stdout/stderr output:\n\n```bash\n$ pytest --json-report --json-report-omit keywords streams\n```\n\nif you don't like to have the report saved, you can specify `none` as the target file name:\n\n```bash\n$ pytest --json-report --json-report-file none\n```\n\n## advanced usage\n\n### metadata\n\nthe easiest way to add your own metadata to a test item is by using the `json_metadata` [test fixture](https://docs.pytest.org/en/stable/fixture.html):\n\n```python\ndef test_something(json_metadata):\n    json_metadata['foo'] = {\"some\": \"thing\"}\n    json_metadata['bar'] = 123\n```\n\nor use the `pytest_json_runtest_metadata` [hook](https://docs.pytest.org/en/stable/reference.html#hooks) (in your `conftest.py`) to add metadata based on the current test run. the dict returned will automatically be merged with any existing metadata. e.g., this adds the start and stop time of each test's `call` stage:\n\n```python\ndef pytest_json_runtest_metadata(item, call):\n    if call.when != 'call':\n        return {}\n    return {'start': call.start, 'stop': call.stop}\n```\n\nalso, you could add metadata using [pytest-metadata's `--metadata` switch](https://github.com/pytest-dev/pytest-metadata#additional-metadata) which will add metadata to the report's `environment` section, but not to a specific test item. you need to make sure all your metadata is json-serializable.\n\n### a note on hooks\n\nif you're using a `pytest_json_*` hook although the plugin is not installed or not active (not using `--json-report`), pytest doesn't recognize it and may fail with an internal error like this:\n```\ninternalerror> pluggy.manager.pluginvalidationerror: unknown hook 'pytest_json_runtest_metadata' in plugin <module 'conftest' from 'conftest.py'>\n```\nyou can avoid this by declaring the hook implementation optional:\n\n```python\nimport pytest\n@pytest.hookimpl(optionalhook=true)\ndef pytest_json_runtest_metadata(item, call):\n    ...\n```\n\n### modifying the report\n\nyou can modify the entire report before it's saved by using the `pytest_json_modifyreport` hook.\n\njust implement the hook in your `conftest.py`, e.g.:\n\n```python\ndef pytest_json_modifyreport(json_report):\n    # add a key to the report\n    json_report['foo'] = 'bar'\n    # delete the summary from the report\n    del json_report['summary']\n```\n\nafter `pytest_sessionfinish`, the report object is also directly available to script via `config._json_report.report`. so you can access it using some built-in hook:\n\n```python\ndef pytest_sessionfinish(session):\n    report = session.config._json_report.report\n    print('exited with', report['exitcode'])\n```\n\nif you *really* want to change how the result of a test stage run is turned into json, you can use the `pytest_json_runtest_stage` hook. it takes a [`testreport`](https://docs.pytest.org/en/latest/reference.html#_pytest.runner.testreport) and returns a json-serializable dict:\n\n```python\ndef pytest_json_runtest_stage(report):\n    return {'outcome': report.outcome}\n```\n\n### direct invocation\n\nyou can use the plugin when invoking `pytest.main()` directly from code:\n\n```python\nimport pytest\nfrom pytest_jsonreport.plugin import jsonreport\n\nplugin = jsonreport()\npytest.main(['--json-report-file=none', 'test_foo.py'], plugins=[plugin])\n```\n\nyou can then access the `report` object:\n\n```python\nprint(plugin.report)\n```\n\nand save the report manually:\n\n```python\nplugin.save_report('/tmp/my_report.json')\n```\n\n\n## format\n\nthe json report contains metadata of the session, a summary, collectors, tests and warnings. you can find a sample report in [`sample_report.json`](sample_report.json).\n\n| key | description |\n| --- | --- |\n| `created` | report creation date. (unix time) |\n| `duration` | session duration in seconds. |\n| `exitcode` | process exit code as listed [in the pytest docs](https://docs.pytest.org/en/latest/usage.html#possible-exit-codes). the exit code is a quick way to tell if any tests failed, an internal error occurred, etc. |\n| `root` | absolute root path from which the session was started. |\n| `environment` | [environment](#environment) entry. |\n| `summary` | [summary](#summary) entry. |\n| `collectors` | [collectors](#collectors) entry. (absent if `--json-report-summary` or if no collectors)  |\n| `tests` | [tests](#tests) entry. (absent if `--json-report-summary`)  |\n| `warnings` | [warnings](#warnings) entry. (absent if `--json-report-summary` or if no warnings)  |\n\n#### example\n\n```python\n{\n    \"created\": 1518371686.7981803,\n    \"duration\": 0.1235666275024414,\n    \"exitcode\": 1,\n    \"root\": \"/path/to/tests\",\n    \"environment\": environment,\n    \"summary\": summary,\n    \"collectors\": collectors,\n    \"tests\": tests,\n    \"warnings\": warnings,\n}\n```\n\n### summary\n\nnumber of outcomes per category and the total number of test items.\n\n| key | description |\n| --- | --- |\n|  `collected` | total number of tests collected. |\n|  `total` | total number of tests run. |\n|  `deselected` | total number of tests deselected. (absent if number is 0) |\n| `<outcome>` | number of tests with that outcome. (absent if number is 0) |\n\n#### example\n\n```python\n{\n    \"collected\": 10,\n    \"passed\": 2,\n    \"failed\": 3,\n    \"xfailed\": 1,\n    \"xpassed\": 1,\n    \"error\": 2,\n    \"skipped\": 1,\n    \"total\": 10\n}\n```\n\n### environment\n\nthe environment section is provided by [pytest-metadata](https://github.com/pytest-dev/pytest-metadata). all metadata given by that plugin will be added here, so you need to make sure it is json-serializable.\n\n#### example\n\n```python\n{\n    \"python\": \"3.6.4\",\n    \"platform\": \"linux-4.56.78-9-arch-x86_64-with-arch\",\n    \"packages\": {\n        \"pytest\": \"3.4.0\",\n        \"py\": \"1.5.2\",\n        \"pluggy\": \"0.6.0\"\n    },\n    \"plugins\": {\n        \"json-report\": \"0.4.1\",\n        \"xdist\": \"1.22.0\",\n        \"metadata\": \"1.5.1\",\n        \"forked\": \"0.2\",\n        \"cov\": \"2.5.1\"\n    },\n    \"foo\": \"bar\", # custom metadata entry passed via pytest-metadata\n}\n```\n\n### collectors\n\na list of collector nodes. these are useful to check what tests are available without running them, or to debug an error during test discovery.\n\n| key | description |\n| --- | --- |\n| `nodeid` | id of the collector node. ([see docs](https://docs.pytest.org/en/latest/example/markers.html#node-id)) the root node has an empty node id. |\n| `outcome` | outcome of the collection. (not the test outcome!) |\n| `result` | nodes collected by the collector. |\n| `longrepr` | representation of the collection error. (absent if no error occurred) |\n\nthe `result` is a list of the collected nodes:\n\n| key | description |\n| --- | --- |\n| `nodeid` | id of the node. |\n| `type` | type of the collected node. |\n| `lineno` | line number. (absent if not applicable) |\n| `deselected` | `true` if the test is deselected. (absent if not deselected) |\n\n#### example\n\n```python\n[\n    {\n        \"nodeid\": \"\",\n        \"outcome\": \"passed\",\n        \"result\": [\n            {\n                \"nodeid\": \"test_foo.py\",\n                \"type\": \"module\"\n            }\n        ]\n    },\n    {\n        \"nodeid\": \"test_foo.py\",\n        \"outcome\": \"passed\",\n        \"result\": [\n            {\n                \"nodeid\": \"test_foo.py::test_pass\",\n                \"type\": \"function\",\n                \"lineno\": 24,\n                \"deselected\": true\n            },\n            ...\n        ]\n    },\n    {\n        \"nodeid\": \"test_bar.py\",\n        \"outcome\": \"failed\",\n        \"result\": [],\n        \"longrepr\": \"/usr/lib/python3.6 ... invalid syntax\"\n    },\n    ...\n]\n```\n\n### tests\n\na list of test nodes. each completed test stage produces a stage object (`setup`, `call`, `teardown`) with its own `outcome`.\n\n| key | description |\n| --- | --- |\n| `nodeid` | id of the test node. |\n| `lineno` | line number where the test starts. |\n| `keywords` | list of keywords and markers associated with the test. |\n| `outcome` | outcome of the test run. |\n| `{setup, call, teardown}` | [test stage](#test-stage) entry. to find the error in a failed test you need to check all stages. (absent if stage didn't run) |\n| `metadata` | [metadata](#metadata) item. (absent if no metadata) |\n\n#### example\n\n```python\n[\n    {\n        \"nodeid\": \"test_foo.py::test_fail\",\n        \"lineno\": 50,\n        \"keywords\": [\n            \"test_fail\",\n            \"test_foo.py\",\n            \"test_foo0\"\n        ],\n        \"outcome\": \"failed\",\n        \"setup\": test_stage,\n        \"call\": test_stage,\n        \"teardown\": test_stage,\n        \"metadata\": {\n            \"foo\": \"bar\",\n        }\n    },\n    ...\n]\n```\n\n\n### test stage\n\na test stage item.\n\n| key | description |\n| --- | --- |\n| `duration` | duration of the test stage in seconds. |\n| `outcome` | outcome of the test stage. (can be different from the overall test outcome) |\n| `crash` | crash entry. (absent if no error occurred) |\n| `traceback` | list of traceback entries. (absent if no error occurred; affected by `--tb` option) |\n| `stdout` | standard output. (absent if none available) |\n| `stderr` | standard error. (absent if none available) |\n| `log` | [log](#log) entry. (absent if none available) |\n| `longrepr` | representation of the error. (absent if no error occurred; format affected by `--tb` option) |\n\n#### example\n\n```python\n{\n    \"duration\": 0.00018835067749023438,\n    \"outcome\": \"failed\",\n    \"crash\": {\n        \"path\": \"/path/to/tests/test_foo.py\",\n        \"lineno\": 54,\n        \"message\": \"typeerror: unsupported operand type(s) for -: 'int' and 'nonetype'\"\n    },\n    \"traceback\": [\n        {\n            \"path\": \"test_foo.py\",\n            \"lineno\": 65,\n            \"message\": \"\"\n        },\n        {\n            \"path\": \"test_foo.py\",\n            \"lineno\": 63,\n            \"message\": \"in foo\"\n        },\n        {\n            \"path\": \"test_foo.py\",\n            \"lineno\": 63,\n            \"message\": \"in <listcomp>\"\n        },\n        {\n            \"path\": \"test_foo.py\",\n            \"lineno\": 54,\n            \"message\": \"typeerror\"\n        }\n    ],\n    \"stdout\": \"foo\\nbar\\n\",\n    \"stderr\": \"baz\\n\",\n    \"log\": log,\n    \"longrepr\": \"def test_fail_nested():\\n ...\"\n}\n```\n\n### log\n\na list of log records. the fields of a log record are the [`logging.logrecord` attributes](https://docs.python.org/3/library/logging.html#logrecord-attributes), with the exception that the fields `exc_info` and `args` are always empty and `msg` contains the formatted log message.\n\nyou can apply [`logging.makelogrecord()`](https://docs.python.org/3/library/logging.html#logging.makelogrecord)  on a log record to convert it back to a `logging.logrecord` object.\n\n#### example\n\n```python\n[\n    {\n        \"name\": \"root\",\n        \"msg\": \"this is a warning.\",\n        \"args\": null,\n        \"levelname\": \"warning\",\n        \"levelno\": 30,\n        \"pathname\": \"/path/to/tests/test_foo.py\",\n        \"filename\": \"test_foo.py\",\n        \"module\": \"test_foo\",\n        \"exc_info\": null,\n        \"exc_text\": null,\n        \"stack_info\": null,\n        \"lineno\": 8,\n        \"funcname\": \"foo\",\n        \"created\": 1519772464.291738,\n        \"msecs\": 291.73803329467773,\n        \"relativecreated\": 332.90839195251465,\n        \"thread\": 140671803118912,\n        \"threadname\": \"mainthread\",\n        \"processname\": \"mainprocess\",\n        \"process\": 31481\n    },\n    ...\n]\n```\n\n\n### warnings\n\na list of warnings that occurred during the session. (see the [pytest docs on warnings](https://docs.pytest.org/en/latest/warnings.html).)\n\n| key | description |\n| --- | --- |\n| `filename` | file name. |\n| `lineno` | line number. |\n| `message` | warning message. |\n| `when` | when the warning was captured. (`\"config\"`, `\"collect\"` or `\"runtest\"` as listed [here](https://docs.pytest.org/en/latest/reference.html#_pytest.hookspec.pytest_warning_captured)) |\n\n#### example\n\n```python\n[\n    {\n        \"code\": \"c1\",\n        \"path\": \"/path/to/tests/test_foo.py\",\n        \"nodeid\": \"test_foo.py::testfoo\",\n        \"message\": \"cannot collect test class 'testfoo' because it has a __init__ constructor\"\n    }\n]\n```\n\n## related tools\n\n- [pytest-json](https://github.com/mattcl/pytest-json) has some great features but appears to be unmaintained. i borrowed some ideas and test cases from there.\n\n- [tox has a switch](http://tox.readthedocs.io/en/latest/example/result.html) to create a json report including a test result summary. however, it just provides the overall outcome without any per-test details.\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "pytest-json-report",
  "package_url": "https://pypi.org/project/pytest-json-report/",
  "project_url": "https://pypi.org/project/pytest-json-report/",
  "project_urls": {
    "Homepage": "https://github.com/numirias/pytest-json-report"
  },
  "release_url": "https://pypi.org/project/pytest-json-report/1.5.0/",
  "requires_dist": [
    "pytest (>=3.8.0)",
    "pytest-metadata"
  ],
  "requires_python": "",
  "summary": "a pytest plugin to report test results as json files",
  "version": "1.5.0",
  "releases": [],
  "developers": [
    "numirias",
    "numirias@users.noreply.github.com"
  ],
  "kwds": "pytest_json_runtest_metadata pytest_json_runtest_stage pytest_json_ pytest_jsonreport pytest_json_modifyreport",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_pytest_json_report",
  "homepage": "https://github.com/numirias/pytest-json-report",
  "release_count": 24,
  "dependency_ids": [
    "pypi_pytest",
    "pypi_pytest_metadata"
  ]
}