{
  "classifiers": [
    "license :: osi approved :: mit license",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.12",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: software development :: libraries :: application frameworks",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "# \ud83d\uddc2\ufe0f llamaindex \ud83e\udd99\n\n[![pypi - downloads](https://img.shields.io/pypi/dm/llama-index)](https://pypi.org/project/llama-index/)\n[![github contributors](https://img.shields.io/github/contributors/jerryjliu/llama_index)](https://github.com/jerryjliu/llama_index/graphs/contributors)\n[![discord](https://img.shields.io/discord/1059199217496772688)](https://discord.gg/dgcwcsnxhu)\n\nllamaindex (gpt index) is a data framework for your llm application.\n\npypi:\n\n- llamaindex: https://pypi.org/project/llama-index/.\n- gpt index (duplicate): https://pypi.org/project/gpt-index/.\n\nllamaindex.ts (typescript/javascript): https://github.com/run-llama/llamaindexts.\n\ndocumentation: https://docs.llamaindex.ai/en/stable/.\n\ntwitter: https://twitter.com/llama_index.\n\ndiscord: https://discord.gg/dgcwcsnxhu.\n\n### ecosystem\n\n- llamahub (community library of data loaders): https://llamahub.ai\n- llamalab (cutting-edge agi projects using llamaindex): https://github.com/run-llama/llama-lab\n\n## \ud83d\ude80 overview\n\n**note**: this readme is not updated as frequently as the documentation. please check out the documentation above for the latest updates!\n\n### context\n\n- llms are a phenomenal piece of technology for knowledge generation and reasoning. they are pre-trained on large amounts of publicly available data.\n- how do we best augment llms with our own private data?\n\nwe need a comprehensive toolkit to help perform this data augmentation for llms.\n\n### proposed solution\n\nthat's where **llamaindex** comes in. llamaindex is a \"data framework\" to help you build llm apps. it provides the following tools:\n\n- offers **data connectors** to ingest your existing data sources and data formats (apis, pdfs, docs, sql, etc.)\n- provides ways to **structure your data** (indices, graphs) so that this data can be easily used with llms.\n- provides an **advanced retrieval/query interface over your data**: feed in any llm input prompt, get back retrieved context and knowledge-augmented output.\n- allows easy integrations with your outer application framework (e.g. with langchain, flask, docker, chatgpt, anything else).\n\nllamaindex provides tools for both beginner users and advanced users. our high-level api allows beginner users to use llamaindex to ingest and query their data in\n5 lines of code. our lower-level apis allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules),\nto fit their needs.\n\n## \ud83d\udca1 contributing\n\ninterested in contributing? see our [contribution guide](contributing.md) for more details.\n\n## \ud83d\udcc4 documentation\n\nfull documentation can be found here: https://gpt-index.readthedocs.io/en/latest/.\n\nplease check it out for the most up-to-date tutorials, how-to guides, references, and other resources!\n\n## \ud83d\udcbb example usage\n\n```\npip install llama-index\n```\n\nexamples are in the `examples` folder. indices are in the `indices` folder (see list of indices below).\n\nto build a simple vector store index using openai:\n\n```python\nimport os\n\nos.environ[\"openai_api_key\"] = \"your_openai_api_key\"\n\nfrom llama_index import vectorstoreindex, simpledirectoryreader\n\ndocuments = simpledirectoryreader(\"your_data_directory\").load_data()\nindex = vectorstoreindex.from_documents(documents)\n```\n\nto build a simple vector store index using non-openai llms, e.g. llama 2 hosted on [replicate](https://replicate.com/), where you can easily create a free trial api token:\n\n```python\nimport os\n\nos.environ[\"replicate_api_token\"] = \"your_replicate_api_token\"\n\nfrom llama_index.llms import replicate\n\nllama2_7b_chat = \"meta/llama-2-7b-chat:8e6975e5ed6174911a6ff3d60540dfd4844201974602551e10e9e87ab143d81e\"\nllm = replicate(\n    model=llama2_7b_chat,\n    temperature=0.01,\n    additional_kwargs={\"top_p\": 1, \"max_new_tokens\": 300},\n)\n\n# set tokenizer to match llm\nfrom llama_index import set_global_tokenizer\nfrom transformers import autotokenizer\n\nset_global_tokenizer(\n    autotokenizer.from_pretrained(\"nousresearch/llama-2-7b-chat-hf\").encode\n)\n\nfrom llama_index.embeddings import huggingfaceembedding\nfrom llama_index import servicecontext\n\nembed_model = huggingfaceembedding(model_name=\"baai/bge-small-en-v1.5\")\nservice_context = servicecontext.from_defaults(\n    llm=llm, embed_model=embed_model\n)\n\nfrom llama_index import vectorstoreindex, simpledirectoryreader\n\ndocuments = simpledirectoryreader(\"your_data_directory\").load_data()\nindex = vectorstoreindex.from_documents(\n    documents, service_context=service_context\n)\n```\n\nto query:\n\n```python\nquery_engine = index.as_query_engine()\nquery_engine.query(\"your_question\")\n```\n\nby default, data is stored in-memory.\nto persist to disk (under `./storage`):\n\n```python\nindex.storage_context.persist()\n```\n\nto reload from disk:\n\n```python\nfrom llama_index import storagecontext, load_index_from_storage\n\n# rebuild storage context\nstorage_context = storagecontext.from_defaults(persist_dir=\"./storage\")\n# load index\nindex = load_index_from_storage(storage_context)\n```\n\n## \ud83d\udd27 dependencies\n\nthe main third-party package requirements are `tiktoken`, `openai`, and `langchain`.\n\nall requirements should be contained within the `setup.py` file.\nto run the package locally without building the wheel, simply run:\n\n```bash\npip install poetry\npoetry install --with dev\n```\n\n## \ud83d\udcd6 citation\n\nreference to cite if you use llamaindex in a paper:\n\n```\n@software{liu_llamaindex_2022,\nauthor = {liu, jerry},\ndoi = {10.5281/zenodo.1234},\nmonth = {11},\ntitle = {{llamaindex}},\nurl = {https://github.com/jerryjliu/llama_index},\nyear = {2022}\n}\n```\n",
  "docs_url": null,
  "keywords": "llm,nlp,rag,data,devtools,index,retrieval",
  "license": "mit",
  "name": "llama-index",
  "package_url": "https://pypi.org/project/llama-index/",
  "project_url": "https://pypi.org/project/llama-index/",
  "project_urls": {
    "Documentation": "https://docs.llamaindex.ai/en/stable/",
    "Homepage": "https://llamaindex.ai",
    "Repository": "https://github.com/run-llama/llama_index"
  },
  "release_url": "https://pypi.org/project/llama-index/0.9.21/",
  "requires_dist": [
    "SQLAlchemy[asyncio] (>=1.4.49)",
    "beautifulsoup4 (>=4.12.2,<5.0.0)",
    "dataclasses-json",
    "deprecated (>=1.2.9.3)",
    "fsspec (>=2023.5.0)",
    "httpx",
    "langchain (>=0.0.303) ; extra == \"langchain\"",
    "nest-asyncio (>=1.5.8,<2.0.0)",
    "nltk (>=3.8.1,<4.0.0)",
    "numpy",
    "openai (>=1.1.0)",
    "pandas",
    "tenacity (>=8.2.0,<9.0.0)",
    "tiktoken (>=0.3.3)",
    "typing-extensions (>=4.5.0)",
    "typing-inspect (>=0.8.0)",
    "requests (>=2.31.0)",
    "gradientai (>=1.4.0) ; extra == \"gradientai\"",
    "asyncpg (>=0.28.0,<0.29.0) ; extra == \"postgres\"",
    "pgvector (>=0.1.0,<0.2.0) ; extra == \"postgres\"",
    "psycopg-binary (>=3.1.12,<4.0.0) ; extra == \"postgres\"",
    "optimum[onnxruntime] (>=1.13.2,<2.0.0) ; extra == \"local-models\"",
    "sentencepiece (>=0.1.99,<0.2.0) ; extra == \"local-models\"",
    "transformers[torch] (>=4.34.0,<5.0.0) ; extra == \"local-models\"",
    "guidance (>=0.0.64,<0.0.65) ; extra == \"query-tools\"",
    "lm-format-enforcer (>=0.4.3,<0.5.0) ; extra == \"query-tools\"",
    "jsonpath-ng (>=1.6.0,<2.0.0) ; extra == \"query-tools\"",
    "rank-bm25 (>=0.2.2,<0.3.0) ; extra == \"query-tools\"",
    "scikit-learn ; extra == \"query-tools\"",
    "spacy (>=3.7.1,<4.0.0) ; extra == \"query-tools\"",
    "aiohttp (>=3.8.6,<4.0.0)"
  ],
  "requires_python": ">=3.8.1,<4.0",
  "summary": "interface between llms and your data",
  "version": "0.9.21",
  "releases": [],
  "developers": [
    "andrei@runllama.ai",
    "andrei_fajardo",
    "jerry@llamaindex.ai",
    "jerry_liu"
  ],
  "kwds": "llama_index llamaindex llamaindexts llamalab llama",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_llama_index",
  "homepage": "https://llamaindex.ai",
  "release_count": 286,
  "dependency_ids": [
    "pypi_aiohttp",
    "pypi_asyncpg",
    "pypi_beautifulsoup4",
    "pypi_dataclasses_json",
    "pypi_deprecated",
    "pypi_fsspec",
    "pypi_gradientai",
    "pypi_guidance",
    "pypi_httpx",
    "pypi_jsonpath_ng",
    "pypi_langchain",
    "pypi_lm_format_enforcer",
    "pypi_nest_asyncio",
    "pypi_nltk",
    "pypi_numpy",
    "pypi_openai",
    "pypi_optimum",
    "pypi_pandas",
    "pypi_pgvector",
    "pypi_psycopg_binary",
    "pypi_rank_bm25",
    "pypi_requests",
    "pypi_scikit_learn",
    "pypi_sentencepiece",
    "pypi_spacy",
    "pypi_sqlalchemy",
    "pypi_tenacity",
    "pypi_tiktoken",
    "pypi_transformers",
    "pypi_typing_extensions",
    "pypi_typing_inspect"
  ]
}