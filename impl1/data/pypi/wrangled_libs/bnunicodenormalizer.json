{
  "classifiers": [
    "development status :: 3 - alpha",
    "intended audience :: education",
    "license :: osi approved :: mit license",
    "operating system :: os independent",
    "programming language :: python :: 3"
  ],
  "description": "# bnunicodenormalizer\nbangla unicode normalization for word normalization\n# install\n```python\npip install bnunicodenormalizer\n```\n# useage\n**initialization and cleaning**\n```python\n# import\nfrom bnunicodenormalizer import normalizer \nfrom pprint import pprint\n# initialize\nbnorm=normalizer()\n# normalize\nword = '\u09be\u099f\u09cb\u09ac\u09be\u0995\u09cb'\nresult=bnorm(word)\nprint(f\"non-norm:{word}; norm:{result['normalized']}\")\nprint(\"--------------------------------------------------\")\npprint(result)\n```\n> output \n\n```\nnon-norm:\u09be\u099f\u09cb\u09ac\u09be\u0995\u09cb; norm:\u099f\u09cb\u09ac\u09be\u0995\u09cb\n--------------------------------------------------\n{'given': '\u09be\u099f\u09cb\u09ac\u09be\u0995\u09cb',\n 'normalized': '\u099f\u09cb\u09ac\u09be\u0995\u09cb',\n 'ops': [{'after': '\u099f\u09cb\u09ac\u09be\u0995\u09cb',\n          'before': '\u09be\u099f\u09cb\u09ac\u09be\u0995\u09cb',\n          'operation': 'invalidunicode'}]}\n```\n\n**call to the normalizer returns a dictionary in the following format**\n\n* ```given``` = provided text\n* ```normalized``` = normalized text (gives none if during the operation length of the text becomes 0)\n* ```ops``` = list of operations (dictionary) that were executed in given text to create normalized text\n*  each dictionary in ops has:\n    * ```operation```: the name of the operation / problem in given text\n    * ```before``` : what the text looked like before the specific operation\n    * ```after```  : what the text looks like after the specific operation  \n\n**allow to use english text**\n\n```python\n# initialize without english (default)\nnorm=normalizer()\nprint(\"without english:\",norm(\"asd123\")[\"normalized\"])\n# --> returns none\nnorm=normalizer(allow_english=true)\nprint(\"with english:\",norm(\"asd123\")[\"normalized\"])\n\n```\n> output\n\n```\nwithout english: none\nwith english: asd123\n```\n\n \n\n# initialization: bangla normalizer\n\n```python\n'''\n    initialize a normalizer\n            args:\n                allow_english                   :   allow english letters numbers and punctuations [default:false]\n                keep_legacy_symbols             :   legacy symbols will be considered as valid unicodes[default:false]\n                                                    '\u09fa':isshar \n                                                    '\u09fb':ganda\n                                                    '\u0980':anji (not '\u09ed')\n                                                    '\u098c':li\n                                                    '\u09e1':dirgho li\n                                                    '\u09bd':avagraha\n                                                    '\u09e0':vocalic rr (not '\u098b')\n                                                    '\u09f2':rupi\n                                                    '\u09f4':currency numerator 1\n                                                    '\u09f5':currency numerator 2\n                                                    '\u09f6':currency numerator 3\n                                                    '\u09f7':currency numerator 4\n                                                    '\u09f8':currency numerator one less than the denominator\n                                                    '\u09f9':currency denominator sixteen\n                legacy_maps                     :   a dictionay for changing legacy symbols into a more used  unicode \n                                                    a default legacy map is included in the language class as well,\n                                                    legacy_maps={'\u0980':'\u09ed',\n                                                                '\u098c':'\u09ef',\n                                                                '\u09e1':'\u09ef',\n                                                                '\u09f5':'\u09ef',\n                                                                '\u09fb':'\u09ce',\n                                                                '\u09e0':'\u098b',\n                                                                '\u09bd':'\u0987'}\n                                            \n                                                    pass-   \n                                                    * legacy_maps=none; for keeping the legacy symbols as they are\n                                                    * legacy_maps=\"default\"; for using the default legacy map\n                                                    * legacy_maps=custom dictionary(type-dict) ; which will map your desired legacy symbol to any of symbol you want\n                                                        * the keys in the custiom dicts must belong to any of the legacy symbols\n                                                        * the values in the custiom dicts must belong to either vowels,consonants,numbers or diacritics  \n                                                        vowels         =   ['\u0985', '\u0986', '\u0987', '\u0988', '\u0989', '\u098a', '\u098b', '\u098f', '\u0990', '\u0993', '\u0994']\n                                                        consonants     =   ['\u0995', '\u0996', '\u0997', '\u0998', '\u0999', '\u099a', '\u099b','\u099c', '\u099d', '\u099e', \n                                                                            '\u099f', '\u09a0', '\u09a1', '\u09a2', '\u09a3', '\u09a4', '\u09a5', '\u09a6', '\u09a7', '\u09a8', \n                                                                            '\u09aa', '\u09ab', '\u09ac', '\u09ad', '\u09ae', '\u09af', '\u09b0', '\u09b2', '\u09b6', '\u09b7', \n                                                                            '\u09b8', '\u09b9','\u09dc', '\u09dd', '\u09df','\u09ce']    \n                                                        numbers        =    ['\u09e6', '\u09e7', '\u09e8', '\u09e9', '\u09ea', '\u09eb', '\u09ec', '\u09ed', '\u09ee', '\u09ef']\n                                                        vowel_diacritics       =   ['\u09be', '\u09bf', '\u09c0', '\u09c1', '\u09c2', '\u09c3', '\u09c7', '\u09c8', '\u09cb', '\u09cc']\n                                                        consonant_diacritics   =   ['\u0981', '\u0982', '\u0983']\n    \n                                                        > for example you may want to map '\u09bd':avagraha as '\u09b9' based on visual similiarity \n                                                            (default:'\u0987')\n\n                ** legacy contions: keep_legacy_symbols and legacy_maps operates as follows \n                    case-1) keep_legacy_symbols=true and legacy_maps=none\n                        : all legacy symbols will be considered valid unicodes. none of them will be changed\n                    case-2) keep_legacy_symbols=true and legacy_maps=valid dictionary example:{'\u0980':'\u0995'}\n                        : all legacy symbols will be considered valid unicodes. only '\u0980' will be changed to '\u0995' , others will be untouched\n                    case-3) keep_legacy_symbols=false and legacy_maps=none\n                        : all legacy symbols will be removed\n                    case-4) keep_legacy_symbols=false and legacy_maps=valid dictionary example:{'\u09bd':'\u0987','\u09e0':'\u098b'}\n                        : '\u09bd' will be changed to '\u0987' and '\u09e0' will be changed to '\u098b'. all other legacy symbols will be removed\n'''\n\n```\n\n```python\nmy_legacy_maps={'\u098c':'\u0987',\n                '\u09e1':'\u0987',\n                '\u09f5':'\u0987',\n                '\u09e0':'\u0987',\n                '\u09bd':'\u0987'}\ntext=\"\u09fa,\u09fb,\u0980,\u098c,\u09e1,\u09bd,\u09e0,\u09f2,\u09f4,\u09f5,\u09f6,\u09f7,\u09f8,\u09f9\"\n# case 1\nnorm=normalizer(keep_legacy_symbols=true,legacy_maps=none)\nprint(\"case-1 normalized text:  \",norm(text)[\"normalized\"])\n# case 2\nnorm=normalizer(keep_legacy_symbols=true,legacy_maps=my_legacy_maps)\nprint(\"case-2 normalized text:  \",norm(text)[\"normalized\"])\n# case 2-defalut\nnorm=normalizer(keep_legacy_symbols=true)\nprint(\"case-2 default normalized text:  \",norm(text)[\"normalized\"])\n\n# case 3\nnorm=normalizer(keep_legacy_symbols=false,legacy_maps=none)\nprint(\"case-3 normalized text:  \",norm(text)[\"normalized\"])\n# case 4\nnorm=normalizer(keep_legacy_symbols=false,legacy_maps=my_legacy_maps)\nprint(\"case-4 normalized text:  \",norm(text)[\"normalized\"])\n# case 4-defalut\nnorm=normalizer(keep_legacy_symbols=false)\nprint(\"case-4 default normalized text:  \",norm(text)[\"normalized\"])\n```\n\n> output\n\n```\ncase-1 normalized text:   \u09fa,\u09fb,\u0980,\u098c,\u09e1,\u09bd,\u09e0,\u09f2,\u09f4,\u09f5,\u09f6,\u09f7,\u09f8,\u09f9\ncase-2 normalized text:   \u09fa,\u09fb,\u0980,\u0987,\u0987,\u0987,\u0987,\u09f2,\u09f4,\u0987,\u09f6,\u09f7,\u09f8,\u09f9\ncase-2 default normalized text:   \u09fa,\u09fb,\u0980,\u098c,\u09e1,\u09bd,\u09e0,\u09f2,\u09f4,\u09f5,\u09f6,\u09f7,\u09f8,\u09f9\ncase-3 normalized text:   ,,,,,,,,,,,,,\ncase-4 normalized text:   ,,,\u0987,\u0987,\u0987,\u0987,,,\u0987,,,,\ncase-4 default normalized text:   ,,,,,,,,,,,,, \n```\n\n# operations\n* base operations available for all indic languages:\n\n```python\nself.word_level_ops={\"legacysymbols\"   :self.maplegacysymbols,\n                    \"brokendiacritics\" :self.fixbrokendiacritics}\n\nself.decomp_level_ops={\"brokennukta\"             :self.fixbrokennukta,\n                    \"invalidunicode\"             :self.cleaninvalidunicodes,\n                    \"invalidconnector\"           :self.cleaninvalidconnector,\n                    \"fixdiacritics\"              :self.cleandiacritics,\n                    \"voweldiacriticaftervowel\"   :self.cleanvoweldiacriticcomingaftervowel}\n```\n* extensions for bangla\n\n```python\nself.decomp_level_ops[\"toandhosontonormalize\"]             =       self.normalizetoandhosonto\n\n# invalid folas \nself.decomp_level_ops[\"normalizeconjunctsdiacritics\"]      =       self.cleaninvalidconjunctdiacritics\n\n# complex root cleanup \nself.decomp_level_ops[\"complexrootnormalization\"]          =       self.convertcomplexroots\n\n```\n\n# normalization problem examples\n**in all examples (a) is the non-normalized form and (b) is the normalized form**\n\n* broken diacritics:\n``` \n# example-1: \n(a)'\u0986\u09b0\u09c7\u09be'==(b)'\u0986\u09b0\u09cb' ->  false \n    (a) breaks as:['\u0986', '\u09b0', '\u09c7', '\u09be']\n    (b) breaks as:['\u0986', '\u09b0', '\u09cb']\n# example-2:\n(a)\u09aa\u09c7\u09d7\u0981\u099b\u09c7==(b)\u09aa\u09cc\u0981\u099b\u09c7 ->  false\n    (a) breaks as:['\u09aa', '\u09c7', '\u09d7', '\u0981', '\u099b', '\u09c7']\n    (b) breaks as:['\u09aa', '\u09cc', '\u0981', '\u099b', '\u09c7']\n# example-3:\n(a)\u09b8\u0982\u09b8\u09cd\u0995\u09c4\u09a4\u09bf==(b)\u09b8\u0982\u09b8\u09cd\u0995\u09c3\u09a4\u09bf ->  false\n    (a) breaks as:['\u09b8', '\u0982', '\u09b8', '\u09cd', '\u0995', '\u09c4', '\u09a4', '\u09bf']\n    (b) breaks as:['\u09b8', '\u0982', '\u09b8', '\u09cd', '\u0995', '\u09c3', '\u09a4', '\u09bf']\n```\n* nukta normalization:\n\n```        \nexample-1:\n(a)\u0995\u09c7\u09a8\u09cd\u09a6\u09cd\u09b0\u09c0\u09af\u09bc==(b)\u0995\u09c7\u09a8\u09cd\u09a6\u09cd\u09b0\u09c0\u09df ->  false\n    (a) breaks as:['\u0995', '\u09c7', '\u09a8', '\u09cd', '\u09a6', '\u09cd', '\u09b0', '\u09c0', '\u09af', '\u09bc']\n    (b) breaks as:['\u0995', '\u09c7', '\u09a8', '\u09cd', '\u09a6', '\u09cd', '\u09b0', '\u09c0', '\u09df']\nexample-2:\n(a)\u09b0\u09af\u09c7\u09bc\u099b\u09c7==(b)\u09b0\u09df\u09c7\u099b\u09c7 ->  false\n    (a) breaks as:['\u09b0', '\u09af', '\u09c7', '\u09bc', '\u099b', '\u09c7']\n    (b) breaks as:['\u09b0', '\u09df', '\u09c7', '\u099b', '\u09c7']\nexample-3: \n(a)\u099c\u09bc\u09a8\u09cd\u09af==(b)\u099c\u09a8\u09cd\u09af ->  false\n    (a) breaks as:['\u099c', '\u09bc', '\u09a8', '\u09cd', '\u09af']\n    (b) breaks as:['\u099c', '\u09a8', '\u09cd', '\u09af']\n``` \n* invalid hosonto\n```\n# example-1:\n(a)\u09a6\u09c1\u0987\u09cd\u099f\u09bf==(b)\u09a6\u09c1\u0987\u099f\u09bf-->false\n    (a) breaks as ['\u09a6', '\u09c1', '\u0987', '\u09cd', '\u099f', '\u09bf']\n    (b) breaks as ['\u09a6', '\u09c1', '\u0987', '\u099f', '\u09bf']\n# example-2:\n(a)\u098f\u09cd\u09a4\u09c7==(b)\u098f\u09a4\u09c7-->false\n    (a) breaks as ['\u098f', '\u09cd', '\u09a4', '\u09c7']\n    (b) breaks as ['\u098f', '\u09a4', '\u09c7']\n# example-3:\n(a)\u09a8\u09c7\u099f\u09cd\u0993\u09df\u09be\u09b0\u09cd\u0995==(b)\u09a8\u09c7\u099f\u0993\u09df\u09be\u09b0\u09cd\u0995-->false\n    (a) breaks as ['\u09a8', '\u09c7', '\u099f', '\u09cd', '\u0993', '\u09df', '\u09be', '\u09b0', '\u09cd', '\u0995']\n    (b) breaks as ['\u09a8', '\u09c7', '\u099f', '\u0993', '\u09df', '\u09be', '\u09b0', '\u09cd', '\u0995']\n# example-4:\n(a)\u098f\u09b8\u09cd\u0986\u0987==(b)\u098f\u09b8\u0986\u0987-->false\n    (a) breaks as ['\u098f', '\u09b8', '\u09cd', '\u0986', '\u0987']\n    (b) breaks as ['\u098f', '\u09b8', '\u0986', '\u0987']\n# example-5: \n(a)'\u099a\u09c1\u09cd\u0995\u09cd\u09a4\u09bf'==(b)'\u099a\u09c1\u0995\u09cd\u09a4\u09bf' ->  false \n    (a) breaks as:['\u099a', '\u09c1', '\u09cd', '\u0995', '\u09cd', '\u09a4', '\u09bf']\n    (b) breaks as:['\u099a', '\u09c1','\u0995', '\u09cd', '\u09a4', '\u09bf']\n# example-6:\n(a)'\u09af\u09c1\u09cd\u0995\u09cd\u09a4'==(b)'\u09af\u09c1\u0995\u09cd\u09a4' ->   false\n    (a) breaks as:['\u09af', '\u09c1', '\u09cd', '\u0995', '\u09cd', '\u09a4']\n    (b) breaks as:['\u09af', '\u09c1', '\u0995', '\u09cd', '\u09a4']\n# example-7:\n(a)'\u0995\u09bf\u099b\u09c1\u09cd\u0987'==(b)'\u0995\u09bf\u099b\u09c1\u0987' ->   false\n    (a) breaks as:['\u0995', '\u09bf', '\u099b', '\u09c1', '\u09cd', '\u0987']\n    (b) breaks as:['\u0995', '\u09bf', '\u099b', '\u09c1','\u0987']\n```\n\n* to+hosonto: \n\n``` \n# example-1:\n(a)\u09ac\u09c1\u09a4\u09cd\u09aa\u09a4\u09cd\u09a4\u09bf==(b)\u09ac\u09c1\u09ce\u09aa\u09a4\u09cd\u09a4\u09bf-->false\n    (a) breaks as ['\u09ac', '\u09c1', '\u09a4', '\u09cd', '\u09aa', '\u09a4', '\u09cd', '\u09a4', '\u09bf']\n    (b) breaks as ['\u09ac', '\u09c1', '\u09ce', '\u09aa', '\u09a4', '\u09cd', '\u09a4', '\u09bf']\n# example-2:\n(a)\u0989\u09a4\u09cd\u09b8==(b)\u0989\u09ce\u09b8-->false\n    (a) breaks as ['\u0989', '\u09a4', '\u09cd', '\u09b8']\n    (b) breaks as ['\u0989', '\u09ce', '\u09b8']\n```\n\n* unwanted doubles(consecutive doubles):\n\n```\n# example-1: \n(a)'\u09af\u09c1\u09c1\u09a6\u09cd\u09a7'==(b)'\u09af\u09c1\u09a6\u09cd\u09a7' ->  false \n    (a) breaks as:['\u09af', '\u09c1', '\u09c1', '\u09a6', '\u09cd', '\u09a7']\n    (b) breaks as:['\u09af', '\u09c1', '\u09a6', '\u09cd', '\u09a7']\n# example-2:\n(a)'\u09a6\u09c1\u09c1\u0987'==(b)'\u09a6\u09c1\u0987' ->   false\n    (a) breaks as:['\u09a6', '\u09c1', '\u09c1', '\u0987']\n    (b) breaks as:['\u09a6', '\u09c1', '\u0987']\n# example-3:\n(a)'\u09aa\u09cd\u09b0\u0995\u09c3\u09c3\u09a4\u09bf\u09b0'==(b)'\u09aa\u09cd\u09b0\u0995\u09c3\u09a4\u09bf\u09b0' ->   false\n    (a) breaks as:['\u09aa', '\u09cd', '\u09b0', '\u0995', '\u09c3', '\u09c3', '\u09a4', '\u09bf', '\u09b0']\n    (b) breaks as:['\u09aa', '\u09cd', '\u09b0', '\u0995', '\u09c3', '\u09a4', '\u09bf', '\u09b0']\n# example-4:\n(a)\u0986\u09ae\u09be\u0995\u09c7\u09be\u09be==(b)'\u0986\u09ae\u09be\u0995\u09cb'->   false\n    (a) breaks as:['\u0986', '\u09ae', '\u09be', '\u0995', '\u09c7', '\u09be', '\u09be']\n    (b) breaks as:['\u0986', '\u09ae', '\u09be', '\u0995', '\u09cb']\n```\n\n* vowwels and modifier followed by vowel diacritics:\n\n```\n# example-1:\n(a)\u0989\u09c1\u09b2\u09c1==(b)\u0989\u09b2\u09c1-->false\n    (a) breaks as ['\u0989', '\u09c1', '\u09b2', '\u09c1']\n    (b) breaks as ['\u0989', '\u09b2', '\u09c1']\n# example-2:\n(a)\u0986\u09b0\u09cd\u0995\u09bf\u0993\u09cb\u09b2\u099c\u09bf==(b)\u0986\u09b0\u09cd\u0995\u09bf\u0993\u09b2\u099c\u09bf-->false\n    (a) breaks as ['\u0986', '\u09b0', '\u09cd', '\u0995', '\u09bf', '\u0993', '\u09cb', '\u09b2', '\u099c', '\u09bf']\n    (b) breaks as ['\u0986', '\u09b0', '\u09cd', '\u0995', '\u09bf', '\u0993', '\u09b2', '\u099c', '\u09bf']\n# example-3:\n(a)\u098f\u0995\u098f\u09c7==(b)\u098f\u0995\u09a4\u09cd\u09b0\u09c7-->false\n    (a) breaks as ['\u098f', '\u0995', '\u098f', '\u09c7']\n    (b) breaks as ['\u098f', '\u0995', '\u09a4', '\u09cd', '\u09b0', '\u09c7']\n```  \n\n* repeated folas:\n\n```\n# example-1:\n(a)\u0997\u09cd\u09b0\u09cd\u09b0\u09be\u09ae\u0995\u09c7==(b)\u0997\u09cd\u09b0\u09be\u09ae\u0995\u09c7-->false\n    (a) breaks as ['\u0997', '\u09cd', '\u09b0', '\u09cd', '\u09b0', '\u09be', '\u09ae', '\u0995', '\u09c7']\n    (b) breaks as ['\u0997', '\u09cd', '\u09b0', '\u09be', '\u09ae', '\u0995', '\u09c7']\n```\n\n## important note\n**the normalization is purely based on how bangla text is used in ```bangladesh```(bn:bd). it does not necesserily cover every variation of textual content available at other regions**\n\n# unit testing\n* clone the repository\n* change working directory to ```tests```\n* run: ```python3 -m unittest test_normalizer.py```\n\n# issue reporting\n* for reporting an issue please provide the specific information\n    *  invalid text\n    *  expected valid text\n    *  why is the output expected \n    *  clone the repository\n    *  add a test case in **tests/test_normalizer.py** after **line no:91**\n\n    ```python\n        # dummy non-bangla,numbers and space cases/ invalid start end cases\n        # english\n        self.assertequal(norm('asd1234')[\"normalized\"],none)\n        self.assertequal(ennorm('asd1234')[\"normalized\"],'asd1234')\n        # random\n        self.assertequal(norm('\u09bf\u09a4')[\"normalized\"],'\u09a4')\n        self.assertequal(norm('\u09b8\u0982\u09cd\u09af\u09c1\u0995\u09cd\u09a4\u09bf')[\"normalized\"],\"\u09b8\u0982\u09af\u09c1\u0995\u09cd\u09a4\u09bf\")\n        # ending\n        self.assertequal(norm(\"\u0985\u099c\u09be\u09a8\u09be\u09cd\")[\"normalized\"],\"\u0985\u099c\u09be\u09a8\u09be\")\n\n        #--------------------------------------------- insert your assertions here----------------------------------------\n        '''\n            ###  case: give a comment about your case\n            ## (a) invalid text==(b) valid text <---- an example of your case\n            self.assertequal(norm(invalid text)[\"normalized\"],expected output)\n                        or\n            self.assertequal(ennorm(invalid text)[\"normalized\"],expected output) <----- for including english text\n            \n        '''\n        # your case goes here-\n            \n    ```\n    * perform the unit testing\n    * make sure the unit test fails under true conditions    \n\n# indic base normalizer\n* to use indic language normalizer for 'devanagari', 'gujarati', 'odiya', 'tamil', 'panjabi', 'malayalam','sylhetinagri'\n\n```python\nfrom bnunicodenormalizer import indicnormalizer\nnorm=indicnormalizer('devanagari')\n```\n* initialization\n\n```python\n'''\n    initialize a normalizer\n    args:\n        language                        :   language identifier from 'devanagari', 'gujarati', 'odiya', 'tamil', 'panjabi', 'malayalam','sylhetinagri'\n        allow_english                   :   allow english letters numbers and punctuations [default:false]\n                \n'''        \n        \n```\n\n\n# about us\n* authors: [bengali.ai](https://bengali.ai/) in association with ocr team , [apsis solutions limited](https://apsissolutions.com/) \n* **cite bengali.ai multipurpose grapheme dataset paper**\n```bibtext\n@inproceedings{alam2021large,\n  title={a large multi-target dataset of common bengali handwritten graphemes},\n  author={alam, samiul and reasat, tahsin and sushmit, asif shahriyar and siddique, sadi mohammad and rahman, fuad and hasan, mahady and humayun, ahmed imtiaz},\n  booktitle={international conference on document analysis and recognition},\n  pages={383--398},\n  year={2021},\n  organization={springer}\n}\n```\n\nchange log\n===========\n\n0.0.5 (9/03/2022)\n-------------------\n- added details for execution map\n- checkop typo correction\n\n0.0.6 (9/03/2022)\n-------------------\n- broken diacritics op addition\n\n0.0.7 (11/03/2022)\n-------------------\n- assemese replacement\n- word op and unicode op mapping\n- modifier list modification\n- doc string for call and initialization\n- verbosity removal\n- typo correction for operation\n- unit test updates\n- '\u098f' replacement correction\n- nongylphunicodes\n- legacy symbols option\n- legacy mapper added \n- added bn:bd declaration\n\n0.0.8 (14/03/2022)\n-------------------\n- multipleconsonantdiacritics handling change\n- to+hosonto correction\n- invalid hosonto correction \n\n0.0.9 (15/04/2022)\n-------------------\n- base normalizer\n- language class\n- bangla extension\n- complex root normalization \n\n0.0.10 (15/04/2022)\n-------------------\n- added conjucts\n- exception for english words\n\n0.0.11 (15/04/2022)\n-------------------\n- fixed no space char issue for bangla\n\n0.0.12 (26/04/2022)\n-------------------\n- fixed consonants orders \n\n0.0.13 (26/04/2022)\n-------------------\n- fixed non char followed by diacritics \n\n0.0.14 (01/05/2022)\n-------------------\n- word based normalization\n- encoding fix\n\n0.0.15 (02/05/2022)\n-------------------\n- import correction\n\n0.0.16 (02/05/2022)\n-------------------\n- local variable issue\n\n0.0.17 (17/05/2022)\n-------------------\n- nukta mod break\n\n0.0.18 (08/06/2022)\n-------------------\n- no space chars fix\n\n\n0.0.19 (15/06/2022)\n-------------------\n- no space chars further fix\n- base_bangla_compose to avoid false op flags\n- added foreign conjuncts\n\n\n0.0.20 (01/08/2022)\n-------------------\n- \u098f\u09cd\u09af\u09be replacement correction\n\n0.0.21 (01/08/2022)\n-------------------\n- \"\u09af\",\"\u09ac\" + hosonto combination correction\n- added '\u09ac\u09cd\u09b2\u09cd\u09af' in conjuncts\n\n0.0.22 (22/08/2022)\n-------------------\n- \\u200d combination limiting\n\n0.0.23 (23/08/2022)\n-------------------\n- \\u200d condition change\n\n0.0.24 (26/08/2022)\n-------------------\n- \\u200d error handling\n\n0.0.25 (10/09/22)\n-------------------\n- removed unnecessary operations: fixreforder,fixordersforcc\n- added conjuncts: '\u09b0\u09cd\u09a8\u09cd\u09a4','\u09a0\u09cd\u09af','\u09ad\u09cd\u09b2'\n\n0.1.0 (20/10/22)\n-------------------\n- added indic parser\n- fixed language class\n\n0.1.1 (21/10/22)\n-------------------\n- added nukta and diacritic maps for indics \n- cleaned conjucts for now \n- fixed issues with no-space and connector\n\n0.1.2 (10/12/22)\n-------------------\n- allow halant ending for indic language except bangla\n\n0.1.3 (10/12/22)\n-------------------\n- broken char break cases for halant \n\n0.1.4 (01/01/23)\n-------------------\n- added sylhetinagri \n\n0.1.5 (01/01/23)\n-------------------\n- cleaned panjabi double quotes in diac map \n\n0.1.6 (15/04/23)\n-------------------\n- added bangla punctuations \n\n",
  "docs_url": null,
  "keywords": "bangla,unicode,text normalization,indic",
  "license": "mit",
  "name": "bnunicodenormalizer",
  "package_url": "https://pypi.org/project/bnunicodenormalizer/",
  "project_url": "https://pypi.org/project/bnunicodenormalizer/",
  "project_urls": {
    "Homepage": "https://github.com/mnansary/bnUnicodeNormalizer"
  },
  "release_url": "https://pypi.org/project/bnunicodenormalizer/0.1.6/",
  "requires_dist": [],
  "requires_python": "",
  "summary": "bangla unicode normalization toolkit",
  "version": "0.1.6",
  "releases": [],
  "developers": [
    "bengali",
    "research.bengaliai@gmail.com"
  ],
  "kwds": "\u09b0\u0995 bengali bnunicodenormalizer \u0986\u09ae unicode",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_bnunicodenormalizer",
  "homepage": "https://github.com/mnansary/bnunicodenormalizer",
  "release_count": 32,
  "dependency_ids": []
}