{
  "classifiers": [
    "license :: osi approved :: apache software license",
    "operating system :: os independent",
    "programming language :: python :: 3"
  ],
  "description": "# getting started: td-pyspark\n\n[treasure data](https://treasuredata.com) extension for using [pyspark](https://spark.apache.org/docs/latest/api/python/index.html).\n\n## installation\n\nyou can install td-pyspark from pypi by using `pip` as follows:\n\n```sh\n$ pip install td-pyspark\n```\n\nif you want to install pyspark via pypi as well, you can install as:\n\n```sh\n$ pip install td-pyspark[spark]\n```\n\n## introduction\n\nfirst contact [support@treasure-data.com](mailto:support@treasure-data.com) to enable td-spark feature. this feature is disabled by default.\n\ntd-pyspark is a library to enable python to access tables in treasure data.\nthe features of td_pyspark include:\n\n- reading tables in treasure data as dataframe\n- writing dataframes to treasure data\n- submitting presto queries and read the query results as dataframes\n\nfor more details, see also [td-spark faqs](https://docs.treasuredata.com/display/public/pd/apache+spark+driver+%28td-spark%29+faqs).\n\n### quick start with docker\n\nyou can try td_pyspark using docker without installing spark nor python.\n\nfirst create __td-spark.conf__ file and set your td api key and site (us, jp, eu01, ap02) configurations:\n\n__td-spark.conf__\n```\nspark.td.apikey (your td api key)\nspark.td.site (your site: us, jp, eu01, ap02)\nspark.serializer org.apache.spark.serializer.kryoserializer\nspark.sql.execution.arrow.pyspark.enabled true\n```\n\nlaunch pyspark docker image. this image already has a pre-installed td_pyspark library:\n\n```shell\n$ docker run -it -e td_spark_conf=td-spark.conf -v $(pwd):/opt/spark/work devtd/td-spark-pyspark:latest_spark3.1.1\npython 3.9.2 (default, feb 19 2021, 17:33:48) \n[gcc 10.2.1 20201203] on linux\ntype \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n21/05/10 09:04:48 warn nativecodeloader: unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nusing spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nsetting default log level to \"warn\".\nto adjust logging level use sc.setloglevel(newlevel). for sparkr, use setloglevel(newlevel).\nwelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.1\n      /_/\n\nusing python version 3.9.2 (default, feb 19 2021 17:33:48)\nsparksession available as 'spark'.\n2021-05-10 09:04:53.268z debug [spark] loading com.treasuredata.spark package - (package.scala:23)\n...\n>>>\n```\n\ntry read a sample table by specifying a time range:\n\n```python\n>>> df = td.table(\"sample_datasets.www_access\").within(\"+2d/2014-10-04\").df()\n>>> df.show()\n2021-05-10 09:07:40.233z  info [partitionscanner] fetching the partition list of sample_datasets.www_access within time range:[2014-10-04 00:00:00z,2014-10-06 00:00:00z) - (partitionscanner.scala:29)\n2021-05-10 09:07:42.262z  info [partitionscanner] retrieved 2 partition entries - (partitionscanner.scala:36)\n+----+---------------+--------------------+--------------------+----+--------------------+----+------+----------+\n|user|           host|                path|             referer|code|               agent|size|method|      time|\n+----+---------------+--------------------+--------------------+----+--------------------+----+------+----------+\n|null|192.225.229.196|  /category/software|                   -| 200|mozilla/5.0 (maci...| 117|   get|1412382292|\n|null|120.168.215.131|  /category/software|                   -| 200|mozilla/5.0 (comp...|  53|   get|1412382284|\n|null|180.198.173.136|/category/electro...| /category/computers| 200|mozilla/5.0 (wind...| 106|   get|1412382275|\n|null| 140.168.145.49|   /item/garden/2832|      /item/toys/230| 200|mozilla/5.0 (maci...| 122|   get|1412382267|\n|null|  52.168.78.222|/category/electro...|    /item/games/2532| 200|mozilla/5.0 (comp...|  73|   get|1412382259|\n|null|  32.42.160.165|   /category/cameras|/category/cameras...| 200|mozilla/5.0 (wind...| 117|   get|1412382251|\n|null|   48.204.59.23|  /category/software|/search/?c=electr...| 200|mozilla/5.0 (maci...|  52|   get|1412382243|\n|null|136.207.150.227|/category/electro...|                   -| 200|mozilla/5.0 (ipad...| 120|   get|1412382234|\n|null| 204.21.174.187|   /category/jewelry|   /item/office/3462| 200|mozilla/5.0 (wind...|  59|   get|1412382226|\n|null|  224.198.88.93|    /category/office|     /category/music| 200|mozilla/4.0 (comp...|  46|   get|1412382218|\n|null|   96.54.24.116|     /category/games|                   -| 200|mozilla/5.0 (wind...|  40|   get|1412382210|\n|null| 184.42.224.210| /category/computers|                   -| 200|mozilla/5.0 (wind...|  95|   get|1412382201|\n|null|  144.72.47.212|/item/giftcards/4684|    /item/books/1031| 200|mozilla/5.0 (wind...|  65|   get|1412382193|\n|null| 40.213.111.170|     /item/toys/1085|   /category/cameras| 200|mozilla/5.0 (wind...|  65|   get|1412382185|\n|null| 132.54.226.209|/item/electronics...|  /category/software| 200|mozilla/5.0 (comp...| 121|   get|1412382177|\n|null|  108.219.68.64|/category/cameras...|                   -| 200|mozilla/5.0 (maci...|  54|   get|1412382168|\n|null| 168.66.149.218| /item/software/4343|  /category/software| 200|mozilla/4.0 (comp...| 139|   get|1412382160|\n|null|  80.66.118.103|  /category/software|                   -| 200|mozilla/4.0 (comp...|  92|   get|1412382152|\n|null|140.171.147.207|     /category/music|   /category/jewelry| 200|mozilla/5.0 (wind...| 119|   get|1412382144|\n|null| 84.132.164.204| /item/software/4783|/category/electro...| 200|mozilla/5.0 (wind...| 137|   get|1412382135|\n+----+---------------+--------------------+--------------------+----+--------------------+----+------+----------+\nonly showing top 20 rows\n>>>\n```\n\n\n## usage\n\n_tdsparkcontext_ is an entry point to access td_pyspark's functionalities. to create tdsparkcontext, pass your sparksession (spark) to tdsparkcontext:\n\n```python\ntd = tdsparkcontext(spark)\n```\n\n### reading tables as dataframes\n\nto read a table, use `td.table(table name)`:\n\n```python\ndf = td.table(\"sample_datasets.www_access\").df()\ndf.show()\n```\n\nto change the context database, use `td.use(database_name)`:\n\n```python\ntd.use(\"sample_datasets\")\n# accesses sample_datasets.www_access\ndf = td.table(\"www_access\").df()\n```\n\nby calling `.df()` your table data will be read as spark's dataframe.\nthe usage of the dataframe is the same with pyspark. see also [pyspark dataframe documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.dataframe).\n\n#### specifying time ranges\n\ntreasure data is a time series database, so reading recent data by specifying a time range is important to reduce the amount of data to be processed.\n`.within(...)` function can be used to specify a target time range in a concise syntax.\n`within` function accepts the same syntax used in [td_interval function in presto](https://docs.treasuredata.com/display/public/pd/supported+presto+and+td+functions#supportedprestoandtdfunctions-td_interval).\n\nfor example, to read the last 1 hour range of data, use `within(\"-1h\")`:\n\n```python\ntd.table(\"tbl\").within(\"-1h\").df()\n```\n\nyou can also read the last day's data:\n\n```python\ntd.table(\"tbl\").within(\"-1d\").df()\n```\n\nyou can also specify an _offset_ of the relative time range. this example reads the last days's data beginning from 7 days ago:\n\n```python\ntd.table(\"tbl\").within(\"-1d/-7d\").df()\n```\n\nif you know an exact time range, `within(\"(start time)/(end time)\")` is useful:\n\n```python\n>>> df = td.table(\"sample_datasets.www_access\").within(\"2014-10-04/2014-10-05\").df()\n>>> df.show()\n2021-05-10 09:10:02.366z  info [partitionscanner] fetching the partition list of sample_datasets.www_access within time range:[2014-10-04 00:00:00z,2014-10-05 00:00:00z) - (partitionscanner.scala:29)\n...\n```\n\nsee [this doc](https://docs.treasuredata.com/display/public/pd/supported+presto+and+td+functions#supportedprestoandtdfunctions-td_interval) for more examples of interval strings.\n\n\n#### submitting presto queries\n\nif your spark cluster is small, reading all of the data as in-memory dataframe might be difficult.\nin this case, you can utilize presto, a distributed sql query engine, to reduce the amount of data processing with pyspark:\n\n```python\n>>> q = td.presto(\"select code, * from sample_datasets.www_access\")\n>>> q.show()\n2019-06-13 20:09:13.245z  info [tdprestojdbcrdd]  - (tdprestorelation.scala:106)\nsubmit presto query:\nselect code, count(*) cnt from sample_datasets.www_access group by 1\n+----+----+\n|code| cnt|\n+----+----+\n| 200|4981|\n| 500|   2|\n| 404|  17|\n+----+----+\n```\n\nthe query result is represented as a dataframe.\n\nto run non query statements (e.g., insert into, create table, etc.) use `execute_presto(sql)`:\n\n```python\ntd.execute_presto(\"create table if not exists a(time bigint, id varchar)\")\n```\n\n#### using sparksql\n\nto use tables in treaure data inside spark sql, create a view with `df.createorreplacetempview(...)`:\n\n```python\n# read td table as a dataframe\ndf = td.table(\"mydb.test1\").df()\n# register the dataframe as a view\ndf.createorreplacetempview(\"test1\")\n\nspark.sql(\"select * from test1\").show()\n```\n\n### create or drop databases and tables\n\ncreate a new table or database:\n\n```python\ntd.create_database_if_not_exists(\"mydb\")\ntd.create_table_if_not_exists(\"mydb.test1\")\n```\n\ndelete unnecessary tables:\n\n```python\ntd.drop_table_if_exists(\"mydb.test1\")\ntd.drop_database_if_exists(\"mydb\")\n```\n\nyou can also check the presence of a table:\n\n```python\ntd.table(\"mydb.test1\").exists() # true if the table exists\n```\n\n### create user-defined partition tables\n\nuser-defined partitioning ([udp](https://docs.treasuredata.com/display/public/pd/defining+partitioning+for+presto)) is useful if\nyou know a column in the table that has unique identifiers (e.g., ids, category values).\n\nyou can create a udp table partitioned by id (string type column) as follows:\n\n```python\ntd.create_udp_s(\"mydb.user_list\", \"id\")\n```\n\nto create a udp table, partitioned by long (bigint) type column, use `td.create_udp_l`:\n\n```python\ntd.create_udp_l(\"mydb.departments\", \"dept_id\")\n```\n\n### swapping table contents\n\nyou can replace the contents of two tables. the input tables must be in the same database:\n\n```python\n# swap the contents of two tables\ntd.swap_tables(\"mydb.tbl1\", \"mydb.tbl2\")\n\n# another way to swap tables\ntd.table(\"mydb.tbl1\").swap_table_with(\"tbl2\")\n```\n\n### uploading dataframes to treasure data\n\nto save your local dataframes as a table, `td.insert_into(df, table)` and `td.create_or_replace(df, table)` can be used:\n\n```python\n# insert the records in the input dataframe to the target table:\ntd.insert_into(df, \"mydb.tbl1\")\n\n# create or replace the target table with the content of the input dataframe:\ntd.create_or_replace(df, \"mydb.tbl2\")\n```\n\n## using multiple td accounts\n\nto specify a new api key aside from the key that is configured in td-spark.conf, just use `td.with_apikey(apikey)`:\n\n```python\n# returns a new tdsparkcontext with the specified key\ntd2 = td.with_apikey(\"key2\")\n```\n\nfor reading tables or uploading dataframes with the new key, use `td2`:\n\n```python\n# read a table with key2\ndf = td2.table(\"sample_datasets.www_access\").df()\n...\n# insert the records with key2\ntd2.insert_into(df, \"mydb.tbl1\")\n```\n\n### running pyspark jobs with spark-submit\n\nto submit your pyspark script to a spark cluster, you will need the following files:\n\n- __td-spark.conf__ file that describes your td api key and `spark.td.site` (see above).\n- __td_pyspark.py__\n  - check the file location using `pip show -f td-pyspark`, and copy td_pyspark.py to your favorite location\n- __td-spark-assembly-latest_xxxx.jar__\n  - get the latest version from [download](https://treasure-data.github.io/td-spark/release_notes.html#download) page.\n- pre-build spark\n  - [download spark 3.1.x](https://spark.apache.org/downloads.html) with hadoop 3.2 (built for scala 2.12)\n  - extract the downloaded archive. this folder location will be your `$spark_home`.\n\nhere is an example pyspark application code:\n__my_app.py__\n\n```python\nimport td_pyspark\nfrom pyspark.sql import sparksession\n\n# create a new sparksession\nspark = sparksession\\\n    .builder\\\n    .appname(\"myapp\")\\\n    .getorcreate()\n\n# create tdsparkcontext\ntd = td_pyspark.tdsparkcontext(spark)\n\n# read the table data within -1d (yesterday) range as dataframe\ndf = td.table(\"sample_datasets.www_access\").within(\"-1d\").df()\ndf.show()\n```\n\nto run `my_app.py` use spark-submit by specifying the necessary files mentioned above:\n\n```bash\n# launching pyspark with the local mode\n$ ${spark_home}/bin/spark-submit --master \"local[4]\"\\\n  --driver-class-path td-spark-assembly.jar\\\n  --properties-file=td-spark.conf\\\n  --py-files td_pyspark.py\\\n  my_app.py\n```\n\n`local[4]` means running a spark cluster locally using 4 threads.\n\nto use a remote spark cluster, specify `master` address, e.g., `--master=spark://(master node ip address):7077`.\n\n### using td-spark assembly included in the pypi package.\n\nthe package contains pre-built binary of td-spark so that you can add it into the classpath as default.\n`tdsparkcontextbuilder.default_jar_path()` returns the path to the default td-spark-assembly.jar file.\npassing the path to `jars` method of tdsparkcontextbuilder will automatically build the sparksession including the default jar.\n\n```python\nimport td_pyspark\nfrom pyspark.sql import sparksession\n\nbuilder = sparksession\\\n    .builder\\\n    .appname(\"td-pyspark-app\")\n\ntd = td_pyspark.tdsparkcontextbuilder(builder)\\\n    .apikey(\"xxxxxxxxxxxxxx\")\\\n    .jars(tdsparkcontextbuilder.default_jar_path())\\\n    .build()\n```\n",
  "docs_url": null,
  "keywords": "spark pyspark treasuredata",
  "license": "apache 2",
  "name": "td-pyspark",
  "package_url": "https://pypi.org/project/td-pyspark/",
  "project_url": "https://pypi.org/project/td-pyspark/",
  "project_urls": {
    "Homepage": "https://docs.treasuredata.com/display/public/INT/Data+Science+and+SQL+Tools"
  },
  "release_url": "https://pypi.org/project/td-pyspark/23.5.0/",
  "requires_dist": [
    "sphinx (>=2.2.0) ; extra == 'docs'",
    "sphinx-rtd-theme (>=0.4.3) ; extra == 'docs'",
    "recommonmark ; extra == 'docs'",
    "pyspark (~=3.4.0) ; extra == 'spark'"
  ],
  "requires_python": "",
  "summary": "treasure data extension for pyspark",
  "version": "23.5.0",
  "releases": [],
  "developers": [
    "dev+pypi@treasure-data.com",
    "treasure_data"
  ],
  "kwds": "td_pyspark __td_pyspark pyspark td_spark_conf treasuredata",
  "license_kwds": "apache 2",
  "libtype": "pypi",
  "id": "pypi_td_pyspark",
  "homepage": "https://docs.treasuredata.com/display/public/int/data+science+and+sql+tools",
  "release_count": 20,
  "dependency_ids": [
    "pypi_pyspark",
    "pypi_recommonmark",
    "pypi_sphinx",
    "pypi_sphinx_rtd_theme"
  ]
}