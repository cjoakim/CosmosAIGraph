{
  "classifiers": [
    "development status :: 2 - pre-alpha",
    "programming language :: python :: 3"
  ],
  "description": "# linearoperator\n\n[![test](https://github.com/cornellius-gp/linear_operator/actions/workflows/run_test_suite.yml/badge.svg)](https://github.com/cornellius-gp/linear_operator/actions/workflows/run_test_suite.yml)\n[![documentation](https://readthedocs.org/projects/linear-operator/badge/?version=latest)](https://linear-operator.readthedocs.io/en/latest/?badge=latest)\n[![license](https://img.shields.io/badge/license-mit-green.svg)](license)\n\n[![python version](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![conda](https://img.shields.io/conda/v/gpytorch/linear_operator.svg)](https://anaconda.org/gpytorch/linear_operator)\n[![pypi](https://img.shields.io/pypi/v/linear_operator.svg)](https://pypi.org/project/linear_operator)\n\n\n<!-- docs_intro_start -->\n\nlinearoperator is a pytorch package for abstracting away the linear algebra routines needed for structured matrices (or operators).\n\n**this package is in beta.**\ncurrently, most of the functionality only supports positive semi-definite and triangular matrices.\npackage development todos:\n - [x] support psd operators\n - [x] support triangular operators\n - [ ] interface to specify structure (i.e. symmetric, triangular, psd, etc.)\n - [ ] add algebraic routines for symmetric operators\n - [ ] add algebraic routines for generic square operators\n - [ ] add algebraic routines for generic rectangular operators\n - [ ] add sparse operators\n\n<!-- docs_intro_end -->\n\nto get started, run either\n```sh\npip install linear_operator\n# or\nconda install linear_operator -c gpytorch\n```\nor [see below](#installation) for more detailed instructions.\n\n\n<!-- docs_index_start -->\n\n\n## why linearoperator\nbefore describing what linear operators are and why they make a useful abstraction, it's easiest to see an example.\nlet's say you wanted to compute a matrix solve:\n\n$$\\boldsymbol a^{-1} \\boldsymbol b.$$\n\nif you didn't know anything about the matrix $\\boldsymbol a$, the simplest (and best) way to accomplish this in code is:\n\n```python\n# a = torch.randn(1000, 1000)\n# b = torch.randn(1000)\ntorch.linalg.solve(a, b)  # computes a^{-1} b\n```\n\nwhile this is easy, the `solve` routine is $\\mathcal o(n^3)$, which gets very slow as $n$ grows large.\n\nhowever, let's imagine that we knew that $\\boldsymbol a$ was equal to a low rank matrix plus a diagonal\n(i.e. $\\boldsymbol a = \\boldsymbol c \\boldsymbol c^\\top + \\boldsymbol d$\nfor some skinny matrix $\\boldsymbol c$ and some diagonal matrix $\\boldsymbol d$.)\nthere's now a very efficient $\\boldsymbol o(n)$ routine to compute $\\boldsymbol a^{-1}$ (the [woodbury formula](https://en.wikipedia.org/wiki/woodbury_matrix_identity)).\n**in general**, if we know that $\\boldsymbol a$ has structure,\nwe want to use efficient linear algebra routines - rather than the general routines -\nthat exploit this structure.\n\n\n### without linearoperator\n\nimplementing the efficient solve that exploits $\\boldsymbol a$'s low-rank-plus-diagonal structure would look something like this:\n\n```python\ndef low_rank_plus_diagonal_solve(c, d, b):\n    # a = c c^t + diag(d)\n    # a^{-1} b = d^{-1} b - d^{-1} c (i + c^t d^{-1} c)^{-1} c^t d^{-1} b\n    #   where d = diag(d)\n\n    d_inv_b = b / d\n    d_inv_c = c / d.unsqueeze(-1)\n    eye = torch.eye(c.size(-2))\n    return (\n        d_inv_b - d_inv_c @ torch.cholesky_solve(\n            c.mt @ d_inv_b,\n            torch.linalg.cholesky(eye + c.mt @ d_inv_c, upper=false),\n            upper=false\n        )\n    )\n\n\n# c = torch.randn(1000, 20)\n# d = torch.randn(1000)\n# b = torch.randn(1000)\nlow_rank_plus_diagonal_solve(c, d, b)  # computes a^{-1} b in o(n) time, instead of o(n^3)\n```\n\nwhile this is efficient code, it's not ideal for a number of reasons:\n1. it's a lot more complicated than `torch.linalg.solve(a, b)`.\n2. there's no object that represents $\\boldsymbol a$.\n   to perform any math with $\\boldsymbol a$, we have to pass around the matrix `c` and the vector `d`.\n\n\n### with linearoperator\n\nthe linearoperator package offers the best of both worlds:\n\n```python\nfrom linear_operator.operators import diaglinearoperator, lowrankrootlinearoperator\n# c = torch.randn(1000, 20)\n# d = torch.randn(1000)\n# b = torch.randn(1000)\na = lowrankrootlinearoperator(c) + diaglinearoperator(d)  # represents c c^t + diag(d)\n```\n\nit provides an interface that lets us treat $\\boldsymbol a$ as if it were a generic tensor,\nusing the standard pytorch api:\n\n```python\ntorch.linalg.solve(a, b)  # computes a^{-1} b efficiently!\n```\n\nunder-the-hood, the `linearoperator` object keeps track of the algebraic structure of $\\boldsymbol a$ (low rank plus diagonal)\nand determines the most efficient routine to use (the woodbury formula).\nthis way, we can get a efficient $\\mathcal o(n)$ solve while abstracting away all of the details.\n\ncrucially, $\\boldsymbol a$ is never explicitly instantiated as a matrix, which makes it possible to scale\nto very large operators without running out of memory:\n\n```python\n# c = torch.randn(10000000, 20)\n# d = torch.randn(10000000)\n# b = torch.randn(10000000)\na = lowrankrootlinearoperator(c) + diaglinearoperator(d)  # represents a 10m x 10m matrix!\ntorch.linalg.solve(a, b)  # computes a^{-1} b efficiently!\n```\n\n\n<!-- docs_index_end -->\n<!-- docs_about_start -->\n\n\n## what is a linear operator?\na linear operator is a generalization of a matrix.\nit is a linear function that is defined in by its application to a vector.\nthe most common linear operators are (potentially structured) matrices,\nwhere the function applying them to a vector are (potentially efficient)\nmatrix-vector multiplication routines.\n\nin code, a `linearoperator` is a class that\n1. specifies the tensor(s) needed to define the linearoperator,\n1. specifies a `_matmul` function (how the linearoperator is applied to a vector),\n1. specifies a `_size` function (how big is the linearoperator if it is represented as a matrix, or batch of matrices), and\n1. specifies a `_transpose_nonbatch` function (the adjoint of the linearoperator).\n1. (optionally) defines other functions (e.g. `logdet`, `eigh`, etc.) to accelerate computations for which efficient sturcture-exploiting routines exist.\n\nfor example:\n\n```python\nclass diaglinearoperator(linear_operator.linearoperator):\n    r\"\"\"\n    a linearoperator representing a diagonal matrix.\n    \"\"\"\n    def __init__(self, diag):\n        # diag: the vector that defines the diagonal of the matrix\n        self.diag = diag\n\n    def _matmul(self, v):\n        return self.diag.unsqueeze(-1) * v\n\n    def _size(self):\n        return torch.size([*self.diag.shape, self.diag.size(-1)])\n\n    def _transpose_nonbatch(self):\n        return self  # diagonal matrices are symmetric\n\n    # this function is optional, but it will accelerate computation\n    def logdet(self):\n        return self.diag.log().sum(dim=-1)\n# ...\n\nd = diaglinearoperator(torch.tensor([1., 2., 3.])\n# represents the matrix\n#   [[1., 0., 0.],\n#    [0., 2., 0.],\n#    [0., 0., 3.]]\ntorch.matmul(d, torch.tensor([4., 5., 6.])\n# returns [4., 10., 18.]\n```\n\nwhile `_matmul`, `_size`, and `_transpose_nonbatch` might seem like a limited set of functions,\nit turns out that most functions on the `torch` and `torch.linalg` namespaces can be efficiently implemented\nusing only these three primitative functions.\n\nmoreover, because `_matmul` is a linear function, it is very easy to compose linear operators in various ways.\nfor example: adding two linear operators (`sumlinearoperator`) just requires adding the output of their `_matmul` functions.\nthis makes it possible to define very complex compositional structures that still yield efficient linear algebraic routines.\n\nfinally, `linearoperator` objects can be composed with one another, yielding new `linearoperator` objects and automatically keeping track of algebraic structure after each computation.\nas a result, users never need to reason about what efficient linear algebra routines to use  (so long as the input elements defined by the user encode known input structure).\n<!-- docs_about_end -->\nsee the [using linearoperator objects](#using-linearoperator-objects) section for more details.\n\n\n<!-- docs_usecases_start -->\n\n\n## use cases\n\nthere are several use cases for the linearoperator package.\nhere we highlight two general themes:\n\n### modular code for structured matrices\n\nfor example, let's say that you have a generative model that involves\nsampling from a high-dimensional multivariate gaussian.\nthis sampling operation will require storing and manipulating a large covariance matrix,\nso to speed things up you might want to experiment with different structured\napproximations of that covariance matrix.\nthis is easy with the linearoperator package.\n\n```python\nfrom gpytorch.distributions import multivariatenormal\n\n# variance = torch.randn(10000)\ncov = diaglinearoperator(variance)\n# or\n# cov = lowrankrootlinearoperator(...) + diaglinearoperator(...)\n# or\n# cov = kroneckerproductlinearoperator(...)\n# or\n# cov = toeplitzlinearoperator(...)\n# or\n# ...\n\nmvn = multivariatenormal(torch.zeros(cov.size(-1), cov) # 10000-dimensional mvn\nmvn.rsample()  # returns a 10000-dimensional vector\n```\n\n### efficient routines for complex operators\n\nmany of the efficient linear algebra routines in linearoperator are iterative algorithms\nbased on matrix-vector multiplication.\nsince matrix-vector multiplication obeys many nice compositional properties\nit is possible to obtain efficient routines for extremely complex compositional lienaroperators:\n\n```python\nfrom linear_operator.operators import kroneckerproductlinearoperator, rootlinearoperator, toeplitzlinearoperator\n\n# mat1 = 200 x 200 psd matrix\n# mat2 = 100 x 100 psd matrix\n# vec3 = 20000 vector\n\na = kroneckerproductlinearoperator(mat1, mat2) + rootlinearoperator(toeplitzlinearoperator(vec3))\n# represents a 20000 x 20000 matrix\n\ntorch.linalg.solve(a, torch.randn(20000))  # sub o(n^3) routine!\n```\n\n\n<!-- docs_usecases_end -->\n<!-- docs_using_start -->\n\n\n## using linearoperator objects\n\nlinearoperator objects share (mostly) the same api as `torch.tensor` objects.\nunder the hood, these objects use `__torch_function__` to dispatch all efficient linear algebra operations\nto the `torch` and `torch.linalg` namespaces.\nthis includes\n- `torch.add`\n- `torch.cat`\n- `torch.clone`\n- `torch.diagonal`\n- `torch.dim`\n- `torch.div`\n- `torch.expand`\n- `torch.logdet`\n- `torch.matmul`\n- `torch.numel`\n- `torch.permute`\n- `torch.prod`\n- `torch.squeeze`\n- `torch.sub`\n- `torch.sum`\n- `torch.transpose`\n- `torch.unsqueeze`\n- `torch.linalg.cholesky`\n- `torch.linalg.eigh`\n- `torch.linalg.eigvalsh`\n- `torch.linalg.solve`\n- `torch.linalg.svd`\n\neach of these functions will either return a `torch.tensor`, or a new `linearoperator` object,\ndepending on the function.\nfor example:\n\n```python\n# a = rootlinearoperator(...)\n# b = toeplitzlinearoperator(...)\n# d = vec\n\nc = torch.matmul(a, b)  # a new lienearoperator representing the product of a and b\ntorch.linalg.solve(c, d)  # a torch.tensor\n```\n\nfor more examples, see the [examples folder](https://github.com/cornellius-gp/linear_operator/blob/main/examples/).\n\n### batch support and broadcasting\n\n`linearoperator` objects operate naturally in batch mode.\nfor example, to represent a batch of 3 `100 x 100` diagonal matrices:\n\n```python\n# d = torch.randn(3, 100)\nd = diaglinearoperator(d)  # reprents an operator of size 3 x 100 x 100\n```\n\nthese objects fully support broadcasted operations:\n\n```python\nd @ torch.randn(100, 2)  # returns a tensor of size 3 x 100 x 2\n\nd2 = diaglinearoperator(torch.randn([2, 1, 100]))  # represents an operator of size 2 x 1 x 100 x 100\nd2 + d  # represents an operator of size 2 x 3 x 100 x 100\n```\n\n### indexing\n\n`linearoperator` objects can be indexed in ways similar to torch tensors. this includes:\n- integer indexing (get a row, column, or batch)\n- slice indexing (get a subset of rows, columns, or batches)\n- longtensor indexing (get a set of individual entries by index)\n- ellipses (support indexing operations with arbitrary batch dimensions)\n\n```python\nd = diaglinearoperator(torch.randn(2, 3, 100))  # represents an operator of size 2 x 3 x 100 x 100\nd[-1]  # returns a 3 x 100 x 100 operator\nd[..., :10, -5:]  # returns a 2 x 3 x 10 x 5 operator\nd[..., torch.longtensor([0, 1, 2, 3]), torch.longtensor([0, 1, 2, 3])]  # returns a 2 x 3 x 4 tensor\n```\n\n### composition and decoration\nlinearoperators can be composed with one another in various ways.\nthis includes\n- addition (`linearopa + linearopb`)\n- matrix multiplication (`linearopa @ linearopb`)\n- concatenation (`torch.cat([linearopa, linearopb], dim=-2)`)\n- kronecker product (`torch.kron(linearopa, linearopb)`)\n\nin addition, there are many ways to \"decorate\" linearoperator objects.\nthis includes:\n- elementwise multiplying by constants (`torch.mul(2., linearopa)`)\n- summing over batches (`torch.sum(linearopa, dim=-3)`)\n- elementwise multiplying over batches (`torch.prod(linearopa, dim=-3)`)\n\nsee the documentation for a [full list of supported composition and decoration operations](https://linear-operator.readthedocs.io/en/latest/composition_decoration_operators.html).\n\n\n<!-- docs_using_end -->\n<!-- docs_install_start -->\n\n\n## installation\n\nlinearoperator requires python >= 3.8.\n\n### standard installation (most recent stable version)\nwe recommend installing via `pip` or anaconda:\n\n```sh\npip install linear_operator\n# or\nconda install linear_operator -c gpytorch\n```\n\nthe installation requires the following packages:\n- pytorch >= 1.11\n- scipy\n\nyou can customize your pytorch installation (i.e. cuda version, cpu only option)\nby following the [pytorch installation instructions](https://pytorch.org/get-started/locally/).\n\n### installing from the `main` branch (latest unsable version)\nto install what is currently on the `main` branch (potentially buggy and unstable):\n\n```sh\npip install --upgrade git+https://github.com/cornellius-gp/linear_operator.git\n```\n\n\n<!-- docs_install_end -->\n\n\n### development installation\nif you are contributing a pull request, it is best to perform a manual installation:\n\n```sh\ngit clone https://github.com/cornellius-gp/linear_operator.git\ncd linear_operator\npip install -e \".[dev,docs,test]\"\n```\n\n\n## contributing\n\nsee the contributing guidelines [contributing.md](https://github.com/cornellius-gp/linear_operator/blob/main/contributing.md)\nfor information on submitting issues and pull requests.\n\n\n## license\n\nlinearoperator is [mit licensed](https://github.com/cornellius-gp/linear_operator/blob/main/license).\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "linear-operator",
  "package_url": "https://pypi.org/project/linear-operator/",
  "project_url": "https://pypi.org/project/linear-operator/",
  "project_urls": {
    "Documentation": "https://linear_operator.readthedocs.io",
    "Source": "https://github.com/cornellius-gp/linear_operator/"
  },
  "release_url": "https://pypi.org/project/linear-operator/0.5.2/",
  "requires_dist": [
    "torch >=1.11",
    "scipy",
    "jaxtyping >=0.2.9",
    "typeguard ~=2.13.3",
    "pre-commit ; extra == 'dev'",
    "setuptools-scm ; extra == 'dev'",
    "ufmt ; extra == 'dev'",
    "twine ; extra == 'dev'",
    "myst-parser ; extra == 'docs'",
    "setuptools-scm ; extra == 'docs'",
    "sphinx ; extra == 'docs'",
    "six ; extra == 'docs'",
    "sphinx-rtd-theme ; extra == 'docs'",
    "sphinx-autodoc-typehints ; extra == 'docs'",
    "uncompyle6 ; extra == 'docs'",
    "flake8 ==5.0.4 ; extra == 'test'",
    "flake8-print ==5.0.0 ; extra == 'test'",
    "pytest ; extra == 'test'"
  ],
  "requires_python": ">=3.8",
  "summary": "a linear operator implementation, primarily designed for finite-dimensional positive definite operators (i.e. kernel matrices).",
  "version": "0.5.2",
  "releases": [],
  "developers": [
    "geoff_pleiss",
    "gpleiss@gmail.com"
  ],
  "kwds": "linearoperator linearoperators linear_operator operators composition_decoration_operators",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_linear_operator",
  "homepage": "",
  "release_count": 8,
  "dependency_ids": [
    "pypi_flake8",
    "pypi_flake8_print",
    "pypi_jaxtyping",
    "pypi_myst_parser",
    "pypi_pre_commit",
    "pypi_pytest",
    "pypi_scipy",
    "pypi_setuptools_scm",
    "pypi_six",
    "pypi_sphinx",
    "pypi_sphinx_autodoc_typehints",
    "pypi_sphinx_rtd_theme",
    "pypi_torch",
    "pypi_twine",
    "pypi_typeguard",
    "pypi_ufmt",
    "pypi_uncompyle6"
  ]
}