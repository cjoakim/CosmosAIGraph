{
  "classifiers": [
    "development status :: 4 - beta",
    "license :: osi approved :: mit license",
    "programming language :: python",
    "programming language :: python :: 3",
    "topic :: scientific/engineering"
  ],
  "description": "# tdigest\n### efficient percentile estimation of streaming or distributed data\n[![pypi version](https://badge.fury.io/py/tdigest.svg)](https://badge.fury.io/py/tdigest)\n[![build status](https://travis-ci.org/camdavidsonpilon/tdigest.svg?branch=master)](https://travis-ci.org/camdavidsonpilon/tdigest)\n\n\nthis is a python implementation of ted dunning's [t-digest](https://github.com/tdunning/t-digest) data structure. the t-digest data structure is designed around computing accurate estimates from either streaming data, or distributed data. these estimates are percentiles, quantiles, trimmed means, etc. two t-digests can be added, making the data structure ideal for map-reduce settings, and can be serialized into much less than 10kb (instead of storing the entire list of data).\n\nsee a blog post about it here: [percentile and quantile estimation of big data: the t-digest](http://dataorigami.net/blogs/napkin-folding/19055451-percentile-and-quantile-estimation-of-big-data-the-t-digest)\n\n\n### installation\n*tdigest* is compatible with both python 2 and python 3. \n\n```\npip install tdigest\n```\n\n### usage\n\n#### update the digest sequentially\n\n```\nfrom tdigest import tdigest\nfrom numpy.random import random\n\ndigest = tdigest()\nfor x in range(5000):\n    digest.update(random())\n\nprint(digest.percentile(15))  # about 0.15, as 0.15 is the 15th percentile of the uniform(0,1) distribution\n```\n\n#### update the digest in batches\n\n```\nanother_digest = tdigest()\nanother_digest.batch_update(random(5000))\nprint(another_digest.percentile(15))\n```\n\n#### sum two digests to create a new digest\n\n```\nsum_digest = digest + another_digest \nsum_digest.percentile(30)  # about 0.3\n```\n\n#### to dict or serializing a digest with json\n\nyou can use the to_dict() method to turn a tdigest object into a standard python dictionary.\n```\ndigest = tdigest()\ndigest.update(1)\ndigest.update(2)\ndigest.update(3)\nprint(digest.to_dict())\n```\nor you can get only a list of centroids with `centroids_to_list()`.\n```\ndigest.centroids_to_list()\n```\n\nsimilarly, you can restore a python dict of digest values with `update_from_dict()`. centroids are merged with any existing ones in the digest.\nfor example, make a fresh digest and restore values from a python dictionary.\n```\ndigest = tdigest()\ndigest.update_from_dict({'k': 25, 'delta': 0.01, 'centroids': [{'c': 1.0, 'm': 1.0}, {'c': 1.0, 'm': 2.0}, {'c': 1.0, 'm': 3.0}]})\n```\n\nk and delta values are optional, or you can provide only a list of centroids with `update_centroids_from_list()`.\n```\ndigest = tdigest()\ndigest.update_centroids([{'c': 1.0, 'm': 1.0}, {'c': 1.0, 'm': 2.0}, {'c': 1.0, 'm': 3.0}])\n```\n\nif you want to serialize with other tools like json, you can first convert to_dict().\n```\njson.dumps(digest.to_dict())\n```\n\nalternatively, make a custom encoder function to provide as default to the standard json module.\n```\ndef encoder(digest_obj):\n    return digest_obj.to_dict()\n```\nthen pass the encoder function as the default parameter.\n```\njson.dumps(digest, default=encoder)\n```\n\n\n### api \n\n`tdigest.`\n\n - `update(x, w=1)`: update the tdigest with value `x` and weight `w`.\n - `batch_update(x, w=1)`: update the tdigest with values in array `x` and weight `w`.\n - `compress()`: perform a compression on the underlying data structure that will shrink the memory footprint of it, without hurting accuracy. good to perform after adding many values. \n - `percentile(p)`: return the `p`th percentile. example: `p=50` is the median.\n - `cdf(x)`: return the cdf the value `x` is at. \n - `trimmed_mean(p1, p2)`: return the mean of data set without the values below and above the `p1` and `p2` percentile respectively. \n - `to_dict()`: return a python dictionary of the tdigest and internal centroid values.\n - `update_from_dict(dict_values)`: update from serialized dictionary values into the tdigest object.\n - `centroids_to_list()`: return a python list of the tdigest object's internal centroid values.\n - `update_centroids_from_list(list_values)`: update centroids from a python list.\n\n\n\n\n\n\n\n",
  "docs_url": null,
  "keywords": "percentile,median,probabilistic data structure,quantile,distributed,qdigest,tdigest,streaming,pyspark",
  "license": "mit",
  "name": "tdigest",
  "package_url": "https://pypi.org/project/tdigest/",
  "project_url": "https://pypi.org/project/tdigest/",
  "project_urls": {
    "Homepage": "https://github.com/CamDavidsonPilon/tdigest"
  },
  "release_url": "https://pypi.org/project/tdigest/0.5.2.2/",
  "requires_dist": [
    "accumulation-tree",
    "pyudorandom",
    "pytest ; extra == 'tests'",
    "pytest-timeout ; extra == 'tests'",
    "pytest-cov ; extra == 'tests'",
    "numpy ; extra == 'tests'"
  ],
  "requires_python": "",
  "summary": "t-digest data structure",
  "version": "0.5.2.2",
  "releases": [],
  "developers": [
    "cam.davidson.pilon@gmail.com",
    "cam_davidson"
  ],
  "kwds": "percentiles percentile quantile quantiles pyspark",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_tdigest",
  "homepage": "https://github.com/camdavidsonpilon/tdigest",
  "release_count": 14,
  "dependency_ids": [
    "pypi_accumulation_tree",
    "pypi_numpy",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_pytest_timeout",
    "pypi_pyudorandom"
  ]
}