{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "operating system :: macos :: macos x",
    "operating system :: microsoft :: windows",
    "operating system :: posix",
    "programming language :: python :: 3 :: only",
    "topic :: scientific/engineering",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: text processing"
  ],
  "description": "# sacrebleu\n\n[![pypi version](https://img.shields.io/pypi/v/sacrebleu)](https://img.shields.io/pypi/v/sacrebleu)\n[![python version](https://img.shields.io/pypi/pyversions/sacrebleu)](https://img.shields.io/pypi/pyversions/sacrebleu)\n[![github issues](https://img.shields.io/github/issues/mjpost/sacrebleu.svg)](https://github.com/mjpost/sacrebleu/issues)\n\nsacrebleu ([post, 2018](http://aclweb.org/anthology/w18-6319)) provides hassle-free computation of shareable, comparable, and reproducible **bleu** scores.\ninspired by rico sennrich's `multi-bleu-detok.perl`, it produces the official wmt scores but works with plain text.\nit also knows all the standard test sets and handles downloading, processing, and tokenization for you.\n\nthe official version is hosted at <https://github.com/mjpost/sacrebleu>.\n\n# motivation\n\ncomparing bleu scores is harder than it should be. every decoder has its own implementation, often borrowed from moses, but maybe with subtle changes.\nmoses itself has a number of implementations as standalone scripts, with little indication of how they differ (note: they mostly don't, but `multi-bleu.pl` expects tokenized input). different flags passed to each of these scripts can produce wide swings in the final score. all of these may handle tokenization in different ways. on top of this, downloading and managing test sets is a moderate annoyance.\n\nsacre bleu! what a mess.\n\n**sacrebleu** aims to solve these problems by wrapping the original reference implementation ([papineni et al., 2002](https://www.aclweb.org/anthology/p02-1040.pdf)) together with other useful features.\nthe defaults are set the way that bleu should be computed, and furthermore, the script outputs a short version string that allows others to know exactly what you did.\nas an added bonus, it automatically downloads and manages test sets for you, so that you can simply tell it to score against `wmt14`, without having to hunt down a path on your local file system.\nit is all designed to take bleu a little more seriously.\nafter all, even with all its problems, bleu is the default and---admit it---well-loved metric of our entire research community.\nsacre bleu.\n\n# features\n\n- it automatically downloads common wmt test sets and processes them to plain text\n- it produces a short version string that facilitates cross-paper comparisons\n- it properly computes scores on detokenized outputs, using wmt ([conference on machine translation](http://statmt.org/wmt17)) standard tokenization\n- it produces the same values as the official script (`mteval-v13a.pl`) used by wmt\n- it outputs the bleu score without the comma, so you don't have to remove it with `sed` (looking at you, `multi-bleu.perl`)\n- it supports different tokenizers for bleu including support for japanese and chinese\n- it supports **chrf, chrf++** and **translation error rate (ter)** metrics\n- it performs paired bootstrap resampling and paired approximate randomization tests for statistical significance reporting\n\n# breaking changes\n\n## v2.0.0\n\nas of v2.0.0, the default output format is changed to `json` for less painful parsing experience. this means that software that parse the output of sacrebleu should be modified to either (i) parse the json using for example the `jq` utility or (ii) pass `-f text` to sacrebleu to preserve the old textual output. the latter change can also be made **persistently** by exporting `sacrebleu_format=text` in relevant shell configuration files.\n\nhere's an example of parsing the `score` key of the json output using `jq`:\n\n```\n$ sacrebleu -i output.detok.txt -t wmt17 -l en-de | jq -r .score\n20.8\n```\n\n# installation\n\ninstall the official python module from pypi (**python>=3.6 only**):\n\n    pip install sacrebleu\n\nin order to install japanese tokenizer support through `mecab-python3`, you need to run the\nfollowing command instead, to perform a full installation with dependencies:\n\n    pip install \"sacrebleu[ja]\"\n\nin order to install korean tokenizer support through `pymecab-ko`, you need to run the\nfollowing command instead, to perform a full installation with dependencies:\n\n    pip install \"sacrebleu[ko]\"\n\n# command-line usage\n\nyou can get a list of available test sets with `sacrebleu --list`. please see [datasets.md](datasets.md)\nfor an up-to-date list of supported datasets. you can also list available test sets for a given language pair\nwith `sacrebleu --list -l en-fr`.\n\n## basics\n\n### downloading test sets\n\ndownloading is triggered when you request a test set. if the dataset is not available, it is downloaded\nand unpacked.\n\ne.g., you can use the following commands to download the source, pass it through your translation system\nin `translate.sh`, and then score it:\n\n```\n$ sacrebleu -t wmt17 -l en-de --echo src > wmt17.en-de.en\n$ cat wmt17.en-de.en | translate.sh | sacrebleu -t wmt17 -l en-de\n```\n\nsome test sets also have the outputs of systems that were submitted to the task.\nfor example, the `wmt/systems` test set.\n\n```bash\n$ sacrebleu -t wmt21/systems -l zh-en --echo niutrans\n```\n\nthis provides a convenient way to score:\n\n```bash\n$ sacrebleu -t wmt21/system -l zh-en --echo niutrans | sacrebleu -t wmt21/systems -l zh-en\n``\n\nyou can see a list of the available outputs by passing an invalid value to `--echo`.\n\n### json output\n\nas of version `>=2.0.0`, sacrebleu prints the computed scores in json format to make parsing less painful:\n\n```\n$ sacrebleu -i output.detok.txt -t wmt17 -l en-de\n```\n\n```json\n{\n \"name\": \"bleu\",\n \"score\": 20.8,\n \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0\",\n \"verbose_score\": \"54.4/26.6/14.9/8.7 (bp = 1.000 ratio = 1.026 hyp_len = 62880 ref_len = 61287)\",\n \"nrefs\": \"1\",\n \"case\": \"mixed\",\n \"eff\": \"no\",\n \"tok\": \"13a\",\n \"smooth\": \"exp\",\n \"version\": \"2.0.0\"\n}\n```\n\nif you want to keep the old behavior, you can pass `-f text` or export `sacrebleu_format=text`:\n\n```\n$ sacrebleu -i output.detok.txt -t wmt17 -l en-de -f text\nbleu|nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0 = 20.8 54.4/26.6/14.9/8.7 (bp = 1.000 ratio = 1.026 hyp_len = 62880 ref_len = 61287)\n```\n\n### scoring\n\n(all examples below assume old-style text output for a compact representation that save space)\n\nlet's say that you just translated the `en-de` test set of wmt17 with your fancy mt system and the **detokenized** translations are in a file called `output.detok.txt`:\n\n```\n# option 1: redirect system output to stdin\n$ cat output.detok.txt | sacrebleu -t wmt17 -l en-de\nbleu|nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0 = 20.8 54.4/26.6/14.9/8.7 (bp = 1.000 ratio = 1.026 hyp_len = 62880 ref_len = 61287)\n\n# option 2: use the --input/-i argument\n$ sacrebleu -t wmt17 -l en-de -i output.detok.txt\nbleu|nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0 = 20.8 54.4/26.6/14.9/8.7 (bp = 1.000 ratio = 1.026 hyp_len = 62880 ref_len = 61287)\n```\n\nyou can obtain a short version of the signature with `--short/-sh`:\n\n```\n$ sacrebleu -t wmt17 -l en-de -i output.detok.txt -sh\nbleu|#:1|c:mixed|e:no|tok:13a|s:exp|v:2.0.0 = 20.8 54.4/26.6/14.9/8.7 (bp = 1.000 ratio = 1.026 hyp_len = 62880 ref_len = 61287)\n```\n\nif you only want the score to be printed, you can use the `--score-only/-b` flag:\n\n```\n$ sacrebleu -t wmt17 -l en-de -i output.detok.txt -b\n20.8\n```\n\nthe precision of the scores can be configured via the `--width/-w` flag:\n\n```\n$ sacrebleu -t wmt17 -l en-de -i output.detok.txt -b -w 4\n20.7965\n```\n\n### using your own reference file\n\nsacrebleu knows about common test sets (as detailed in the `--list` example above), but you can also use it to score system outputs with arbitrary references. in this case, do not forget to provide **detokenized** reference and hypotheses files:\n\n```\n# let's save the reference to a text file\n$ sacrebleu -t wmt17 -l en-de --echo ref > ref.detok.txt\n\n#\u00a0option 1: pass the reference file as a positional argument to sacrebleu\n$ sacrebleu ref.detok.txt -i output.detok.txt -m bleu -b -w 4\n20.7965\n\n# option 2: redirect the system into stdin (compatible with multi-bleu.perl way of doing things)\n$ cat output.detok.txt | sacrebleu ref.detok.txt -m bleu -b -w 4\n20.7965\n```\n\n### using multiple metrics\n\nlet's first compute bleu, chrf and ter with the default settings:\n\n```\n$ sacrebleu -t wmt17 -l en-de -i output.detok.txt -m bleu chrf ter\n        bleu|nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0 = 20.8 <stripped>\n      chrf2|nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.0.0 = 52.0\nter|nrefs:1|case:lc|tok:tercom|norm:no|punct:yes|asian:no|version:2.0.0 = 69.0\n```\n\nlet's now enable `chrf++` which is a revised version of chrf that takes into account word n-grams.\nobserve how the `nw:0` gets changed into `nw:2` in the signature:\n\n```\n$ sacrebleu -t wmt17 -l en-de -i output.detok.txt -m bleu chrf ter --chrf-word-order 2\n        bleu|nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0 = 20.8 <stripped>\n    chrf2++|nrefs:1|case:mixed|eff:yes|nc:6|nw:2|space:no|version:2.0.0 = 49.0\nter|nrefs:1|case:lc|tok:tercom|norm:no|punct:yes|asian:no|version:2.0.0 = 69.0\n```\n\nmetric-specific arguments are detailed in the output of `--help`:\n\n```\nbleu related arguments:\n  --smooth-method {none,floor,add-k,exp}, -s {none,floor,add-k,exp}\n                        smoothing method: exponential decay, floor (increment zero counts), add-k (increment num/denom by k for n>1), or none. (default: exp)\n  --smooth-value bleu_smooth_value, -sv bleu_smooth_value\n                        the smoothing value. only valid for floor and add-k. (defaults: floor: 0.1, add-k: 1)\n  --tokenize {none,zh,13a,char,intl,ja-mecab,ko-mecab}, -tok {none,zh,13a,char,intl,ja-mecab,ko-mecab}\n                        tokenization method to use for bleu. if not provided, defaults to `zh` for chinese, `ja-mecab` for japanese, `ko-mecab` for korean and `13a` (mteval) otherwise.\n  --lowercase, -lc      if true, enables case-insensitivity. (default: false)\n  --force               insist that your tokenized input is actually detokenized.\n\nchrf related arguments:\n  --chrf-char-order chrf_char_order, -cc chrf_char_order\n                        character n-gram order. (default: 6)\n  --chrf-word-order chrf_word_order, -cw chrf_word_order\n                        word n-gram order (default: 0). if equals to 2, the metric is referred to as chrf++.\n  --chrf-beta chrf_beta\n                        determine the importance of recall w.r.t precision. (default: 2)\n  --chrf-whitespace     include whitespaces when extracting character n-grams. (default: false)\n  --chrf-lowercase      enable case-insensitivity. (default: false)\n  --chrf-eps-smoothing  enables epsilon smoothing similar to chrf++.py, nltk and moses; instead of effective order smoothing. (default: false)\n\nter related arguments (the defaults replicate tercom's behavior):\n  --ter-case-sensitive  enables case sensitivity (default: false)\n  --ter-asian-support   enables special treatment of asian characters (default: false)\n  --ter-no-punct        removes punctuation. (default: false)\n  --ter-normalized      applies basic normalization and tokenization. (default: false)\n```\n\n### version signatures\nas you may have noticed, sacrebleu generates version strings such as `bleu|nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0` for reproducibility reasons. it's strongly recommended to share these signatures in your papers!\n\n### outputting other metadata\n\nsacrebleu knows about metadata for some test sets, and you can output it like this:\n\n```\n$ sacrebleu -t wmt21 -l en-de --echo src docid ref | head 2\ncouple maced at california dog park for not wearing face masks while having lunch (video) - rt usa news\trt.com.131279\tpaar in hundepark in kalifornien mit pfefferspray bespr\u00fcht, weil es beim mittagessen keine masken trug (video) - rt usa news\nthere's mask-shaming and then there's full on assault.\trt.com.131279\tmasken-shaming ist eine sache, k\u00f6rperverletzung eine andere.\n```\n\nif multiple fields are requested, they are output as tab-separated columns (a tsv).\n\nto see the available fields, add `--echo asdf` (or some other garbage data):\n\n```\n$ sacrebleu -t wmt21 -l en-de --echo asdf\nsacrebleu: no such field asdf in test set wmt21 for language pair en-de.\nsacrebleu: available fields for wmt21/en-de: src, ref:a, ref, docid, origlang\n```\n\n## translationese support\n\nif you are interested in the translationese effect, you can evaluate bleu on a subset of sentences\nwith a given original language (identified based on the `origlang` tag in the raw sgm files).\ne.g., to evaluate only against originally german sentences translated to english use:\n\n    $ sacrebleu -t wmt13 -l de-en --origlang=de -i my-wmt13-output.txt\n\nand to evaluate against the complement (in this case `origlang` en, fr, cs, ru, de) use:\n\n    $ sacrebleu -t wmt13 -l de-en --origlang=non-de -i my-wmt13-output.txt\n\n**please note** that the evaluator will return a bleu score only on the requested subset,\nbut it expects that you pass through the entire translated test set.\n\n## languages & preprocessing\n\n### bleu\n\n- you can compute case-insensitive bleu by passing `--lowercase` to sacrebleu\n- the default tokenizer for bleu is `13a` which mimics the `mteval-v13a` script from moses.\n- other tokenizers are:\n   - `none` which will not apply any kind of tokenization at all\n   - `char` for language-agnostic character-level tokenization\n   - `intl` applies international tokenization and mimics the `mteval-v14` script from moses\n   - `zh` separates out **chinese** characters and tokenizes the non-chinese parts using `13a` tokenizer\n   - `ja-mecab` tokenizes **japanese** inputs using the [mecab](https://pypi.org/project/mecab-python3) morphological analyzer\n   - `ko-mecab` tokenizes **korean** inputs using the [mecab-ko](https://pypi.org/project/mecab-ko) morphological analyzer\n   - `flores101` and `flores200` uses the sentencepiece model built from the flores-101 and [flores-200](https://github.com/facebookresearch/flores/blob/main/flores200/readme.md#languages-in-flores-200) dataset, respectively. note: the canonical .spm file will be automatically fetched if not found locally.\n- you can switch tokenizers using the `--tokenize` flag of sacrebleu. alternatively, if you provide language-pair strings\n  using `--language-pair/-l`, `zh`, `ja-mecab` and `ko-mecab` tokenizers will be used if the target language is `zh` or `ja` or `ko`, respectively.\n- **note that** there's no automatic language detection from the hypotheses so you need to make sure that you are correctly\n  selecting the tokenizer for **japanese**, **korean** and **chinese**.\n\n\ndefault 13a tokenizer will produce poor results for japanese:\n\n```\n$ sacrebleu kyoto-test.ref.ja -i kyoto-test.hyp.ja -b\n2.1\n```\n\nlet's use the `ja-mecab` tokenizer:\n```\n$ sacrebleu kyoto-test.ref.ja -i kyoto-test.hyp.ja --tokenize ja-mecab -b\n14.5\n```\n\nif you provide the language-pair, sacrebleu will use ja-mecab automatically:\n\n```\n$ sacrebleu kyoto-test.ref.ja -i kyoto-test.hyp.ja -l en-ja -b\n14.5\n```\n\n### chrf / chrf++\n\nchrf applies minimum to none pre-processing as it deals with character n-grams:\n\n- if you pass `--chrf-whitespace`, whitespace characters will be preserved when computing character n-grams.\n- if you pass `--chrf-lowercase`, sacrebleu will compute case-insensitive chrf.\n- if you enable non-zero `--chrf-word-order` (pass `2` for `chrf++`), a very simple punctuation tokenization will be internally applied.\n\n\n### ter\n\ntranslation error rate (ter) has its own special tokenizer that you can configure through the command line.\nthe defaults provided are **compatible with the upstream ter implementation (tercom)** but you can nevertheless modify the\nbehavior through the command-line:\n\n- ter is by default case-insensitive. pass `--ter-case-sensitive` to enable case-sensitivity.\n- pass `--ter-normalize` to apply a general western tokenization\n- pass `--ter-asian-support` to enable the tokenization of asian characters. if provided with `--ter-normalize`,\n  both will be applied.\n- pass `--ter-no-punct` to strip punctuation.\n\n## multi-reference evaluation\n\nall three metrics support the use of multiple references during evaluation. let's first pass all references as positional arguments:\n\n```\n$ sacrebleu ref1 ref2 -i system -m bleu chrf ter\n        bleu|nrefs:2|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0 = 61.8 <stripped>\n      chrf2|nrefs:2|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.0.0 = 75.0\nter|nrefs:2|case:lc|tok:tercom|norm:no|punct:yes|asian:no|version:2.0.0 = 31.2\n```\n\nalternatively (less recommended), we can concatenate references using tabs as delimiters as well. don't forget to pass `--num-refs/-nr` in this case!\n\n```\n$ paste ref1 ref2 > refs.tsv\n\n$ sacrebleu refs.tsv --num-refs 2 -i system -m bleu\nbleu|nrefs:2|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0 = 61.8 <stripped>\n```\n\n## multi-system evaluation\nas of version `>=2.0.0`, sacrebleu supports evaluation of an arbitrary number of systems for a particular\ntest set and language-pair. this has the advantage of seeing all results in a\nnicely formatted table.\n\nlet's pass all system output files that match the shell glob `newstest2017.online-*` to sacrebleu for evaluation:\n\n```\n$ sacrebleu -t wmt17 -l en-de -i newstest2017.online-* -m bleu chrf\n\u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555\n\u2502                        system \u2502  bleu  \u2502  chrf2  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 newstest2017.online-a.0.en-de \u2502  20.8  \u2502  52.0   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 newstest2017.online-b.0.en-de \u2502  26.7  \u2502  56.3   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 newstest2017.online-f.0.en-de \u2502  15.5  \u2502  49.3   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 newstest2017.online-g.0.en-de \u2502  18.2  \u2502  51.6   \u2502\n\u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b\n\n-----------------\nmetric signatures\n-----------------\n - bleu       nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0\n - chrf2      nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.0.0\n```\n\nyou can also change the output format to `latex`:\n\n```\n$ sacrebleu -t wmt17 -l en-de -i newstest2017.online-* -m bleu chrf -f latex\n\\begin{tabular}{rcc}\n\\toprule\n                        system &  bleu  &  chrf2  \\\\\n\\midrule\n newstest2017.online-a.0.en-de &  20.8  &  52.0   \\\\\n newstest2017.online-b.0.en-de &  26.7  &  56.3   \\\\\n newstest2017.online-f.0.en-de &  15.5  &  49.3   \\\\\n newstest2017.online-g.0.en-de &  18.2  &  51.6   \\\\\n\\bottomrule\n\\end{tabular}\n\n...\n```\n\n## confidence intervals for single system evaluation\n\nwhen enabled with the `--confidence` flag, sacrebleu will print\n(1) the actual system score, (2) the true mean estimated from bootstrap resampling and (3),\nthe 95% [confidence interval](https://en.wikipedia.org/wiki/confidence_interval) around the mean.\nby default, the number of bootstrap resamples is 1000 (`bs:1000` in the signature)\nand can be changed with `--confidence-n`:\n\n```\n$ sacrebleu -t wmt17 -l en-de -i output.detok.txt -m bleu chrf --confidence -f text --short\n   bleu|#:1|bs:1000|rs:12345|c:mixed|e:no|tok:13a|s:exp|v:2.0.0 = 22.675 (\u03bc = 22.669 \u00b1 0.598) ...\nchrf2|#:1|bs:1000|rs:12345|c:mixed|e:yes|nc:6|nw:0|s:no|v:2.0.0 = 51.953 (\u03bc = 51.953 \u00b1 0.462)\n```\n\n**note:** although provided as a functionality, having access to confidence intervals for just one system\nmay not reveal much information about the underlying model. it often makes more sense to perform\n**paired statistical tests** across multiple systems.\n\n**note:** when resampling, the seed of the `numpy`'s random number generator (rng)\nis fixed to `12345`. if you want to relax this and set your own seed, you can\nexport the environment variable `sacrebleu_seed` to an integer. alternatively, you can export\n`sacrebleu_seed=none` to skip initializing the rng's seed and allow for non-deterministic\nbehavior.\n\n## paired significance tests for multi system evaluation\nideally, one would have access to many systems in cases such as (1) investigating\nwhether a newly added feature yields significantly different scores than the baseline or\n(2) evaluating submissions for a particular shared task. sacrebleu offers two different paired significance tests that are widely used in mt research.\n\n### paired bootstrap resampling (--paired-bs)\n\nthis is an efficient implementation of the paper [statistical significance tests for machine translation evaluation](https://www.aclweb.org/anthology/w04-3250.pdf) and is result-compliant with the [reference moses implementation](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/analysis/bootstrap-hypothesis-difference-significance.pl). the number of bootstrap resamples can be changed with the `--paired-bs-n` flag and its default is 1000.\n\nwhen launched, paired bootstrap resampling will perform:\n - bootstrap resampling to estimate 95% ci for all systems and the baseline\n - a significance test between the **baseline** and each **system** to compute a [p-value](https://en.wikipedia.org/wiki/p-value).\n\n### paired approximate randomization (--paired-ar)\n\npaired approximate randomization (ar) is another type of paired significance test that is claimed to be more accurate than paired bootstrap resampling when it comes to type-i errors ([riezler and maxwell iii, 2005](https://www.aclweb.org/anthology/w05-0908.pdf)). type-i errors indicate failures to reject the null hypothesis when it is true. in other words, ar should in theory be more robust to subtle changes across systems.\n\nour implementation is verified to be result-compliant with the [multeval toolkit](https://github.com/jhclark/multeval) that also uses paired ar test for pairwise comparison. the number of approximate randomization trials is set to 10,000 by default. this can be changed with the `--paired-ar-n` flag.\n\n### running the tests\n\n- the **first system** provided to `--input/-i` will be automatically taken as the **baseline system** against which you want to compare **other systems.**\n- when `--input/-i` is used, the system output files will be automatically named according to the file paths. for the sake of simplicity, sacrebleu will automatically discard the **baseline system** if it also appears amongst **other systems**. this is useful if you would like to run the tool by passing `-i systems/baseline.txt systems/*.txt`. here, the `baseline.txt` file will not be also considered as a candidate system.\n- alternatively, you can also use a tab-separated input file redirected to sacrebleu. in this case, the first column hypotheses will be taken as the **baseline system**. however, this method is **not recommended** as it won't allow naming your systems in a human-readable way. it will instead enumerate the systems from 1 to n following the column order in the tab-separated input.\n- on linux and mac os x, you can launch the tests on multiple cpu's by passing the flag `--paired-jobs n`. if `n == 0`, sacrebleu will launch one worker for each pairwise comparison. if `n > 0`, `n` worker processes will be spawned. this feature will substantially speed up the runtime especially if you want the **ter** metric to be computed.\n\n#### example: paired bootstrap resampling\nin the example below, we select `newstest2017.lium-nmt.4900.en-de` as the baseline and compare it to 4 other wmt17 submissions using paired bootstrap resampling. according to the results, the null hypothesis (i.e. the two systems being essentially the same) could not be rejected (at the significance level of 0.05) for the following comparisons:\n\n- 0.1 bleu difference between the baseline and the online-b system (p = 0.3077)\n\n```\n$ sacrebleu -t wmt17 -l en-de -i newstest2017.lium-nmt.4900.en-de newstest2017.online-* -m bleu chrf --paired-bs\n\u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555\n\u2502                                     system \u2502  bleu (\u03bc \u00b1 95% ci)  \u2502  chrf2 (\u03bc \u00b1 95% ci)  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 baseline: newstest2017.lium-nmt.4900.en-de \u2502  26.6 (26.6 \u00b1 0.6)  \u2502  55.9 (55.9 \u00b1 0.5)   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              newstest2017.online-a.0.en-de \u2502  20.8 (20.8 \u00b1 0.6)  \u2502  52.0 (52.0 \u00b1 0.4)   \u2502\n\u2502                                            \u2502    (p = 0.0010)*    \u2502    (p = 0.0010)*     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              newstest2017.online-b.0.en-de \u2502  26.7 (26.6 \u00b1 0.7)  \u2502  56.3 (56.3 \u00b1 0.5)   \u2502\n\u2502                                            \u2502    (p = 0.3077)     \u2502    (p = 0.0240)*     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              newstest2017.online-f.0.en-de \u2502  15.5 (15.4 \u00b1 0.5)  \u2502  49.3 (49.3 \u00b1 0.4)   \u2502\n\u2502                                            \u2502    (p = 0.0010)*    \u2502    (p = 0.0010)*     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              newstest2017.online-g.0.en-de \u2502  18.2 (18.2 \u00b1 0.5)  \u2502  51.6 (51.6 \u00b1 0.4)   \u2502\n\u2502                                            \u2502    (p = 0.0010)*    \u2502    (p = 0.0010)*     \u2502\n\u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b\n\n------------------------------------------------------------\npaired bootstrap resampling test with 1000 resampling trials\n------------------------------------------------------------\n - each system is pairwise compared to baseline: newstest2017.lium-nmt.4900.en-de.\n   actual system score / bootstrap estimated true mean / 95% ci are provided for each metric.\n\n - null hypothesis: the system and the baseline translations are essentially\n   generated by the same underlying process. for a given system and the baseline,\n   the p-value is roughly the probability of the absolute score difference (delta)\n   or higher occurring due to chance, under the assumption that the null hypothesis is correct.\n\n - assuming a significance threshold of 0.05, the null hypothesis can be rejected\n   for p-values < 0.05 (marked with \"*\"). this means that the delta is unlikely to be attributed\n   to chance, hence the system is significantly \"different\" than the baseline.\n   otherwise, the p-values are highlighted in red.\n\n - note: significance does not tell whether a system is \"better\" than the baseline but rather\n   emphasizes the \"difference\" of the systems in terms of the replicability of the delta.\n\n-----------------\nmetric signatures\n-----------------\n - bleu       nrefs:1|bs:1000|seed:12345|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0\n - chrf2      nrefs:1|bs:1000|seed:12345|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.0.0\n```\n\n#### example: paired approximate randomization\n\nlet's now run the paired approximate randomization test for the same comparison. according to the results, the findings are compatible with the paired bootstrap resampling test. however, the p-value for the `baseline vs. online-b` comparison is much higher (`0.8066`) than the paired bootstrap resampling test.\n\n(**note that** the ar test does not provide confidence intervals around the true mean as it does not perform bootstrap resampling.)\n\n```\n$ sacrebleu -t wmt17 -l en-de -i newstest2017.lium-nmt.4900.en-de newstest2017.online-* -m bleu chrf --paired-ar\n\u2552\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2555\n\u2502                                     system \u2502     bleu      \u2502     chrf2     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 baseline: newstest2017.lium-nmt.4900.en-de \u2502     26.6      \u2502     55.9      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              newstest2017.online-a.0.en-de \u2502     20.8      \u2502     52.0      \u2502\n\u2502                                            \u2502 (p = 0.0001)* \u2502 (p = 0.0001)* \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              newstest2017.online-b.0.en-de \u2502     26.7      \u2502     56.3      \u2502\n\u2502                                            \u2502 (p = 0.8066)  \u2502 (p = 0.0385)* \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              newstest2017.online-f.0.en-de \u2502     15.5      \u2502     49.3      \u2502\n\u2502                                            \u2502 (p = 0.0001)* \u2502 (p = 0.0001)* \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              newstest2017.online-g.0.en-de \u2502     18.2      \u2502     51.6      \u2502\n\u2502                                            \u2502 (p = 0.0001)* \u2502 (p = 0.0001)* \u2502\n\u2558\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255b\n\n-------------------------------------------------------\npaired approximate randomization test with 10000 trials\n-------------------------------------------------------\n - each system is pairwise compared to baseline: newstest2017.lium-nmt.4900.en-de.\n   actual system score is provided for each metric.\n\n - null hypothesis: the system and the baseline translations are essentially\n   generated by the same underlying process. for a given system and the baseline,\n   the p-value is roughly the probability of the absolute score difference (delta)\n   or higher occurring due to chance, under the assumption that the null hypothesis is correct.\n\n - assuming a significance threshold of 0.05, the null hypothesis can be rejected\n   for p-values < 0.05 (marked with \"*\"). this means that the delta is unlikely to be attributed\n   to chance, hence the system is significantly \"different\" than the baseline.\n   otherwise, the p-values are highlighted in red.\n\n - note: significance does not tell whether a system is \"better\" than the baseline but rather\n   emphasizes the \"difference\" of the systems in terms of the replicability of the delta.\n\n-----------------\nmetric signatures\n-----------------\n - bleu       nrefs:1|ar:10000|seed:12345|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0\n - chrf2      nrefs:1|ar:10000|seed:12345|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.0.0\n```\n\n# using sacrebleu from python\n\nfor evaluation, it may be useful to compute bleu, chrf or ter from a python script. the recommended\nway of doing this is to use the object-oriented api, by creating an instance of the `metrics.bleu` class\nfor example:\n\n```python\nin [1]: from sacrebleu.metrics import bleu, chrf, ter\n   ...:\n   ...: refs = [ # first set of references\n   ...:          ['the dog bit the man.', 'it was not unexpected.', 'the man bit him first.'],\n   ...:          # second set of references\n   ...:          ['the dog had bit the man.', 'no one was surprised.', 'the man had bitten the dog.'],\n   ...:        ]\n   ...: sys = ['the dog bit the man.', \"it wasn't surprising.\", 'the man had just bitten him.']\n\nin [2]: bleu = bleu()\n\nin [3]: bleu.corpus_score(sys, refs)\nout[3]: bleu = 48.53 82.4/50.0/45.5/37.5 (bp = 0.943 ratio = 0.944 hyp_len = 17 ref_len = 18)\n\nin [4]: bleu.get_signature()\nout[4]: nrefs:2|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0\n\nin [5]: chrf = chrf()\n\nin [6]: chrf.corpus_score(sys, refs)\nout[6]: chrf2 = 59.73\n```\n\n### variable number of references\n\nlet's now remove the first reference sentence for the first system sentence `the dog bit the man.` by replacing it with either `none` or the empty string `''`.\nthis allows using a variable number of reference segments per hypothesis. observe how the signature changes from `nrefs:2` to `nrefs:var`:\n\n```python\nin [1]: from sacrebleu.metrics import bleu, chrf, ter\n   ...:\n   ...: refs = [ # first set of references\n                 # 1st sentence does not have a ref here\n   ...:          ['', 'it was not unexpected.', 'the man bit him first.'],\n   ...:          # second set of references\n   ...:          ['the dog had bit the man.', 'no one was surprised.', 'the man had bitten the dog.'],\n   ...:        ]\n   ...: sys = ['the dog bit the man.', \"it wasn't surprising.\", 'the man had just bitten him.']\n\nin [2]: bleu = bleu()\n\nin [3]: bleu.corpus_score(sys, refs)\nout[3]: bleu = 29.44 82.4/42.9/27.3/12.5 (bp = 0.889 ratio = 0.895 hyp_len = 17 ref_len = 19)\n\nin [4]: bleu.get_signature()\nout[4]: nrefs:var|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0\n```\n\n## compatibility api\n\nyou can also use the compatibility api that provides wrapper functions around the object-oriented api to\ncompute sentence-level and corpus-level bleu, chrf and ter: (it should be noted that this api can be\nremoved in future releases)\n\n```python\nin [1]: import sacrebleu\n   ...: \n   ...: refs = [ # first set of references\n   ...:          ['the dog bit the man.', 'it was not unexpected.', 'the man bit him first.'],\n   ...:          # second set of references\n   ...:          ['the dog had bit the man.', 'no one was surprised.', 'the man had bitten the dog.'],\n   ...:        ]\n   ...: sys = ['the dog bit the man.', \"it wasn't surprising.\", 'the man had just bitten him.']\n\nin [2]: sacrebleu.corpus_bleu(sys, refs)\nout[2]: bleu = 48.53 82.4/50.0/45.5/37.5 (bp = 0.943 ratio = 0.944 hyp_len = 17 ref_len = 18)\n```\n\n# license\n\nsacrebleu is licensed under the [apache 2.0 license](license.txt).\n\n# credits\n\nthis was all rico sennrich's idea.\noriginally written by matt post.\nnew features and ongoing support provided by martin popel (@martinpopel) and ozan caglayan (@ozancaglayan).\n\nif you use sacrebleu, please cite the following:\n\n```\n@inproceedings{post-2018-call,\n  title = \"a call for clarity in reporting {bleu} scores\",\n  author = \"post, matt\",\n  booktitle = \"proceedings of the third conference on machine translation: research papers\",\n  month = oct,\n  year = \"2018\",\n  address = \"belgium, brussels\",\n  publisher = \"association for computational linguistics\",\n  url = \"https://www.aclweb.org/anthology/w18-6319\",\n  pages = \"186--191\",\n}\n```\n\n# release notes\n\n\n- 2.4.0 (2023-11-07)\n  added:\n  - wmt23 test sets (test set `wmt23`)\n\n- 2.3.2 (2023-11-06)\n  fixed:\n  - special treatment of empty references in ter (#232)\n  - bump in mecab version for ja (#234)\n\n  added:\n  - warning if `-tok spm` is used (use explicit `flores101` instead) (#238)\n\n- 2.3.1 (2022-10-18)\n  bugfix:\n  - set lru_cache to 2^16 for spm tokenizer (was set to infinite)\n\n- 2.3.0 (2022-10-18)\n  features:\n  - (#203) added `-tok flores101` and `-tok flores200`, a.k.a. `spbleu`.\n    these are multilingual tokenizations that make use of the\n    multilingual spm models released by facebook and described in the\n    following papers:\n    * flores-101: https://arxiv.org/abs/2106.03193\n    * flores-200: https://arxiv.org/abs/2207.04672\n  - (#213) added json formatting for multi-system output (thanks to manikanta inugurthi @me-manikanta)\n  - (#211) you can now list all test sets for a language pair with `--list src-trg`.\n    thanks to jaume zaragoza (@zjaume) for adding this feature.\n  - added wmt22 test sets (test set `wmt22`)\n  - system outputs: include with wmt22. also added wmt21/systems which will produce wmt21 submitted systems.\n    to see available systems, give a dummy system to `--echo`, e.g., `sacrebleu -t wmt22 -l en-de --echo ?`\n\n- 2.2.1 (2022-09-13)\n  bugfix: standard usage was returning (and using) each reference twice.\n\n- 2.2.0 (2022-07-25)\n  features:\n  - added wmt21 datasets (thanks to @brighxiaohan)\n  - `--echo` now exposes document metadata where available (e.g., docid, genre, origlang)\n  - bugfix: allow empty references (#161)\n  - adds a korean tokenizer (thanks to @nounique)\n\n  under the hood:\n  - moderate code refactoring\n  - processed files have adopted a more sensible internal naming scheme under ~/.sacrebleu\n    (e.g., wmt17_ms.zh-en.src instead of zh-en.zh)\n  - processed file extensions correspond to the values passed to `--echo` (e.g., \"src\")\n  - now explicitly representing nonetokenizer\n  - got rid of the \".lock\" lockfile for downloading (using the tarball itself)\n\n  many thanks to @brightxiaohan (https://github.com/brightxiaohan) for the bulk of\n  the code contributions in this release.\n\n- 2.1.0 (2022-05-19)\n  features:\n  - added `-tok spm` for multilingual spm tokenization (#168)\n    (thanks to naman goyal and james cross at facebook)\n\n  fixes:\n  - handle potential memory usage issues due to lru caching in tokenizers (#167)\n  - bugfix: bleu.corpus_score() now using max_ngram_order (#173)\n  - upgraded ja-mecab to 1.0.5 (#196)\n\n- 2.0.0 (2021-07-18)\n  - build: add windows and os x testing to travis ci.\n  - improve documentation and type annotations.\n  - drop `python < 3.6` support and migrate to f-strings.\n  - relax `portalocker` version pinning, add `regex, tabulate, numpy` dependencies.\n  - drop input type manipulation through `isinstance` checks. if the user does not obey\n    to the expected annotations, exceptions will be raised. robustness attempts lead to\n    confusions and obfuscated score errors in the past (#121)\n  - variable # references per segment is supported for all metrics by default. it is\n    still only available through the api.\n  - use colored strings in tabular outputs (multi-system evaluation mode) through\n    the help of `colorama` package.\n  - tokenizers: add caching to tokenizers which seem to speed up things a bit.\n  - `intl` tokenizer: use `regex` module. speed goes from ~4 seconds to ~0.6 seconds\n    for a particular test set evaluation. (#46)\n  - signature: formatting changed (mostly to remove '+' separator as it was\n    interfering with chrf++). the field separator is now '|' and key values\n    are separated with ':' rather than '.'.\n  - signature: boolean true / false values are shortened to yes / no.\n  - signature: number of references is `var` if variable number of references is used.\n  - signature: add effective order (yes/no) to bleu and chrf signatures.\n  - metrics: scale all metrics into the [0, 100] range (#140)\n  - metrics api: use explicit argument names and defaults for the metrics instead of\n    passing obscure `argparse.namespace` objects.\n  - metrics api: a base abstract `metric` class is introduced to guide further\n    metric development. this class defines the methods that should be implemented\n    in the derived classes and offers boilerplate methods for the common functionality.\n    a new metric implemented this way will automatically support significance testing.\n  - metrics api: all metrics now receive an optional `references` argument at\n    initialization time to process and cache the references. further evaluations\n    of different systems against the same references becomes faster this way\n    for example when using significance testing.\n  - bleu: in case of no n-gram matches at all, skip smoothing and return 0.0 bleu (#141).\n  - chrf: added multi-reference support, verified the scores against chrf++.py, added test case.\n  - chrf: added chrf+ support through `word_order` argument. added test cases against chrf++.py.\n    exposed it through the cli (--chrf-word-order) (#124)\n  - chrf: add possibility to disable effective order smoothing (pass --chrf-eps-smoothing).\n    this way, the scores obtained are exactly the same as chrf++, moses and nltk implementations.\n    we keep the effective ordering as the default for compatibility, since this only\n    affects sentence-level scoring with very short sentences. (#144)\n  - cli: `--input/-i` can now ingest multiple systems. for this reason, the positional\n    `references` should always preceed the `-i` flag.\n  - cli: allow modifying ter arguments through cli. we still keep the tercom defaults.\n  - cli: prefix metric-specific arguments with --chrf and --ter. to maintain compatibility,\n    bleu argument names are kept the same.\n  - cli: separate metric-specific arguments for clarity when `--help` is printed.\n  - cli: added `--format/-f` flag. the single-system output mode is now `json` by default.\n    if you want to keep the old text format persistently, you can export `sacrebleu_format=text` into your\n    shell.\n  - cli: for multi-system mode, `json` falls back to plain text. `latex` output can only\n    be generated for multi-system mode.\n  - cli: sacrebleu now supports evaluating multiple systems for a given test set\n    in an efficient way. through the use of `tabulate` package, the results are\n    nicely rendered into a plain text table, latex, html or rst (cf. --format/-f argument).\n    the systems can be either given as a list of plain text files to `-i/--input` or\n    as a tab-separated single stream redirected into `stdin`. in the former case,\n    the basenames of the files will be automatically used as system names.\n  - statistical tests: sacrebleu now supports confidence interval estimation\n    through bootstrap resampling for single-system evaluation (`--confidence` flag)\n    as well as paired bootstrap resampling (`--paired-bs`) and paired approximate\n    randomization tests (`--paired-ar`) when evaluating multiple systems (#40 and #78).\n\n- 1.5.1 (2021-03-05)\n  - fix extraction error for wmt18 extra test sets (test-ts) (#142)\n  - validation and test datasets are added for multilingual tedx\n\n- 1.5.0 (2021-01-15)\n  - fix an assertion error in chrf (#121)\n  - add missing `__repr__()` methods for bleu and ter\n  - ter: fix exception when `--short` is used (#131)\n  - pin mecab version to 1.0.3 for python 3.5 support\n  - [api change]: default value for `floor` smoothing is now 0.1 instead of 0.\n  - [api change]: `sacrebleu.sentence_bleu()` now uses the `exp` smoothing method,\n    exactly the same as the cli's --sentence-level behavior. this was mainly done\n    to make two methods behave the same.\n  - add smoothing value to bleu signature (#98)\n  - dataset: fix iwslt links (#128)\n  - allow variable number of references for bleu (only via api) (#130).\n    thanks to ondrej dusek (@tuetschek)\n\n- 1.4.14 (2020-09-13)\n  - added character-based tokenization (`-tok char`).\n    thanks to christian federmann.\n  - added ter (`-m ter`). thanks to ales tamchyna! (fixes #90)\n  - allow calling the script as a standalone utility (fixes #86)\n  - fix type annotation issues (fixes #100) and mark sacrebleu as supporting mypy\n  - added wmt20 robustness test sets:\n    - wmt20/robust/set1 (en-ja, en-de)\n    - wmt20/robust/set2 (en-ja, ja-en)\n    - wmt20/robust/set3 (de-en)\n\n- 1.4.13 (2020-07-30)\n  - added wmt20 newstest test sets (#103)\n  - make mecab3-python an extra dependency, adapt code to new mecab3-python\n    this fixes the recent windows installation issues as well (#104)\n    japanese support should now be explicitly installed through sacrebleu[ja] package.\n  - fix return type annotation of corpus_bleu()\n  - improve sentence_score's documentation, do not allow single ref string (#98)\n\n- 1.4.12 (2020-07-03)\n  - fix a deployment bug (#96)\n\n- 1.4.11 (2020-07-03)\n  - added multi30k multimodal mt test set metadata\n  - refactored all tokenizers into respective classes (fixes #85)\n  - refactored all metrics into respective classes\n  - moved utility functions into `utils.py`\n  - implemented signatures using `bleusignature` and `chrfsignature` classes\n  - simplified checking of chinese characters (fixes #5)\n  - unified common regexp tokenization codes for tokenizers (fixes #27)\n  - fixed --detail failing when no test sets are provided\n  - fixed multi-reference bleu failing when tab-delimited reference stream is used\n  - removed lowercase option for chrf which was not functional (#85)\n  - simplified chrf and used the same i/o logic as bleu to allow for future\n    multi-reference reading\n  - added score regression tests for chrf using reference chrf++ implementation\n  - added multi-reference & tokenizer & signature tests\n\n- 1.4.10 (2020-05-30)\n  - fixed bug in signature with mecab tokenizer\n  - cleaned up deprecation warnings (thanks to karthikeyan singaravelan @tirkarthi)\n  - now only lists the external [typing](https://pypi.org/project/typing/)\n    module as a dependency for python `<= 3.4`, as it was integrated in the standard\n    library in python 3.5 (thanks to erwan de l\u00e9pinau @erwandl).\n  - added license to pypi (thanks to mark harfouche @hmaarrfk)\n\n- 1.4.9 (2020-04-30)\n  - changed `get_available_testsets()` to return a list\n  - remove japanese mecab tokenizer from requirements.\n    (must be installed manually to avoid windows incompatibility).\n    many thanks to makoto morishita (@morinoseimorizo).\n\n- 1.4.8 (2020-04-26)\n  - added to api:\n    - get_source_file()\n    - get_reference_files()\n    - get_available_testsets()\n    - get_langpairs_for_testset()\n  - some internal refactoring\n  - fixed descriptions of some wmt19/google test sets\n  - added api test case (test/test_apy.py)\n\n- 1.4.7 (2020-04-19)\n  - added google's extra wmt19/en-de refs (-t wmt19/google/{ar,arp,hqall,hqp,hqr,wmtp})\n    (freitag, grangier, & caswell\n     bleu might be guilty but references are not innocent\n     https://arxiv.org/abs/2004.06063)\n  - restored sacrebleu_dir and smart_open to exports (thanks to thomas liao @tholiao)\n\n- 1.4.6 (2020-03-28)\n  - large internal reorganization as a module (thanks to thamme gowda @thammegowda)\n\n- 1.4.5 (2020-03-28)\n  - added japanese mecab tokenizer (`-tok ja-mecab`) (thanks to makoto morishita @morinoseimorizo)\n  - added wmt20/dev test sets (thanks to martin popel @martinpopel)\n\n- 1.4.4 (2020-03-10)\n  - smoothing changes (sebastian nickels @sn1c)\n    - fixed bug that only applied smoothing to n-grams for n > 2\n    - added default smoothing values for methods \"floor\" (0) and \"add-k\" (1)\n  - `--list` now returns a list of all language pairs for a task when combined with `-t`\n    (e.g., `sacrebleu -t wmt19 --list`)\n  - added missing languages for iwslt17\n  - minor code improvements (thomas liao @tholiao)\n\n- 1.4.3 (2019-12-02)\n  - bugfix: handling of result object for chrf\n  - improved api example\n\n- 1.4.2 (2019-10-11)\n  - tokenization variant omitted from the chrf signature; it is relevant only for bleu (thanks to martin popel)\n  - bugfix: call to sentence_bleu (thanks to rachel bawden)\n  - documentation example for python api (thanks to vlad lyalin)\n  - calls to corpus_chrf and sentence_chrf now return a an object instead of a float (use result.score)\n\n- 1.4.1 (2019-09-11)\n   - added sentence-level scoring via -sl (--sentence-level)\n\n- 1.4.0 (2019-09-10)\n   - many thanks to martin popel for all the changes below!\n   - added evaluation on concatenated test sets (e.g., `-t wmt17,wmt18`).\n     works as long as they all have the same language pair.\n   - added `sacrebleu --origlang` (both for evaluation on a subset and for `--echo`).\n     note that while echoing prints just the subset, evaluation expects the complete\n     test set (and just skips the irrelevant parts).\n   - added `sacrebleu --detail` for breakdown by domain-specific subsets of the test sets.\n     (available for wmt19).\n   - minor changes\n     - improved display of `sacrebleu -h`\n     - added `sacrebleu --list`\n     - code refactoring\n     - documentation and tests updates\n     - fixed a race condition bug (`os.makedirs(outdir, exist_ok=true)` instead of `if os.path.exists`)\n\n- 1.3.7 (2019-07-12)\n   - lazy loading of regexes cuts import time from ~1s to nearly nothing (thanks, @louismartin!)\n   - added a simple (non-atomic) lock on downloading\n   - can now read multiple refs from a single tab-delimited file.\n     you need to pass `--num-refs n` to tell it to run the split.\n     only works with a single reference file passed from the command line.\n\n- 1.3.6 (2019-06-10)\n   - removed another f-string for python 3.5 compatibility\n\n- 1.3.5 (2019-06-07)\n   - restored python 3.5 compatibility\n\n- 1.3.4 (2019-05-28)\n   - added mtnt 2019 test sets\n   - added a bleu object\n\n- 1.3.3 (2019-05-08)\n   - added wmt'19 test sets\n\n- 1.3.2 (2018-04-24)\n   - bugfix in test case (thanks to adam roberts, @adarob)\n   - passing smoothing method through `sentence_bleu`\n\n- 1.3.1 (2019-03-20)\n   - added another smoothing approach (add-k) and a command-line option for choosing the smoothing method\n     (`--smooth exp|floor|add-n|none`) and the associated value (`--smooth-value`), when relevant.\n   - changed interface to some functions (backwards incompatible)\n     - 'smooth' is now 'smooth_method'\n     - 'smooth_floor' is now 'smooth_value'\n\n- 1.2.21 (19 march 2019)\n   - ctrl-m characters are now treated as normal characters, previously treated as newline.\n\n- 1.2.20 (28 february 2018)\n   - tokenization now defaults to \"zh\" when language pair is known\n\n- 1.2.19 (19 february 2019)\n   - updated checksum for wmt19/dev (seems to have changed)\n\n- 1.2.18 (19 february 2019)\n   - fixed checksum for wmt17/dev (copy-paste error)\n\n- 1.2.17 (6 february 2019)\n   - added kk-en and en-kk to wmt19/dev\n\n- 1.2.16 (4 february 2019)\n   - added gu-en and en-gu to wmt19/dev\n\n- 1.2.15 (30 january 2019)\n   - added md5 checksumming of downloaded files for all datasets.\n\n- 1.2.14 (22 january 2019)\n   - added mtnt1.1/train mtnt1.1/valid mtnt1.1/test data from [mtnt](http://www.cs.cmu.edu/~pmichel1/mtnt/)\n\n- 1.2.13 (22 january 2019)\n   - added 'wmt19/dev' task for 'lt-en' and 'en-lt' (development data for new tasks).\n   - added md5 checksum for downloaded tarballs.\n\n- 1.2.12 (8 november 2018)\n   - now outputs only only digit after the decimal\n\n- 1.2.11 (29 august 2018)\n   - added a function for sentence-level, smoothed bleu\n\n- 1.2.10 (23 may 2018)\n   - added wmt18 test set (with references)\n\n- 1.2.9 (15 may 2018)\n   - added zh-en, en-zh, tr-en, and en-tr datasets for wmt18/test-ts\n\n- 1.2.8 (14 may 2018)\n   - added wmt18/test-ts, the test sources (only) for [wmt18](http://statmt.org/wmt18/translation-task.html)\n   - moved readme out of `sacrebleu.py` and the changelog into a separate file\n\n- 1.2.7 (10 april 2018)\n   - fixed another locale issue (with --echo)\n   - grudgingly enabled `-tok none` from the command line\n\n- 1.2.6 (22 march 2018)\n   - added wmt17/ms (microsoft's [additional zh-en references](https://github.com/microsofttranslator/translator-humanparitydata)).\n     try `sacrebleu -t wmt17/ms --cite`.\n   - `--echo ref` now pastes together all references, if there is more than one\n\n- 1.2.5 (13 march 2018)\n   - added wmt18/dev datasets (en-et and et-en)\n   - fixed logic with --force\n   - locale-independent installation\n   - added \"--echo both\" (tab-delimited)\n\n- 1.2.3 (28 january 2018)\n   - metrics (`-m`) are now printed in the order requested\n   - chrf now prints a version string (including the beta parameter, importantly)\n   - attempt to remove dependence on locale setting\n\n- 1.2 (17 january 2018)\n   - added the chrf metric (`-m chrf` or `-m bleu chrf` for both)\n     see 'chrf: character n-gram f-score for automatic mt evaluation' by maja popovic (wmt 2015)\n     [http://www.statmt.org/wmt15/pdf/wmt49.pdf]\n   - added iwslt 2017 test and tuning sets for de, fr, and zh\n     (thanks to mauro cettolo and marcello federico).\n   - added `--cite` to produce the citation for easy inclusion in papers\n   - added `--input` (`-i`) to set input to a file instead of stdin\n   - removed accent mark after objection from un official\n\n- 1.1.7 (27 november 2017)\n   - corpus_bleu() now raises an exception if input streams are different lengths\n   - thanks to martin popel for:\n      - small bugfix in tokenization_13a (not affecting wmt references)\n      - adding `--tok intl` (international tokenization)\n   - added wmt17/dev and wmt17/dev sets (for languages intro'd those years)\n\n- 1.1.6 (15 november 2017)\n   - bugfix for tokenization warning\n\n- 1.1.5 (12 november 2017)\n   - added -b option (only output the bleu score)\n   - removed fi-en from list of wmt16/17 systems with more than one reference\n   - added wmt16/tworefs and wmt17/tworefs for scoring with both en-fi references\n\n- 1.1.4 (10 november 2017)\n   - added effective order for sentence-level bleu computation\n   - added unit tests from sockeye\n\n- 1.1.3 (8 november 2017).\n   - factored code a bit to facilitate api:\n      - compute_bleu: works from raw stats\n      - corpus_bleu for use from the command line\n      - raw_corpus_bleu: turns off tokenization, command-line sanity checks, floor smoothing\n   - smoothing (type 'exp', now the default) fixed to produce mteval-v13a.pl results\n   - added 'floor' smoothing (adds 0.01 to 0 counts, more versatile via api), 'none' smoothing (via api)\n   - small bugfixes, windows compatibility (h/t christian federmann)\n\n- 1.0.3 (4 november 2017).\n   - contributions from christian federmann:\n      - added explicit support for encoding\n      - fixed windows support\n      - bugfix in handling reference length with multiple refs\n\n- version 1.0.1 (1 november 2017).\n   - small bugfix affecting some versions of python.\n   - code reformatting due to ozan \u00e7a\u011flayan.\n\n- version 1.0 (23 october 2017).\n   - support for wmt 2008--2017.\n   - single tokenization (v13a) with lowercase fix (proper lower() instead of just a-z).\n   - chinese tokenization.\n   - tested to match all wmt17 scores on all arcs.\n\n",
  "docs_url": null,
  "keywords": "machine translation, evaluation, nlp, natural language processing, computational linguistics",
  "license": "apache license 2.0",
  "name": "sacrebleu",
  "package_url": "https://pypi.org/project/sacrebleu/",
  "project_url": "https://pypi.org/project/sacrebleu/",
  "project_urls": {
    "Homepage": "https://github.com/mjpost/sacrebleu"
  },
  "release_url": "https://pypi.org/project/sacrebleu/2.4.0/",
  "requires_dist": [
    "portalocker",
    "regex",
    "tabulate (>=0.8.9)",
    "numpy (>=1.17)",
    "colorama",
    "lxml",
    "mecab-python3 (<=1.0.6,>=1.0.5) ; extra == 'ja'",
    "ipadic (<2.0,>=1.0) ; extra == 'ja'",
    "mecab-ko (<=1.0.1,>=1.0.0) ; extra == 'ko'",
    "mecab-ko-dic (<2.0,>=1.0) ; extra == 'ko'"
  ],
  "requires_python": ">=3.6",
  "summary": "hassle-free computation of shareable, comparable, and reproducible bleu, chrf, and ter scores",
  "version": "2.4.0",
  "releases": [],
  "developers": [
    "matt_post",
    "post@cs.jhu.edu"
  ],
  "kwds": "compute_bleu sacrebleu_seed sacrebleu sacrebleu_format raw_corpus_bleu",
  "license_kwds": "apache license 2.0",
  "libtype": "pypi",
  "id": "pypi_sacrebleu",
  "homepage": "https://github.com/mjpost/sacrebleu",
  "release_count": 67,
  "dependency_ids": [
    "pypi_colorama",
    "pypi_ipadic",
    "pypi_lxml",
    "pypi_mecab_ko",
    "pypi_mecab_ko_dic",
    "pypi_mecab_python3",
    "pypi_numpy",
    "pypi_portalocker",
    "pypi_regex",
    "pypi_tabulate"
  ]
}