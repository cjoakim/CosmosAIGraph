{
  "classifiers": [
    "license :: osi approved :: mit license",
    "natural language :: english",
    "programming language :: python :: 3.5",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: utilities"
  ],
  "description": "# google search results in python\n\n[![package](https://badge.fury.io/py/google-search-results.svg)](https://badge.fury.io/py/google-search-results)\n[![build](https://github.com/serpapi/google-search-results-python/actions/workflows/python-package.yml/badge.svg)](https://github.com/serpapi/google-search-results-python/actions/workflows/python-package.yml)\n\nthis python package is meant to scrape and parse search results from google, bing, baidu, yandex, yahoo, home depot, ebay and more, using [serpapi](https://serpapi.com). \n\nthe following services are provided:\n- [search api](https://serpapi.com/search-api)\n- [search archive api](https://serpapi.com/search-archive-api)\n- [account api](https://serpapi.com/account-api)\n- [location api](https://serpapi.com/locations-api) (google only)\n\nserpapi provides a [script builder](https://serpapi.com/demo) to get you started quickly.\n\n## installation\n\npython 3.7+\n```bash\npip install google-search-results\n```\n\n[link to the python package page](https://pypi.org/project/google-search-results/)\n\n## quick start\n\n```python\nfrom serpapi import googlesearch\nsearch = googlesearch({\n    \"q\": \"coffee\", \n    \"location\": \"austin,texas\",\n    \"api_key\": \"<your secret api key>\"\n  })\nresult = search.get_dict()\n```\n\nthis example runs a search for \"coffee\" using your secret api key.\n\nthe serpapi service (backend)\n- searches google using the search: q = \"coffee\"\n- parses the messy html responses\n- returns a standardized json response\nthe googlesearch class\n- formats the request\n- executes a get http request against serpapi service\n- parses the json response into a dictionary\n\net voil\u00e0...\n\nalternatively, you can search:\n- bing using bingsearch class\n- baidu using baidusearch class\n- yahoo using yahoosearch class\n- duckduckgo using duckduckgosearch class\n- ebay using ebaysearch class\n- yandex using yandexsearch class\n- homedepot using homedepotsearch class\n- googlescholar using googlescholarsearch class\n- youtube using youtubesearch class\n- walmart using walmartsearch\n- apple app store using appleappstoresearch class\n- naver using naversearch class\n\n\nsee the [playground to generate your code.](https://serpapi.com/playground)\n\n## summary\n- [google search results in python](#google-search-results-in-python)\n  - [installation](#installation)\n  - [quick start](#quick-start)\n  - [summary](#summary)\n    - [google search api capability](#google-search-api-capability)\n    - [how to set serp api key](#how-to-set-serp-api-key)\n    - [example by specification](#example-by-specification)\n    - [location api](#location-api)\n    - [search archive api](#search-archive-api)\n    - [account api](#account-api)\n    - [search bing](#search-bing)\n    - [search baidu](#search-baidu)\n    - [search yandex](#search-yandex)\n    - [search yahoo](#search-yahoo)\n    - [search ebay](#search-ebay)\n    - [search home depot](#search-home-depot)\n    - [search youtube](#search-youtube)\n    - [search google scholar](#search-google-scholar)\n    - [generic search with serpapiclient](#generic-search-with-serpapiclient)\n    - [search google images](#search-google-images)\n    - [search google news](#search-google-news)\n    - [search google shopping](#search-google-shopping)\n    - [google search by location](#google-search-by-location)\n    - [batch asynchronous searches](#batch-asynchronous-searches)\n    - [python object as a result](#python-object-as-a-result)\n    - [python paginate using iterator](#pagination-using-iterator)\n    - [error management](#error-management)\n  - [change log](#change-log)\n  - [conclusion](#conclusion)\n\n### google search api capability\nsource code.\n```python\nparams = {\n  \"q\": \"coffee\",\n  \"location\": \"location requested\", \n  \"device\": \"desktop|mobile|tablet\",\n  \"hl\": \"google ui language\",\n  \"gl\": \"google country\",\n  \"safe\": \"safe search flag\",\n  \"num\": \"number of results\",\n  \"start\": \"pagination offset\",\n  \"api_key\": \"your serp api key\", \n  # to be match\n  \"tbm\": \"nws|isch|shop\", \n  # to be search\n  \"tbs\": \"custom to be search criteria\",\n  # allow async request\n  \"async\": \"true|false\",\n  # output format\n  \"output\": \"json|html\"\n}\n\n# define the search search\nsearch = googlesearch(params)\n# override an existing parameter\nsearch.params_dict[\"location\"] = \"portland\"\n# search format return as raw html\nhtml_results = search.get_html()\n# parse results\n#  as python dictionary\ndict_results = search.get_dict()\n#  as json using json package\njson_results = search.get_json()\n#  as dynamic python object\nobject_result = search.get_object()\n```\n[link to the full documentation](https://serpapi.com/search-api)\n\nsee below for more hands-on examples.\n\n### how to set serp api key\n\nyou can get an api key here if you don't already have one: https://serpapi.com/users/sign_up\n\nthe serpapi `api_key` can be set globally:\n```python\ngooglesearch.serp_api_key = \"your private key\"\n```\nthe serpapi `api_key` can be provided for each search:\n```python\nquery = googlesearch({\"q\": \"coffee\", \"serp_api_key\": \"your private key\"})\n```\n\n### example by specification\n\nwe love true open source, continuous integration and test driven development (tdd). \n we are using rspec to test [our infrastructure around the clock](https://travis-ci.org/serpapi/google-search-results-python) to achieve the best quality of service (qos).\n \nthe directory test/ includes specification/examples.\n\nset your api key.\n```bash\nexport api_key=\"your secret key\"\n```\n\nrun test\n```python\nmake test\n```\n\n### location api\n\n```python\nfrom serpapi import googlesearch\nsearch = googlesearch({})\nlocation_list = search.get_location(\"austin\", 3)\nprint(location_list)\n```\n\nthis prints the first 3 locations matching austin (texas, texas, rochester).\n```python\n[   {   'canonical_name': 'austin,tx,texas,united states',\n        'country_code': 'us',\n        'google_id': 200635,\n        'google_parent_id': 21176,\n        'gps': [-97.7430608, 30.267153],\n        'id': '585069bdee19ad271e9bc072',\n        'keys': ['austin', 'tx', 'texas', 'united', 'states'],\n        'name': 'austin, tx',\n        'reach': 5560000,\n        'target_type': 'dma region'},\n        ...]\n```\n\n### search archive api\n\nthe search results are stored in a temporary cache.\nthe previous search can be retrieved from the cache for free.\n\n```python\nfrom serpapi import googlesearch\nsearch = googlesearch({\"q\": \"coffee\", \"location\": \"austin,texas\"})\nsearch_result = search.get_dictionary()\nassert search_result.get(\"error\") == none\nsearch_id = search_result.get(\"search_metadata\").get(\"id\")\nprint(search_id)\n```\n\nnow let's retrieve the previous search from the archive.\n\n```python\narchived_search_result = googlesearch({}).get_search_archive(search_id, 'json')\nprint(archived_search_result.get(\"search_metadata\").get(\"id\"))\n```\nthis prints the search result from the archive.\n\n### account api\n```python\nfrom serpapi import googlesearch\nsearch = googlesearch({})\naccount = search.get_account()\n```\nthis prints your account information.\n\n### search bing\n```python\nfrom serpapi import bingsearch\nsearch = bingsearch({\"q\": \"coffee\", \"location\": \"austin,texas\"})\ndata = search.get_dict()\n```\nthis code prints bing search results for coffee as a dictionary. \n\nhttps://serpapi.com/bing-search-api\n\n### search baidu\n```python\nfrom serpapi import baidusearch\nsearch = baidusearch({\"q\": \"coffee\"})\ndata = search.get_dict()\n```\nthis code prints baidu search results for coffee as a dictionary. \nhttps://serpapi.com/baidu-search-api\n\n### search yandex\n```python\nfrom serpapi import yandexsearch\nsearch = yandexsearch({\"text\": \"coffee\"})\ndata = search.get_dict()\n```\nthis code prints yandex search results for coffee as a dictionary. \n\nhttps://serpapi.com/yandex-search-api\n\n### search yahoo\n```python\nfrom serpapi import yahoosearch\nsearch = yahoosearch({\"p\": \"coffee\"})\ndata = search.get_dict()\n```\nthis code prints yahoo search results for coffee as a dictionary. \n\nhttps://serpapi.com/yahoo-search-api\n\n\n### search ebay\n```python\nfrom serpapi import ebaysearch\nsearch = ebaysearch({\"_nkw\": \"coffee\"})\ndata = search.get_dict()\n```\nthis code prints ebay search results for coffee as a dictionary. \n\nhttps://serpapi.com/ebay-search-api\n\n### search home depot\n```python\nfrom serpapi import homedepotsearch\nsearch = homedepotsearch({\"q\": \"chair\"})\ndata = search.get_dict()\n```\nthis code prints home depot search results for chair as dictionary. \n\nhttps://serpapi.com/home-depot-search-api\n\n### search youtube\n```python\nfrom serpapi import homedepotsearch\nsearch = youtubesearch({\"q\": \"chair\"})\ndata = search.get_dict()\n```\nthis code prints youtube search results for chair as dictionary. \n\nhttps://serpapi.com/youtube-search-api\n\n### search google scholar\n```python\nfrom serpapi import googlescholarsearch\nsearch = googlescholarsearch({\"q\": \"coffee\"})\ndata = search.get_dict()\n```\nthis code prints google scholar search results.\n\n### search walmart\n```python\nfrom serpapi import walmartsearch\nsearch = walmartsearch({\"query\": \"chair\"})\ndata = search.get_dict()\n```\nthis code prints walmart search results.\n\n### search youtube\n```python\nfrom serpapi import youtubesearch\nsearch = youtubesearch({\"search_query\": \"chair\"})\ndata = search.get_dict()\n```\nthis code prints youtube search results.\n\n### search apple app store\n```python\nfrom serpapi import appleappstoresearch\nsearch = appleappstoresearch({\"term\": \"coffee\"})\ndata = search.get_dict()\n```\nthis code prints apple app store search results.\n\n### search naver\n```python\nfrom serpapi import naversearch\nsearch = naversearch({\"query\": \"chair\"})\ndata = search.get_dict()\n```\nthis code prints naver search results.\n\n### generic search with serpapiclient\n```python\nfrom serpapi import serpapiclient\nquery = {\"q\": \"coffee\", \"location\": \"austin,texas\", \"engine\": \"google\"}\nsearch = serpapiclient(query)\ndata = search.get_dict()\n```\nthis class enables interaction with any search engine supported by serpapi.com \n\n### search google images\n\n```python\nfrom serpapi import googlesearch\nsearch = googlesearch({\"q\": \"coffe\", \"tbm\": \"isch\"})\nfor image_result in search.get_dict()['images_results']:\n    link = image_result[\"original\"]\n    try:\n        print(\"link: \" + link)\n        # wget.download(link, '.')\n    except:\n        pass\n```\n\nthis code prints all the image links, \n and downloads the images if you un-comment the line with wget (linux/os x tool to download files).\n\nthis tutorial covers more ground on this topic.\nhttps://github.com/serpapi/showcase-serpapi-tensorflow-keras-image-training\n\n### search google news\n\n```python\nfrom serpapi import googlesearch\nsearch = googlesearch({\n    \"q\": \"coffe\",   # search search\n    \"tbm\": \"nws\",  # news\n    \"tbs\": \"qdr:d\", # last 24h\n    \"num\": 10\n})\nfor offset in [0,1,2]:\n    search.params_dict[\"start\"] = offset * 10\n    data = search.get_dict()\n    for news_result in data['news_results']:\n        print(str(news_result['position'] + offset * 10) + \" - \" + news_result['title'])\n```\n\nthis script prints the first 3 pages of the news headlines for the last 24 hours.\n\n### search google shopping\n\n```python\nfrom serpapi import googlesearch\nsearch = googlesearch({\n    \"q\": \"coffe\",   # search search\n    \"tbm\": \"shop\",  # news\n    \"tbs\": \"p_ord:rv\", # last 24h\n    \"num\": 100\n})\ndata = search.get_dict()\nfor shopping_result in data['shopping_results']:\n    print(shopping_result['position']) + \" - \" + shopping_result['title'])\n\n```\n\nthis script prints all the shopping results, ordered by review order.\n\n### google search by location\n\nwith serpapi, we can build a google search from anywhere in the world.\nthis code looks for the best coffee shop for the given cities.\n\n```python\nfrom serpapi import googlesearch\nfor city in [\"new york\", \"paris\", \"berlin\"]:\n  location = googlesearch({}).get_location(city, 1)[0][\"canonical_name\"]\n  search = googlesearch({\n      \"q\": \"best coffee shop\",   # search search\n      \"location\": location,\n      \"num\": 1,\n      \"start\": 0\n  })\n  data = search.get_dict()\n  top_result = data[\"organic_results\"][0][\"title\"]\n```\n\n### batch asynchronous searches\n\nwe offer two ways to boost your searches thanks to the`async` parameter.\n - blocking - async=false - more compute intensive because the search needs to maintain many connections. (default) \n- non-blocking - async=true - the way to go for large batches of queries  (recommended)\n\n```python\n# operating system\nimport os\n\n# regular expression library\nimport re\n\n# safe queue (named queue in python2)\nfrom queue import queue\n\n# time utility\nimport time\n\n# serpapi search\nfrom serpapi import googlesearch\n\n# store searches\nsearch_queue = queue()\n\n# serpapi search\nsearch = googlesearch({\n    \"location\": \"austin,texas\",\n    \"async\": true,\n    \"api_key\": os.getenv(\"api_key\")\n})\n\n# loop through a list of companies\nfor company in ['amd', 'nvidia', 'intel']:\n    print(\"execute async search: q = \" + company)\n    search.params_dict[\"q\"] = company\n    result = search.get_dict()\n    if \"error\" in result:\n        print(\"oops error: \", result[\"error\"])\n        continue\n    print(\"add search to the queue where id: \", result['search_metadata'])\n    # add search to the search_queue\n    search_queue.put(result)\n\nprint(\"wait until all search statuses are cached or success\")\n\n# create regular search\nwhile not search_queue.empty():\n    result = search_queue.get()\n    search_id = result['search_metadata']['id']\n\n    # retrieve search from the archive - blocker\n    print(search_id + \": get search from archive\")\n    search_archived = search.get_search_archive(search_id)\n    print(search_id + \": status = \" +\n          search_archived['search_metadata']['status'])\n\n    # check status\n    if re.search('cached|success',\n                 search_archived['search_metadata']['status']):\n        print(search_id + \": search done with q = \" +\n              search_archived['search_parameters']['q'])\n    else:\n        # requeue search_queue\n        print(search_id + \": requeue search\")\n        search_queue.put(result)\n\n        # wait 1s\n        time.sleep(1)\n\nprint('all searches completed')\n```\n\nthis code shows how to run searches asynchronously.\nthe search parameters must have {async: true}. this indicates that the client shouldn't wait for the search to be completed.\nthe current thread that executes the search is now non-blocking, which allows it to execute thousands of searches in seconds. the serpapi backend will do the processing work.\nthe actual search result is deferred to a later call from the search archive using get_search_archive(search_id).\nin this example the non-blocking searches are persisted in a queue: search_queue.\na loop through the search_queue allows it to fetch individual search results.\nthis process can easily be multithreaded to allow a large number of concurrent search requests.\nto keep things simple, this example only explores search results one at a time (single threaded).\n\n[see example.](https://github.com/serpapi/google-search-results-python/blob/master/tests/test_example.py)\n\n### python object as a result\n\nthe search results can be automatically wrapped in dynamically generated python object.\nthis solution offers a more dynamic, fully oriented object programming approach over the regular dictionary / json data structure.\n\n```python\nfrom serpapi import googlesearch\nsearch = googlesearch({\"q\": \"coffee\", \"location\": \"austin,texas\"})\nr = search.get_object()\nassert type(r.organic_results), list\nassert r.organic_results[0].title\nassert r.search_metadata.id\nassert r.search_metadata.google_url\nassert r.search_parameters.q, \"coffee\"\nassert r.search_parameters.engine, \"google\"\n```\n\n### pagination using iterator\nlet's collect links across multiple search results pages.\n```python\n# to get 2 pages\nstart = 0\nend = 40\npage_size = 10\n\n# basic search parameters\nparameter = {\n  \"q\": \"coca cola\",\n  \"tbm\": \"nws\",\n  \"api_key\": os.getenv(\"api_key\"),\n  # optional pagination parameter\n  #  the pagination method can take argument directly\n  \"start\": start,\n  \"end\": end,\n  \"num\": page_size\n}\n\n# as proof of concept \n# urls collects\nurls = []\n\n# initialize a search\nsearch = googlesearch(parameter)\n\n# create a python generator using parameter\npages = search.pagination()\n# or set custom parameter\npages = search.pagination(start, end, page_size)\n\n# fetch one search result per iteration \n# using a basic python for loop \n# which invokes python iterator under the hood.\nfor page in pages:\n  print(f\"current page: {page['serpapi_pagination']['current']}\")\n  for news_result in page[\"news_results\"]:\n    print(f\"title: {news_result['title']}\\nlink: {news_result['link']}\\n\")\n    urls.append(news_result['link'])\n  \n# check if the total number pages is as expected\n# note: the exact number if variable depending on the search engine backend\nif len(urls) == (end - start):\n  print(\"all search results count match!\")\nif len(urls) == len(set(urls)):\n  print(\"all search results are unique!\")\n```\n\nexamples to fetch links with pagination: [test file](https://github.com/serpapi/google-search-results-python/blob/master/tests/test_example_paginate.py), [online ide](https://replit.com/@dimitryzub1/scrape-google-news-with-pagination-python-serpapi)\n\n### error management\n\nserpapi keeps error management simple.\n - backend service error or search fail\n - client error\n\nif it's a backend error, a simple error message is returned as string in the server response.\n```python\nfrom serpapi import googlesearch\nsearch = googlesearch({\"q\": \"coffee\", \"location\": \"austin,texas\", \"api_key\": \"<secret_key>\"})\ndata = search.get_json()\nassert data[\"error\"] == none\n```\nin some cases, there are more details available in the data object.\n\nif it's a client error, then a serpapiclientexception is raised.\n\n## change log\n2023-03-10 @ 2.4.2\n - change long description to readme.md\n\n2021-12-22 @ 2.4.1\n - add more search engine \n   - youtube\n   - walmart\n   - apple_app_store\n   - naver \n - raise serpapiclientexception instead of raw string in order to follow python guideline 3.5+\n - add more unit error tests for serp_api_client\n\n2021-07-26 @ 2.4.0\n - add page size support using num parameter\n - add youtube search engine\n\n2021-06-05 @ 2.3.0\n - add pagination support\n\n2021-04-28 @ 2.2.0\n - add get_response method to provide raw requests.response object\n\n2021-04-04 @ 2.1.0\n - add home depot search engine\n - get_object() returns dynamic python object\n \n2020-10-26 @ 2.0.0\n - reduce class name to <engine>search\n - add get_raw_json\n\n2020-06-30 @ 1.8.3\n - simplify import\n - improve package for python 3.5+\n - add support for python 3.5 and 3.6\n\n2020-03-25 @ 1.8\n - add support for yandex, yahoo, ebay\n - clean-up test\n\n2019-11-10 @ 1.7.1\n - increase engine parameter priority over engine value set in the class\n\n2019-09-12 @ 1.7\n - change  namespace \"from lib.\" instead: \"from serpapi import googlesearch\"\n - support for bing and baidu\n\n2019-06-25 @ 1.6\n - new search engine supported: baidu and bing\n\n## conclusion\nserpapi supports all the major search engines. google has the more advance support with all the major services available: images, news, shopping and more..\nto enable a type of search, the field tbm (to be matched) must be set to:\n\n * isch: google images api.\n * nws: google news api.\n * shop: google shopping api.\n * any other google service should work out of the box.\n * (no tbm parameter): regular google search.\n\nthe field `tbs` allows to customize the search even more.\n\n[the full documentation is available here.](https://serpapi.com/search-api)\n",
  "docs_url": null,
  "keywords": "scrape,serp,api,json,search,localized,rank,google,bing,baidu,yandex,yahoo,ebay,scale,datamining,training,machine,ml,youtube,naver,walmart,apple,store,app",
  "license": "mit",
  "name": "google-search-results",
  "package_url": "https://pypi.org/project/google-search-results/",
  "project_url": "https://pypi.org/project/google-search-results/",
  "project_urls": {
    "Homepage": "https://github.com/serpapi/google-search-results-python"
  },
  "release_url": "https://pypi.org/project/google-search-results/2.4.2/",
  "requires_dist": [],
  "requires_python": ">=3.5",
  "summary": "scrape and search localized results from google, bing, baidu, yahoo, yandex, ebay, homedepot, youtube at scale using serpapi.com",
  "version": "2.4.2",
  "releases": [],
  "developers": [
    "victor@serpapi.com",
    "vikoky"
  ],
  "kwds": "googlesearch yandexsearch api walmartsearch bingsearch",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_google_search_results",
  "homepage": "https://github.com/serpapi/google-search-results-python",
  "release_count": 41,
  "dependency_ids": []
}