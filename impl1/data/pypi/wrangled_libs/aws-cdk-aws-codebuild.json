{
  "classifiers": [
    "development status :: 7 - inactive",
    "framework :: aws cdk",
    "framework :: aws cdk :: 1",
    "intended audience :: developers",
    "license :: osi approved",
    "operating system :: os independent",
    "programming language :: javascript",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "typing :: typed"
  ],
  "description": "# aws codebuild construct library\n\n<!--begin stability banner-->---\n\n\n![end-of-support](https://img.shields.io/badge/end--of--support-critical.svg?style=for-the-badge)\n\n> aws cdk v1 has reached end-of-support on 2023-06-01.\n> this package is no longer being updated, and users should migrate to aws cdk v2.\n>\n> for more information on how to migrate, see the [*migrating to aws cdk v2* guide](https://docs.aws.amazon.com/cdk/v2/guide/migrating-v2.html).\n\n---\n<!--end stability banner-->\n\naws codebuild is a fully managed continuous integration service that compiles\nsource code, runs tests, and produces software packages that are ready to\ndeploy. with codebuild, you don\u2019t need to provision, manage, and scale your own\nbuild servers. codebuild scales continuously and processes multiple builds\nconcurrently, so your builds are not left waiting in a queue. you can get\nstarted quickly by using prepackaged build environments, or you can create\ncustom build environments that use your own build tools. with codebuild, you are\ncharged by the minute for the compute resources you use.\n\n## installation\n\ninstall the module:\n\n```console\n$ npm i @aws-cdk/aws-codebuild\n```\n\nimport it into your code:\n\n```python\nimport aws_cdk.aws_codebuild as codebuild\n```\n\nthe `codebuild.project` construct represents a build project resource. see the\nreference documentation for a comprehensive list of initialization properties,\nmethods and attributes.\n\n## source\n\nbuild projects are usually associated with a *source*, which is specified via\nthe `source` property which accepts a class that extends the `source`\nabstract base class.\nthe default is to have no source associated with the build project;\nthe `buildspec` option is required in that case.\n\nhere's a codebuild project with no source which simply prints `hello, codebuild!`:\n\n```python\ncodebuild.project(self, \"myproject\",\n    build_spec=codebuild.buildspec.from_object({\n        \"version\": \"0.2\",\n        \"phases\": {\n            \"build\": {\n                \"commands\": [\"echo \\\"hello, codebuild!\\\"\"\n                ]\n            }\n        }\n    })\n)\n```\n\n### `codecommitsource`\n\nuse an aws codecommit repository as the source of this build:\n\n```python\nimport aws_cdk.aws_codecommit as codecommit\n\n\nrepository = codecommit.repository(self, \"myrepo\", repository_name=\"foo\")\ncodebuild.project(self, \"myfirstcodecommitproject\",\n    source=codebuild.source.code_commit(repository=repository)\n)\n```\n\n### `s3source`\n\ncreate a codebuild project with an s3 bucket as the source:\n\n```python\nbucket = s3.bucket(self, \"mybucket\")\n\ncodebuild.project(self, \"myproject\",\n    source=codebuild.source.s3(\n        bucket=bucket,\n        path=\"path/to/file.zip\"\n    )\n)\n```\n\nthe codebuild role will be granted to read just the given path from the given `bucket`.\n\n### `githubsource` and `githubenterprisesource`\n\nthese source types can be used to build code from a github repository.\nexample:\n\n```python\ngit_hub_source = codebuild.source.git_hub(\n    owner=\"awslabs\",\n    repo=\"aws-cdk\",\n    webhook=true,  # optional, default: true if `webhookfilters` were provided, false otherwise\n    webhook_triggers_batch_build=true,  # optional, default is false\n    webhook_filters=[\n        codebuild.filtergroup.in_event_of(codebuild.eventaction.push).and_branch_is(\"master\").and_commit_message_is(\"the commit message\")\n    ]\n)\n```\n\nto provide github credentials, please either go to aws codebuild console to connect\nor call `importsourcecredentials` to persist your personal access token.\nexample:\n\n```console\naws codebuild import-source-credentials --server-type github --auth-type personal_access_token --token <token_value>\n```\n\n### `bitbucketsource`\n\nthis source type can be used to build code from a bitbucket repository.\n\n```python\nbb_source = codebuild.source.bit_bucket(\n    owner=\"owner\",\n    repo=\"repo\"\n)\n```\n\n### for all git sources\n\nfor all git sources, you can fetch submodules while cloing git repo.\n\n```python\ngit_hub_source = codebuild.source.git_hub(\n    owner=\"awslabs\",\n    repo=\"aws-cdk\",\n    fetch_submodules=true\n)\n```\n\n## artifacts\n\ncodebuild projects can produce artifacts and upload them to s3. for example:\n\n```python\n# bucket: s3.bucket\n\n\nproject = codebuild.project(self, \"myproject\",\n    build_spec=codebuild.buildspec.from_object({\n        \"version\": \"0.2\"\n    }),\n    artifacts=codebuild.artifacts.s3(\n        bucket=bucket,\n        include_build_id=false,\n        package_zip=true,\n        path=\"another/path\",\n        identifier=\"addartifact1\"\n    )\n)\n```\n\nif you'd prefer your buildspec to be rendered as yaml in the template,\nuse the `fromobjecttoyaml()` method instead of `fromobject()`.\n\nbecause we've not set the `name` property, this example will set the\n`overrideartifactname` parameter, and produce an artifact named as defined in\nthe buildspec file, uploaded to an s3 bucket (`bucket`). the path will be\n`another/path` and the artifact will be a zipfile.\n\n## codepipeline\n\nto add a codebuild project as an action to codepipeline,\nuse the `pipelineproject` class instead of `project`.\nit's a simple class that doesn't allow you to specify `sources`,\n`secondarysources`, `artifacts` or `secondaryartifacts`,\nas these are handled by setting input and output codepipeline `artifact` instances on the action,\ninstead of setting them on the project.\n\n```python\nproject = codebuild.pipelineproject(self, \"project\")\n```\n\nfor more details, see the readme of the `@aws-cdk/@aws-codepipeline-actions` package.\n\n## caching\n\nyou can save time when your project builds by using a cache. a cache can store reusable pieces of your build environment and use them across multiple builds. your build project can use one of two types of caching: amazon s3 or local. in general, s3 caching is a good option for small and intermediate build artifacts that are more expensive to build than to download. local caching is a good option for large intermediate build artifacts because the cache is immediately available on the build host.\n\n### s3 caching\n\nwith s3 caching, the cache is stored in an s3 bucket which is available\nregardless from what codebuild instance gets selected to run your codebuild job\non. when using s3 caching, you must also add in a `cache` section to your\nbuildspec which indicates the files to be cached:\n\n```python\n# my_caching_bucket: s3.bucket\n\n\ncodebuild.project(self, \"project\",\n    source=codebuild.source.bit_bucket(\n        owner=\"awslabs\",\n        repo=\"aws-cdk\"\n    ),\n\n    cache=codebuild.cache.bucket(my_caching_bucket),\n\n    # buildspec with a 'cache' section necessary for s3 caching. this can\n    # also come from 'buildspec.yml' in your source.\n    build_spec=codebuild.buildspec.from_object({\n        \"version\": \"0.2\",\n        \"phases\": {\n            \"build\": {\n                \"commands\": [\"...\"]\n            }\n        },\n        \"cache\": {\n            \"paths\": [\"/root/cachedir/**/*\"\n            ]\n        }\n    })\n)\n```\n\nnote that two different codebuild projects using the same s3 bucket will *not*\nshare their cache: each project will get a unique file in the s3 bucket to store\nthe cache in.\n\n### local caching\n\nwith local caching, the cache is stored on the codebuild instance itself. this\nis simple, cheap and fast, but codebuild cannot guarantee a reuse of instance\nand hence cannot guarantee cache hits. for example, when a build starts and\ncaches files locally, if two subsequent builds start at the same time afterwards\nonly one of those builds would get the cache. three different cache modes are\nsupported, which can be turned on individually.\n\n* `localcachemode.source` caches git metadata for primary and secondary sources.\n* `localcachemode.docker_layer` caches existing docker layers.\n* `localcachemode.custom` caches directories you specify in the buildspec file.\n\n```python\ncodebuild.project(self, \"project\",\n    source=codebuild.source.git_hub_enterprise(\n        https_clone_url=\"https://my-github-enterprise.com/owner/repo\"\n    ),\n\n    # enable docker and custom caching\n    cache=codebuild.cache.local(codebuild.localcachemode.docker_layer, codebuild.localcachemode.custom),\n\n    # buildspec with a 'cache' section necessary for 'custom' caching. this can\n    # also come from 'buildspec.yml' in your source.\n    build_spec=codebuild.buildspec.from_object({\n        \"version\": \"0.2\",\n        \"phases\": {\n            \"build\": {\n                \"commands\": [\"...\"]\n            }\n        },\n        \"cache\": {\n            \"paths\": [\"/root/cachedir/**/*\"\n            ]\n        }\n    })\n)\n```\n\n## environment\n\nby default, projects use a small instance with an ubuntu 18.04 image. you\ncan use the `environment` property to customize the build environment:\n\n* `buildimage` defines the docker image used. see [images](#images) below for\n  details on how to define build images.\n* `certificate` defines the location of a pem encoded certificate to import.\n* `computetype` defines the instance type used for the build.\n* `privileged` can be set to `true` to allow privileged access.\n* `environmentvariables` can be set at this level (and also at the project\n  level).\n\n## images\n\nthe codebuild library supports both linux and windows images via the\n`linuxbuildimage` (or `linuxarmbuildimage`), and `windowsbuildimage` classes, respectively.\n\nyou can specify one of the predefined windows/linux images by using one\nof the constants such as `windowsbuildimage.win_server_core_2019_base`,\n`windowsbuildimage.windows_base_2_0`, `linuxbuildimage.standard_2_0`, or\n`linuxarmbuildimage.amazon_linux_2_arm`.\n\nalternatively, you can specify a custom image using one of the static methods on\n`linuxbuildimage`:\n\n* `linuxbuildimage.fromdockerregistry(image[, { secretsmanagercredentials }])` to reference an image in any public or private docker registry.\n* `linuxbuildimage.fromecrrepository(repo[, tag])` to reference an image available in an\n  ecr repository.\n* `linuxbuildimage.fromasset(parent, id, props)` to use an image created from a\n  local asset.\n* `linuxbuildimage.fromcodebuildimageid(id)` to reference a pre-defined, codebuild-provided docker image.\n\nor one of the corresponding methods on `windowsbuildimage`:\n\n* `windowsbuildimage.fromdockerregistry(image[, { secretsmanagercredentials }, imagetype])`\n* `windowsbuildimage.fromecrrepository(repo[, tag, imagetype])`\n* `windowsbuildimage.fromasset(parent, id, props, [, imagetype])`\n\nor one of the corresponding methods on `linuxarmbuildimage`:\n\n* `linuxarmbuildimage.fromecrrepository(repo[, tag])`\n\nnote that the `windowsbuildimage` version of the static methods accepts an optional parameter of type `windowsimagetype`,\nwhich can be either `windowsimagetype.standard`, the default, or `windowsimagetype.server_2019`:\n\n```python\n# ecr_repository: ecr.repository\n\n\ncodebuild.project(self, \"project\",\n    environment=codebuild.buildenvironment(\n        build_image=codebuild.windowsbuildimage.from_ecr_repository(ecr_repository, \"v1.0\", codebuild.windowsimagetype.server_2019),\n        # optional certificate to include in the build image\n        certificate=codebuild.buildenvironmentcertificate(\n            bucket=s3.bucket.from_bucket_name(self, \"bucket\", \"my-bucket\"),\n            object_key=\"path/to/cert.pem\"\n        )\n    )\n)\n```\n\nthe following example shows how to define an image from a docker asset:\n\n```python\nenvironment=codebuild.buildenvironment(\n    build_image=codebuild.linuxbuildimage.from_asset(self, \"myimage\",\n        directory=path.join(__dirname, \"demo-image\")\n    )\n)\n```\n\nthe following example shows how to define an image from an ecr repository:\n\n```python\nenvironment=codebuild.buildenvironment(\n    build_image=codebuild.linuxbuildimage.from_ecr_repository(ecr_repository, \"v1.0\")\n)\n```\n\nthe following example shows how to define an image from a private docker registry:\n\n```python\nenvironment=codebuild.buildenvironment(\n    build_image=codebuild.linuxbuildimage.from_docker_registry(\"my-registry/my-repo\",\n        secrets_manager_credentials=secrets\n    )\n)\n```\n\n### gpu images\n\nthe class `linuxgpubuildimage` contains constants for working with\n[aws deep learning container images](https://aws.amazon.com/releasenotes/available-deep-learning-containers-images):\n\n```python\ncodebuild.project(self, \"project\",\n    environment=codebuild.buildenvironment(\n        build_image=codebuild.linuxgpubuildimage.dlc_tensorflow_2_1_0_inference\n    )\n)\n```\n\none complication is that the repositories for the dlc images are in\ndifferent accounts in different aws regions.\nin most cases, the cdk will handle providing the correct account for you;\nin rare cases (for example, deploying to new regions)\nwhere our information might be out of date,\nyou can always specify the account\n(along with the repository name and tag)\nexplicitly using the `awsdeeplearningcontainersimage` method:\n\n```python\ncodebuild.project(self, \"project\",\n    environment=codebuild.buildenvironment(\n        build_image=codebuild.linuxgpubuildimage.aws_deep_learning_containers_image(\"tensorflow-inference\", \"2.1.0-gpu-py36-cu101-ubuntu18.04\", \"123456789012\")\n    )\n)\n```\n\nalternatively, you can reference an image available in an ecr repository using the `linuxgpubuildimage.fromecrrepository(repo[, tag])` method.\n\n## logs\n\ncodebuild lets you specify an s3 bucket, cloudwatch log group or both to receive logs from your projects.\n\nby default, logs will go to cloudwatch.\n\n### cloudwatch logs example\n\n```python\ncodebuild.project(self, \"project\",\n    logging=codebuild.loggingoptions(\n        cloud_watch=codebuild.cloudwatchloggingoptions(\n            log_group=logs.loggroup(self, \"myloggroup\")\n        )\n    )\n)\n```\n\n### s3 logs example\n\n```python\ncodebuild.project(self, \"project\",\n    logging=codebuild.loggingoptions(\n        s3=codebuild.s3loggingoptions(\n            bucket=s3.bucket(self, \"logbucket\")\n        )\n    )\n)\n```\n\n## credentials\n\ncodebuild allows you to store credentials used when communicating with various sources,\nlike github:\n\n```python\ncodebuild.githubsourcecredentials(self, \"codebuildgithubcreds\",\n    access_token=secretvalue.secrets_manager(\"my-token\")\n)\n```\n\nand bitbucket:\n\n```python\ncodebuild.bitbucketsourcecredentials(self, \"codebuildbitbucketcreds\",\n    username=secretvalue.secrets_manager(\"my-bitbucket-creds\", json_field=\"username\"),\n    password=secretvalue.secrets_manager(\"my-bitbucket-creds\", json_field=\"password\")\n)\n```\n\n**note**: the credentials are global to a given account in a given region -\nthey are not defined per codebuild project.\ncodebuild only allows storing a single credential of a given type\n(github, github enterprise or bitbucket)\nin a given account in a given region -\nany attempt to save more than one will result in an error.\nyou can use the [`list-source-credentials` aws cli operation](https://docs.aws.amazon.com/cli/latest/reference/codebuild/list-source-credentials.html)\nto inspect what credentials are stored in your account.\n\n## test reports\n\nyou can specify a test report in your buildspec:\n\n```python\nproject = codebuild.project(self, \"project\",\n    build_spec=codebuild.buildspec.from_object({\n        # ...\n        \"reports\": {\n            \"my_report\": {\n                \"files\": \"**/*\",\n                \"base-directory\": \"build/test-results\"\n            }\n        }\n    })\n)\n```\n\nthis will create a new test report group,\nwith the name `<projectname>-myreport`.\n\nthe project's role in the cdk will always be granted permissions to create and use report groups\nwith names starting with the project's name;\nif you'd rather not have those permissions added,\nyou can opt out of it when creating the project:\n\n```python\n# source: codebuild.source\n\n\nproject = codebuild.project(self, \"project\",\n    source=source,\n    grant_report_group_permissions=false\n)\n```\n\nalternatively, you can specify an arn of an existing resource group,\ninstead of a simple name, in your buildspec:\n\n```python\n# source: codebuild.source\n\n\n# create a new reportgroup\nreport_group = codebuild.reportgroup(self, \"reportgroup\")\n\nproject = codebuild.project(self, \"project\",\n    source=source,\n    build_spec=codebuild.buildspec.from_object({\n        # ...\n        \"reports\": {\n            \"report_group.report_group_arn\": {\n                \"files\": \"**/*\",\n                \"base-directory\": \"build/test-results\"\n            }\n        }\n    })\n)\n```\n\nif you do that, you need to grant the project's role permissions to write reports to that report group:\n\n```python\n# project: codebuild.project\n# report_group: codebuild.reportgroup\n\n\nreport_group.grant_write(project)\n```\n\nfor more information on the test reports feature,\nsee the [aws codebuild documentation](https://docs.aws.amazon.com/codebuild/latest/userguide/test-reporting.html).\n\n## events\n\ncodebuild projects can be used either as a source for events or be triggered\nby events via an event rule.\n\n### using project as an event target\n\nthe `@aws-cdk/aws-events-targets.codebuildproject` allows using an aws codebuild\nproject as a aws cloudwatch event rule target:\n\n```python\n# start build when a commit is pushed\nimport aws_cdk.aws_codecommit as codecommit\nimport aws_cdk.aws_events_targets as targets\n\n# code_commit_repository: codecommit.repository\n# project: codebuild.project\n\n\ncode_commit_repository.on_commit(\"oncommit\",\n    target=targets.codebuildproject(project)\n)\n```\n\n### using project as an event source\n\nto define amazon cloudwatch event rules for build projects, use one of the `onxxx`\nmethods:\n\n```python\nimport aws_cdk.aws_events_targets as targets\n# fn: lambda.function\n# project: codebuild.project\n\n\nrule = project.on_state_change(\"buildstatechange\",\n    target=targets.lambdafunction(fn)\n)\n```\n\n## codestar notifications\n\nto define codestar notification rules for projects, use one of the `notifyonxxx()` methods.\nthey are very similar to `onxxx()` methods for cloudwatch events:\n\n```python\nimport aws_cdk.aws_chatbot as chatbot\n\n# project: codebuild.project\n\n\ntarget = chatbot.slackchannelconfiguration(self, \"myslackchannel\",\n    slack_channel_configuration_name=\"your_channel_name\",\n    slack_workspace_id=\"your_slack_workspace_id\",\n    slack_channel_id=\"your_slack_channel_id\"\n)\n\nrule = project.notify_on_build_succeeded(\"notifyonbuildsucceeded\", target)\n```\n\n## secondary sources and artifacts\n\ncodebuild projects can get their sources from multiple places, and produce\nmultiple outputs. for example:\n\n```python\nimport aws_cdk.aws_codecommit as codecommit\n# repo: codecommit.repository\n# bucket: s3.bucket\n\n\nproject = codebuild.project(self, \"myproject\",\n    secondary_sources=[\n        codebuild.source.code_commit(\n            identifier=\"source2\",\n            repository=repo\n        )\n    ],\n    secondary_artifacts=[\n        codebuild.artifacts.s3(\n            identifier=\"artifact2\",\n            bucket=bucket,\n            path=\"some/path\",\n            name=\"file.zip\"\n        )\n    ]\n)\n```\n\nnote that the `identifier` property is required for both secondary sources and\nartifacts.\n\nthe contents of the secondary source is available to the build under the\ndirectory specified by the `codebuild_src_dir_<identifier>` environment variable\n(so, `codebuild_src_dir_source2` in the above case).\n\nthe secondary artifacts have their own section in the buildspec, under the\nregular `artifacts` one. each secondary artifact has its own section, beginning\nwith their identifier.\n\nso, a buildspec for the above project could look something like this:\n\n```python\nproject = codebuild.project(self, \"myproject\",\n    # secondary sources and artifacts as above...\n    build_spec=codebuild.buildspec.from_object({\n        \"version\": \"0.2\",\n        \"phases\": {\n            \"build\": {\n                \"commands\": [\"cd $codebuild_src_dir_source2\", \"touch output2.txt\"\n                ]\n            }\n        },\n        \"artifacts\": {\n            \"secondary-artifacts\": {\n                \"artifact2\": {\n                    \"base-directory\": \"$codebuild_src_dir_source2\",\n                    \"files\": [\"output2.txt\"\n                    ]\n                }\n            }\n        }\n    })\n)\n```\n\n### definition of vpc configuration in codebuild project\n\ntypically, resources in an vpc are not accessible by aws codebuild. to enable\naccess, you must provide additional vpc-specific configuration information as\npart of your codebuild project configuration. this includes the vpc id, the\nvpc subnet ids, and the vpc security group ids. vpc-enabled builds are then\nable to access resources inside your vpc.\n\nfor further information see https://docs.aws.amazon.com/codebuild/latest/userguide/vpc-support.html\n\n**use cases**\nvpc connectivity from aws codebuild builds makes it possible to:\n\n* run integration tests from your build against data in an amazon rds database that's isolated on a private subnet.\n* query data in an amazon elasticache cluster directly from tests.\n* interact with internal web services hosted on amazon ec2, amazon ecs, or services that use internal elastic load balancing.\n* retrieve dependencies from self-hosted, internal artifact repositories, such as pypi for python, maven for java, and npm for node.js.\n* access objects in an amazon s3 bucket configured to allow access through an amazon vpc endpoint only.\n* query external web services that require fixed ip addresses through the elastic ip address of the nat gateway or nat instance associated with your subnet(s).\n\nyour builds can access any resource that's hosted in your vpc.\n\n**enable amazon vpc access in your codebuild projects**\n\npass the vpc when defining your project, then make sure to\ngive the codebuild's security group the right permissions\nto access the resources that it needs by using the\n`connections` object.\n\nfor example:\n\n```python\n# load_balancer: elbv2.applicationloadbalancer\n\n\nvpc = ec2.vpc(self, \"myvpc\")\nproject = codebuild.project(self, \"myproject\",\n    vpc=vpc,\n    build_spec=codebuild.buildspec.from_object({})\n)\n\nproject.connections.allow_to(load_balancer, ec2.port.tcp(443))\n```\n\n## project file system location efs\n\nadd support for codebuild to build on aws efs file system mounts using\nthe new projectfilesystemlocation.\nthe `filesystemlocations` property which accepts a list `projectfilesystemlocation`\nas represented by the interface `ifilesystemlocations`.\nthe only supported file system type is `efs`.\n\nfor example:\n\n```python\ncodebuild.project(self, \"myproject\",\n    build_spec=codebuild.buildspec.from_object({\n        \"version\": \"0.2\"\n    }),\n    file_system_locations=[\n        codebuild.filesystemlocation.efs(\n            identifier=\"myidentifier2\",\n            location=\"myclodation.mydnsroot.com:/loc\",\n            mount_point=\"/media\",\n            mount_options=\"opts\"\n        )\n    ]\n)\n```\n\nhere's a codebuild project with a simple example that creates a project mounted on aws efs:\n\n[minimal example](./test/integ.project-file-system-location.ts)\n\n## batch builds\n\nto enable batch builds you should call `enablebatchbuilds()` on the project instance.\n\nit returns an object containing the batch service role that was created,\nor `undefined` if batch builds could not be enabled, for example if the project was imported.\n\n```python\n# source: codebuild.source\n\n\nproject = codebuild.project(self, \"myproject\", source=source)\n\nif project.enable_batch_builds():\n    print(\"batch builds were enabled\")\n```\n\n## timeouts\n\nthere are two types of timeouts that can be set when creating your project.\nthe `timeout` property can be used to set an upper limit on how long your project is able to run without being marked as completed.\nthe default is 60 minutes.\nan example of overriding the default follows.\n\n```python\ncodebuild.project(self, \"myproject\",\n    timeout=duration.minutes(90)\n)\n```\n\nthe `queuedtimeout` property can be used to set an upper limit on how your project remains queued to run.\nthere is no default value for this property.\nas an example, to allow your project to queue for up to thirty (30) minutes before the build fails,\nuse the following code.\n\n```python\ncodebuild.project(self, \"myproject\",\n    queued_timeout=duration.minutes(30)\n)\n```\n\n## limiting concurrency\n\nby default if a new build is triggered it will be run even if there is a previous build already in progress.\nit is possible to limit the maximum concurrent builds to value between 1 and the account specific maximum limit.\nby default there is no explicit limit.\n\n```python\ncodebuild.project(self, \"myproject\",\n    concurrent_build_limit=1\n)\n```\n",
  "docs_url": null,
  "keywords": "",
  "license": "apache-2.0",
  "name": "aws-cdk.aws-codebuild",
  "package_url": "https://pypi.org/project/aws-cdk.aws-codebuild/",
  "project_url": "https://pypi.org/project/aws-cdk.aws-codebuild/",
  "project_urls": {
    "Homepage": "https://github.com/aws/aws-cdk",
    "Source": "https://github.com/aws/aws-cdk.git"
  },
  "release_url": "https://pypi.org/project/aws-cdk.aws-codebuild/1.204.0/",
  "requires_dist": [
    "aws-cdk.assets (==1.204.0)",
    "aws-cdk.aws-cloudwatch (==1.204.0)",
    "aws-cdk.aws-codecommit (==1.204.0)",
    "aws-cdk.aws-codestarnotifications (==1.204.0)",
    "aws-cdk.aws-ec2 (==1.204.0)",
    "aws-cdk.aws-ecr-assets (==1.204.0)",
    "aws-cdk.aws-ecr (==1.204.0)",
    "aws-cdk.aws-events (==1.204.0)",
    "aws-cdk.aws-iam (==1.204.0)",
    "aws-cdk.aws-kms (==1.204.0)",
    "aws-cdk.aws-logs (==1.204.0)",
    "aws-cdk.aws-s3-assets (==1.204.0)",
    "aws-cdk.aws-s3 (==1.204.0)",
    "aws-cdk.aws-secretsmanager (==1.204.0)",
    "aws-cdk.core (==1.204.0)",
    "aws-cdk.region-info (==1.204.0)",
    "constructs (<4.0.0,>=3.3.69)",
    "jsii (<2.0.0,>=1.84.0)",
    "publication (>=0.0.3)",
    "typeguard (~=2.13.3)"
  ],
  "requires_python": "~=3.7",
  "summary": "the cdk construct library for aws::codebuild",
  "version": "1.204.0",
  "releases": [],
  "developers": [
    "amazon_web_services"
  ],
  "kwds": "aws_codebuild aws_cdk aws_codecommit awslabs aws",
  "license_kwds": "apache-2.0",
  "libtype": "pypi",
  "id": "pypi_aws_cdk.aws_codebuild",
  "homepage": "https://github.com/aws/aws-cdk",
  "release_count": 258,
  "dependency_ids": [
    "pypi_aws_cdk.assets",
    "pypi_aws_cdk.aws_cloudwatch",
    "pypi_aws_cdk.aws_codecommit",
    "pypi_aws_cdk.aws_codestarnotifications",
    "pypi_aws_cdk.aws_ec2",
    "pypi_aws_cdk.aws_ecr",
    "pypi_aws_cdk.aws_ecr_assets",
    "pypi_aws_cdk.aws_events",
    "pypi_aws_cdk.aws_iam",
    "pypi_aws_cdk.aws_kms",
    "pypi_aws_cdk.aws_logs",
    "pypi_aws_cdk.aws_s3",
    "pypi_aws_cdk.aws_s3_assets",
    "pypi_aws_cdk.aws_secretsmanager",
    "pypi_aws_cdk.core",
    "pypi_aws_cdk.region_info",
    "pypi_constructs",
    "pypi_jsii",
    "pypi_publication",
    "pypi_typeguard"
  ]
}