{
  "classifiers": [
    "development status :: 4 - beta",
    "intended audience :: developers",
    "programming language :: python :: 3",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "# saliency library\n## updates\n\n&#x1f534;&nbsp;&nbsp; now framework-agnostic! [(example core notebook)](examples_core.ipynb) &nbsp;&#x1f534;\n\n&#x1f517;&nbsp;&nbsp; for further explanation of the methods and more examples of the resulting maps, see our [github pages website](https://pair-code.github.io/saliency)  &nbsp;&#x1f517;\n\nif upgrading from an older version, update old imports to `import saliency.tf1 as saliency`. we provide wrappers to make the framework-agnostic version compatible with tf1 models. [(example tf1 notebook)](examples_tf1.ipynb)\n\n&#x1f534;&nbsp;&nbsp; added performance information curve (pic) - a human\nindependent metric for evaluating the quality of saliency methods.\n([example notebook](https://github.com/pair-code/saliency/blob/master/pic_metrics.ipynb)) &nbsp;&#x1f534;\n\n## saliency methods\n\nthis repository contains code for the following saliency techniques:\n\n*   guided integrated gradients* ([paper](https://arxiv.org/abs/2106.09788), [poster](https://github.com/pair-code/saliency/blob/master/docs/cvpr_guided_ig_poster.pdf))\n*   xrai* ([paper](https://arxiv.org/abs/1906.02825), [poster](https://github.com/pair-code/saliency/blob/master/docs/iccv_xrai_poster.pdf))\n*   smoothgrad* ([paper](https://arxiv.org/abs/1706.03825))\n*   vanilla gradients\n    ([paper](https://scholar.google.com/scholar?q=visualizing+higher-layer+features+of+a+deep+network&btng=&hl=en&as_sdt=0%2c22),\n    [paper](https://arxiv.org/abs/1312.6034))\n*   guided backpropogation ([paper](https://arxiv.org/abs/1412.6806))\n*   integrated gradients ([paper](https://arxiv.org/abs/1703.01365))\n*   occlusion\n*   grad-cam ([paper](https://arxiv.org/abs/1610.02391))\n*   blur ig ([paper](https://arxiv.org/abs/2004.03383))\n\n\\*developed by pair.\n\nthis list is by no means comprehensive. we are accepting pull requests to add\nnew methods!\n\n## evaluation of saliency methods\n\nthe repository provides an implementation of performance information curve (pic) -\na human independent metric for evaluating the quality of saliency methods\n([paper](https://arxiv.org/abs/1906.02825),\n[poster](https://github.com/pair-code/saliency/blob/master/docs/iccv_xrai_poster.pdf),\n[code](https://github.com/pair-code/saliency/blob/master/saliency/metrics/pic.py),\n[notebook](https://github.com/pair-code/saliency/blob/master/pic_metrics.ipynb)).\n\n\n## download\n\n```\n# to install the core subpackage:\npip install saliency\n\n# to install core and tf1 subpackages:\npip install saliency[tf1]\n\n```\n\nor for the development version:\n```\ngit clone https://github.com/pair-code/saliency\ncd saliency\n```\n\n\n## usage\n\nthe saliency library has two subpackages:\n*\t`core` uses a generic `call_model_function` which can be used with any ml \n\tframework.\n*\t`tf1` accepts input/output tensors directly, and sets up the necessary \n\tgraph operations for each method.\n\n### core\n\neach saliency mask class extends from the `coresaliency` base class. this class\ncontains the following methods:\n\n*   `getmask(x_value, call_model_function, call_model_args=none)`: returns a mask\n    of\n    the shape of non-batched `x_value` given by the saliency technique.\n*   `getsmoothedmask(x_value, call_model_function, call_model_args=none, stdev_spread=.15, nsamples=25, magnitude=true)`: \n    returns a mask smoothed of the shape of non-batched `x_value` with the \n    smoothgrad technique.\n\n\nthe visualization module contains two methods for saliency visualization:\n\n* ```visualizeimagegrayscale(image_3d, percentile)```: marginalizes across the\n  absolute value of each channel to create a 2d single channel image, and clips\n  the image at the given percentile of the distribution. this method returns a\n  2d tensor normalized between 0 to 1.\n* ```visualizeimagediverging(image_3d, percentile)```: marginalizes across the\n  value of each channel to create a 2d single channel image, and clips the\n  image at the given percentile of the distribution. this method returns a\n  2d tensor normalized between -1 to 1 where zero remains unchanged.\n\nif the sign of the value given by the saliency mask is not important, then use\n```visualizeimagegrayscale```, otherwise use ```visualizeimagediverging```. see\nthe smoothgrad paper for more details on which visualization method to use.\n\n##### call_model_function\n`call_model_function` is how we pass inputs to a given model and receive the outputs\nnecessary to compute saliency masks. the description of this method and expected \noutput format is in the `coresaliency` description, as well as separately for each method.\n\n\n##### examples\n\n[this example ipython notebook](http://github.com/pair-code/saliency/blob/master/examples_core.ipynb)\nshowing these techniques is a good starting place.\n\nhere is a condensed example of using ig+smoothgrad with tensorflow 2:\n\n```\nimport saliency.core as saliency\nimport tensorflow as tf\n\n...\n\n# call_model_function construction here.\ndef call_model_function(x_value_batched, call_model_args, expected_keys):\n\ttape = tf.gradienttape()\n\tgrads = np.array(tape.gradient(output_layer, images))\n\treturn {saliency.input_output_gradients: grads}\n\n...\n\n# load data.\nimage = getimagepng(...)\n\n# compute ig+smoothgrad.\nig_saliency = saliency.integratedgradients()\nsmoothgrad_ig = ig_saliency.getsmoothedmask(image, \n\t\t\t\t\t\t\t\t\t\t\tcall_model_function, \n                                            call_model_args=none)\n\n# compute a 2d tensor for visualization.\ngrayscale_visualization = saliency.visualizeimagegrayscale(\n    smoothgrad_ig)\n```\n\n### tf1\n\neach saliency mask class extends from the `tf1saliency` base class. this class\ncontains the following methods:\n\n*   `__init__(graph, session, y, x)`: constructor of the saliencymask. this can\n    modify the graph, or sometimes create a new graph. often this will add nodes\n    to the graph, so this shouldn't be called continuously. `y` is the output\n    tensor to compute saliency masks with respect to, `x` is the input tensor\n    with the outer most dimension being batch size.\n*   `getmask(x_value, feed_dict)`: returns a mask of the shape of non-batched\n    `x_value` given by the saliency technique.\n*   `getsmoothedmask(x_value, feed_dict)`: returns a mask smoothed of the shape\n    of non-batched `x_value` with the smoothgrad technique.\n\nthe visualization module contains two visualization methods:\n\n* ```visualizeimagegrayscale(image_3d, percentile)```: marginalizes across the\n  absolute value of each channel to create a 2d single channel image, and clips\n  the image at the given percentile of the distribution. this method returns a\n  2d tensor normalized between 0 to 1.\n* ```visualizeimagediverging(image_3d, percentile)```: marginalizes across the\n  value of each channel to create a 2d single channel image, and clips the\n  image at the given percentile of the distribution. this method returns a\n  2d tensor normalized between -1 to 1 where zero remains unchanged.\n\nif the sign of the value given by the saliency mask is not important, then use\n```visualizeimagegrayscale```, otherwise use ```visualizeimagediverging```. see\nthe smoothgrad paper for more details on which visualization method to use.\n\n##### examples\n\n[this example ipython notebook](http://github.com/pair-code/saliency/blob/master/examples_tf1.ipynb) shows\nthese techniques is a good starting place.\n\nanother example of using guidedbackprop with smoothgrad from tensorflow:\n\n```\nfrom saliency.tf1 import guidedbackprop\nfrom saliency.tf1 import visualizeimagegrayscale\nimport tensorflow.compat.v1 as tf\n\n...\n# tensorflow graph construction here.\ny = logits[5]\nx = tf.placeholder(...)\n...\n\n# compute guided backprop.\n# note: this creates another graph that gets cached, try to avoid creating many\n# of these.\nguided_backprop_saliency = guidedbackprop(graph, session, y, x)\n\n...\n# load data.\nimage = getimagepng(...)\n...\n\nsmoothgrad_guided_backprop =\n    guided_backprop_saliency.getmask(image, feed_dict={...})\n\n# compute a 2d tensor for visualization.\ngrayscale_visualization = visualization.visualizeimagegrayscale(\n    smoothgrad_guided_backprop)\n```\n\n## conclusion/disclaimer\n\nif you have any questions or suggestions for improvements to this library,\nplease contact the owners of the `pair-code/saliency` repository.\n\nthis is not an official google product.\n\n",
  "docs_url": null,
  "keywords": "saliency mask neural network deep learning",
  "license": "apache 2.0",
  "name": "saliency",
  "package_url": "https://pypi.org/project/saliency/",
  "project_url": "https://pypi.org/project/saliency/",
  "project_urls": {
    "Homepage": "https://github.com/pair-code/saliency"
  },
  "release_url": "https://pypi.org/project/saliency/0.2.0/",
  "requires_dist": [
    "numpy",
    "scikit-image",
    "tensorflow (>=1.15) ; extra == 'full'",
    "tensorflow (>=1.15) ; extra == 'tf1'"
  ],
  "requires_python": "",
  "summary": "framework-agnostic saliency methods",
  "version": "0.2.0",
  "releases": [],
  "developers": [
    "tf-saliency-dev@google.com",
    "the_saliency_authors"
  ],
  "kwds": "guided_backprop_saliency saliency saliencymask ig_saliency tensorflow",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_saliency",
  "homepage": "https://github.com/pair-code/saliency",
  "release_count": 10,
  "dependency_ids": [
    "pypi_numpy",
    "pypi_scikit_image",
    "pypi_tensorflow"
  ]
}