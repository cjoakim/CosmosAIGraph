{
  "classifiers": [
    "development status :: 5 - production/stable",
    "framework :: pytest",
    "intended audience :: developers",
    "license :: osi approved :: mit license",
    "operating system :: os independent",
    "programming language :: python",
    "programming language :: python :: 2",
    "programming language :: python :: 2.7",
    "programming language :: python :: 3",
    "programming language :: python :: 3.4",
    "programming language :: python :: 3.5",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: implementation :: cpython",
    "programming language :: python :: implementation :: pypy",
    "topic :: software development :: testing"
  ],
  "description": "# pytest-depends\n\nthis pytest plugin allows you to declare dependencies between pytest tests, where dependent tests will not run if the\ntests they depend on did not succeed.\n\nof course, tests should be self contained whenever possible, but that doesn't mean this doesn't have good uses.\n\nthis can be useful for when the failing of a test means that another test cannot possibly succeed either, especially\nwith slower tests. this isn't a dependency in the sense of test a sets up stuff for test b, but more in the sense of if\ntest a failed there's no reason to bother with test b either.\n\n## installation\n\nsimply install using `pip` (or `easy_install`):\n\n```\npip install pytest-depends\n```\n\n## usage\n\n``` python\nbuild_path = 'build'\n\ndef test_build_exists():\n    assert os.path.exists(build_path)\n\n@pytest.mark.depends(on=['test_build_exists'])\ndef test_build_version():\n    result = subprocess.run([build_path, '--version'], stdout=subprocess.pipe)\n    assert result.returncode == 0\n    assert '1.2.3' in result.stdout\n```\n\nthis is a simple example of the situation mentioned earlier. in this case, the first test checks whether the build file\neven exists. if this fails, the other test will not be ran, as there is no point in doing to.\n\n## order\n\nthis plugin will automatically re-order the tests so that tests are run after the tests they depend on. if another\nplugin also reorders tests (such as `pytest-randomly`), this may cause problems, as dependencies that haven't ran yet\nare considered failures.\n\nthis plugin attempts to make sure it runs last to prevent this issue, but there are no guarantees this is successful. if\nyou run into issues with this in combination with another plugin, feel free to open an issue.\n\n## naming\n\nthere are multiple ways to refer to each test. let's start with an example, which we'll call `test_file.py`:\n\n``` python\nclass testclass(object):\n    @pytest.mark.depends(name='foo')\n    def test_in_class(self):\n        pass\n\n@pytest.mark.depends(name='foo')\ndef test_outside_class():\n    pass\n\ndef test_without_name(num):\n    pass\n```\n\nthe `test_in_class` test will be available under the following names:\n\n- `test_file.py::testclass::test_in_class`\n- `test_file.py::testclass`\n- `test_file.py`\n- `foo`\n\nthe `test_outside_class` test will be available under the following names:\n\n- `test_file.py::test_outside_class`\n- `test_file.py`\n- `foo`\n\nthe `test_without_name` test will be available under the following names:\n\n- `test_file.py::test_without_name`\n- `test_file.py`\n\nnote how some names apply to multiple tests. depending on `foo` in this case would mean depending on both\n`test_in_class` and `test_outside_class`, and depending on `test_file.py` would mean depending on all 3 tests in this\nfile.\n\nanother example, with parametrization. we'll call this one `test_params.py`:\n\n``` python\n@pytest.mark.depends(name='bar')\n@pytest.mark.parametrize('num', [\n    pytest.param(1, marks=pytest.mark.depends(name='baz')),\n    2,\n])\ndef test_with_params(num):\n    pass\n```\n\nthe first run of the test, with `num = 1`, will be available under the following names:\n\n- `test_params.py::test_with_params[num0]`\n- `test_params.py::test_with_params`\n- `test_params.py`\n- `bar`\n- `baz`\n\nthe second run of the test, with `num = 2`, will be available under the following names:\n\n- `test_params.py::test_with_params[num1]`\n- `test_params.py::test_with_params`\n- `test_params.py`\n- `bar`\n\nnote that the first name has a partially autogenerated name. if you want to depend on a single instance of a\nparametrized test, it's recommended to use the `pytest.depends` syntax to give it a name rather than depending on the\nautogenerated one.\n\n\n",
  "docs_url": null,
  "keywords": "pytest",
  "license": "mit",
  "name": "pytest-depends",
  "package_url": "https://pypi.org/project/pytest-depends/",
  "project_url": "https://pypi.org/project/pytest-depends/",
  "project_urls": {
    "Homepage": "https://gitlab.com/maienm/pytest-depends"
  },
  "release_url": "https://pypi.org/project/pytest-depends/1.0.1/",
  "requires_dist": [
    "colorama",
    "future-fstrings",
    "networkx",
    "pytest (>=3)"
  ],
  "requires_python": "",
  "summary": "tests that depend on other tests",
  "version": "1.0.1",
  "releases": [],
  "developers": [
    "michon1992@gmail.com",
    "michon_van_dooren"
  ],
  "kwds": "test_build_version test_build_exists dependencies dependency dependent",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_pytest_depends",
  "homepage": "https://gitlab.com/maienm/pytest-depends",
  "release_count": 4,
  "dependency_ids": [
    "pypi_colorama",
    "pypi_future_fstrings",
    "pypi_networkx",
    "pypi_pytest"
  ]
}