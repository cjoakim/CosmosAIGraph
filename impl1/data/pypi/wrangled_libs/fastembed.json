{
  "classifiers": [
    "license :: other/proprietary license",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "# \u26a1\ufe0f what is fastembed?\n\nfastembed is a lightweight, fast, python library built for embedding generation. we [support popular text models](https://qdrant.github.io/fastembed/examples/supported_models/). please [open a github issue](https://github.com/qdrant/fastembed/issues/new) if you want us to add a new model.\n\nthe default embedding supports \"query\" and \"passage\" prefixes for the input text. the default model is flag embedding, which is top of the [mteb](https://huggingface.co/spaces/mteb/leaderboard) leaderboard. here is an example for [retrieval embedding generation](https://qdrant.github.io/fastembed/examples/retrieval_with_fastembed/) and how to use [fastembed with qdrant](https://qdrant.github.io/fastembed/examples/usage_with_qdrant/).\n\n1. light & fast\n    - quantized model weights\n    - onnx runtime, no pytorch dependency\n    - cpu-first design\n    - data-parallelism for encoding of large datasets\n\n2. accuracy/recall\n    - better than openai ada-002\n    - default is flag embedding, which is top of the [mteb](https://huggingface.co/spaces/mteb/leaderboard) leaderboard\n    - list of [supported models](https://qdrant.github.io/fastembed/examples/supported_models/) - including multilingual models\n\n## \ud83d\ude80 installation\n\nto install the fastembed library, pip works: \n\n```bash\npip install fastembed\n```\n\n## \ud83d\udcd6 usage\n\n```python\nfrom fastembed.embedding import flagembedding as embedding\nfrom typing import list\nimport numpy as np\n\ndocuments: list[str] = [\n    \"passage: hello, world!\",\n    \"query: hello, world!\", # these are two different embedding\n    \"passage: this is an example passage.\",\n    \"fastembed is supported by and maintained by qdrant.\" # you can leave out the prefix but it's recommended\n]\nembedding_model = embedding(model_name=\"baai/bge-base-en\", max_length=512) \nembeddings: list[np.ndarray] = list(embedding_model.embed(documents)) # note the list() call - this is a generator \n```\n\n## usage with qdrant\n\ninstallation with qdrant client in python:\n\n```bash\npip install qdrant-client[fastembed]\n```\n\nmight have to use ```pip install 'qdrant-client[fastembed]'``` on zsh. \n\n```python\nfrom qdrant_client import qdrantclient\n\n# initialize the client\nclient = qdrantclient(\":memory:\")  # or qdrantclient(path=\"path/to/db\")\n\n# prepare your documents, metadata, and ids\ndocs = [\"qdrant has langchain integrations\", \"qdrant also has llama index integrations\"]\nmetadata = [\n    {\"source\": \"langchain-docs\"},\n    {\"source\": \"linkedin-docs\"},\n]\nids = [42, 2]\n\n# use the new add method\nclient.add(\n    collection_name=\"demo_collection\",\n    documents=docs,\n    metadata=metadata,\n    ids=ids\n)\n\nsearch_result = client.query(\n    collection_name=\"demo_collection\",\n    query_text=\"this is a query document\"\n)\nprint(search_result)\n```\n\n#### similar work\n\nilyas m. wrote about using [flagembeddings with optimum](https://twitter.com/ilysmoutawwakil/status/1705215192425288017) over cuda.\n",
  "docs_url": null,
  "keywords": "vector,embedding,neural,search,qdrant,sentence-transformers",
  "license": "apache license",
  "name": "fastembed",
  "package_url": "https://pypi.org/project/fastembed/",
  "project_url": "https://pypi.org/project/fastembed/",
  "project_urls": {
    "Homepage": "https://github.com/qdrant/fastembed",
    "Repository": "https://github.com/qdrant/fastembed"
  },
  "release_url": "https://pypi.org/project/fastembed/0.1.3/",
  "requires_dist": [
    "onnx (>=1.11,<2.0)",
    "onnxruntime (>=1.15,<2.0)",
    "tqdm (>=4.65,<5.0)",
    "requests (>=2.31,<3.0)",
    "tokenizers (>=0.15.0,<0.16.0)",
    "huggingface-hub (==0.19.4)"
  ],
  "requires_python": ">=3.8.0,<3.12",
  "summary": "fast, light, accurate library built for retrieval embedding generation",
  "version": "0.1.3",
  "releases": [],
  "developers": [
    "nirant.bits@gmail.com",
    "nirantk"
  ],
  "kwds": "retrieval_with_fastembed embedding_model embeddings embedding fastembed",
  "license_kwds": "apache license",
  "libtype": "pypi",
  "id": "pypi_fastembed",
  "homepage": "https://github.com/qdrant/fastembed",
  "release_count": 16,
  "dependency_ids": [
    "pypi_huggingface_hub",
    "pypi_onnx",
    "pypi_onnxruntime",
    "pypi_requests",
    "pypi_tokenizers",
    "pypi_tqdm"
  ]
}