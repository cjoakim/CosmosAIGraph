{
  "classifiers": [],
  "description": "[![tests](https://github.com/alex-petrenko/faster-fifo/actions/workflows/test-ci.yml/badge.svg)](https://github.com/alex-petrenko/faster-fifo/actions/workflows/test-ci.yml)\n[![downloads](https://pepy.tech/badge/faster-fifo)](https://pepy.tech/project/faster-fifo)\n[<img src=\"https://img.shields.io/discord/987232982798598164?label=discord\">](https://discord.gg/d9vzmmwejh)\n\n<!-- [![codecov](https://codecov.io/gh/alex-petrenko/faster-fifo/branch/master/graph/badge.svg)](https://codecov.io/gh/alex-petrenko/faster-fifo) -->\n\n# faster-fifo\n\nfaster alternative to python's standard multiprocessing.queue (ipc fifo queue). up to 30x faster in some configurations.\n\nimplemented in c++ using posix mutexes with pthread_process_shared attribute. based on a circular buffer, low footprint, brokerless.\ncompletely mimics the interface of the standard multiprocessing.queue, so can be used as a drop-in replacement.\n\nadds `get_many()` and `put_many()` methods to receive/send multiple messages at once for the price of a single lock.\n\n## requirements \n\n- linux or macos\n- python 3.6 or newer\n- gcc 4.9.0 or newer\n\n## installation\n\n```pip install faster-fifo```\n\n(on a fresh linux installation you might need some basic compiling tools `sudo apt install --reinstall build-essential`)\n\n## manual build instructions\n\n```\npip install cython\npython setup.py build_ext --inplace\npip install -e .\n```\n\n## usage example\n\n```python\nfrom faster_fifo import queue\nfrom queue import full, empty\n\nq = queue(1000 * 1000)  # specify the size of the circular buffer in the ctor\n\n# any pickle-able python object can be added to the queue\npy_obj = dict(a=42, b=33, c=(1, 2, 3), d=[1, 2, 3], e='123', f=b'kkk')\nq.put(py_obj)\nassert q.qsize() == 1\n\nretrieved = q.get()\nassert q.empty()\nassert py_obj == retrieved\n\nfor i in range(100):\n    try:\n        q.put(py_obj, timeout=0.1)\n    except full:\n        log.debug('queue is full!')\n\nnum_received = 0\nwhile num_received < 100:\n    # get multiple messages at once, returns a list of messages for better performance in many-to-few scenarios\n    # get_many does not guarantee that all max_messages_to_get will be received on the first call, in fact\n    # no such guarantee can be made in multiprocessing systems.\n    # get_many() will retrieve as many messages as there are available and can fit in the pre-allocated memory\n    # buffer. the size of the buffer is increased gradually to match demand.\n    messages = q.get_many(max_messages_to_get=100)\n    num_received += len(messages)\n\ntry:\n    q.get(timeout=0.1)\n    assert true, 'this won\\'t be called'\nexcept empty:\n    log.debug('queue is empty')\n\n```\n\n## performance comparison (faster-fifo vs multiprocessing.queue)\n\n##### system #1 (intel(r) core(tm) i9-7900x cpu @ 3.30ghz, 10 cores, ubuntu 18.04)\n\n*(measured execution times in seconds)*\n\n|                                                   | multiprocessing.queue |   faster-fifo, get()    |  faster-fifo, get_many()  |\n|---------------------------------------------------|:---------------------:|:-----------------------:|:-------------------------:|\n|   1 producer 1 consumer (200k msgs per producer)  |         2.54          |          0.86           |           0.92            |\n|  1 producer 10 consumers (200k msgs per producer) |         4.00          |          1.39           |           1.36            |           \n|  10 producers 1 consumer (100k msgs per producer) |         13.19         |          6.74           |           0.94            |\n| 3 producers 20 consumers (100k msgs per producer) |         9.30          |          2.22           |           2.17            |\n|  20 producers 3 consumers (50k msgs per producer) |         18.62         |          7.41           |           0.64            |\n| 20 producers 20 consumers (50k msgs per producer) |         36.51         |          1.32           |           3.79            |\n\n\n##### system #2 (intel(r) core(tm) i5-4200u cpu @ 1.60ghz, 2 cores, ubuntu 18.04)\n\n*(measured execution times in seconds)*\n\n|                                                   | multiprocessing.queue |   faster-fifo, get()    |  faster-fifo, get_many()  |\n|---------------------------------------------------|:---------------------:|:-----------------------:|:-------------------------:|\n|   1 producer 1 consumer (200k msgs per producer)  |         7.86          |          2.09           |            2.2            |\n|  1 producer 10 consumers (200k msgs per producer) |         11.68         |          4.01           |           3.88            |           \n|  10 producers 1 consumer (100k msgs per producer) |         44.48         |          16.68          |           5.98            |\n| 3 producers 20 consumers (100k msgs per producer) |         22.59         |          7.83           |           7.49            |\n|  20 producers 3 consumers (50k msgs per producer) |         66.3          |          22.3           |           6.35            |\n| 20 producers 20 consumers (50k msgs per producer) |         78.75         |          14.39          |           15.78           |\n\n## run tests\n\n`python -m unittest`\n\n(there are also c++ unit tests, should run them if c++ code was altered)\n\n## recent pypi releases\n\n##### v1.4.5\n\n* added method `data_size()` to query the total size of the messages in queue (in bytes). thank you [@lucanicosia](https://github.com/lucanicosia)!\n\n##### v1.4.4\n\n* fixed an obscure issue with the tlsbuffer ctor being called without arguments (guessing it's cython's weirdness)\n\n##### v1.4.3\n\n* simplified usage with \"spawn\" multiprocessing context. no need to use `faster_fifo_reduction` anymore. thank you [@mosbas](https://github.com/mosbas)!\n\n##### v1.4.2\n\n* fixed an issue with the custom queue pickler\n\n##### v1.4.1\n\n* fixed multithreading issues using threading.local for message recv buffer (huge thanks to [@brianmacy](https://github.com/brianmacy)!)\n* better error reporting in cython and c++\n* added threading tests\n\n##### v1.4.0\n\n* increase default receive buffer size from 10 bytes to 5000 bytes.\n\n##### v1.3.1\n\n* minor change: better debugging messages + improved c++ tests\n\n##### v1.3.0\n* now support custom serializers and deserializers instead of pickle (thank you [@beasteers](https://github.com/beasteers)!):\n```python\nq = queue(max_size_bytes=100000, loads=custom_deserializer, dumps=custom_serializer)\n```\n\n## footnote\n\noriginally designed for samplefactory, a high-throughput asynchronous rl codebase https://github.com/alex-petrenko/sample-factory.\n\nprogrammed by [aleksei petrenko](https://alex-petrenko.github.io/) and tushar kumar at [usc resl](https://robotics.usc.edu/resl/people/).\n\ndeveloped under mit license, feel free to use for any purpose, commercial or not, at your own risk.\n\nif you wish to cite this repository:\n\n```\n@misc{faster-fifo,\n    author={petrenko, aleksei and kumar, tushar},\n    title={a faster alternative to python's multiprocessing.queue},\n    publisher={github},\n    journal = {github repository},\n    howpublished = {\\url{https://github.com/alex-petrenko/faster-fifo}},\n    year={2020},\n}\n```\n",
  "docs_url": null,
  "keywords": "multiprocessing data structures",
  "license": "mit",
  "name": "faster-fifo",
  "package_url": "https://pypi.org/project/faster-fifo/",
  "project_url": "https://pypi.org/project/faster-fifo/",
  "project_urls": {
    "Homepage": "https://github.com/alex-petrenko/faster-fifo"
  },
  "release_url": "https://pypi.org/project/faster-fifo/1.4.5/",
  "requires_dist": [],
  "requires_python": ">=3.6",
  "summary": "a faster alternative to python's standard multiprocessing.queue (ipc fifo queue)",
  "version": "1.4.5",
  "releases": [],
  "developers": [
    "aleksei_petrenko"
  ],
  "kwds": "faster_fifo multiprocessing faster_fifo_reduction codecov badge",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_faster_fifo",
  "homepage": "https://github.com/alex-petrenko/faster-fifo",
  "release_count": 24,
  "dependency_ids": []
}