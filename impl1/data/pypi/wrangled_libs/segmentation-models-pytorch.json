{
  "classifiers": [
    "license :: osi approved :: mit license",
    "programming language :: python",
    "programming language :: python :: 3",
    "programming language :: python :: implementation :: cpython",
    "programming language :: python :: implementation :: pypy"
  ],
  "description": "\n<div align=\"center\">\n\n![logo](https://i.ibb.co/dc1xdht/segmentation-models-v2-side-1-1.png)  \n**python library with neural networks for image  \nsegmentation based on [pytorch](https://pytorch.org/).**  \n\n[![generic badge](https://img.shields.io/badge/license-mit-<color>.svg?style=for-the-badge)](https://github.com/qubvel/segmentation_models.pytorch/blob/master/license) \n[![github workflow status (branch)](https://img.shields.io/github/actions/workflow/status/qubvel/segmentation_models.pytorch/tests.yml?branch=master&style=for-the-badge)](https://github.com/qubvel/segmentation_models.pytorch/actions/workflows/tests.yml) \n[![read the docs](https://img.shields.io/readthedocs/smp?style=for-the-badge&logo=readthedocs&logocolor=white)](https://smp.readthedocs.io/en/latest/) \n<br>\n[![pypi](https://img.shields.io/pypi/v/segmentation-models-pytorch?color=blue&style=for-the-badge&logo=pypi&logocolor=white)](https://pypi.org/project/segmentation-models-pytorch/) \n[![pypi - downloads](https://img.shields.io/pypi/dm/segmentation-models-pytorch?style=for-the-badge&color=blue)](https://pepy.tech/project/segmentation-models-pytorch) \n<br>\n[![pytorch - version](https://img.shields.io/badge/pytorch-1.4+-red?style=for-the-badge&logo=pytorch)](https://pepy.tech/project/segmentation-models-pytorch) \n[![python - version](https://img.shields.io/badge/python-3.7+-red?style=for-the-badge&logo=python&logocolor=white)](https://pepy.tech/project/segmentation-models-pytorch) \n\n</div>\n\nthe main features of this library are:\n\n - high level api (just two lines to create a neural network)\n - 9 models architectures for binary and multi class segmentation (including legendary unet)\n - 124 available encoders (and 500+ encoders from [timm](https://github.com/rwightman/pytorch-image-models))\n - all encoders have pre-trained weights for faster and better convergence\n - popular metrics and losses for training routines\n\n### [\ud83d\udcda project documentation \ud83d\udcda](http://smp.readthedocs.io/)\n\nvisit [read the docs project page](https://smp.readthedocs.io/) or read following readme to know more about segmentation models pytorch (smp for short) library\n\n### \ud83d\udccb table of content\n 1. [quick start](#start)\n 2. [examples](#examples)\n 3. [models](#models)\n    1. [architectures](#architectures)\n    2. [encoders](#encoders)\n    3. [timm encoders](#timm)\n 4. [models api](#api)\n    1. [input channels](#input-channels)\n    2. [auxiliary classification output](#auxiliary-classification-output)\n    3. [depth](#depth)\n 5. [installation](#installation)\n 6. [competitions won with the library](#competitions-won-with-the-library)\n 7. [contributing](#contributing)\n 8. [citing](#citing)\n 9. [license](#license)\n\n### \u23f3 quick start <a name=\"start\"></a>\n\n#### 1. create your first segmentation model with smp\n\nsegmentation model is just a pytorch nn.module, which can be created as easy as:\n\n```python\nimport segmentation_models_pytorch as smp\n\nmodel = smp.unet(\n    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for rgb, etc.)\n    classes=3,                      # model output channels (number of classes in your dataset)\n)\n```\n - see [table](#architectures) with available model architectures\n - see [table](#encoders) with available encoders and their corresponding weights\n\n#### 2. configure data preprocessing\n\nall encoders have pretrained weights. preparing your data the same way as during weights pre-training may give you better results (higher metric score and faster convergence). it is **not necessary** in case you train the whole model, not only decoder.\n\n```python\nfrom segmentation_models_pytorch.encoders import get_preprocessing_fn\n\npreprocess_input = get_preprocessing_fn('resnet18', pretrained='imagenet')\n```\n\ncongratulations! you are done! now you can train your model with your favorite framework!\n\n### \ud83d\udca1 examples <a name=\"examples\"></a>\n - training model for pets binary segmentation with pytorch-lightning [notebook](https://github.com/qubvel/segmentation_models.pytorch/blob/master/examples/binary_segmentation_intro.ipynb) and [![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/qubvel/segmentation_models.pytorch/blob/master/examples/binary_segmentation_intro.ipynb)\n - training model for cars segmentation on camvid dataset [here](https://github.com/qubvel/segmentation_models.pytorch/blob/master/examples/cars%20segmentation%20(camvid).ipynb).\n - training smp model with [catalyst](https://github.com/catalyst-team/catalyst) (high-level framework for pytorch), [ttach](https://github.com/qubvel/ttach) (tta library for pytorch) and [albumentations](https://github.com/albu/albumentations) (fast image augmentation library) - [here](https://github.com/catalyst-team/catalyst/blob/v21.02rc0/examples/notebooks/segmentation-tutorial.ipynb) [![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catalyst-team/catalyst/blob/v21.02rc0/examples/notebooks/segmentation-tutorial.ipynb)\n - training smp model with [pytorch-lightning](https://pytorch-lightning.readthedocs.io) framework - [here](https://github.com/ternaus/cloths_segmentation) (clothes binary segmentation by [@ternaus](https://github.com/ternaus)).\n\n### \ud83d\udce6 models <a name=\"models\"></a>\n\n#### architectures <a name=\"architectures\"></a>\n - unet [[paper](https://arxiv.org/abs/1505.04597)] [[docs](https://smp.readthedocs.io/en/latest/models.html#unet)]\n - unet++ [[paper](https://arxiv.org/pdf/1807.10165.pdf)] [[docs](https://smp.readthedocs.io/en/latest/models.html#id2)]\n - manet [[paper](https://ieeexplore.ieee.org/abstract/document/9201310)] [[docs](https://smp.readthedocs.io/en/latest/models.html#manet)]\n - linknet [[paper](https://arxiv.org/abs/1707.03718)] [[docs](https://smp.readthedocs.io/en/latest/models.html#linknet)]\n - fpn [[paper](http://presentations.cocodataset.org/coco17-stuff-fair.pdf)] [[docs](https://smp.readthedocs.io/en/latest/models.html#fpn)]\n - pspnet [[paper](https://arxiv.org/abs/1612.01105)] [[docs](https://smp.readthedocs.io/en/latest/models.html#pspnet)]\n - pan [[paper](https://arxiv.org/abs/1805.10180)] [[docs](https://smp.readthedocs.io/en/latest/models.html#pan)]\n - deeplabv3 [[paper](https://arxiv.org/abs/1706.05587)] [[docs](https://smp.readthedocs.io/en/latest/models.html#deeplabv3)]\n - deeplabv3+ [[paper](https://arxiv.org/abs/1802.02611)] [[docs](https://smp.readthedocs.io/en/latest/models.html#id9)]\n\n#### encoders <a name=\"encoders\"></a>\n\nthe following is a list of supported encoders in the smp. select the appropriate family of encoders and click to expand the table and select a specific encoder and its pre-trained weights (`encoder_name` and `encoder_weights` parameters).\n\n<details>\n<summary style=\"margin-left: 25px;\">resnet</summary>\n<div style=\"margin-left: 25px;\">\n\n|encoder                         |weights                         |params, m                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|resnet18                        |imagenet / ssl / swsl           |11m                             |\n|resnet34                        |imagenet                        |21m                             |\n|resnet50                        |imagenet / ssl / swsl           |23m                             |\n|resnet101                       |imagenet                        |42m                             |\n|resnet152                       |imagenet                        |58m                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">resnext</summary>\n<div style=\"margin-left: 25px;\">\n\n|encoder                         |weights                         |params, m                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|resnext50_32x4d                 |imagenet / ssl / swsl           |22m                             |\n|resnext101_32x4d                |ssl / swsl                      |42m                             |\n|resnext101_32x8d                |imagenet / instagram / ssl / swsl|86m                         |\n|resnext101_32x16d               |instagram / ssl / swsl          |191m                            |\n|resnext101_32x32d               |instagram                       |466m                            |\n|resnext101_32x48d               |instagram                       |826m                            |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">resnest</summary>\n<div style=\"margin-left: 25px;\">\n\n|encoder                         |weights                         |params, m                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-resnest14d                 |imagenet                        |8m                              |\n|timm-resnest26d                 |imagenet                        |15m                             |\n|timm-resnest50d                 |imagenet                        |25m                             |\n|timm-resnest101e                |imagenet                        |46m                             |\n|timm-resnest200e                |imagenet                        |68m                             |\n|timm-resnest269e                |imagenet                        |108m                            |\n|timm-resnest50d_4s2x40d         |imagenet                        |28m                             |\n|timm-resnest50d_1s4x24d         |imagenet                        |23m                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">res2ne(x)t</summary>\n<div style=\"margin-left: 25px;\">\n\n|encoder                         |weights                         |params, m                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-res2net50_26w_4s           |imagenet                        |23m                             |\n|timm-res2net101_26w_4s          |imagenet                        |43m                             |\n|timm-res2net50_26w_6s           |imagenet                        |35m                             |\n|timm-res2net50_26w_8s           |imagenet                        |46m                             |\n|timm-res2net50_48w_2s           |imagenet                        |23m                             |\n|timm-res2net50_14w_8s           |imagenet                        |23m                             |\n|timm-res2next50                 |imagenet                        |22m                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">regnet(x/y)</summary>\n<div style=\"margin-left: 25px;\">\n\n|encoder                         |weights                         |params, m                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-regnetx_002                |imagenet                        |2m                              |\n|timm-regnetx_004                |imagenet                        |4m                              |\n|timm-regnetx_006                |imagenet                        |5m                              |\n|timm-regnetx_008                |imagenet                        |6m                              |\n|timm-regnetx_016                |imagenet                        |8m                              |\n|timm-regnetx_032                |imagenet                        |14m                             |\n|timm-regnetx_040                |imagenet                        |20m                             |\n|timm-regnetx_064                |imagenet                        |24m                             |\n|timm-regnetx_080                |imagenet                        |37m                             |\n|timm-regnetx_120                |imagenet                        |43m                             |\n|timm-regnetx_160                |imagenet                        |52m                             |\n|timm-regnetx_320                |imagenet                        |105m                            |\n|timm-regnety_002                |imagenet                        |2m                              |\n|timm-regnety_004                |imagenet                        |3m                              |\n|timm-regnety_006                |imagenet                        |5m                              |\n|timm-regnety_008                |imagenet                        |5m                              |\n|timm-regnety_016                |imagenet                        |10m                             |\n|timm-regnety_032                |imagenet                        |17m                             |\n|timm-regnety_040                |imagenet                        |19m                             |\n|timm-regnety_064                |imagenet                        |29m                             |\n|timm-regnety_080                |imagenet                        |37m                             |\n|timm-regnety_120                |imagenet                        |49m                             |\n|timm-regnety_160                |imagenet                        |80m                             |\n|timm-regnety_320                |imagenet                        |141m                            |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">gernet</summary>\n<div style=\"margin-left: 25px;\">\n\n|encoder                         |weights                         |params, m                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-gernet_s                   |imagenet                        |6m                              |\n|timm-gernet_m                   |imagenet                        |18m                             |\n|timm-gernet_l                   |imagenet                        |28m                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">se-net</summary>\n<div style=\"margin-left: 25px;\">\n\n|encoder                         |weights                         |params, m                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|senet154                        |imagenet                        |113m                            |\n|se_resnet50                     |imagenet                        |26m                             |\n|se_resnet101                    |imagenet                        |47m                             |\n|se_resnet152                    |imagenet                        |64m                             |\n|se_resnext50_32x4d              |imagenet                        |25m                             |\n|se_resnext101_32x4d             |imagenet                        |46m                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">sk-resne(x)t</summary>\n<div style=\"margin-left: 25px;\">\n\n|encoder                         |weights                         |params, m                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-skresnet18                 |imagenet                        |11m                             |\n|timm-skresnet34                 |imagenet                        |21m                             |\n|timm-skresnext50_32x4d          |imagenet                        |25m                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">densenet</summary>\n<div style=\"margin-left: 25px;\">\n\n|encoder                         |weights                         |params, m                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|densenet121                     |imagenet                        |6m                              |\n|densenet169                     |imagenet                        |12m                             |\n|densenet201                     |imagenet                        |18m                             |\n|densenet161                     |imagenet                        |26m                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">inception</summary>\n<div style=\"margin-left: 25px;\">\n\n|encoder                         |weights                         |params, m                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|inceptionresnetv2               |imagenet /  imagenet+background |54m                             |\n|inceptionv4                     |imagenet /  imagenet+background |41m                             |\n|xception                        |imagenet                        |22m                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">efficientnet</summary>\n<div style=\"margin-left: 25px;\">\n\n|encoder                         |weights                         |params, m                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|efficientnet-b0                 |imagenet                        |4m                              |\n|efficientnet-b1                 |imagenet                        |6m                              |\n|efficientnet-b2                 |imagenet                        |7m                              |\n|efficientnet-b3                 |imagenet                        |10m                             |\n|efficientnet-b4                 |imagenet                        |17m                             |\n|efficientnet-b5                 |imagenet                        |28m                             |\n|efficientnet-b6                 |imagenet                        |40m                             |\n|efficientnet-b7                 |imagenet                        |63m                             |\n|timm-efficientnet-b0            |imagenet / advprop / noisy-student|4m                              |\n|timm-efficientnet-b1            |imagenet / advprop / noisy-student|6m                              |\n|timm-efficientnet-b2            |imagenet / advprop / noisy-student|7m                              |\n|timm-efficientnet-b3            |imagenet / advprop / noisy-student|10m                             |\n|timm-efficientnet-b4            |imagenet / advprop / noisy-student|17m                             |\n|timm-efficientnet-b5            |imagenet / advprop / noisy-student|28m                             |\n|timm-efficientnet-b6            |imagenet / advprop / noisy-student|40m                             |\n|timm-efficientnet-b7            |imagenet / advprop / noisy-student|63m                             |\n|timm-efficientnet-b8            |imagenet / advprop             |84m                             |\n|timm-efficientnet-l2            |noisy-student                   |474m                            |\n|timm-efficientnet-lite0         |imagenet                        |4m                              |\n|timm-efficientnet-lite1         |imagenet                        |5m                              |\n|timm-efficientnet-lite2         |imagenet                        |6m                              |\n|timm-efficientnet-lite3         |imagenet                        |8m                             |\n|timm-efficientnet-lite4         |imagenet                        |13m                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">mobilenet</summary>\n<div style=\"margin-left: 25px;\">\n\n|encoder                         |weights                         |params, m                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|mobilenet_v2                    |imagenet                        |2m                              |\n|timm-mobilenetv3_large_075      |imagenet                        |1.78m                       |\n|timm-mobilenetv3_large_100      |imagenet                        |2.97m                       |\n|timm-mobilenetv3_large_minimal_100|imagenet                        |1.41m                       |\n|timm-mobilenetv3_small_075      |imagenet                        |0.57m                        |\n|timm-mobilenetv3_small_100      |imagenet                        |0.93m                       |\n|timm-mobilenetv3_small_minimal_100|imagenet                        |0.43m                       |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">dpn</summary>\n<div style=\"margin-left: 25px;\">\n\n|encoder                         |weights                         |params, m                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|dpn68                           |imagenet                        |11m                             |\n|dpn68b                          |imagenet+5k                     |11m                             |\n|dpn92                           |imagenet+5k                     |34m                             |\n|dpn98                           |imagenet                        |58m                             |\n|dpn107                          |imagenet+5k                     |84m                             |\n|dpn131                          |imagenet                        |76m                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">vgg</summary>\n<div style=\"margin-left: 25px;\">\n\n|encoder                         |weights                         |params, m                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|vgg11                           |imagenet                        |9m                              |\n|vgg11_bn                        |imagenet                        |9m                              |\n|vgg13                           |imagenet                        |9m                              |\n|vgg13_bn                        |imagenet                        |9m                              |\n|vgg16                           |imagenet                        |14m                             |\n|vgg16_bn                        |imagenet                        |14m                             |\n|vgg19                           |imagenet                        |20m                             |\n|vgg19_bn                        |imagenet                        |20m                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">mix vision transformer</summary>\n<div style=\"margin-left: 25px;\">\n\nbackbone from segformer pretrained on imagenet! can be used with other decoders from package, you can combine mix vision transformer with unet, fpn and others!\n\nlimitations:  \n\n   - encoder is **not** supported by linknet, unet++\n   - encoder is supported by fpn only for encoder **depth = 5**\n\n|encoder                         |weights                         |params, m                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|mit_b0                          |imagenet                        |3m                              |\n|mit_b1                          |imagenet                        |13m                             |\n|mit_b2                          |imagenet                        |24m                             |\n|mit_b3                          |imagenet                        |44m                             |\n|mit_b4                          |imagenet                        |60m                             |\n|mit_b5                          |imagenet                        |81m                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">mobileone</summary>\n<div style=\"margin-left: 25px;\">\n\napple's \"sub-one-ms\" backbone pretrained on imagenet! can be used with all decoders.\n\nnote: in the official github repo the s0 variant has additional num_conv_branches, leading to more params than s1.\n\n|encoder                         |weights                         |params, m                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|mobileone_s0                    |imagenet                        |4.6m                              |\n|mobileone_s1                    |imagenet                        |4.0m                              |\n|mobileone_s2                    |imagenet                        |6.5m                              |\n|mobileone_s3                    |imagenet                        |8.8m                              |\n|mobileone_s4                    |imagenet                        |13.6m                             |\n\n</div>\n</details>\n\n\n\\* `ssl`, `swsl` - semi-supervised and weakly-supervised learning on imagenet ([repo](https://github.com/facebookresearch/semi-supervised-imagenet1k-models)).\n\n#### timm encoders <a name=\"timm\"></a>\n\n[docs](https://smp.readthedocs.io/en/latest/encoders_timm.html)\n\npytorch image models (a.k.a. timm) has a lot of pretrained models and interface which allows using these models as encoders in smp, however, not all models are supported\n\n - not all transformer models have ``features_only`` functionality implemented that is required for encoder\n - some models have inappropriate strides\n\ntotal number of supported encoders: 549\n - [table with available encoders](https://smp.readthedocs.io/en/latest/encoders_timm.html)\n\n### \ud83d\udd01 models api <a name=\"api\"></a>\n\n - `model.encoder` - pretrained backbone to extract features of different spatial resolution\n - `model.decoder` - depends on models architecture (`unet`/`linknet`/`pspnet`/`fpn`)\n - `model.segmentation_head` - last block to produce required number of mask channels (include also optional upsampling and activation)\n - `model.classification_head` - optional block which create classification head on top of encoder\n - `model.forward(x)` - sequentially pass `x` through model\\`s encoder, decoder and segmentation head (and classification head if specified)\n\n##### input channels\ninput channels parameter allows you to create models, which process tensors with arbitrary number of channels.\nif you use pretrained weights from imagenet - weights of first convolution will be reused. for\n1-channel case it would be a sum of weights of first convolution layer, otherwise channels would be \npopulated with weights like `new_weight[:, i] = pretrained_weight[:, i % 3]` and than scaled with `new_weight * 3 / new_in_channels`.\n```python\nmodel = smp.fpn('resnet34', in_channels=1)\nmask = model(torch.ones([1, 1, 64, 64]))\n```\n\n##### auxiliary classification output  \nall models support `aux_params` parameters, which is default set to `none`. \nif `aux_params = none` then classification auxiliary output is not created, else\nmodel produce not only `mask`, but also `label` output with shape `nc`.\nclassification head consists of globalpooling->dropout(optional)->linear->activation(optional) layers, which can be \nconfigured by `aux_params` as follows:\n```python\naux_params=dict(\n    pooling='avg',             # one of 'avg', 'max'\n    dropout=0.5,               # dropout ratio, default is none\n    activation='sigmoid',      # activation function, default is none\n    classes=4,                 # define number of output labels\n)\nmodel = smp.unet('resnet34', classes=4, aux_params=aux_params)\nmask, label = model(x)\n```\n\n##### depth\ndepth parameter specify a number of downsampling operations in encoder, so you can make\nyour model lighter if specify smaller `depth`.\n```python\nmodel = smp.unet('resnet34', encoder_depth=4)\n```\n\n\n### \ud83d\udee0 installation <a name=\"installation\"></a>\npypi version:\n```bash\n$ pip install segmentation-models-pytorch\n````\nlatest version from source:\n```bash\n$ pip install git+https://github.com/qubvel/segmentation_models.pytorch\n````\n\n### \ud83c\udfc6 competitions won with the library\n\n`segmentation models` package is widely used in the image segmentation competitions.\n[here](https://github.com/qubvel/segmentation_models.pytorch/blob/master/halloffame.md) you can find competitions, names of the winners and links to their solutions.\n\n### \ud83e\udd1d contributing\n\n#### install smp  \n\n```bash\nmake install_dev  # create .venv, install smp in dev mode\n```\n\n#### run tests and code checks  \n\n```bash\nmake all          # run flake8, black, tests\n```\n\n#### update table with encoders  \n\n```bash\nmake table        # generate table with encoders and print to stdout\n```\n\n### \ud83d\udcdd citing\n```\n@misc{iakubovskii:2019,\n  author = {pavel iakubovskii},\n  title = {segmentation models pytorch},\n  year = {2019},\n  publisher = {github},\n  journal = {github repository},\n  howpublished = {\\url{https://github.com/qubvel/segmentation_models.pytorch}}\n}\n```\n\n### \ud83d\udee1\ufe0f license <a name=\"license\"></a>\nproject is distributed under [mit license](https://github.com/qubvel/segmentation_models.pytorch/blob/master/license)\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "segmentation-models-pytorch",
  "package_url": "https://pypi.org/project/segmentation-models-pytorch/",
  "project_url": "https://pypi.org/project/segmentation-models-pytorch/",
  "project_urls": {
    "Homepage": "https://github.com/qubvel/segmentation_models.pytorch"
  },
  "release_url": "https://pypi.org/project/segmentation-models-pytorch/0.3.3/",
  "requires_dist": [
    "torchvision (>=0.5.0)",
    "pretrainedmodels (==0.7.4)",
    "efficientnet-pytorch (==0.7.1)",
    "timm (==0.9.2)",
    "tqdm",
    "pillow",
    "pytest ; extra == 'test'",
    "mock ; extra == 'test'",
    "pre-commit ; extra == 'test'",
    "black (==22.3.0) ; extra == 'test'",
    "flake8 (==4.0.1) ; extra == 'test'",
    "flake8-docstrings (==1.6.0) ; extra == 'test'"
  ],
  "requires_python": ">=3.7.0",
  "summary": "image segmentation models with pre-trained backbones. pytorch.",
  "version": "0.3.3",
  "releases": [],
  "developers": [
    "pavel_iakubovskii",
    "qubvel@gmail.com"
  ],
  "kwds": "segmentation_models_pytorch segmentation_models badge qubvel segmentation",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_segmentation_models_pytorch",
  "homepage": "https://github.com/qubvel/segmentation_models.pytorch",
  "release_count": 13,
  "dependency_ids": [
    "pypi_black",
    "pypi_efficientnet_pytorch",
    "pypi_flake8",
    "pypi_flake8_docstrings",
    "pypi_mock",
    "pypi_pillow",
    "pypi_pre_commit",
    "pypi_pretrainedmodels",
    "pypi_pytest",
    "pypi_timm",
    "pypi_torchvision",
    "pypi_tqdm"
  ]
}