{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: developers",
    "license :: osi approved :: mit license",
    "operating system :: os independent",
    "programming language :: python",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.12",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: database :: front-ends"
  ],
  "description": "sqlalchemy dialect for bigquery\n===============================\n\n|ga| |pypi| |versions|\n\n`sqlalchemy dialects`_\n\n- `dialect documentation`_\n- `product documentation`_\n\n.. |ga| image:: https://img.shields.io/badge/support-ga-gold.svg\n   :target: https://github.com/googleapis/google-cloud-python/blob/main/readme.rst#general-availability\n.. |pypi| image:: https://img.shields.io/pypi/v/sqlalchemy-bigquery.svg\n   :target: https://pypi.org/project/sqlalchemy-bigquery/\n.. |versions| image:: https://img.shields.io/pypi/pyversions/sqlalchemy-bigquery.svg\n   :target: https://pypi.org/project/sqlalchemy-bigquery/\n.. _sqlalchemy dialects: https://docs.sqlalchemy.org/en/14/dialects/\n.. _dialect documentation: https://googleapis.dev/python/sqlalchemy-bigquery/latest\n.. _product documentation: https://cloud.google.com/bigquery/docs/\n\n\nquick start\n-----------\n\nin order to use this library, you first need to go through the following steps:\n\n1. `select or create a cloud platform project.`_\n2. [optional] `enable billing for your project.`_\n3. `enable the bigquery storage api.`_\n4. `setup authentication.`_\n\n.. _select or create a cloud platform project.: https://console.cloud.google.com/project\n.. _enable billing for your project.: https://cloud.google.com/billing/docs/how-to/modify-project#enable_billing_for_a_project\n.. _enable the bigquery storage api.: https://console.cloud.google.com/apis/library/bigquery.googleapis.com\n.. _setup authentication.: https://googleapis.dev/python/google-api-core/latest/auth.html\n\n.. note::\n   this library is only compatible with sqlalchemy versions < 2.0.0\n\ninstallation\n------------\n\ninstall this library in a `virtualenv`_ using pip. `virtualenv`_ is a tool to\ncreate isolated python environments. the basic problem it addresses is one of\ndependencies and versions, and indirectly permissions.\n\nwith `virtualenv`_, it's possible to install this library without needing system\ninstall permissions, and without clashing with the installed system\ndependencies.\n\n.. _`virtualenv`: https://virtualenv.pypa.io/en/latest/\n\n\nsupported python versions\n^^^^^^^^^^^^^^^^^^^^^^^^^\npython >= 3.8\n\nunsupported python versions\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\npython <= 3.7.\n\n\nmac/linux\n^^^^^^^^^\n\n.. code-block:: console\n\n    pip install virtualenv\n    virtualenv <your-env>\n    source <your-env>/bin/activate\n    <your-env>/bin/pip install sqlalchemy-bigquery\n\n\nwindows\n^^^^^^^\n\n.. code-block:: console\n\n    pip install virtualenv\n    virtualenv <your-env>\n    <your-env>\\scripts\\activate\n    <your-env>\\scripts\\pip.exe install sqlalchemy-bigquery\n\n\ninstallations when processing large datasets\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nwhen handling large datasets, you may see speed increases by also installing the\n`bqstorage` dependencies. see the instructions above about creating a virtual \nenvironment and then install `sqlalchemy-bigquery` using the `bqstorage` extras:\n\n.. code-block:: console\n\n    source <your-env>/bin/activate\n    <your-env>/bin/pip install sqlalchemy-bigquery[bqstorage]\n\n\nusage\n-----\n\nsqlalchemy\n^^^^^^^^^^\n\n.. code-block:: python\n\n    from sqlalchemy import *\n    from sqlalchemy.engine import create_engine\n    from sqlalchemy.schema import *\n    engine = create_engine('bigquery://project')\n    table = table('dataset.table', metadata(bind=engine), autoload=true)\n    print(select([func.count('*')], from_obj=table).scalar())\n\nproject\n^^^^^^^\n\n``project`` in ``bigquery://project`` is used to instantiate bigquery client with the specific project id. to infer project from the environment, use ``bigquery://`` \u2013\u00a0without ``project``\n\nauthentication\n^^^^^^^^^^^^^^\n\nfollow the `google cloud library guide <https://google-cloud-python.readthedocs.io/en/latest/core/auth.html>`_ for authentication. \n\nalternatively, you can choose either of the following approaches:\n\n* provide the path to a service account json file in ``create_engine()`` using the ``credentials_path`` parameter:\n\n.. code-block:: python\n\n    # provide the path to a service account json file\n    engine = create_engine('bigquery://', credentials_path='/path/to/keyfile.json')\n\n* pass the credentials in ``create_engine()`` as a python dictionary using the ``credentials_info`` parameter:\n\n.. code-block:: python\n    \n    # provide credentials as a python dictionary\n    credentials_info = {\n        \"type\": \"service_account\", \n        \"project_id\": \"your-service-account-project-id\"\n    },\n    engine = create_engine('bigquery://', credentials_info=credentials_info)\n\nlocation\n^^^^^^^^\n\nto specify location of your datasets pass ``location`` to ``create_engine()``:\n\n.. code-block:: python\n\n    engine = create_engine('bigquery://project', location=\"asia-northeast1\")\n\n\ntable names\n^^^^^^^^^^^\n\nto query tables from non-default projects or datasets, use the following format for the sqlalchemy schema name: ``[project.]dataset``, e.g.:\n\n.. code-block:: python\n\n    # if neither dataset nor project are the default\n    sample_table_1 = table('natality', schema='bigquery-public-data.samples')\n    # if just dataset is not the default\n    sample_table_2 = table('natality', schema='bigquery-public-data')\n\nbatch size\n^^^^^^^^^^\n\nby default, ``arraysize`` is set to ``5000``. ``arraysize`` is used to set the batch size for fetching results. to change it, pass ``arraysize`` to ``create_engine()``:\n\n.. code-block:: python\n\n    engine = create_engine('bigquery://project', arraysize=1000)\n\npage size for dataset.list_tables\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nby default, ``list_tables_page_size`` is set to ``1000``. ``list_tables_page_size`` is used to set the max_results for `dataset.list_tables`_ operation. to change it, pass ``list_tables_page_size`` to ``create_engine()``:\n\n.. _`dataset.list_tables`: https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/list\n.. code-block:: python\n\n    engine = create_engine('bigquery://project', list_tables_page_size=100)\n\nadding a default dataset\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nif you want to have the ``client`` use a default dataset, specify it as the \"database\" portion of the connection string.\n\n.. code-block:: python\n\n    engine = create_engine('bigquery://project/dataset')\n\nwhen using a default dataset, don't include the dataset name in the table name, e.g.:\n\n.. code-block:: python\n\n    table = table('table_name')\n\nnote that specifying a default dataset doesn't restrict execution of queries to that particular dataset when using raw queries, e.g.:\n\n.. code-block:: python\n\n    # set default dataset to dataset_a\n    engine = create_engine('bigquery://project/dataset_a')\n\n    # this will still execute and return rows from dataset_b\n    engine.execute('select * from dataset_b.table').fetchall()\n\n\nconnection string parameters\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nthere are many situations where you can't call ``create_engine`` directly, such as when using tools like `flask sqlalchemy <http://flask-sqlalchemy.pocoo.org/2.3/>`_. for situations like these, or for situations where you want the ``client`` to have a `default_query_job_config <https://googlecloudplatform.github.io/google-cloud-python/latest/bigquery/generated/google.cloud.bigquery.client.client.html#google.cloud.bigquery.client.client>`_, you can pass many arguments in the query of the connection string.\n\nthe ``credentials_path``, ``credentials_info``, ``credentials_base64``, ``location``, ``arraysize`` and ``list_tables_page_size`` parameters are used by this library, and the rest are used to create a `queryjobconfig <https://googlecloudplatform.github.io/google-cloud-python/latest/bigquery/generated/google.cloud.bigquery.job.queryjobconfig.html#google.cloud.bigquery.job.queryjobconfig>`_\n\nnote that if you want to use query strings, it will be more reliable if you use three slashes, so ``'bigquery:///?a=b'`` will work reliably, but ``'bigquery://?a=b'`` might be interpreted as having a \"database\" of ``?a=b``, depending on the system being used to parse the connection string.\n\nhere are examples of all the supported arguments. any not present are either for legacy sql (which isn't supported by this library), or are too complex and are not implemented.\n\n.. code-block:: python\n\n    engine = create_engine(\n        'bigquery://some-project/some-dataset' '?'\n        'credentials_path=/some/path/to.json' '&'\n        'location=some-location' '&'\n        'arraysize=1000' '&'\n        'list_tables_page_size=100' '&'\n        'clustering_fields=a,b,c' '&'\n        'create_disposition=create_if_needed' '&'\n        'destination=different-project.different-dataset.table' '&'\n        'destination_encryption_configuration=some-configuration' '&'\n        'dry_run=true' '&'\n        'labels=a:b,c:d' '&'\n        'maximum_bytes_billed=1000' '&'\n        'priority=interactive' '&'\n        'schema_update_options=allow_field_addition,allow_field_relaxation' '&'\n        'use_query_cache=true' '&'\n        'write_disposition=write_append'\n    )\n\nin cases where you wish to include the full credentials in the connection uri you can base64 the credentials json file and supply the encoded string to the ``credentials_base64`` parameter.\n\n.. code-block:: python\n\n    engine = create_engine(\n        'bigquery://some-project/some-dataset' '?'\n        'credentials_base64=eyjrzxkioij2ywx1zsj9cg==' '&'\n        'location=some-location' '&'\n        'arraysize=1000' '&'\n        'list_tables_page_size=100' '&'\n        'clustering_fields=a,b,c' '&'\n        'create_disposition=create_if_needed' '&'\n        'destination=different-project.different-dataset.table' '&'\n        'destination_encryption_configuration=some-configuration' '&'\n        'dry_run=true' '&'\n        'labels=a:b,c:d' '&'\n        'maximum_bytes_billed=1000' '&'\n        'priority=interactive' '&'\n        'schema_update_options=allow_field_addition,allow_field_relaxation' '&'\n        'use_query_cache=true' '&'\n        'write_disposition=write_append'\n    )\n\nto create the base64 encoded string you can use the command line tool ``base64``, or ``openssl base64``, or ``python -m base64``.\n\nalternatively, you can use an online generator like `www.base64encode.org <https://www.base64encode.org>_` to paste your credentials json file to be encoded.\n\n\nsupplying your own bigquery client\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nthe above connection string parameters allow you to influence how the bigquery client used to execute your queries will be instantiated.\nif you need additional control, you can supply a bigquery client of your own:\n\n.. code-block:: python\n\n    from google.cloud import bigquery\n\n    custom_bq_client = bigquery.client(...)\n\n    engine = create_engine(\n        'bigquery://some-project/some-dataset?user_supplied_client=true',\n\tconnect_args={'client': custom_bq_client},\n    )\n\n\ncreating tables\n^^^^^^^^^^^^^^^\n\nto add metadata to a table:\n\n.. code-block:: python\n\n    table = table('mytable', ..., bigquery_description='my table description', bigquery_friendly_name='my table friendly name')\n\nto add metadata to a column:\n\n.. code-block:: python\n\n    column('mycolumn', doc='my column description')\n\n\nthreading and multiprocessing\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nbecause this client uses the `grpc` library, it's safe to\nshare instances across threads.\n\nin multiprocessing scenarios, the best\npractice is to create client instances *after* the invocation of\n`os.fork` by `multiprocessing.pool.pool` or\n`multiprocessing.process`.\n",
  "docs_url": null,
  "keywords": "bigquery,sqlalchemy",
  "license": "",
  "name": "sqlalchemy-bigquery",
  "package_url": "https://pypi.org/project/sqlalchemy-bigquery/",
  "project_url": "https://pypi.org/project/sqlalchemy-bigquery/",
  "project_urls": {
    "Homepage": "https://github.com/googleapis/python-bigquery-sqlalchemy"
  },
  "release_url": "https://pypi.org/project/sqlalchemy-bigquery/1.9.0/",
  "requires_dist": [
    "google-api-core !=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5",
    "google-auth <3.0.0dev,>=1.25.0",
    "google-cloud-bigquery <4.0.0dev,>=2.25.2",
    "packaging",
    "sqlalchemy <2.0.0dev,>=1.2.0",
    "alembic ; extra == 'alembic'",
    "alembic ; extra == 'all'",
    "GeoAlchemy2 ; extra == 'all'",
    "google-cloud-bigquery-storage <3.0.0dev,>=2.0.0 ; extra == 'all'",
    "grpcio <2.0dev,>=1.47.0 ; extra == 'all'",
    "shapely ; extra == 'all'",
    "packaging ; extra == 'all'",
    "pytz ; extra == 'all'",
    "pyarrow >=3.0.0 ; extra == 'all'",
    "grpcio <2.0dev,>=1.49.1 ; (python_version >= \"3.11\") and extra == 'all'",
    "google-cloud-bigquery-storage <3.0.0dev,>=2.0.0 ; extra == 'bqstorage'",
    "grpcio <2.0dev,>=1.47.0 ; extra == 'bqstorage'",
    "pyarrow >=3.0.0 ; extra == 'bqstorage'",
    "grpcio <2.0dev,>=1.49.1 ; (python_version >= \"3.11\") and extra == 'bqstorage'",
    "GeoAlchemy2 ; extra == 'geography'",
    "shapely ; extra == 'geography'",
    "packaging ; extra == 'tests'",
    "pytz ; extra == 'tests'"
  ],
  "requires_python": ">=3.8, <3.13",
  "summary": "sqlalchemy dialect for bigquery",
  "version": "1.9.0",
  "releases": [],
  "developers": [
    "googleapis-packages@google.com",
    "the_sqlalchemy"
  ],
  "kwds": "_sqlalchemy sqlalchemy bigquery_description bigquery_friendly_name bigquery",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_sqlalchemy_bigquery",
  "homepage": "https://github.com/googleapis/python-bigquery-sqlalchemy",
  "release_count": 26,
  "dependency_ids": [
    "pypi_alembic",
    "pypi_geoalchemy2",
    "pypi_google_api_core",
    "pypi_google_auth",
    "pypi_google_cloud_bigquery",
    "pypi_google_cloud_bigquery_storage",
    "pypi_grpcio",
    "pypi_packaging",
    "pypi_pyarrow",
    "pypi_pytz",
    "pypi_shapely",
    "pypi_sqlalchemy"
  ]
}