{
  "classifiers": [
    "development status :: 4 - beta",
    "environment :: console",
    "intended audience :: developers",
    "intended audience :: education",
    "intended audience :: information technology",
    "intended audience :: science/research",
    "license :: osi approved :: gnu general public license v3 or later (gplv3+)",
    "operating system :: macos :: macos x",
    "operating system :: microsoft :: windows",
    "operating system :: posix :: linux",
    "programming language :: python",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.12",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: internet :: www/http",
    "topic :: scientific/engineering :: information analysis",
    "topic :: text processing :: filters",
    "topic :: text processing :: linguistic"
  ],
  "description": "courlan: clean, filter, normalize, and sample urls\n==================================================\n\n\n.. image:: https://img.shields.io/pypi/v/courlan.svg\n    :target: https://pypi.python.org/pypi/courlan\n    :alt: python package\n\n.. image:: https://img.shields.io/pypi/pyversions/courlan.svg\n    :target: https://pypi.python.org/pypi/courlan\n    :alt: python versions\n\n.. image:: https://img.shields.io/codecov/c/github/adbar/courlan.svg\n    :target: https://codecov.io/gh/adbar/courlan\n    :alt: code coverage\n\n.. image:: https://img.shields.io/badge/code%20style-black-000000.svg\n   :target: https://github.com/psf/black\n   :alt: code style: black\n\n\nwhy courlan?\n------------\n\n    \u201cit is important for the crawler to visit \"important\" pages first, so that the fraction of the web that is visited (and kept up to date) is more meaningful.\u201d (cho et al. 1998)\n\n    \u201cgiven that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\u201d (edwards et al. 2001)\n\n\nthis library provides an additional \u201cbrain\u201d for web crawling, scraping and document management. it facilitates web navigation through a set of filters, enhancing the quality of resulting document collections:\n\n- save bandwidth and processing time by steering clear of pages deemed low-value\n- identify specific pages based on language or text content\n- pinpoint pages relevant for efficient link gathering\n\nadditional utilities needed include url storage, filtering, and deduplication.\n\n\nfeatures\n--------\n\nseparate the wheat from the chaff and optimize document discovery and retrieval:\n\n- url handling\n   - validation\n   - normalization\n   - sampling\n- heuristics for link filtering\n   - spam, trackers, and content-types\n   - language/locale-aware processing\n   - web crawling (frontier, scheduling)\n- data store specifically designed for urls\n- usable with python or on the command-line\n\n\n**let the courlan fish up juicy bits for you!**\n\n.. image:: courlan_harns-march.jpg\n    :alt: courlan \n    :align: center\n    :width: 65%\n    :target: https://commons.wikimedia.org/wiki/file:limpkin,_harns_marsh_(33723700146).jpg\n\nhere is a `courlan <https://en.wiktionary.org/wiki/courlan>`_ (source: `limpkin at harn's marsh by russ <https://commons.wikimedia.org/wiki/file:limpkin,_harns_marsh_(33723700146).jpg>`_, cc by 2.0).\n\n\n\ninstallation\n------------\n\nthis package is compatible with with all common versions of python, it is tested on linux, macos and windows systems.\n\ncourlan is available on the package repository `pypi <https://pypi.org/>`_ and can notably be installed with the python package manager ``pip``:\n\n.. code-block:: bash\n\n    $ pip install courlan # pip3 install on systems where both python 2 and 3 are installed\n    $ pip install --upgrade courlan # to make sure you have the latest version\n    $ pip install git+https://github.com/adbar/courlan.git # latest available code (see build status above)\n\n\npython\n------\n\nmost filters revolve around the ``strict`` and ``language`` arguments.\n\n\ncheck_url()\n~~~~~~~~~~~\n\nall useful operations chained in ``check_url(url)``:\n\n.. code-block:: python\n\n    >>> from courlan import check_url\n\n    # return url and domain name\n    >>> check_url('https://github.com/adbar/courlan')\n    ('https://github.com/adbar/courlan', 'github.com')\n\n    # filter out bogus domains\n    >>> check_url('http://666.0.0.1/')\n    >>>\n\n    # tracker removal\n    >>> check_url('http://test.net/foo.html?utm_source=twitter#gclid=123')\n    ('http://test.net/foo.html', 'test.net')\n\n    # use strict for further trimming\n    >>> my_url = 'https://httpbin.org/redirect-to?url=http%3a%2f%2fexample.org'\n    >>> check_url(my_url, strict=true)\n    ('https://httpbin.org/redirect-to', 'httpbin.org')\n\n    # check for redirects (head request)\n    >>> url, domain_name = check_url(my_url, with_redirects=true)\n\n\nlanguage-aware heuristics, notably internationalization in urls, are available in ``lang_filter(url, language)``:\n\n.. code-block:: python\n\n    # optional language argument\n    >>> url = 'https://www.un.org/en/about-us'\n\n    # success: returns clean url and domain name\n    >>> check_url(url, language='en')\n    ('https://www.un.org/en/about-us', 'un.org')\n\n    # failure: doesn't return anything\n    >>> check_url(url, language='de')\n    >>>\n\n    # optional argument: strict\n    >>> url = 'https://en.wikipedia.org/'\n    >>> check_url(url, language='de', strict=false)\n    ('https://en.wikipedia.org', 'wikipedia.org')\n    >>> check_url(url, language='de', strict=true)\n    >>>\n\n\ndefine stricter restrictions on the expected content type with ``strict=true``. also blocks certain platforms and pages types crawlers should stay away from if they don't target them explicitly and other black holes where machines get lost.\n\n.. code-block:: python\n\n    # strict filtering: blocked as it is a major platform\n    >>> check_url('https://www.twitch.com/', strict=true)\n    >>>\n\n\n\nsampling by domain name\n~~~~~~~~~~~~~~~~~~~~~~~\n\n\n.. code-block:: python\n\n    >>> from courlan import sample_urls\n    >>> my_urls = ['https://example.org/' + str(x) for x in range(100)]\n    >>> my_sample = sample_urls(my_urls, 10)\n    # optional: exclude_min=none, exclude_max=none, strict=false, verbose=false\n\n\nweb crawling and url handling\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\ndetermine if a link leads to another host:\n\n.. code-block:: python\n\n    >>> from courlan import is_external\n    >>> is_external('https://github.com/', 'https://www.microsoft.com/')\n    true\n    # default\n    >>> is_external('https://google.com/', 'https://www.google.co.uk/', ignore_suffix=true)\n    false\n    # taking suffixes into account\n    >>> is_external('https://google.com/', 'https://www.google.co.uk/', ignore_suffix=false)\n    true\n\n\nother useful functions dedicated to url handling:\n\n- ``extract_domain(url, fast=true)``: find domain and subdomain or just domain with ``fast=false``\n- ``get_base_url(url)``: strip the url of some of its parts\n- ``get_host_and_path(url)``: decompose urls in two parts: protocol + host/domain and path\n- ``get_hostinfo(url)``: extract domain and host info (protocol + host/domain)\n- ``fix_relative_urls(baseurl, url)``: prepend necessary information to relative links\n\n\n.. code-block:: python\n\n    >>> from courlan import *\n    >>> url = 'https://www.un.org/en/about-us'\n\n    >>> get_base_url(url)\n    'https://www.un.org'\n\n    >>> get_host_and_path(url)\n    ('https://www.un.org', '/en/about-us')\n\n    >>> get_hostinfo(url)\n    ('un.org', 'https://www.un.org')\n\n    >>> fix_relative_urls('https://www.un.org', 'en/about-us')\n    'https://www.un.org/en/about-us'\n\n\nother filters dedicated to crawl frontier management:\n\n- ``is_not_crawlable(url)``: check for deep web or pages generally not usable in a crawling context\n- ``is_navigation_page(url)``: check for navigation and overview pages\n\n\n.. code-block:: python\n\n    >>> from courlan import is_navigation_page, is_not_crawlable\n    >>> is_navigation_page('https://www.randomblog.net/category/myposts')\n    true\n    >>> is_not_crawlable('https://www.randomblog.net/login')\n    true\n\n\npython helpers\n~~~~~~~~~~~~~~\n\nhelper function, scrub and normalize:\n\n.. code-block:: python\n\n    >>> from courlan import clean_url\n    >>> clean_url('https://www.dwds.de:80/')\n    'https://www.dwds.de'\n\n\nbasic scrubbing only:\n\n.. code-block:: python\n\n    >>> from courlan import scrub_url\n\n\nbasic canonicalization/normalization only, i.e. modifying and standardizing urls in a consistent manner:\n\n.. code-block:: python\n\n    >>> from urllib.parse import urlparse\n    >>> from courlan import normalize_url\n    >>> my_url = normalize_url(urlparse(my_url))\n    # passing url strings directly also works\n    >>> my_url = normalize_url(my_url)\n    # remove unnecessary components and re-order query elements\n    >>> normalize_url('http://test.net/foo.html?utm_source=twitter&post=abc&page=2#fragment', strict=true)\n    'http://test.net/foo.html?page=2&post=abc'\n\n\nbasic url validation only:\n\n.. code-block:: python\n\n    >>> from courlan import validate_url\n    >>> validate_url('http://1234')\n    (false, none)\n    >>> validate_url('http://www.example.org/')\n    (true, parseresult(scheme='http', netloc='www.example.org', path='/', params='', query='', fragment=''))\n\n\ntroubleshooting\n~~~~~~~~~~~~~~~\n\ncourlan uses an internal cache to speed up url parsing. it can be reset as follows:\n\n.. code-block:: python\n\n    >>> from courlan.meta import clear_caches\n    >>> clear_caches()\n\n\n\nurlstore class\n~~~~~~~~~~~~~~\n\nthe ``urlstore`` class allow for storing and retrieving domain-classified urls, where a url like ``https://example.org/path/testpage`` is stored as the path ``/path/testpage`` within the domain ``https://example.org``. it features the following methods:\n\n- url management\n   - ``add_urls(urls=[], appendleft=none, visited=false)``: add a list of urls to the (possibly) existing one. optional: append certain urls to the left, specify if the urls have already been visited.\n   - ``add_from_html(htmlstring, url, external=false, lang=none, with_nav=true)``: extract and filter links in a html string.\n   - ``discard(domains)``: declare domains void and prune the store.\n   - ``dump_urls()``: return a list of all known urls.\n   - ``print_urls()``: print all urls in store (url + tab + visited or not).\n   - ``print_unvisited_urls()``: print all unvisited urls in store.\n   - ``get_all_counts()``: return all download counts for the hosts in store.\n   - ``get_known_domains()``: return all known domains as a list.\n   - ``get_unvisited_domains()``: find all domains for which there are unvisited urls.\n   - ``total_url_number()``: find number of all urls in store.\n   - ``is_known(url)``: check if the given url has already been stored.\n   - ``has_been_visited(url)``: check if the given url has already been visited.\n   - ``filter_unknown_urls(urls)``: take a list of urls and return the currently unknown ones.\n   - ``filter_unvisited_urls(urls)``: take a list of urls and return the currently unvisited ones.\n   - ``find_known_urls(domain)``: get all already known urls for the given domain (ex. \"https://example.org\").\n   - ``find_unvisited_urls(domain)``: get all unvisited urls for the given domain.\n   - ``get_unvisited_domains()``: return all domains which have not been all visited.\n   - ``reset()``: re-initialize the url store.\n- crawling and downloads\n   - ``get_url(domain)``: retrieve a single url and consider it to be visited (with corresponding timestamp).\n   - ``get_rules(domain)``: return the stored crawling rules for the given website.\n   - ``store_rules(website, rules=none)``: store crawling rules for a given website.\n   - ``get_crawl_delay()``: return the delay as extracted from robots.txt, or a given default.\n   - ``get_download_urls(timelimit=10)``: get a list of immediately downloadable urls according to the given time limit per domain.\n   - ``establish_download_schedule(max_urls=100, time_limit=10)``: get up to the specified number of urls along with a suitable backoff schedule (in seconds).\n   - ``download_threshold_reached(threshold)``: find out if the download limit (in seconds) has been reached for one of the websites in store.\n   - ``unvisited_websites_number()``: return the number of websites for which there are still urls to visit.\n   - ``is_exhausted_domain(domain)``: tell if all known urls for the website have been visited.\n\noptional settings:\n- ``compressed=true``: activate compression of urls and rules\n- ``language=xx``: focus on a particular target language (two-letter code)\n- ``strict=true``: stricter url filtering\n- ``verbose=true``: dump urls if interrupted (requires use of ``signal``)\n\n\ncommand-line\n------------\n\nthe main fonctions are also available through a command-line utility.\n\n.. code-block:: bash\n\n    $ courlan --inputfile url-list.txt --outputfile cleaned-urls.txt\n    $ courlan --help\n    usage: courlan [-h] -i inputfile -o outputfile [-d discardedfile] [-v]\n                   [-p parallel] [--strict] [-l language] [-r] [--sample sample]\n                   [--exclude-max exclude_max] [--exclude-min exclude_min]\n\n\noptional arguments:\n  -h, --help            show this help message and exit\n\ni/o:\n  manage input and output\n\n  -i inputfile, --inputfile inputfile\n                        name of input file (required)\n  -o outputfile, --outputfile outputfile\n                        name of output file (required)\n  -d discardedfile, --discardedfile discardedfile\n                        name of file to store discarded urls (optional)\n  -v, --verbose         increase output verbosity\n  -p parallel, --parallel parallel\n                        number of parallel processes (not used for sampling)\n\nfiltering:\n  configure url filters\n\n  --strict              perform more restrictive tests\n  -l language, --language language\n                        use language filter (iso 639-1 code)\n  -r, --redirects       check redirects\n\nsampling:\n  use sampling by host, configure sample size\n\n  --sample sample       size of sample per domain\n  --exclude-max exclude_max\n                        exclude domains with more than n urls\n  --exclude-min exclude_min\n                        exclude domains with less than n urls\n\n\nlicense\n-------\n\n*courlan* is distributed under the `gnu general public license v3.0 <https://github.com/adbar/courlan/blob/master/license>`_. if you wish to redistribute this library but feel bounded by the license conditions please try interacting `at arms length <https://www.gnu.org/licenses/gpl-faq.html#gplinproprietarysystem>`_, `multi-licensing <https://en.wikipedia.org/wiki/multi-licensing>`_ with `compatible licenses <https://en.wikipedia.org/wiki/gnu_general_public_license#compatibility_and_multi-licensing>`_, or `contacting me <https://github.com/adbar/courlan#author>`_.\n\nsee also `gpl and free software licensing: what's in it for business? <https://web.archive.org/web/20230127221311/https://www.techrepublic.com/article/gpl-and-free-software-licensing-whats-in-it-for-business/>`_\n\n\n\nsettings\n--------\n\n``courlan`` is optimized for english and german but its generic approach is also usable in other contexts.\n\ndetails of strict url filtering can be reviewed and changed in the file ``settings.py``. to override the default settings, `clone the repository <https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository-from-github>`_ and `re-install the package locally <https://packaging.python.org/tutorials/installing-packages/#installing-from-a-local-src-tree>`_.\n\n\n\ncontributing\n------------\n\n`contributions <https://github.com/adbar/courlan/blob/master/contributing.md>`_ are welcome!\n\nfeel free to file issues on the `dedicated page <https://github.com/adbar/courlan/issues>`_.\n\n\nauthor\n------\n\nthis effort is part of methods to derive information from web documents in order to build `text databases for research <https://www.dwds.de/d/k-web>`_ (chiefly linguistic analysis and natural language processing). extracting and pre-processing web texts to the exacting standards of scientific research presents a substantial challenge for those who conduct such research. web corpus construction involves numerous design decisions, and this software package can help facilitate text data collection and enhance corpus quality.\n\n- barbaresi, a. \"`trafilatura: a web scraping library and command-line tool for text discovery and extraction <https://aclanthology.org/2021.acl-demo.15/>`_.\" *proceedings of acl/ijcnlp 2021: system demonstrations*, 2021, pp. 122-131.\n- barbaresi, a. \"`generic web content extraction with open-source software <https://konvens.org/proceedings/2019/papers/kaleidoskop/camera_ready_barbaresi.pdf>`_.\" *proceedings of the 15th conference on natural language processing (konvens 2019)*, 2019, pp. 267-268.\n\ncontact: see `homepage <https://adrien.barbaresi.eu/>`_ or `github <https://github.com/adbar>`_.\n\nsoftware ecosystem: see `this graphic <https://github.com/adbar/trafilatura/blob/master/docs/software-ecosystem.png>`_.\n\n\n\nsimilar work\n------------\n\nthese python libraries perform similar handling and normalization tasks but do not entail language or content filters. they also do not primarily focus on crawl optimization:\n\n- `furl <https://github.com/gruns/furl>`_\n- `ural <https://github.com/medialab/ural>`_\n- `yarl <https://github.com/aio-libs/yarl>`_\n\n\nreferences\n----------\n\n- cho, j., garcia-molina, h., & page, l. (1998). efficient crawling through url ordering. *computer networks and isdn systems*, 30(1-7), 161\u2013172.\n- edwards, j., mccurley, k. s., and tomlin, j. a. (2001). \"an adaptive model for optimizing performance of an incremental web crawler\". in *proceedings of the 10th international conference on world wide web - www '01*, pp. 106\u2013113.\n",
  "docs_url": null,
  "keywords": "cleaner,crawler,preprocessing,url-parsing,url-manipulation,urls,validation,webcrawling",
  "license": "gplv3+",
  "name": "courlan",
  "package_url": "https://pypi.org/project/courlan/",
  "project_url": "https://pypi.org/project/courlan/",
  "project_urls": {
    "Blog": "https://adrien.barbaresi.eu/blog/",
    "Homepage": "https://github.com/adbar/courlan",
    "Tracker": "https://github.com/adbar/courlan/issues"
  },
  "release_url": "https://pypi.org/project/courlan/0.9.5/",
  "requires_dist": [
    "langcodes >=3.3.0",
    "tld ==0.12.6 ; python_version < \"3.7\"",
    "urllib3 <2,>=1.26 ; python_version < \"3.7\"",
    "tld >=0.13 ; python_version >= \"3.7\"",
    "urllib3 <3,>=1.26 ; python_version >= \"3.7\""
  ],
  "requires_python": ">=3.6",
  "summary": "clean, filter and sample urls to optimize data collection \u2013 includes spam, content type and language filters.",
  "version": "0.9.5",
  "releases": [],
  "developers": [
    "adrien_barbaresi",
    "barbaresi@bbaw.de"
  ],
  "kwds": "urlparse crawler webcrawling find_unvisited_urls scraping",
  "license_kwds": "gplv3+",
  "libtype": "pypi",
  "id": "pypi_courlan",
  "homepage": "https://github.com/adbar/courlan",
  "release_count": 25,
  "dependency_ids": [
    "pypi_langcodes",
    "pypi_tld",
    "pypi_urllib3"
  ]
}