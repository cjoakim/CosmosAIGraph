{
  "classifiers": [
    "development status :: 5 - production/stable",
    "license :: osi approved :: apache software license",
    "operating system :: macos :: macos x",
    "operating system :: microsoft :: windows",
    "operating system :: posix :: linux",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "## dbt-duckdb\n\n[duckdb](http://duckdb.org) is an embedded database, similar to sqlite, but designed for olap-style analytics.\nit is crazy fast and allows you to read and write data stored in csv, json, and parquet files directly, without requiring you to load\nthem into the database first.\n\n[dbt](http://getdbt.com) is the best way to manage a collection of data transformations written in sql or python for analytics\nand data science. `dbt-duckdb` is the project that ties duckdb and dbt together, allowing you to create a [modern data stack in\na box](https://duckdb.org/2022/10/12/modern-data-stack-in-a-box.html) or a simple and powerful data lakehouse with python.\n\n### installation\n\nthis project is hosted on pypi, so you should be able to install it and the necessary dependencies via:\n\n`pip3 install dbt-duckdb`\n\nthe latest supported version targets `dbt-core` 1.6.x and `duckdb` version 0.9.x, but we work hard to ensure that newer\nversions of duckdb will continue to work with the adapter as they are released. if you would like to use our new (and experimental!)\nsupport for persisting the tables that duckdb creates to the [aws glue catalog](https://aws.amazon.com/glue/), you should install\n`dbt-duckdb[glue]` in order to get the aws dependencies as well.\n\n### configuring your profile\n\na super-minimal dbt-duckdb profile only needs *one* setting:\n\n````\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n  target: dev\n````\n\nthis will run your dbt-duckdb pipeline against an in-memory duckdb database that will not be persisted after your run completes. this may\nnot seem very useful at first, but it turns out to be a powerful tool for a) testing out data pipelines, either locally or in ci jobs and\nb) running data pipelines that operate purely on external csv, parquet, or json files. more details on how to work with external data files\nin dbt-duckdb are provided in the docs on [reading and writing external files](#reading-and-writing-external-files).\n\nto have your dbt pipeline persist relations in a duckdb file, set the `path` field in your profile to the path\nof the duckdb file that you would like to read and write on your local filesystem. (for in-memory pipelines, the `path`\nis automatically set to the special value `:memory:`).\n\n`dbt-duckdb` also supports common profile fields like `schema` and `threads`, but the `database` property is special: its value is automatically set\nto the basename of the file in the `path` argument with the suffix removed. for example, if the `path` is `/tmp/a/dbfile.duckdb`, the `database`\nfield will be set to `dbfile`. if you are running in in-memory mode, then the `database` property will be automatically set to `memory`.\n\n#### using motherduck\n\nas of `dbt-duckdb` 1.5.2, you can connect to a duckdb instance running on [motherduck](http://www.motherduck.com) by setting your `path` to use a [md:<database> connection string](https://motherduck.com/docs/getting-started/connect-query-from-python/installation-authentication), just as you would with the duckdb cli\nor the python api.\n\nmotherduck databases generally work the same way as local duckdb databases from the perspective of dbt, but\nthere are a [few differences to be aware of](https://motherduck.com/docs/architecture-and-capabilities#considerations-and-limitations):\n1. for the moment, motherduck _requires_ duckdb version `0.9.1`.\n1. motherduck databases do not suppport transactions, so there is a new `disable_transactions` profile.\noption that will be automatically enabled if you are connecting to a motherduck database in your `path`.\n1. motherduck preloads a set of the most common duckdb extensions for you, but does not support loading custom extensions or user-defined functions.\n1. a small subset of advanced sql features are currently unsupported; the only impact of this on the dbt adapter is that the [dbt.listagg](https://docs.getdbt.com/reference/dbt-jinja-functions/cross-database-macros#listagg) macro and foreign-key constraints will work against a local duckdb database, but will not work against a motherduck database.\n\n#### duckdb extensions, settings, and filesystems\n\nyou can load any supported [duckdb extensions](https://duckdb.org/docs/extensions/overview) by listing them in\nthe `extensions` field in your profile. you can also set any additional [duckdb configuration options](https://duckdb.org/docs/sql/configuration)\nvia the `settings` field, including options that are supported in any loaded extensions. for example, to be able to connect to s3 and read/write\nparquet files using an aws access key and secret, your profile would look something like this:\n\n```\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n      extensions:\n        - httpfs\n        - parquet\n      settings:\n        s3_region: my-aws-region\n        s3_access_key_id: \"{{ env_var('s3_access_key_id') }}\"\n        s3_secret_access_key: \"{{ env_var('s3_secret_access_key') }}\"\n  target: dev\n```\n\nas of version `1.4.1`, we have added (experimental!) support for duckdb's (experimental!) support for filesystems\nimplemented via [fsspec](https://duckdb.org/docs/guides/python/filesystems.html). the `fsspec` library provides\nsupport for reading and writing files from a [variety of cloud data storage systems](https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations)\nincluding s3, gcs, and azure blob storage. you can configure a list of fsspec-compatible implementations for use with your dbt-duckdb project by installing the relevant python modules\nand configuring your profile like so:\n\n```\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n      filesystems:\n        - fs: s3\n          anon: false\n          key: \"{{ env_var('s3_access_key_id') }}\"\n          secret: \"{{ env_var('s3_secret_access_key') }}\"\n          client_kwargs:\n            endpoint_url: \"http://localhost:4566\"\n  target: dev\n```\n\nhere, the `filesystems` property takes a list of configurations, where each entry must have a property named `fs` that indicates which `fsspec` protocol\nto load (so `s3`, `gcs`, `abfs`, etc.) and then an arbitrary set of other key-value pairs that are used to configure the `fsspec` implementation. you can see a simple example project that\nillustrates the usage of this feature to connect to a localstack instance running s3 from dbt-duckdb [here](https://github.com/jwills/s3-demo).\n\n#### fetching credentials from context\ninstead of specifying the credentials through the settings block, you can also use the use_credential_provider property. if you set this to `aws` (currently the only supported implementation) and you have `boto3` installed in your python environment, we will fetch your aws credentials using the credential provider chain as described [here](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html). this means that you can use any supported mechanism from aws to obtain credentials (e.g., web identity tokens).\n\n#### attaching additional databases\n\nduckdb version `0.7.0` added support for [attaching additional databases](https://duckdb.org/docs/sql/statements/attach.html) to your dbt-duckdb run so that you can read\nand write from multiple databases. additional databases may be configured using [dbt run hooks](https://docs.getdbt.com/docs/build/hooks-operations) or via the `attach` argument\nin your profile that was added in dbt-duckdb `1.4.0`:\n\n```\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n      attach:\n        - path: /tmp/other.duckdb\n        - path: ./yet/another.duckdb\n          alias: yet_another\n        - path: s3://yep/even/this/works.duckdb\n          read_only: true\n        - path: sqlite.db\n          type: sqlite\n```\n\nthe attached databases may be referred to in your dbt sources and models by either the basename of the database file minus its suffix (e.g., `/tmp/other.duckdb` is the `other` database\nand `s3://yep/even/this/works.duckdb` is the `works` database) or by an alias that you specify (so the `./yet/another.duckdb` database in the above configuration is referred to\nas `yet_another` instead of `another`.) note that these additional databases do not necessarily have to be duckdb files: duckdb's storage and catalog engines are pluggable, and\nduckdb `0.7.0` ships with support for reading and writing from attached sqlite databases. you can indicate the type of the database you are connecting to via the `type` argument,\nwhich currently supports `duckdb` and `sqlite`.\n\n#### configuring dbt-duckdb plugins\n\ndbt-duckdb has its own [plugin](dbt/adapters/duckdb/plugins/__init__.py) system to enable advanced users to extend\ndbt-duckdb with additional functionality, including:\n\n* defining [custom python udfs](https://duckdb.org/docs/api/python/function.html) on the duckdb database connection\nso that they can be used in your sql models\n* loading source data from [excel](dbt/adapters/duckdb/plugins/excel.py), [google sheets](dbt/adapters/duckdb/plugins/gsheet.py), or [sqlalchemy](dbt/adapters/duckdb/plugins/sqlalchemy.py) tables\n\nyou can find more details on [how to write your own plugins here](#writing-your-own-plugins). to configure a plugin for use\nin your dbt project, use the `plugins` property on the profile:\n\n```\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n      plugins:\n        - module: gsheet\n          config:\n            method: oauth\n        - module: sqlalchemy\n          alias: sql\n          config:\n            connection_url: \"{{ env_var('dbt_env_secret_sqlalchemy_uri') }}\"\n        - module: path.to.custom_udf_module\n```\n\nevery plugin must have a `module` property that indicates where the `plugin` class to load is defined. there is\na set of built-in plugins that are defined in [dbt.adapters.duckdb.plugins](dbt/adapters/duckdb/plugins/) that\nmay be referenced by their base filename (e.g., `excel` or `gsheet`), while user-defined plugins (which are\ndescribed later in this document) should be referred to via their full module path name (e.g. a `lib.my.custom` module that defines a class named `plugin`.)\n\neach plugin instance has a name for logging and reference purposes that defaults to the name of the module\nbut that may be overridden by the user by setting the `alias` property in the configuration. finally,\nmodules may be initialized using an arbitrary set of key-value pairs that are defined in the\n`config` dictionary. in this example, we initialize the `gsheet` plugin with the setting `method: oauth` and we\ninitialize the `sqlalchemy` plugin (aliased as \"sql\") with a `connection_url` that is set via an environment variable.\n\nplease remember that using plugins may require you to add additional dependencies to the python environment that your dbt-duckdb pipeline runs in:\n\n* `excel` depends on `pandas`, and `openpyxl` or `xlsxwriter` to perform writes\n* `gsheet` depends on `gspread` and `pandas`\n*  `iceberg` depends on `pyiceberg` and python >= 3.8\n* `sqlalchemy` depends on `pandas`, `sqlalchemy`, and the driver(s) you need\n\n**experimental:**\n\n* `delta` depends on `deltalake`, [an example project](https://github.com/milicevica23/dbt-duckdb-delta-plugin-demo)\n\n**note:** be aware that experimental features can change over time, and we would like your feedback on config and possible different use cases.\n\n#### using local python modules\n\nin dbt-duckdb 1.6.0, we added a new profile setting named `module_paths` that allows users to specify a list\nof paths on the filesystem that contain additional python modules that should be added to the python processes'\n`sys.path` property. this allows users to include additional helper python modules in their dbt projects that\ncan be accessed by the running dbt process and used to define custom dbt-duckdb plugins or library code that is\nhelpful for creating dbt python models.\n\n### reading and writing external files\n\none of duckdb's most powerful features is its ability to read and write csv, json, and parquet files directly, without needing to import/export\nthem from the database first.\n\n#### reading from external files\n\nyou may reference external files in your dbt models either directly or as dbt `source`s by configuring the `external_location`\nmeta option on the source:\n\n```\nsources:\n  - name: external_source\n    meta:\n      external_location: \"s3://my-bucket/my-sources/{name}.parquet\"\n    tables:\n      - name: source1\n      - name: source2\n```\n\nhere, the `meta` options on `external_source` defines `external_location` as an [f-string](https://peps.python.org/pep-0498/) that\nallows us to express a pattern that indicates the location of any of the tables defined for that source. so a dbt model like:\n\n```\nselect *\nfrom {{ source('external_source', 'source1') }}\n```\n\nwill be compiled as:\n\n```\nselect *\nfrom 's3://my-bucket/my-sources/source1.parquet'\n```\n\nif one of the source tables deviates from the pattern or needs some other special handling, then the `external_location` can also be set on the `meta`\noptions for the table itself, for example:\n\n```\nsources:\n  - name: external_source\n    meta:\n      external_location: \"s3://my-bucket/my-sources/{name}.parquet\"\n    tables:\n      - name: source1\n      - name: source2\n        meta:\n          external_location: \"read_parquet(['s3://my-bucket/my-sources/source2a.parquet', 's3://my-bucket/my-sources/source2b.parquet'])\"\n```\n\nin this situation, the `external_location` setting on the `source2` table will take precedence, so a dbt model like:\n\n```\nselect *\nfrom {{ source('external_source', 'source2') }}\n```\n\nwill be compiled to the sql query:\n\n```\nselect *\nfrom read_parquet(['s3://my-bucket/my-sources/source2a.parquet', 's3://my-bucket/my-sources/source2b.parquet'])\n```\n\nnote that the value of the `external_location` property does not need to be a path-like string; it can also be a function\ncall, which is helpful in the case that you have an external source that is a csv file which requires special handling for duckdb to load it correctly:\n\n```\nsources:\n  - name: flights_source\n    tables:\n      - name: flights\n        meta:\n          external_location: \"read_csv('flights.csv', types={'flightdate': 'date'}, names=['flightdate', 'uniquecarrier'])\"\n          formatter: oldstyle\n```\n\nnote that we need to override the default `str.format` string formatting strategy for this example\nbecause the `types={'flightdate': 'date'}` argument to the `read_csv` function will be interpreted by\n`str.format` as a template to be matched on, which will cause a `keyerror: \"'flightdate'\"` when we attempt\nto parse the source in a dbt model. the `formatter` configuration option for the source indicates whether\nwe should use `newstyle` string formatting (the default), `oldstyle` string formatting, or `template` string\nformatting. you can read up on the strategies the various string formatting techniques use at this\n[stack overflow answer](https://stackoverflow.com/questions/13451989/pythons-many-ways-of-string-formatting-are-the-older-ones-going-to-be-depre) and see examples of their use\nin this [dbt-duckdb integration test](https://github.com/jwills/dbt-duckdb/blob/master/tests/functional/adapter/test_sources.py).\n\n#### writing to external files\n\nwe support creating dbt models that are backed by external files via the `external` materialization strategy:\n\n```\n{{ config(materialized='external', location='local/directory/file.parquet') }}\nselect m.*, s.id is not null as has_source_id\nfrom {{ ref('upstream_model') }} m\nleft join {{ source('upstream', 'source') }} s using (id)\n```\n\n| option | default | description\n| :---:    |  :---:    | ---\n| location | [external_location](dbt/include/duckdb/macros/utils/external_location.sql) macro | the path to write the external materialization to. see below for more details.\n| format | parquet | the format of the external file (parquet, csv, or json)\n| delimiter | ,    | for csv files, the delimiter to use for fields.\n| options | none | any other options to pass to duckdb's `copy` operation (e.g., `partition_by`, `codec`, etc.)\n| glue_register | false | if true, try to register the file created by this model with the aws glue catalog.\n| glue_database | default | the name of the aws glue database to register the model with.\n\nif the `location` argument is specified, it must be a filename (or s3 bucket/path), and dbt-duckdb will attempt to infer\nthe `format` argument from the file extension of the `location` if the `format` argument is unspecified (this functionality was\nadded in version 1.4.1.)\n\nif the `location` argument is _not_ specified, then the external file will be named after the model.sql (or model.py) file that defined it\nwith an extension that matches the `format` argument (`parquet`, `csv`, or `json`). by default, the external files are created\nrelative to the current working directory, but you can change the default directory (or s3 bucket/prefix) by specifying the\n`external_root` setting in your duckdb profile.\n\n#### re-running external models with an in-memory version of dbt-duckdb\nwhen using `:memory:` as the duckdb database, subsequent dbt runs can fail when selecting a subset of models that depend on external tables. this is because external files are only registered as  duckdb views when they are created, not when they are referenced. to overcome this issue we have provided the `register_upstream_external_models` macro that can be triggered at the beginning of a run. to enable this automatic registration, place the following in your `dbt_project.yml` file:\n\n```yaml\non-run-start:\n  - \"{{ register_upstream_external_models() }}\"\n```\n\n### python support\n\ndbt added support for [python models in version 1.3.0](https://docs.getdbt.com/docs/build/python-models). for most data platforms,\ndbt will package up the python code defined in a `.py` file and ship it off to be executed in whatever python environment that\ndata platform supports (e.g., snowpark for snowflake or dataproc for bigquery.) in dbt-duckdb, we execute python models in the same\nprocess that owns the connection to the duckdb database, which by default, is the python process that is created when you run dbt.\nto execute the python model, we treat the `.py` file that your model is defined in as a python module and load it into the\nrunning process using [importlib](https://docs.python.org/3/library/importlib.html). we then construct the arguments to the `model`\nfunction that you defined (a `dbt` object that contains the names of any `ref` and `source` information your model needs and a\n`duckdbpyconnection` object for you to interact with the underlying duckdb database), call the `model` function, and then materialize\nthe returned object as a table in duckdb.\n\nthe value of the `dbt.ref` and `dbt.source` functions inside of a python model will be a [duckdb relation](https://duckdb.org/docs/api/python/reference/)\nobject that can be easily converted into a pandas/polars dataframe or an arrow table. the return value of the `model` function can be\nany python object that duckdb knows how to turn into a table, including a pandas/polars `dataframe`, a duckdb `relation`, or an arrow `table`,\n`dataset`, `recordbatchreader`, or `scanner`.\n\n#### batch processing with python models\n\nas of version 1.6.1, it is possible to both read and write data in chunks, which allows for larger-than-memory\ndatasets to be manipulated in python models. here is a basic example:\n```\nimport pyarrow as pa\n\ndef batcher(batch_reader: pa.recordbatchreader):\n    for batch in batch_reader:\n        df = batch.to_pandas()\n        # do some operations on the df...\n        # ...then yield back a new batch\n        yield pa.recordbatch.from_pandas(df)\n\ndef model(dbt, session):\n    big_model = dbt.ref(\"big_model\")\n    batch_reader = big_model.record_batch(100_000)\n    batch_iter = batcher(batch_reader)\n    return pa.recordbatchreader.from_batches(batch_reader.schema, batch_iter)\n```\n\n### writing your own plugins\n\ndefining your own dbt-duckdb plugin is as simple as creating a python module that defines a class named `plugin` that\ninherits from [dbt.adapters.duckdb.plugins.baseplugin](dbt/adapters/duckdb/plugins/__init__.py). there are currently\nfour methods that may be implemented in your plugin class:\n\n1. `initialize`: takes in the `config` dictionary for the plugin that is defined in the profile to enable any\nadditional configuration for the module based on the project; this method is called once when an instance of the\n`plugin` class is created.\n1. `configure_connection`: takes an instance of the `duckdbpyconnection` object used to connect to the duckdb\ndatabase and may perform any additional configuration of that object that is needed by the plugin, like defining\ncustom user-defined functions.\n1. `load`: takes a [sourceconfig](dbt/adapters/duckdb/utils.py) instance, which encapsulates the configuration for a\na dbt source and can optionally return a dataframe-like object that duckdb knows how to turn into a table (this is\nsimilar to a dbt-duckdb python model, but without the ability to `ref` any models or access any information beyond\nthe source config.)\n1. `store`: takes a [targetconfig](dbt/adapters/duckdb/utils.py) instance, which encapsulates the configuration for\nan `external` materialization and can perform additional operations once the csv/parquet/json file is written. the\n[glue](dbt/adapters/duckdb/plugins/glue.py) and [sqlalchemy](dbt/adapters/duckdb/plugins/sqlalchemy.py) are examples\nthat demonstrate how to use the `store` operation to register an aws glue database table or upload a dataframe to\nan external database, respectively.\n\ndbt-duckdb ships with a number of [built-in plugins](dbt/adapters/duckdb/plugins/) that can be used as examples\nfor implementing your own.\n\n### roadmap\n\nthings that we would like to add in the near future:\n\n* support for delta and iceberg external table formats (both as sources and destinations)\n* make dbt's incremental models and snapshots work with external materializations\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "dbt-duckdb",
  "package_url": "https://pypi.org/project/dbt-duckdb/",
  "project_url": "https://pypi.org/project/dbt-duckdb/",
  "project_urls": {
    "Homepage": "https://github.com/jwills/dbt-duckdb"
  },
  "release_url": "https://pypi.org/project/dbt-duckdb/1.7.0/",
  "requires_dist": [
    "dbt-core ~=1.7.0",
    "duckdb >=0.7.0",
    "boto3 ; extra == 'glue'",
    "mypy-boto3-glue ; extra == 'glue'"
  ],
  "requires_python": ">=3.8",
  "summary": "the duckdb adapter plugin for dbt (data build tool)",
  "version": "1.7.0",
  "releases": [],
  "developers": [
    "josh_wills",
    "joshwills+dbt@gmail.com"
  ],
  "kwds": "databases duckdb db dbfile dbt",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_dbt_duckdb",
  "homepage": "https://github.com/jwills/dbt-duckdb",
  "release_count": 29,
  "dependency_ids": [
    "pypi_boto3",
    "pypi_dbt_core",
    "pypi_duckdb",
    "pypi_mypy_boto3_glue"
  ]
}