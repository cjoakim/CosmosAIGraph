{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: developers",
    "license :: osi approved :: mit license",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: software development :: libraries"
  ],
  "description": "# asyncio dataloader\n\ndataloader is a generic utility to be used as part of your application's data\nfetching layer to provide a simplified and consistent api over various remote\ndata sources such as databases or web services via batching and caching.\n\n[![pypi version](https://badge.fury.io/py/aiodataloader.svg)](https://badge.fury.io/py/aiodataloader)\n![test status](https://github.com/syrusakbary/aiodataloader/actions/workflows/test.yml/badge.svg)\n![lint status](https://github.com/syrusakbary/aiodataloader/actions/workflows/lint.yml/badge.svg)\n[![coverage status](https://coveralls.io/repos/syrusakbary/aiodataloader/badge.svg?branch=master&service=github)](https://coveralls.io/github/syrusakbary/aiodataloader?branch=master)\n\na port of the \"loader\" api originally developed by [@schrockn][] at facebook in\n2010 as a simplifying force to coalesce the sundry key-value store back-end\napis which existed at the time. at facebook, \"loader\" became one of the\nimplementation details of the \"ent\" framework, a privacy-aware data entity\nloading and caching layer within web server product code. this ultimately became\nthe underpinning for facebook's graphql server implementation and type\ndefinitions.\n\nasyncio dataloader is a python port of the original javascript [dataloader][]\nimplementation. dataloader is often used when implementing a [graphql][] service,\nthough it is also broadly useful in other situations.\n\n\n## getting started\n\nfirst, install dataloader using pip.\n\n```sh\npip install aiodataloader\n```\n\nto get started, create a `dataloader`. each `dataloader` instance represents a\nunique cache. typically instances are created per request when used within a\nweb-server like [sanic][] if different users can see different things.\n\n> note: dataloader assumes a asyncio environment with `async/await`\n  available only in python 3.5+.\n\n\n## batching\n\nbatching is not an advanced feature, it's dataloader's primary feature.\ncreate loaders by providing a batch loading function.\n\n```python\nfrom aiodataloader import dataloader\n\nclass userloader(dataloader):\n    async def batch_load_fn(self, keys):\n        return await my_batch_get_users(keys)\n\nuser_loader = userloader()\n```\n\na batch loading function accepts an iterable of keys, and returns a promise which\nresolves to a list of values[<sup>*</sup>](#batch-function).\n\nthen load individual values from the loader. dataloader will coalesce all\nindividual loads which occur within a single frame of execution (a single tick\nof the event loop) and then call your batch function with all requested keys.\n\n```python\nuser1_future = user_loader.load(1)\nuser2_future = user_loader.load(2)\n\nuser1 = await user1_future\nuser2 = await user2_future\n\nuser1_invitedby = user_loader.load(user1.invited_by_id)\nuser2_invitedby = user_loader.load(user2.invited_by_id)\n\nprint(\"user 1 was invited by\", await user1_invitedby)\nprint(\"user 2 was invited by\", await user2_invitedby)\n```\n\na naive application may have issued four round-trips to a backend for the\nrequired information, but with dataloader this application will make at most\ntwo.\n\ndataloader allows you to decouple unrelated parts of your application without\nsacrificing the performance of batch data-loading. while the loader presents an\napi that loads individual values, all concurrent requests will be coalesced and\npresented to your batch loading function. this allows your application to safely\ndistribute data fetching requirements throughout your application and maintain\nminimal outgoing data requests.\n\n#### batch function\n\na batch loading function accepts an list of keys, and returns a future which\nresolves to a list of values. there are a few constraints that must be upheld:\n\n * the list of values must be the same length as the list of keys.\n * each index in the list of values must correspond to the same index in the list of keys.\n\nfor example, if your batch function was provided the list of keys: `[ 2, 9, 6, 1 ]`,\nand loading from a back-end service returned the values:\n\n```python\n{ 'id': 9, 'name': 'chicago' }\n{ 'id': 1, 'name': 'new york' }\n{ 'id': 2, 'name': 'san francisco' }\n```\n\nour back-end service returned results in a different order than we requested, likely\nbecause it was more efficient for it to do so. also, it omitted a result for key `6`,\nwhich we can interpret as no value existing for that key.\n\nto uphold the constraints of the batch function, it must return an list of values\nthe same length as the list of keys, and re-order them to ensure each index aligns\nwith the original keys `[ 2, 9, 6, 1 ]`:\n\n```python\n[\n  { 'id': 2, 'name': 'san francisco' },\n  { 'id': 9, 'name': 'chicago' },\n  none,\n  { 'id': 1, 'name': 'new york' }\n]\n```\n\n\n## caching\n\ndataloader provides a memoization cache for all loads which occur in a single\nrequest to your application. after `.load()` is called once with a given key,\nthe resulting value is cached to eliminate redundant loads.\n\nin addition to relieving pressure on your data storage, caching results per-request\nalso creates fewer objects which may relieve memory pressure on your application:\n\n```python\nuser_future1 = user_loader.load(1)\nuser_future2 = user_loader.load(1)\n\nassert user_future1 == user_future2\n```\n\n#### caching per-request\n\ndataloader caching *does not* replace redis, memcache, or any other shared\napplication-level cache. dataloader is first and foremost a data loading mechanism,\nand its cache only serves the purpose of not repeatedly loading the same data in\nthe context of a single request to your application. to do this, it maintains a\nsimple in-memory memoization cache (more accurately: `.load()` is a memoized function).\n\navoid multiple requests from different users using the dataloader instance, which\ncould result in cached data incorrectly appearing in each request. typically,\ndataloader instances are created when a request begins, and are not used once the\nrequest ends.\n\nfor example, when using with [sanic][]:\n\n```python\ndef create_loaders(auth_token) {\n    return {\n      'users': user_loader,\n    }\n}\n\n\napp = sanic(__name__)\n\n@app.route(\"/\")\nasync def test(request):\n    auth_token = authenticate_user(request)\n    loaders = create_loaders(auth_token)\n    return render_page(request, loaders)\n```\n\n#### clearing cache\n\nin certain uncommon cases, clearing the request cache may be necessary.\n\nthe most common example when clearing the loader's cache is necessary is after\na mutation or update within the same request, when a cached value could be out of\ndate and future loads should not use any possibly cached value.\n\nhere's a simple example using sql update to illustrate.\n\n```python\n# request begins...\nuser_loader = ...\n\n# and a value happens to be loaded (and cached).\nuser4 = await user_loader.load(4)\n\n# a mutation occurs, invalidating what might be in cache.\nawait sql_run('update users where id=4 set username=\"zuck\"')\nuser_loader.clear(4)\n\n# later the value load is loaded again so the mutated data appears.\nuser4 = await user_loader.load(4)\n\n# request completes.\n```\n\n#### caching exceptions\n\nif a batch load fails (that is, a batch function throws or returns a rejected\npromise), then the requested values will not be cached. however if a batch\nfunction returns an `exception` instance for an individual value, that `exception` will\nbe cached to avoid frequently loading the same `exception`.\n\nin some circumstances you may wish to clear the cache for these individual errors:\n\n```python\ntry:\n    user_loader.load(1)\nexcept exception as e:\n    user_loader.clear(1)\n    raise\n```\n\n#### disabling cache\n\nin certain uncommon cases, a dataloader which *does not* cache may be desirable.\ncalling `dataloader(batch_fn, cache=false)` will ensure that every\ncall to `.load()` will produce a *new* future, and requested keys will not be\nsaved in memory.\n\nhowever, when the memoization cache is disabled, your batch function will\nreceive an array of keys which may contain duplicates! each key will be\nassociated with each call to `.load()`. your batch loader should provide a value\nfor each instance of the requested key.\n\nfor example:\n\n```python\nclass myloader(dataloader):\n    cache = false\n    async def batch_load_fn(self, keys):\n        print(keys)\n        return keys\n\nmy_loader = myloader()\n\nmy_loader.load('a')\nmy_loader.load('b')\nmy_loader.load('a')\n\n# > [ 'a', 'b', 'a' ]\n```\n\nmore complex cache behavior can be achieved by calling `.clear()` or `.clear_all()`\nrather than disabling the cache completely. for example, this dataloader will\nprovide unique keys to a batch function due to the memoization cache being\nenabled, but will immediately clear its cache when the batch function is called\nso later requests will load new values.\n\n```python\nclass myloader(dataloader):\n    cache = false\n    async def batch_load_fn(self, keys):\n        self.clear_all()\n        return keys\n```\n\n\n## api\n\n#### class dataloader\n\ndataloader creates a public api for loading data from a particular\ndata back-end with unique keys such as the `id` column of a sql table or\ndocument name in a mongodb database, given a batch loading function.\n\neach `dataloader` instance contains a unique memoized cache. use caution when\nused in long-lived applications or those which serve many users with different\naccess permissions and consider creating a new instance per web request.\n\n##### `dataloader(batch_load_fn, **options)`\n\ncreate a new `dataloader` given a batch loading function and options.\n\n- *batch_load_fn*: an async function (coroutine) which accepts an list of keys\n  and returns a future which resolves to an list of values.\n\n- *options*:\n\n  - *batch*: default `true`. set to `false` to disable batching, instead\n    immediately invoking `batch_load_fn` with a single load key.\n\n  - *max_batch_size*: default `infinity`. limits the number of items that get\n    passed in to the `batch_load_fn`.\n\n  - *cache*: default `true`. set to `false` to disable memoization caching,\n    instead creating a new promise and new key in the `batch_load_fn` for every\n    load of the same key.\n\n  - *cache_key_fn*: a function to produce a cache key for a given load key.\n    defaults to `key => key`. useful to provide when python objects are keys\n    and two similarly shaped objects should be considered equivalent.\n\n  - *cache_map*: an instance of [dict][] (or an object with a similar api) to be\n    used as the underlying cache for this loader. default `{}`.\n\n##### `load(key)`\n\nloads a key, returning a `future` for the value represented by that key.\n\n- *key*: an key value to load.\n\n##### `load_many(keys)`\n\nloads multiple keys, promising an array of values:\n\n```python\na, b = await my_loader.load_many([ 'a', 'b' ]);\n```\n\nthis is equivalent to the more verbose:\n\n```python\nfrom asyncio import gather\na, b = await gather(\n    my_loader.load('a'),\n    my_loader.load('b')\n)\n```\n\n- *keys*: a list of key values to load.\n\n##### `clear(key)`\n\nclears the value at `key` from the cache, if it exists. returns itself for\nmethod chaining.\n\n- *key*: an key value to clear.\n\n##### `clear_all()`\n\nclears the entire cache. to be used when some event results in unknown\ninvalidations across this particular `dataloader`. returns itself for\nmethod chaining.\n\n##### `prime(key, value)`\n\nprimes the cache with the provided key and value. if the key already exists, no\nchange is made. (to forcefully prime the cache, clear the key first with\n`loader.clear(key).prime(key, value)`.) returns itself for method chaining.\n\n\n## using with graphql\n\ndataloader pairs nicely well with [graphql][]. graphql fields are\ndesigned to be stand-alone functions. without a caching or batching mechanism,\nit's easy for a naive graphql server to issue new database requests each time a\nfield is resolved.\n\nconsider the following graphql request:\n\n```\n{\n  me {\n    name\n    bestfriend {\n      name\n    }\n    friends(first: 5) {\n      name\n      bestfriend {\n        name\n      }\n    }\n  }\n}\n```\n\nnaively, if `me`, `bestfriend` and `friends` each need to request the backend,\nthere could be at most 13 database requests!\n\nwhen using dataloader with [graphene][], we could define the `user` type with clearer code and\nat most 4 database requests, and possibly fewer if there are cache hits.\n\n```python\nclass user(graphene.objecttype):\n    name = graphene.string()\n    best_friend = graphene.field(lambda: user)\n    friends = graphene.list(lambda: user)\n\n    def resolve_best_friend(self, args, context, info):\n        return user_loader.load(self.best_friend_id)\n\n    def resolve_friends(self, args, context, info):\n        return user_loader.load_many(self.friend_ids)\n```\n\n\n## common patterns\n\n### creating a new dataloader per request.\n\nin many applications, a web server using dataloader serves requests to many\ndifferent users with different access permissions. it may be dangerous to use\none cache across many users, and is encouraged to create a new dataloader\nper request:\n\n```python\ndef create_loaders(auth_token):\n  return {\n    'users': dataloader(lambda ids: gen_users(auth_token, ids)),\n    'cdn_urls': dataloader(lambda raw_urls: gen_cdn_urls(auth_token, raw_urls)),\n    'stories': dataloader(lambda keys: gen_stories(auth_token, keys)),\n  }\n}\n\n# when handling an incoming web request:\nloaders = create_loaders(request.query.auth_token)\n\n# then, within application logic:\nuser = await loaders.users.load(4)\npic = await loaders.cdn_urls.load(user.raw_pic_url)\n```\n\ncreating an object where each key is a `dataloader` is one common pattern which\nprovides a single value to pass around to code which needs to perform\ndata loading, such as part of the `root_value` in a [graphql][] request.\n\n### loading by alternative keys.\n\noccasionally, some kind of value can be accessed in multiple ways. for example,\nperhaps a \"user\" type can be loaded not only by an \"id\" but also by a \"username\"\nvalue. if the same user is loaded by both keys, then it may be useful to fill\nboth caches when a user is loaded from either source:\n\n```python\nasync def user_by_id_batch_fn(ids):\n    users = await gen_users_by_id(ids)\n    for user in users:\n        username_loader.prime(user.username, user)\n    return users\n\nuser_by_id_loader = dataloader(user_by_id_batch_fn)\n\nasync def username_batch_fn(names):\n    users = await gen_usernames(names)\n    for user in users:\n        user_by_id_loader.prime(user.id, user)\n    return users\n\nusername_loader = dataloader(username_batch_fn)\n```\n\n\n## custom caches\n\ndataloader can optionaly be provided a custom dict instance to use as its\nmemoization cache. more specifically, any object that implements the methods `get()`,\n`set()`, `delete()` and `clear()` can be provided. this allows for custom dicts\nwhich implement various [cache algorithms][] to be provided. by default,\ndataloader uses the standard [dict][] which simply grows until the dataloader\nis released. the default is appropriate when requests to your application are\nshort-lived.\n\n\n\n## video source code walkthrough\n\n**dataloader source code walkthrough (youtube):**\n\n<a href=\"https://youtu.be/oqtnxncdywa\" target=\"_blank\" alt=\"dataloader source code walkthrough\"><img src=\"https://img.youtube.com/vi/oqtnxncdywa/0.jpg\" /></a>\n\n\n[@schrockn]: https://github.com/schrockn\n[dataloader]: https://github.com/graphql/dataloader\n[graphql]: https://graphql.org\n[dict]: https://docs.python.org/3/tutorial/datastructures.html#dictionaries\n[graphene]: https://github.com/graphql-python/graphene\n[graphql-core]: https://github.com/graphql-python/graphql-core\n[cache algorithms]: https://en.wikipedia.org/wiki/cache_algorithms\n[sanic]: https://sanic.readthedocs.io/en/latest/\n",
  "docs_url": null,
  "keywords": "aiodataloader,concurrent,deferred,future",
  "license": "",
  "name": "aiodataloader",
  "package_url": "https://pypi.org/project/aiodataloader/",
  "project_url": "https://pypi.org/project/aiodataloader/",
  "project_urls": {
    "Homepage": "https://github.com/syrusakbary/aiodataloader"
  },
  "release_url": "https://pypi.org/project/aiodataloader/0.4.0/",
  "requires_dist": [
    "typing-extensions>=4.1.1",
    "black; extra == 'lint'",
    "flake8; extra == 'lint'",
    "flake8-import-order; extra == 'lint'",
    "mypy; extra == 'lint'",
    "coveralls; extra == 'test'",
    "mock; extra == 'test'",
    "pytest-asyncio; extra == 'test'",
    "pytest-cov; extra == 'test'",
    "pytest>=3.6; extra == 'test'"
  ],
  "requires_python": ">=3.7",
  "summary": "asyncio dataloader implementation for python",
  "version": "0.4.0",
  "releases": [],
  "developers": [
    "me@syrusakbary.com"
  ],
  "kwds": "async dataloader asyncio apis await",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_aiodataloader",
  "homepage": "",
  "release_count": 7,
  "dependency_ids": [
    "pypi_black",
    "pypi_coveralls",
    "pypi_flake8",
    "pypi_flake8_import_order",
    "pypi_mock",
    "pypi_mypy",
    "pypi_pytest",
    "pypi_pytest_asyncio",
    "pypi_pytest_cov",
    "pypi_typing_extensions"
  ]
}