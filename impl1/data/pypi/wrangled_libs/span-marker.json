{
  "classifiers": [],
  "description": "<div align=\"center\">\r\n<h1>\r\nspanmarker for named entity recognition\r\n</h1>\r\n<a href=\"https://huggingface.co/tomaarsen/span-marker-roberta-large-ontonotes5\" target=\"_blank\">\r\n    <img src=\"https://github.com/tomaarsen/spanmarkerner/assets/37621491/c76d6393-bb0b-44c3-9412-fd9c8313dcc1\">\r\n</a>\r\n\r\n[\ud83e\udd17 models](https://huggingface.co/models?library=span-marker) |\r\n[\ud83d\udee0\ufe0f getting started in google colab](https://colab.research.google.com/github/tomaarsen/spanmarkerner/blob/main/notebooks/getting_started.ipynb) |\r\n[\ud83d\udcc4 documentation](https://tomaarsen.github.io/spanmarkerner) | \ud83d\udcca [thesis](https://raw.githubusercontent.com/tomaarsen/spanmarkerner/main/thesis.pdf)\r\n</div>\r\n\r\nspanmarker is a framework for training powerful named entity recognition models using familiar encoders such as bert, roberta and electra.\r\nbuilt on top of the familiar [\ud83e\udd17 transformers](https://github.com/huggingface/transformers) library, spanmarker inherits a wide range of powerful functionalities, such as easily loading and saving models, hyperparameter optimization, automatic logging in various tools, checkpointing, callbacks, mixed precision training, 8-bit inference, and more.\r\n\r\n<!--tightly implemented on top of the [\ud83e\udd17 transformers](https://github.com/huggingface/transformers/) library, spanmarker can take advantage of its valuable functionality.-->\r\n<!-- like performance dashboard integration, automatic mixed precision, 8-bit inference-->\r\n\r\nbased on the [pl-marker](https://arxiv.org/pdf/2109.06067.pdf) paper, spanmarker breaks the mold through its accessibility and ease of use. crucially, spanmarker works out of the box with many common encoders such as `bert-base-cased`, `roberta-large` and `bert-base-multilingual-cased`, and automatically works with datasets using the `iob`, `iob2`, `bioes`, `bilou` or no label annotation scheme.\r\n\r\nadditionally, the spanmarker library has been integrated with the hugging face hub and the hugging face inference api. see the spanmarker documentation on [hugging face](https://huggingface.co/docs/hub/span_marker) or see [all spanmarker models on the hugging face hub](https://huggingface.co/models?library=span-marker).\r\nthrough the inference api integration, users can test any spanmarker model on the hugging face hub for free using a widget on the [model page](https://huggingface.co/tomaarsen/span-marker-bert-base-fewnerd-fine-super). furthermore, each public spanmarker model offers a free api for fast prototyping and can be deployed to production using hugging face inference endpoints.\r\n\r\n| inference api widget (on a model page) | free inference api (`deploy` > `inference api` on a model page) |\r\n| ------------- | ------------- |\r\n|  ![image](https://github.com/tomaarsen/spanmarkerner/assets/37621491/234078b7-22c8-491c-8686-faccd394f683) |  ![image](https://github.com/tomaarsen/spanmarkerner/assets/37621491/410e5191-9354-4e27-b718-2d69af678eb7) |\r\n\r\n## documentation\r\nfeel free to have a look at the [documentation](https://tomaarsen.github.io/spanmarkerner).\r\n\r\n## installation\r\nyou may install the [`span_marker`](https://pypi.org/project/span-marker) python module via `pip` like so:\r\n```\r\npip install span_marker\r\n```\r\n\r\n## quick start\r\n### training\r\nplease have a look at our [getting started](notebooks/getting_started.ipynb) notebook for details on how spanmarker is commonly used. it explains the following snippet in more detail. alternatively, have a look at the [training scripts](training_scripts) that have been successfully used in the past.\r\n\r\n| colab                                                                                                                                                                                                         | kaggle                                                                                                                                                                                                             | gradient                                                                                                                                                                                         | studio lab                                                                                                                                                                                                             |\r\n|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\r\n| [![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tomaarsen/spanmarkerner/blob/main/notebooks/getting_started.ipynb)                       | [![kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/tomaarsen/spanmarkerner/blob/main/notebooks/getting_started.ipynb)                       | [![gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/tomaarsen/spanmarkerner/blob/main/notebooks/getting_started.ipynb)                       | [![open in sagemaker studio lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/tomaarsen/spanmarkerner/blob/main/notebooks/getting_started.ipynb)                       |\r\n\r\n```python\r\nfrom pathlib import path\r\nfrom datasets import load_dataset\r\nfrom transformers import trainingarguments\r\nfrom span_marker import spanmarkermodel, trainer, spanmarkermodelcarddata\r\n\r\n\r\ndef main() -> none:\r\n    # load the dataset, ensure \"tokens\" and \"ner_tags\" columns, and get a list of labels\r\n    dataset_id = \"dfki-slt/few-nerd\"\r\n    dataset_name = \"fewnerd\"\r\n    dataset = load_dataset(dataset_id, \"supervised\")\r\n    dataset = dataset.remove_columns(\"ner_tags\")\r\n    dataset = dataset.rename_column(\"fine_ner_tags\", \"ner_tags\")\r\n    labels = dataset[\"train\"].features[\"ner_tags\"].feature.names\r\n    # ['o', 'art-broadcastprogram', 'art-film', 'art-music', 'art-other', ...\r\n\r\n    # initialize a spanmarker model using a pretrained bert-style encoder\r\n    encoder_id = \"bert-base-cased\"\r\n    model_id = f\"tomaarsen/span-marker-{encoder_id}-fewnerd-fine-super\"\r\n    model = spanmarkermodel.from_pretrained(\r\n        encoder_id,\r\n        labels=labels,\r\n        # spanmarker hyperparameters:\r\n        model_max_length=256,\r\n        marker_max_length=128,\r\n        entity_max_length=8,\r\n        # model card arguments\r\n        model_card_data=spanmarkermodelcarddata(\r\n            model_id=model_id,\r\n            encoder_id=encoder_id,\r\n            dataset_name=dataset_name,\r\n            dataset_id=dataset_id,\r\n            license=\"cc-by-sa-4.0\",\r\n            language=\"en\",\r\n        ),\r\n    )\r\n\r\n    # prepare the \ud83e\udd17 transformers training arguments\r\n    output_dir = path(\"models\") / model_id\r\n    args = trainingarguments(\r\n        output_dir=output_dir,\r\n        # training hyperparameters:\r\n        learning_rate=5e-5,\r\n        per_device_train_batch_size=32,\r\n        per_device_eval_batch_size=32,\r\n        num_train_epochs=3,\r\n        weight_decay=0.01,\r\n        warmup_ratio=0.1,\r\n        bf16=true,  # replace `bf16` with `fp16` if your hardware can't use bf16.\r\n        # other training parameters\r\n        logging_first_step=true,\r\n        logging_steps=50,\r\n        evaluation_strategy=\"steps\",\r\n        save_strategy=\"steps\",\r\n        eval_steps=3000,\r\n        save_total_limit=2,\r\n        dataloader_num_workers=2,\r\n    )\r\n\r\n    # initialize the trainer using our model, training args & dataset, and train\r\n    trainer = trainer(\r\n        model=model,\r\n        args=args,\r\n        train_dataset=dataset[\"train\"],\r\n        eval_dataset=dataset[\"validation\"],\r\n    )\r\n    trainer.train()\r\n\r\n    # compute & save the metrics on the test set\r\n    metrics = trainer.evaluate(dataset[\"test\"], metric_key_prefix=\"test\")\r\n    trainer.save_metrics(\"test\", metrics)\r\n\r\n    # save the final checkpoint\r\n    trainer.save_model(output_dir / \"checkpoint-final\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n### inference\r\n```python\r\nfrom span_marker import spanmarkermodel\r\n\r\n# download from the \ud83e\udd17 hub\r\nmodel = spanmarkermodel.from_pretrained(\"tomaarsen/span-marker-bert-base-fewnerd-fine-super\")\r\n# run inference\r\nentities = model.predict(\"amelia earhart flew her single engine lockheed vega 5b across the atlantic to paris.\")\r\n[{'span': 'amelia earhart', 'label': 'person-other', 'score': 0.7659597396850586, 'char_start_index': 0, 'char_end_index': 14},\r\n {'span': 'lockheed vega 5b', 'label': 'product-airplane', 'score': 0.9725785851478577, 'char_start_index': 38, 'char_end_index': 54},\r\n {'span': 'atlantic', 'label': 'location-bodiesofwater', 'score': 0.7587679028511047, 'char_start_index': 66, 'char_end_index': 74},\r\n {'span': 'paris', 'label': 'location-gpe', 'score': 0.9892390966415405, 'char_start_index': 78, 'char_end_index': 83}]\r\n```\r\n\r\n## pretrained models\r\n\r\nall models in this list contain `train.py` files that show the training scripts used to generate them. additionally, all training scripts used are stored in the [training_scripts](training_scripts) directory.\r\nthese trained models have hosted inference api widgets that you can use to experiment with the models on their hugging face model pages. additionally, hugging face provides each model with a free api (`deploy` > `inference api` on the model page).\r\n\r\nthese models are further elaborated on in my [thesis](https://raw.githubusercontent.com/tomaarsen/spanmarkerner/main/thesis.pdf).\r\n\r\n### fewnerd\r\n* [`tomaarsen/span-marker-bert-base-fewnerd-fine-super`](https://huggingface.co/tomaarsen/span-marker-bert-base-fewnerd-fine-super) is a model that i have trained in 2 hours on the finegrained, supervised [few-nerd dataset](https://huggingface.co/datasets/dfki-slt/few-nerd). it reached a 70.53 test f1, competitive in the all-time [few-nerd leaderboard](https://paperswithcode.com/sota/named-entity-recognition-on-few-nerd-sup) using `bert-base`. my training script resembles the one that you can see above.\r\n\r\n* [`tomaarsen/span-marker-roberta-large-fewnerd-fine-super`](https://huggingface.co/tomaarsen/span-marker-roberta-large-fewnerd-fine-super) was trained in 6 hours on the finegrained, supervised [few-nerd dataset](https://huggingface.co/datasets/dfki-slt/few-nerd) using `roberta-large`. it reached a 71.03 test f1, reaching a new state of the art in the all-time [few-nerd leaderboard](https://paperswithcode.com/sota/named-entity-recognition-on-few-nerd-sup).\r\n* [`tomaarsen/span-marker-xlm-roberta-base-fewnerd-fine-super`](https://huggingface.co/tomaarsen/span-marker-xlm-roberta-base-fewnerd-fine-super) is a multilingual model that i have trained in 1.5 hours on the finegrained, supervised [few-nerd dataset](https://huggingface.co/datasets/dfki-slt/few-nerd). it reached a 68.6 test f1 on english, and works well on other languages like spanish, french, german, russian, dutch, polish, icelandic, greek and many more.\r\n\r\n### ontonotes v5.0\r\n* [`tomaarsen/span-marker-roberta-large-ontonotes5`](https://huggingface.co/tomaarsen/span-marker-roberta-large-ontonotes5) was trained in 3 hours on the ontonotes v5.0 dataset, reaching a performance of 91.54 f1. for reference, the current strongest spacy model (`en_core_web_trf`) reaches 89.8 f1. this spanmarker model uses a `roberta-large` encoder under the hood.\r\n\r\n### conll03\r\n* [`tomaarsen/span-marker-xlm-roberta-large-conll03`](https://huggingface.co/tomaarsen/span-marker-xlm-roberta-large-conll03) is a spanmarker model using `xlm-roberta-large` that was trained in 45 minutes. it reaches a state of the art 93.1 f1 on conll03 without using document-level context. for reference, the current strongest spacy model (`en_core_web_trf`) reaches 91.6.\r\n* [`tomaarsen/span-marker-xlm-roberta-large-conll03-doc-context`](https://huggingface.co/tomaarsen/span-marker-xlm-roberta-large-conll03-doc-context) is another spanmarker model using the `xlm-roberta-large` encoder. it uses [document-level context](https://tomaarsen.github.io/spanmarkerner/notebooks/document_level_context.html) to reach a state of the art 94.4 f1. for the best performance, inference should be performed using document-level context ([docs](https://tomaarsen.github.io/spanmarkerner/notebooks/document_level_context.html#inference)). this model was trained in 1 hour.\r\n\r\n### conll++\r\n* [`tomaarsen/span-marker-xlm-roberta-large-conllpp-doc-context`](https://huggingface.co/tomaarsen/span-marker-xlm-roberta-large-conllpp-doc-context) was trained in an hour using the `xlm-roberta-large` encoder on the conll++ dataset. using [document-level context](https://tomaarsen.github.io/spanmarkerner/notebooks/document_level_context.html), it reaches a very competitive 95.5 f1. for the best performance, inference should be performed using document-level context ([docs](https://tomaarsen.github.io/spanmarkerner/notebooks/document_level_context.html#inference)).\r\n\r\n### multinerd\r\n* [`tomaarsen/span-marker-xlm-roberta-base-multinerd`](https://huggingface.co/tomaarsen/span-marker-xlm-roberta-base-multinerd) is a multilingual spanmarker model using the `xlm-roberta-large` encoder trained on the huge [multinerd](https://huggingface.co/datasets/babelscape/multinerd) dataset. it reaches a 91.31 f1 on all 10 training languages and 94.55 f1 on english only. the model can predict between 15 classes. for best performance, separate punctuation from your words as described [here](https://huggingface.co/tomaarsen/span-marker-xlm-roberta-base-multinerd#limitations). note that [`tomaarsen/span-marker-mbert-base-multinerd`](https://huggingface.co/tomaarsen/span-marker-mbert-base-multinerd) does not have this limitation and performs better.\r\n\r\n* [`tomaarsen/span-marker-mbert-base-multinerd`](https://huggingface.co/tomaarsen/span-marker-mbert-base-multinerd) is the successor of [`tomaarsen/span-marker-xlm-roberta-base-multinerd`](https://huggingface.co/tomaarsen/span-marker-xlm-roberta-base-multinerd). it's a multilingual spanmarker model using `bert-base-multilingual-cased` trained on the [multinerd](https://huggingface.co/datasets/babelscape/multinerd) dataset. it reaches a state-of-the-art 92.48 f1 on all 10 training languages and 95.18 f1 on english only. this model generalizes well to languages using the latin and cyrillic script.\r\n\r\n## using pretrained spanmarker models with spacy\r\nall [spanmarker models on the hugging face hub](https://huggingface.co/models?library=span-marker) can also be easily used in spacy. it's as simple as including 1 line to add the `span_marker` pipeline. see the [documentation](https://tomaarsen.github.io/spanmarkerner/notebooks/spacy_integration.html) or [api reference](https://tomaarsen.github.io/spanmarkerner/api/span_marker.spacy_integration.html) for more information.\r\n```python\r\nimport spacy\r\n\r\n# load the spacy model with the span_marker pipeline component\r\nnlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])\r\nnlp.add_pipe(\"span_marker\", config={\"model\": \"tomaarsen/span-marker-roberta-large-ontonotes5\"})\r\n\r\n# feed some text through the model to get a spacy doc\r\ntext = \"\"\"cleopatra vii, also known as cleopatra the great, was the last active ruler of the \\\r\nptolemaic kingdom of egypt. she was born in 69 bce and ruled egypt from 51 bce until her \\\r\ndeath in 30 bce.\"\"\"\r\ndoc = nlp(text)\r\n\r\n# and look at the entities\r\nprint([(entity, entity.label_) for entity in doc.ents])\r\n\"\"\"\r\n[(cleopatra vii, \"person\"), (cleopatra the great, \"person\"), (the ptolemaic kingdom of egypt, \"gpe\"),\r\n(69 bce, \"date\"), (egypt, \"gpe\"), (51 bce, \"date\"), (30 bce, \"date\")]\r\n\"\"\"\r\n```\r\n![image](https://user-images.githubusercontent.com/37621491/246170623-6351cb7e-bbb0-4472-af16-9a351a253da9.png)\r\n\r\n## context\r\n<h1 align=\"center\">\r\n    <a href=\"https://github.com/argilla-io/argilla\">\r\n    <img src=\"https://github.com/dvsrepo/imgs/raw/main/rg.svg\" alt=\"argilla\" width=\"150\">\r\n    </a>\r\n</h1>\r\n\r\ni have developed this library as a part of my thesis work at [argilla](https://github.com/argilla-io/argilla). feel free to read my finished thesis [here](https://raw.githubusercontent.com/tomaarsen/spanmarkerner/main/thesis.pdf) in this repository!\r\n\r\n## changelog\r\nsee [changelog.md](changelog.md) for news on all spanmarker versions.\r\n\r\n## license\r\nsee [license](license) for the current license.\r\n",
  "docs_url": null,
  "keywords": "data-science,natural-language-processing,artificial-intelligence,mlops,nlp,machine-learning,transformers",
  "license": "apache-2.0",
  "name": "span-marker",
  "package_url": "https://pypi.org/project/span-marker/",
  "project_url": "https://pypi.org/project/span-marker/",
  "project_urls": {
    "Documentation": "https://tomaarsen.github.io/SpanMarkerNER",
    "Repository": "https://github.com/tomaarsen/SpanMarkerNER"
  },
  "release_url": "https://pypi.org/project/span-marker/1.5.0/",
  "requires_dist": [
    "torch",
    "accelerate",
    "transformers (>=4.19.0)",
    "datasets (>=2.14.0)",
    "packaging (>=20.0)",
    "evaluate",
    "seqeval",
    "jinja2",
    "huggingface-hub",
    "codecarbon ; extra == 'codecarbon'",
    "pre-commit ; extra == 'dev'",
    "ruff ; extra == 'dev'",
    "black ; extra == 'dev'",
    "pytest ; extra == 'dev'",
    "pytest-cov ; extra == 'dev'",
    "spacy ; extra == 'dev'",
    "nltk-theme ; extra == 'docs'",
    "sphinx ; extra == 'docs'",
    "m2r2 ; extra == 'docs'",
    "better-apidoc ; extra == 'docs'",
    "nbsphinx ; extra == 'docs'",
    "nbconvert (<7) ; extra == 'docs'",
    "pandoc (<3) ; extra == 'docs'",
    "ipython ; extra == 'docs'",
    "spacy ; extra == 'docs'",
    "wandb ; extra == 'wandb'"
  ],
  "requires_python": ">=3.8",
  "summary": "named entity recognition using span markers",
  "version": "1.5.0",
  "releases": [],
  "developers": [
    "tom_aarsen"
  ],
  "kwds": "spanmarker spanmarkermodel span_marker spanmarkerner spanmarkermodelcarddata",
  "license_kwds": "apache-2.0",
  "libtype": "pypi",
  "id": "pypi_span_marker",
  "homepage": "",
  "release_count": 18,
  "dependency_ids": [
    "pypi_accelerate",
    "pypi_better_apidoc",
    "pypi_black",
    "pypi_codecarbon",
    "pypi_datasets",
    "pypi_evaluate",
    "pypi_huggingface_hub",
    "pypi_ipython",
    "pypi_jinja2",
    "pypi_m2r2",
    "pypi_nbconvert",
    "pypi_nbsphinx",
    "pypi_nltk_theme",
    "pypi_packaging",
    "pypi_pandoc",
    "pypi_pre_commit",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_ruff",
    "pypi_seqeval",
    "pypi_spacy",
    "pypi_sphinx",
    "pypi_torch",
    "pypi_transformers",
    "pypi_wandb"
  ]
}