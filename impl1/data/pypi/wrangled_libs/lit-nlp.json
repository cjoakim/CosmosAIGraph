{
  "classifiers": [
    "development status :: 5 - production/stable",
    "framework :: jupyter",
    "framework :: pytest",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "operating system :: os independent",
    "programming language :: javascript",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: 3.10",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: scientific/engineering :: information analysis",
    "topic :: scientific/engineering :: visualization",
    "typing :: typed"
  ],
  "description": "# \ud83d\udd25 learning interpretability tool (lit)\n\n<!--* freshness: { owner: 'lit-dev' reviewed: '2023-09-22' } *-->\n\nthe learning interpretability tool (\ud83d\udd25lit, formerly known as the language\ninterpretability tool) is a visual, interactive ml model-understanding tool that\nsupports text, image, and tabular data. it can be run as a standalone server, or\ninside of notebook environments such as colab, jupyter, and google cloud vertex\nai notebooks.\n\nlit is built to answer questions such as:\n\n*   **what kind of examples** does my model perform poorly on?\n*   **why did my model make this prediction?** can this prediction be attributed\n    to adversarial behavior, or to undesirable priors in the training set?\n*   **does my model behave consistently** if i change things like textual style,\n    verb tense, or pronoun gender?\n\n![example of lit ui](https://pair-code.github.io/lit/assets/images/readme-fig-1.png)\n\nlit supports a variety of debugging workflows through a browser-based ui.\nfeatures include:\n\n*   **local explanations** via salience maps, attention, and rich visualization\n    of model predictions.\n*   **aggregate analysis** including custom metrics, slicing and binning, and\n    visualization of embedding spaces.\n*   **counterfactual generation** via manual edits or generator plug-ins to\n    dynamically create and evaluate new examples.\n*   **side-by-side mode** to compare two or more models, or one model on a pair\n    of examples.\n*   **highly extensible** to new model types, including classification,\n    regression, span labeling, seq2seq, and language modeling. supports\n    multi-head models and multiple input features out of the box.\n*   **framework-agnostic** and compatible with tensorflow, pytorch, and more.\n\nlit has a [website](https://pair-code.github.io/lit) with live demos, tutorials,\na setup guide and more.\n\nstay up to date on lit by joining the\n[lit-announcements mailing list](https://groups.google.com/g/lit-annoucements).\n\nfor a broader overview, check out [our paper](https://arxiv.org/abs/2008.05122) and the\n[user guide](https://pair-code.github.io/lit/documentation/ui_guide).\n\n## documentation\n\n*   [documentation index](https://pair-code.github.io/lit/documentation/)\n*   [faq](https://pair-code.github.io/lit/documentation/faq)\n*   [release notes](./release.md)\n\n## download and installation\n\nlit can be run via container image, installed via `pip` or built from source.\nbuilding from source is necessary if you update any of the front-end or core\nback-end code.\n\n### build container image\n\nbuild the image using `docker` or `podman`:\n```sh\ngit clone https://github.com/pair-code/lit.git && cd lit\ndocker build --file dockerfile --tag lit-nlp .\n```\n\nsee the [advanced guide](https://pair-code.github.io/lit/documentation/docker) for detailed instructions on using the\ndefault lit docker image, running lit as a containerized web app in different\nscenarios, and how to creating your own lit images.\n\n### pip installation\n\n```sh\npip install lit-nlp\n```\n\nthe `pip` installation will install all necessary prerequisite packages for use\nof the core lit package.\n\nit **does not** install the prerequisites for the provided demos, so you need to\ninstall those yourself. see\n[requirements_examples.txt](./requirements_examples.txt) for the list of\npackages required to run the demos.\n\n### install from source\n\nclone the repo:\n\n```sh\ngit clone https://github.com/pair-code/lit.git && cd lit\n```\n\n\nnote: be sure you are running python 3.10. if you have a different version on  \nyour system, use the `conda` instructions below to set up a python 3.10 \nenvironment.\n\nset up a python environment with `venv`:\n\n```sh\npython -m venv .venv\nsource .venv/bin/activate\n```\n\nor set up a python environment using `conda`:\n\n```sh\nconda create --name lit-nlp\nconda activate lit-nlp\nconda install python=3.10\nconda install pip\n```\n\nonce you have the environment, install lit's dependencies:\n```sh\npython -m pip install -r requirements.txt\npython -m pip install cudnn cupti  # optional, for gpu support\npython -m pip install torch  # optional, for pytorch\n\n# build the frontend\n(cd lit_nlp; yarn && yarn build)\n```\n\nnote: use the `-r requirements.txt` option to install every dependency required\nfor the lit library, its test suite, and the built-in examples. you can also\ninstall subsets of these using the `-r requirements_core.txt` (core library),\n`-r requirements_test.txt` (test suite), `-r requirements_examples.txt`\n(examples), and/or any combination thereof.\n\nnote: if you see [an error](https://github.com/yarnpkg/yarn/issues/2821)\nrunning `yarn` on ubuntu/debian, be sure you have the\n[correct version installed](https://yarnpkg.com/en/docs/install#linux-tab).\n\n\n## running lit\n\nexplore a collection of hosted demos on the\n[demos page](https://pair-code.github.io/lit/demos).\n\n### quick-start: classification and regression\n\nto explore classification and regression models tasks from the popular\n[glue benchmark](https://gluebenchmark.com/):\n\n```sh\npython -m lit_nlp.examples.glue_demo --port=5432 --quickstart\n```\n\nor, using `docker`:\n\n```sh\ndocker run --rm -e demo_name=glue_demo -p 5432:5432 -t lit-nlp --quickstart\n```\n\nnavigate to http://localhost:5432 to access the lit ui.\n\nyour default view will be a\n[small bert-based model](https://arxiv.org/abs/1908.08962) fine-tuned on the\n[stanford sentiment treebank](https://nlp.stanford.edu/sentiment/treebank.html),\nbut you can switch to\n[sts-b](http://ixa2.si.ehu.es/stswiki/index.php/stsbenchmark) or\n[multinli](https://cims.nyu.edu/~sbowman/multinli/) using the toolbar or the\ngear icon in the upper right.\n\n### quick-start: language modeling\n\nto explore predictions from a pre-trained language model (bert or gpt-2), run:\n\n```sh\npython -m lit_nlp.examples.lm_demo --models=bert-base-uncased --port=5432\n```\n\nor, using `docker`:\n\n```sh\ndocker run --rm -e demo_name=lm_demo -p 5432:5432 -t lit-nlp --models=bert-base-uncased\n```\n\nand navigate to http://localhost:5432 for the ui.\n\n### notebook usage\n\ncolab notebooks showing the use of lit inside of notebooks can be found at\n[lit_nlp/examples/notebooks](./lit_nlp/examples/notebooks).\n\nwe provide a simple\n[colab demo](https://colab.research.google.com/github/pair-code/lit/blob/main/lit_nlp/examples/notebooks/lit_sentiment_classifier.ipynb).\nrun all the cells to see lit on an example classification model in the notebook.\n\n### more examples\n\nsee [lit_nlp/examples](./lit_nlp/examples). most are run similarly to the\nquickstart example above:\n\n```sh\npython -m lit_nlp.examples.<example_name> --port=5432 [optional --args]\n```\n\n## user guide\n\nto learn about lit's features, check out the [user guide](https://pair-code.github.io/lit/documentation/ui_guide), or\nwatch this [video](https://www.youtube.com/watch?v=curi_vk83du).\n\n## adding your own models or data\n\nyou can easily run lit with your own model by creating a custom `demo.py`\nlauncher, similar to those in [lit_nlp/examples](./lit_nlp/examples). the\nbasic steps are:\n\n*   write a data loader which follows the [`dataset` api](https://pair-code.github.io/lit/documentation/api#datasets)\n*   write a model wrapper which follows the [`model` api](https://pair-code.github.io/lit/documentation/api#models)\n*   pass models, datasets, and any additional\n    [components](https://pair-code.github.io/lit/documentation/api#interpretation-components) to the lit server class\n\nfor a full walkthrough, see\n[adding models and data](https://pair-code.github.io/lit/documentation/api#adding-models-and-data).\n\n## extending lit with new components\n\nlit is easy to extend with new interpretability components, generators, and\nmore, both on the frontend or the backend. see our [documentation](https://pair-code.github.io/lit/documentation/) to get\nstarted.\n\n## pull request process\n\nto make code changes to lit, please work off of the `dev` branch and\n[create pull requests](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request)\n(prs) against that branch. the `main` branch is for stable releases, and it is\nexpected that the `dev` branch will always be ahead of `main`.\n\n[draft prs](https://github.blog/2019-02-14-introducing-draft-pull-requests/) are\nencouraged, especially for first-time contributors or contributors working on\ncomplex tasks (e.g., google summer of code contributors). please use these to\ncommunicate ideas and implementations with the lit team, in addition to issues.\n\nprior to sending your pr or marking a draft pr as \"ready for review\", please run\nthe python and typescript linters on your code to ensure compliance with\ngoogle's [python](https://google.github.io/styleguide/pyguide.html) and\n[typescript](https://google.github.io/styleguide/tsguide.html) style guides.\n\n```sh\n# run pylint on your code using the following command from the root of this repo\npushd lit_nlp & pylint & popd\n\n# run eslint on your code using the following command from the root of this repo\npushd lit_nlp & yarn lint & popd\n```\n\n## citing lit\n\nif you use lit as part of your work, please cite\n[our emnlp paper](https://arxiv.org/abs/2008.05122):\n\n```\n@misc{tenney2020language,\n    title={the language interpretability tool: extensible, interactive visualizations and analysis for {nlp} models},\n    author={ian tenney and james wexler and jasmijn bastings and tolga bolukbasi and andy coenen and sebastian gehrmann and ellen jiang and mahima pushkarna and carey radebaugh and emily reif and ann yuan},\n    booktitle = \"proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations\",\n    year = \"2020\",\n    publisher = \"association for computational linguistics\",\n    pages = \"107--118\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.15\",\n}\n```\n\n## disclaimer\n\nthis is not an official google product.\n\nlit is a research project and under active development by a small team. there\nwill be some bugs and rough edges, but we're releasing at an early stage because\nwe think it's pretty useful already. we want lit to be an open platform, not a\nwalled garden, and we would love your suggestions and feedback - drop us a line\nin the [issues](https://github.com/pair-code/lit/issues).\n",
  "docs_url": null,
  "keywords": "interpretability,interpretable artifical intelligence,explainable,explainable artifical intelligence,xai,computer vision,natural language processing,artifical intelligence,machine learning,deep learning,visualization,visual analytics,tensorflow,tf,torch",
  "license": " apache license version 2.0, january 2004 http://www.apache.org/licenses/  terms and conditions for use, reproduction, and distribution  1. definitions.  \"license\" shall mean the terms and conditions for use, reproduction, and distribution as defined by sections 1 through 9 of this document.  \"licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the license.  \"legal entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. for the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.  \"you\" (or \"your\") shall mean an individual or legal entity exercising permissions granted by this license.  \"source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.  \"object\" form shall mean any form resulting from mechanical transformation or translation of a source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.  \"work\" shall mean the work of authorship, whether in source or object form, made available under the license, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the appendix below).  \"derivative works\" shall mean any work, whether in source or object form, that is based on (or derived from) the work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. for the purposes of this license, derivative works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the work and derivative works thereof.  \"contribution\" shall mean any work of authorship, including the original version of the work and any modifications or additions to that work or derivative works thereof, that is intentionally submitted to licensor for inclusion in the work by the copyright owner or by an individual or legal entity authorized to submit on behalf of the copyright owner. for the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the licensor for the purpose of discussing and improving the work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"not a contribution.\"  \"contributor\" shall mean licensor and any individual or legal entity on behalf of whom a contribution has been received by licensor and subsequently incorporated within the work.  2. grant of copyright license. subject to the terms and conditions of this license, each contributor hereby grants to you a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, sublicense, and distribute the work and such derivative works in source or object form.  3. grant of patent license. subject to the terms and conditions of this license, each contributor hereby grants to you a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the work, where such license applies only to those patent claims licensable by such contributor that are necessarily infringed by their contribution(s) alone or by combination of their contribution(s) with the work to which such contribution(s) was submitted. if you institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the work or a contribution incorporated within the work constitutes direct or contributory patent infringement, then any patent licenses granted to you under this license for that work shall terminate as of the date such litigation is filed.  4. redistribution. you may reproduce and distribute copies of the work or derivative works thereof in any medium, with or without modifications, and in source or object form, provided that you meet the following conditions:  (a) you must give any other recipients of the work or derivative works a copy of this license; and  (b) you must cause any modified files to carry prominent notices stating that you changed the files; and  (c) you must retain, in the source form of any derivative works that you distribute, all copyright, patent, trademark, and attribution notices from the source form of the work, excluding those notices that do not pertain to any part of the derivative works; and  (d) if the work includes a \"notice\" text file as part of its distribution, then any derivative works that you distribute must include a readable copy of the attribution notices contained within such notice file, excluding those notices that do not pertain to any part of the derivative works, in at least one of the following places: within a notice text file distributed as part of the derivative works; within the source form or documentation, if provided along with the derivative works; or, within a display generated by the derivative works, if and wherever such third-party notices normally appear. the contents of the notice file are for informational purposes only and do not modify the license. you may add your own attribution notices within derivative works that you distribute, alongside or as an addendum to the notice text from the work, provided that such additional attribution notices cannot be construed as modifying the license.  you may add your own copyright statement to your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of your modifications, or for any such derivative works as a whole, provided your use, reproduction, and distribution of the work otherwise complies with the conditions stated in this license.  5. submission of contributions. unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you to the licensor shall be under the terms and conditions of this license, without any additional terms or conditions. notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with licensor regarding such contributions.  6. trademarks. this license does not grant permission to use the trade names, trademarks, service marks, or product names of the licensor, except as required for reasonable and customary use in describing the origin of the work and reproducing the content of the notice file.  7. disclaimer of warranty. unless required by applicable law or agreed to in writing, licensor provides the work (and each contributor provides its contributions) on an \"as is\" basis, without warranties or conditions of any kind, either express or implied, including, without limitation, any warranties or conditions of title, non-infringement, merchantability, or fitness for a particular purpose. you are solely responsible for determining the appropriateness of using or redistributing the work and assume any risks associated with your exercise of permissions under this license.  8. limitation of liability. in no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any contributor be liable to you for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this license or out of the use or inability to use the work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such contributor has been advised of the possibility of such damages.  9. accepting warranty or additional liability. while redistributing the work or derivative works thereof, you may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this license. however, in accepting such obligations, you may act only on your own behalf and on your sole responsibility, not on behalf of any other contributor, and only if you agree to indemnify, defend, and hold each contributor harmless for any liability incurred by, or claims asserted against, such contributor by reason of your accepting any such warranty or additional liability.  end of terms and conditions  appendix: how to apply the apache license to your work.  to apply the apache license to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (don't include the brackets!)  the text should be enclosed in the appropriate comment syntax for the file format. we also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives.  copyright [yyyy] [name of copyright owner]  licensed under the apache license, version 2.0 (the \"license\"); you may not use this file except in compliance with the license. you may obtain a copy of the license at  http://www.apache.org/licenses/license-2.0  unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \"as is\" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. ",
  "name": "lit-nlp",
  "package_url": "https://pypi.org/project/lit-nlp/",
  "project_url": "https://pypi.org/project/lit-nlp/",
  "project_urls": {
    "Bug Tracker": "https://github.com/PAIR-code/lit/issues",
    "Homepage": "https://github.com/pair-code/lit",
    "Repository": "https://github.com/PAIR-code/lit"
  },
  "release_url": "https://pypi.org/project/lit-nlp/1.0.2/",
  "requires_dist": [
    "absl-py ==1.4.0",
    "annoy ==1.17.3",
    "attrs ==22.1.0",
    "etils[epath] ==1.3.0",
    "filelock ==3.12.2",
    "google-cloud-translate ==3.11.1",
    "ipython ==8.14.0",
    "Levenshtein ==0.21.1",
    "matplotlib ==3.6.1",
    "ml-collections ==0.1.1",
    "numpy ==1.24.1",
    "pandas ==1.5.3",
    "Pillow ==10.0.0",
    "portpicker ==1.5.2",
    "requests ==2.31.0",
    "rouge-score ==0.1.2",
    "sacrebleu ==2.3.1",
    "saliency ==0.1.3",
    "scikit-learn ==1.0.2",
    "scipy ==1.10.1",
    "shap ==0.37.0",
    "six ==1.16.0",
    "tensorflow ==2.10.0",
    "tensorflow-text ==2.10.0",
    "termcolor ==2.3.0",
    "tqdm ==4.64.0",
    "umap-learn ==0.5.1",
    "werkzeug ==2.3.6",
    "gunicorn ==20.1.0 ; extra == 'examples'",
    "tensorflow-datasets ==4.8.0 ; extra == 'examples'",
    "torch ==2.0.1 ; extra == 'examples'",
    "transformers ==4.27.1 ; extra == 'examples'",
    "lime ==0.2.0.1 ; extra == 'test'",
    "pytest ==7.4.0 ; extra == 'test'"
  ],
  "requires_python": ">=3.10",
  "summary": "\ud83d\udd25lit: the learning interpretability tool",
  "version": "1.0.2",
  "releases": [],
  "developers": [
    "lit-dev@google.com"
  ],
  "kwds": "interpretability interpretable lit_nlp lit_sentiment_classifier lit",
  "license_kwds": "licenses license apache ownership licensed",
  "libtype": "pypi",
  "id": "pypi_lit_nlp",
  "homepage": "",
  "release_count": 11,
  "dependency_ids": [
    "pypi_absl_py",
    "pypi_annoy",
    "pypi_attrs",
    "pypi_etils",
    "pypi_filelock",
    "pypi_google_cloud_translate",
    "pypi_gunicorn",
    "pypi_ipython",
    "pypi_levenshtein",
    "pypi_lime",
    "pypi_matplotlib",
    "pypi_ml_collections",
    "pypi_numpy",
    "pypi_pandas",
    "pypi_pillow",
    "pypi_portpicker",
    "pypi_pytest",
    "pypi_requests",
    "pypi_rouge_score",
    "pypi_sacrebleu",
    "pypi_saliency",
    "pypi_scikit_learn",
    "pypi_scipy",
    "pypi_shap",
    "pypi_six",
    "pypi_tensorflow",
    "pypi_tensorflow_datasets",
    "pypi_tensorflow_text",
    "pypi_termcolor",
    "pypi_torch",
    "pypi_tqdm",
    "pypi_transformers",
    "pypi_umap_learn",
    "pypi_werkzeug"
  ]
}