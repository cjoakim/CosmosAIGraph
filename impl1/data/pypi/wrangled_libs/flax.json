{
  "classifiers": [
    "development status :: 3 - alpha",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/google/flax/main/images/flax_logo_250px.png\" alt=\"logo\"></img>\n</div>\n\n# flax: a neural network library and ecosystem for jax designed for flexibility\n\n![build](https://github.com/google/flax/workflows/build/badge.svg?branch=main) [![coverage](https://badgen.net/codecov/c/gh/google/flax)](https://codecov.io/gh/google/flax)\n\n\n[**overview**](#overview)\n| [**quick install**](#quick-install)\n| [**what does flax look like?**](#what-does-flax-look-like)\n| [**documentation**](https://flax.readthedocs.io/)\n\nthis readme is a very short intro. **to learn everything you need to know about flax, refer to our [full documentation](https://flax.readthedocs.io/).**\n\nflax was originally started by engineers and researchers within the brain team in google research (in close collaboration with the jax team), and is now developed jointly with the open source community.\n\nflax is being used by a growing\ncommunity of hundreds of folks in various alphabet research departments\nfor their daily work, as well as a [growing community\nof open source\nprojects](https://github.com/google/flax/network/dependents?dependent_type=repository).\n\nthe flax team's mission is to serve the growing jax neural network\nresearch ecosystem -- both within alphabet and with the broader community,\nand to explore the use-cases where jax shines. we use github for almost\nall of our coordination and planning, as well as where we discuss\nupcoming design changes. we welcome feedback on any of our discussion,\nissue and pull request threads. we are in the process of moving some\nremaining internal design docs and conversation threads to github\ndiscussions, issues and pull requests. we hope to increasingly engage\nwith the needs and clarifications of the broader ecosystem. please let\nus know how we can help!\n\nplease report any feature requests,\nissues, questions or concerns in our [discussion\nforum](https://github.com/google/flax/discussions), or just let us\nknow what you're working on!\n\nwe expect to improve flax, but we don't anticipate significant\nbreaking changes to the core api. we use [changelog](https://github.com/google/flax/tree/main/changelog.md)\nentries and deprecation warnings when possible.\n\nin case you want to reach us directly, we're at flax-dev@google.com.\n\n## overview\n\nflax is a high-performance neural network library and ecosystem for\njax that is **designed for flexibility**:\ntry new forms of training by forking an example and by modifying the training\nloop, not by adding features to a framework.\n\nflax is being developed in close collaboration with the jax team and\ncomes with everything you need to start your research, including:\n\n* **neural network api** (`flax.linen`): dense, conv, {batch|layer|group} norm, attention, pooling, {lstm|gru} cell, dropout\n\n* **utilities and patterns**: replicated training, serialization and checkpointing, metrics, prefetching on device\n\n* **educational examples** that work out of the box: mnist, lstm seq2seq, graph neural networks, sequence tagging\n\n* **fast, tuned large-scale end-to-end examples**: cifar10, resnet on imagenet, transformer lm1b\n\n## quick install\n\nyou will need python 3.6 or later, and a working [jax](https://github.com/google/jax/blob/main/readme.md)\ninstallation (with or without gpu support - refer to [the instructions](https://github.com/google/jax/blob/main/readme.md)).\nfor a cpu-only version of jax:\n\n```\npip install --upgrade pip # to support manylinux2010 wheels.\npip install --upgrade jax jaxlib # cpu-only\n```\n\nthen, install flax from pypi:\n\n```\npip install flax\n```\n\nto upgrade to the latest version of flax, you can use:\n\n```\npip install --upgrade git+https://github.com/google/flax.git\n```\nto install some additional dependencies (like `matplotlib`) that are required but not included\nby some dependencies, you can use:\n\n```bash\npip install flax[all]\n```\n\n## what does flax look like?\n\nwe provide three examples using the flax api: a simple multi-layer perceptron, a cnn and an auto-encoder.\n\nto learn more about the `module` abstraction, check out our [docs](https://flax.readthedocs.io/), our [broad intro to the module abstraction](https://github.com/google/flax/blob/main/docs/notebooks/linen_intro.ipynb). for additional concrete demonstrations of best practices, refer to our\n[guides](https://flax.readthedocs.io/en/latest/guides/index.html) and\n[developer notes](https://flax.readthedocs.io/en/latest/developer_notes/index.html).\n\n```py\nfrom typing import sequence\n\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nimport flax.linen as nn\n\nclass mlp(nn.module):\n  features: sequence[int]\n\n  @nn.compact\n  def __call__(self, x):\n    for feat in self.features[:-1]:\n      x = nn.relu(nn.dense(feat)(x))\n    x = nn.dense(self.features[-1])(x)\n    return x\n\nmodel = mlp([12, 8, 4])\nbatch = jnp.ones((32, 10))\nvariables = model.init(jax.random.key(0), batch)\noutput = model.apply(variables, batch)\n```\n\n```py\nclass cnn(nn.module):\n  @nn.compact\n  def __call__(self, x):\n    x = nn.conv(features=32, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = nn.conv(features=64, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = x.reshape((x.shape[0], -1))  # flatten\n    x = nn.dense(features=256)(x)\n    x = nn.relu(x)\n    x = nn.dense(features=10)(x)\n    x = nn.log_softmax(x)\n    return x\n\nmodel = cnn()\nbatch = jnp.ones((32, 64, 64, 10))  # (n, h, w, c) format\nvariables = model.init(jax.random.key(0), batch)\noutput = model.apply(variables, batch)\n```\n\n```py\nclass autoencoder(nn.module):\n  encoder_widths: sequence[int]\n  decoder_widths: sequence[int]\n  input_shape: sequence[int]\n\n  def setup(self):\n    input_dim = np.prod(self.input_shape)\n    self.encoder = mlp(self.encoder_widths)\n    self.decoder = mlp(self.decoder_widths + (input_dim,))\n\n  def __call__(self, x):\n    return self.decode(self.encode(x))\n\n  def encode(self, x):\n    assert x.shape[1:] == self.input_shape\n    return self.encoder(jnp.reshape(x, (x.shape[0], -1)))\n\n  def decode(self, z):\n    z = self.decoder(z)\n    x = nn.sigmoid(z)\n    x = jnp.reshape(x, (x.shape[0],) + self.input_shape)\n    return x\n\nmodel = autoencoder(encoder_widths=[20, 10, 5],\n                    decoder_widths=[5, 10, 20],\n                    input_shape=(12,))\nbatch = jnp.ones((16, 12))\nvariables = model.init(jax.random.key(0), batch)\nencoded = model.apply(variables, batch, method=model.encode)\ndecoded = model.apply(variables, encoded, method=model.decode)\n```\n\n## \ud83e\udd17 hugging face\n\nin-detail examples to train and evaluate a variety of flax models for\nnatural language processing, computer vision, and speech recognition are\nactively maintained in the [\ud83e\udd17 transformers repository](https://github.com/huggingface/transformers/tree/main/examples/flax).\n\nas of october 2021, the [19 most-used transformer architectures](https://huggingface.co/transformers/#supported-frameworks) are supported in flax\nand over 5000 pretrained checkpoints in flax have been uploaded to the [\ud83e\udd17 hub](https://huggingface.co/models?library=jax&sort=downloads).\n\n## citing flax\n\nto cite this repository:\n\n```\n@software{flax2020github,\n  author = {jonathan heek and anselm levskaya and avital oliver and marvin ritter and bertrand rondepierre and andreas steiner and marc van {z}ee},\n  title = {{f}lax: a neural network library and ecosystem for {jax}},\n  url = {http://github.com/google/flax},\n  version = {0.7.5},\n  year = {2023},\n}\n```\n\nin the above bibtex entry, names are in alphabetical order, the version number\nis intended to be that from [flax/version.py](https://github.com/google/flax/blob/main/flax/version.py), and the year corresponds to the project's open-source release.\n\n## note\n\nflax is an open source project maintained by a dedicated team in google research, but is not an official google product.\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "flax",
  "package_url": "https://pypi.org/project/flax/",
  "project_url": "https://pypi.org/project/flax/",
  "project_urls": {
    "homepage": "https://github.com/google/flax"
  },
  "release_url": "https://pypi.org/project/flax/0.7.5/",
  "requires_dist": [
    "numpy >=1.22",
    "jax >=0.4.19",
    "msgpack",
    "optax",
    "orbax-checkpoint",
    "tensorstore",
    "rich >=11.1",
    "typing-extensions >=4.2",
    "PyYAML >=5.4.1",
    "numpy >=1.23.2 ; python_version >= \"3.11\"",
    "numpy >=1.26.0 ; python_version >= \"3.12\"",
    "matplotlib ; extra == 'all'",
    "clu ; extra == 'testing'",
    "einops ; extra == 'testing'",
    "gymnasium[accept-rom-license,atari] ; extra == 'testing'",
    "jaxlib ; extra == 'testing'",
    "jraph >=0.0.6dev0 ; extra == 'testing'",
    "ml-collections ; extra == 'testing'",
    "mypy ; extra == 'testing'",
    "opencv-python ; extra == 'testing'",
    "pytest ; extra == 'testing'",
    "pytest-cov ; extra == 'testing'",
    "pytest-custom-exit-code ; extra == 'testing'",
    "pytest-xdist ==1.34.0 ; extra == 'testing'",
    "pytype ; extra == 'testing'",
    "sentencepiece ; extra == 'testing'",
    "tensorflow-text >=2.11.0 ; extra == 'testing'",
    "tensorflow-datasets ; extra == 'testing'",
    "tensorflow ; extra == 'testing'",
    "torch ; extra == 'testing'",
    "nbstripout ; extra == 'testing'",
    "black[jupyter] ==23.7.0 ; extra == 'testing'",
    "clu <=0.0.9 ; (python_version < \"3.10\") and extra == 'testing'"
  ],
  "requires_python": ">=3.9",
  "summary": "flax: a neural network library for jax designed for flexibility",
  "version": "0.7.5",
  "releases": [],
  "developers": [
    "flax-dev@google.com"
  ],
  "kwds": "flax_logo_250px flax flax2020github jaxlib logo",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_flax",
  "homepage": "",
  "release_count": 38,
  "dependency_ids": [
    "pypi_black",
    "pypi_clu",
    "pypi_einops",
    "pypi_gymnasium",
    "pypi_jax",
    "pypi_jaxlib",
    "pypi_jraph",
    "pypi_matplotlib",
    "pypi_ml_collections",
    "pypi_msgpack",
    "pypi_mypy",
    "pypi_nbstripout",
    "pypi_numpy",
    "pypi_opencv_python",
    "pypi_optax",
    "pypi_orbax_checkpoint",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_pytest_custom_exit_code",
    "pypi_pytest_xdist",
    "pypi_pytype",
    "pypi_pyyaml",
    "pypi_rich",
    "pypi_sentencepiece",
    "pypi_tensorflow",
    "pypi_tensorflow_datasets",
    "pypi_tensorflow_text",
    "pypi_tensorstore",
    "pypi_torch",
    "pypi_typing_extensions"
  ]
}