{
  "classifiers": [
    "development status :: 5 - production/stable",
    "intended audience :: developers",
    "intended audience :: education",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "operating system :: os independent",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "[![onnx runtime](https://github.com/huggingface/optimum/actions/workflows/test_onnxruntime.yml/badge.svg)](https://github.com/huggingface/optimum/actions/workflows/test_onnxruntime.yml)\n\n# hugging face optimum\n\n\ud83e\udd17 optimum is an extension of \ud83e\udd17 transformers and diffusers, providing a set of optimization tools enabling maximum efficiency to train and run models on targeted hardware, while keeping things easy to use.\n\n## installation\n\n\ud83e\udd17 optimum can be installed using `pip` as follows:\n\n```bash\npython -m pip install optimum\n```\n\nif you'd like to use the accelerator-specific features of \ud83e\udd17 optimum, you can install the required dependencies according to the table below:\n\n| accelerator                                                                                                            | installation                                      |\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\n| [onnx runtime](https://huggingface.co/docs/optimum/onnxruntime/overview)                                                                           | `pip install --upgrade-strategy eager optimum[onnxruntime]`       |\n| [intel neural compressor](https://huggingface.co/docs/optimum/intel/index)       | `pip install --upgrade-strategy eager optimum[neural-compressor]`|\n| [openvino](https://huggingface.co/docs/optimum/intel/index)                                                                 | `pip install --upgrade-strategy eager optimum[openvino,nncf]`    |\n| [amd instinct gpus and ryzen ai npu](https://huggingface.co/docs/optimum/amd/index)                     | `pip install --upgrade-strategy eager optimum[amd]`              |\n| [habana gaudi processor (hpu)](https://huggingface.co/docs/optimum/habana/index)                                                            | `pip install --upgrade-strategy eager optimum[habana]`           |\n| [furiosaai](https://huggingface.co/docs/optimum/furiosa/index)                                                                                   | `pip install --upgrade-strategy eager optimum[furiosa]`          |\n\nthe `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version.\n\nto install from source:\n\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\n\nfor the accelerator-specific features, append `optimum[accelerator_type]` to the above command:\n\n```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```\n\n## accelerated inference\n\n\ud83e\udd17 optimum provides multiple tools to export and run optimized models on various ecosystems: \n\n- [onnx](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model) / [onnx runtime](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models)\n- tensorflow lite\n- [openvino](https://huggingface.co/docs/optimum/intel/inference)\n- habana first-gen gaudi / gaudi2, more details [here](https://huggingface.co/docs/optimum/main/en/habana/usage_guides/accelerate_inference)\n\nthe [export](https://huggingface.co/docs/optimum/exporters/overview) and optimizations can be done both programmatically and with a command line.\n\n### features summary\n\n| features                           | [onnx runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview)| [neural compressor](https://huggingface.co/docs/optimum/main/en/intel/optimization_inc)| [openvino](https://huggingface.co/docs/optimum/main/en/intel/inference)| [tensorflow lite](https://huggingface.co/docs/optimum/main/en/exporters/tflite/overview)|\n|:----------------------------------:|:------------------:|:------------------:|:------------------:|:------------------:|\n| graph optimization                 | :heavy_check_mark: | n/a                | :heavy_check_mark: | n/a                |\n| post-training dynamic quantization | :heavy_check_mark: | :heavy_check_mark: | n/a                | :heavy_check_mark: |\n| post-training static quantization  | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n| quantization aware training (qat)  | n/a                | :heavy_check_mark: | :heavy_check_mark: | n/a                |\n| fp16 (half precision)              | :heavy_check_mark: | n/a                | :heavy_check_mark: | :heavy_check_mark: |\n| pruning                            | n/a                | :heavy_check_mark: | :heavy_check_mark: | n/a                |\n| knowledge distillation             | n/a                | :heavy_check_mark: | :heavy_check_mark: | n/a                |\n\n\n### openvino\n\nbefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install --upgrade-strategy eager optimum[openvino,nncf]\n```\n\nit is possible to export \ud83e\udd17 transformers and diffusers models to the openvino format easily:\n\n```bash\noptimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbert_sst2_ov\n```\n\nif you add `--int8`, the weights will be quantized to int8. static quantization can also be applied on the activations using [nncf](https://github.com/openvinotoolkit/nncf), more information can be found in the [documentation](https://huggingface.co/docs/optimum/main/en/intel/optimization_ov).\n\nto load a model and run inference with openvino runtime, you can just replace your `automodelforxxx` class with the corresponding `ovmodelforxxx` class. to load a pytorch checkpoint and convert it to the openvino format on-the-fly, you can set `export=true` when loading your model.\n\n```diff\n- from transformers import automodelforsequenceclassification\n+ from optimum.intel import ovmodelforsequenceclassification\n  from transformers import autotokenizer, pipeline\n\n  model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n  tokenizer = autotokenizer.from_pretrained(model_id)\n- model = automodelforsequenceclassification.from_pretrained(model_id)\n+ model = ovmodelforsequenceclassification.from_pretrained(\"distilbert_sst2_ov\")\n\n  classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n  results = classifier(\"he's a dreadful magician.\")\n```\n\nyou can find more examples in the [documentation](https://huggingface.co/docs/optimum/intel/inference) and in the [examples](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino).\n\n### neural compressor\n\nbefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install --upgrade-strategy eager optimum[neural-compressor]\n```\n\ndynamic quantization can be applied on your model:\n\n```bash\noptimum-cli inc quantize --model distilbert-base-cased-distilled-squad --output ./quantized_distilbert\n```\n\nto load a model quantized with intel neural compressor, hosted locally or on the \ud83e\udd17 hub, you can do as follows :\n```python\nfrom optimum.intel import incmodelforsequenceclassification\n\nmodel_id = \"intel/distilbert-base-uncased-finetuned-sst-2-english-int8-dynamic\"\nmodel = incmodelforsequenceclassification.from_pretrained(model_id)\n```\n\nyou can find more examples in the [documentation](https://huggingface.co/docs/optimum/intel/optimization_inc) and in the [examples](https://github.com/huggingface/optimum-intel/tree/main/examples/neural_compressor).\n\n### onnx + onnx runtime\n\nbefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install optimum[exporters,onnxruntime]\n```\n\nit is possible to export \ud83e\udd17 transformers and diffusers models to the [onnx](https://onnx.ai/) format and perform graph optimization as well as quantization easily:\n\n```plain\noptimum-cli export onnx -m deepset/roberta-base-squad2 --optimize o2 roberta_base_qa_onnx\n```\n\nthe model can then be quantized using `onnxruntime`:\n\n```bash\noptimum-cli onnxruntime quantize \\\n  --avx512 \\\n  --onnx_model roberta_base_qa_onnx \\\n  -o quantized_roberta_base_qa_onnx\n```\n\nthese commands will export `deepset/roberta-base-squad2` and perform [o2 graph optimization](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization#optimization-configuration) on the exported model, and finally quantize it with the [avx512 configuration](https://huggingface.co/docs/optimum/main/en/onnxruntime/package_reference/configuration#optimum.onnxruntime.autoquantizationconfig.avx512).\n\nfor more information on the onnx export, please check the [documentation](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model).\n\n#### run the exported model using onnx runtime\n\nonce the model is exported to the onnx format, we provide python classes enabling you to run the exported onnx model in a seemless manner using [onnx runtime](https://onnxruntime.ai/) in the backend:\n\n```diff\n- from transformers import automodelforquestionanswering\n+ from optimum.onnxruntime import ortmodelforquestionanswering\n  from transformers import autotokenizer, pipeline\n\n  model_id = \"deepset/roberta-base-squad2\"\n  tokenizer = autotokenizer.from_pretrained(model_id)\n- model = automodelforquestionanswering.from_pretrained(model_id)\n+ model = ortmodelforquestionanswering.from_pretrained(\"roberta_base_qa_onnx\")\n  qa_pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n  question = \"what's optimum?\"\n  context = \"optimum is an awesome library everyone should use!\"\n  results = qa_pipe(question=question, context=context)\n```\n\nmore details on how to run onnx models with `ortmodelforxxx` classes [here](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/models).\n\n### tensorflow lite\n\nbefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install optimum[exporters-tf]\n```\n\njust as for onnx, it is possible to export models to [tensorflow lite](https://www.tensorflow.org/lite) and quantize them:\n\n```plain\noptimum-cli export tflite \\\n  -m deepset/roberta-base-squad2 \\\n  --sequence_length 384  \\\n  --quantize int8-dynamic roberta_tflite_model\n```\n\n## accelerated training\n\n\ud83e\udd17 optimum provides wrappers around the original \ud83e\udd17 transformers [trainer](https://huggingface.co/docs/transformers/main_classes/trainer) to enable training on powerful hardware easily.\nwe support many providers:\n\n- habana's gaudi processors\n- onnx runtime (optimized for gpus)\n\n### habana\n\nbefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install --upgrade-strategy eager optimum[habana]\n```\n\n```diff\n- from transformers import trainer, trainingarguments\n+ from optimum.habana import gauditrainer, gauditrainingarguments\n\n  # download a pretrained model from the hub\n  model = automodelforxxx.from_pretrained(\"bert-base-uncased\")\n\n  # define the training arguments\n- training_args = trainingarguments(\n+ training_args = gauditrainingarguments(\n      output_dir=\"path/to/save/folder/\",\n+     use_habana=true,\n+     use_lazy_mode=true,\n+     gaudi_config_name=\"habana/bert-base-uncased\",\n      ...\n  )\n\n  # initialize the trainer\n- trainer = trainer(\n+ trainer = gauditrainer(\n      model=model,\n      args=training_args,\n      train_dataset=train_dataset,\n      ...\n  )\n\n  # use habana gaudi processor for training!\n  trainer.train()\n```\n\nyou can find more examples in the [documentation](https://huggingface.co/docs/optimum/habana/quickstart) and in the [examples](https://github.com/huggingface/optimum-habana/tree/main/examples).\n\n### onnx runtime\n\n```diff\n- from transformers import trainer, trainingarguments\n+ from optimum.onnxruntime import orttrainer, orttrainingarguments\n\n  # download a pretrained model from the hub\n  model = automodelforsequenceclassification.from_pretrained(\"bert-base-uncased\")\n\n  # define the training arguments\n- training_args = trainingarguments(\n+ training_args = orttrainingarguments(\n      output_dir=\"path/to/save/folder/\",\n      optim=\"adamw_ort_fused\",\n      ...\n  )\n\n  # create a onnx runtime trainer\n- trainer = trainer(\n+ trainer = orttrainer(\n      model=model,\n      args=training_args,\n      train_dataset=train_dataset,\n      ...\n  )\n\n  # use onnx runtime for training!\n  trainer.train()\n```\n\nyou can find more examples in the [documentation](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer) and in the [examples](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training).\n",
  "docs_url": null,
  "keywords": "transformers,quantization,pruning,optimization,training,inference,onnx,onnx runtime,intel,habana,graphcore,neural compressor,ipu,hpu",
  "license": "apache",
  "name": "optimum",
  "package_url": "https://pypi.org/project/optimum/",
  "project_url": "https://pypi.org/project/optimum/",
  "project_urls": {
    "Homepage": "https://github.com/huggingface/optimum"
  },
  "release_url": "https://pypi.org/project/optimum/1.16.1/",
  "requires_dist": [
    "coloredlogs",
    "sympy",
    "transformers[sentencepiece] >=4.26.0",
    "torch >=1.9",
    "packaging",
    "numpy",
    "huggingface-hub >=0.8.0",
    "datasets",
    "optimum-amd ; extra == 'amd'",
    "optuna ; extra == 'benchmark'",
    "tqdm ; extra == 'benchmark'",
    "scikit-learn ; extra == 'benchmark'",
    "seqeval ; extra == 'benchmark'",
    "torchvision ; extra == 'benchmark'",
    "evaluate >=0.2.0 ; extra == 'benchmark'",
    "accelerate ; extra == 'dev'",
    "pytest ; extra == 'dev'",
    "requests ; extra == 'dev'",
    "parameterized ; extra == 'dev'",
    "pytest-xdist ; extra == 'dev'",
    "Pillow ; extra == 'dev'",
    "sacremoses ; extra == 'dev'",
    "torchvision ; extra == 'dev'",
    "diffusers >=0.17.0 ; extra == 'dev'",
    "torchaudio ; extra == 'dev'",
    "einops ; extra == 'dev'",
    "invisible-watermark ; extra == 'dev'",
    "black ~=23.1 ; extra == 'dev'",
    "ruff ==0.1.5 ; extra == 'dev'",
    "diffusers ; extra == 'diffusers'",
    "accelerate ; extra == 'doc-build'",
    "onnx ; extra == 'exporters'",
    "onnxruntime ; extra == 'exporters'",
    "timm ; extra == 'exporters'",
    "onnx ; extra == 'exporters-gpu'",
    "onnxruntime-gpu ; extra == 'exporters-gpu'",
    "timm ; extra == 'exporters-gpu'",
    "tensorflow <=2.12.1,>=2.4 ; extra == 'exporters-tf'",
    "tf2onnx ; extra == 'exporters-tf'",
    "onnx ; extra == 'exporters-tf'",
    "onnxruntime ; extra == 'exporters-tf'",
    "timm ; extra == 'exporters-tf'",
    "h5py ; extra == 'exporters-tf'",
    "numpy <1.24.0 ; extra == 'exporters-tf'",
    "optimum-furiosa ; extra == 'furiosa'",
    "optimum-graphcore ; extra == 'graphcore'",
    "optimum-habana ; extra == 'habana'",
    "transformers <4.35.0,>=4.33.0 ; extra == 'habana'",
    "optimum-intel >=1.12.0 ; extra == 'intel'",
    "optimum-intel[neural-compressor] >=1.12.0 ; extra == 'neural-compressor'",
    "optimum-neuron[neuron] ; extra == 'neuron'",
    "optimum-neuron[neuronx] ; extra == 'neuronx'",
    "optimum-intel[nncf] >=1.12.0 ; extra == 'nncf'",
    "onnx ; extra == 'onnxruntime'",
    "onnxruntime >=1.11.0 ; extra == 'onnxruntime'",
    "datasets >=1.2.1 ; extra == 'onnxruntime'",
    "evaluate ; extra == 'onnxruntime'",
    "protobuf >=3.20.1 ; extra == 'onnxruntime'",
    "onnx ; extra == 'onnxruntime-gpu'",
    "onnxruntime-gpu >=1.11.0 ; extra == 'onnxruntime-gpu'",
    "datasets >=1.2.1 ; extra == 'onnxruntime-gpu'",
    "evaluate ; extra == 'onnxruntime-gpu'",
    "protobuf >=3.20.1 ; extra == 'onnxruntime-gpu'",
    "accelerate ; extra == 'onnxruntime-gpu'",
    "optimum-intel[openvino] >=1.12.0 ; extra == 'openvino'",
    "black ~=23.1 ; extra == 'quality'",
    "ruff ==0.1.5 ; extra == 'quality'",
    "accelerate ; extra == 'tests'",
    "pytest ; extra == 'tests'",
    "requests ; extra == 'tests'",
    "parameterized ; extra == 'tests'",
    "pytest-xdist ; extra == 'tests'",
    "Pillow ; extra == 'tests'",
    "sacremoses ; extra == 'tests'",
    "torchvision ; extra == 'tests'",
    "diffusers >=0.17.0 ; extra == 'tests'",
    "torchaudio ; extra == 'tests'",
    "einops ; extra == 'tests'",
    "invisible-watermark ; extra == 'tests'"
  ],
  "requires_python": ">=3.7.0",
  "summary": "optimum library is an extension of the hugging face transformers library, providing a framework to integrate third-party libraries from hardware partners and interface with their specific functionality.",
  "version": "1.16.1",
  "releases": [],
  "developers": [
    "hardware@huggingface.co",
    "huggingface_inc"
  ],
  "kwds": "neural_compressor onnx_model optimized onnxruntime optim",
  "license_kwds": "apache",
  "libtype": "pypi",
  "id": "pypi_optimum",
  "homepage": "https://github.com/huggingface/optimum",
  "release_count": 53,
  "dependency_ids": [
    "pypi_accelerate",
    "pypi_black",
    "pypi_coloredlogs",
    "pypi_datasets",
    "pypi_diffusers",
    "pypi_einops",
    "pypi_evaluate",
    "pypi_h5py",
    "pypi_huggingface_hub",
    "pypi_invisible_watermark",
    "pypi_numpy",
    "pypi_onnx",
    "pypi_onnxruntime",
    "pypi_onnxruntime_gpu",
    "pypi_optimum_amd",
    "pypi_optimum_furiosa",
    "pypi_optimum_graphcore",
    "pypi_optimum_habana",
    "pypi_optimum_intel",
    "pypi_optimum_neuron",
    "pypi_optuna",
    "pypi_packaging",
    "pypi_parameterized",
    "pypi_pillow",
    "pypi_protobuf",
    "pypi_pytest",
    "pypi_pytest_xdist",
    "pypi_requests",
    "pypi_ruff",
    "pypi_sacremoses",
    "pypi_scikit_learn",
    "pypi_seqeval",
    "pypi_sympy",
    "pypi_tensorflow",
    "pypi_tf2onnx",
    "pypi_timm",
    "pypi_torch",
    "pypi_torchaudio",
    "pypi_torchvision",
    "pypi_tqdm",
    "pypi_transformers"
  ]
}