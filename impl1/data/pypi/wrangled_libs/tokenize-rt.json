{
  "classifiers": [
    "license :: osi approved :: mit license",
    "programming language :: python :: 3",
    "programming language :: python :: 3 :: only",
    "programming language :: python :: implementation :: cpython",
    "programming language :: python :: implementation :: pypy"
  ],
  "description": "[![build status](https://github.com/asottile/tokenize-rt/actions/workflows/main.yml/badge.svg)](https://github.com/asottile/tokenize-rt/actions/workflows/main.yml)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/asottile/tokenize-rt/main.svg)](https://results.pre-commit.ci/latest/github/asottile/tokenize-rt/main)\n\ntokenize-rt\n===========\n\nthe stdlib `tokenize` module does not properly roundtrip.  this wrapper\naround the stdlib provides two additional tokens `escaped_nl` and\n`unimportant_ws`, and a `token` data type.  use `src_to_tokens` and\n`tokens_to_src` to roundtrip.\n\nthis library is useful if you're writing a refactoring tool based on the\npython tokenization.\n\n## installation\n\n```bash\npip install tokenize-rt\n```\n\n## usage\n\n### datastructures\n\n#### `tokenize_rt.offset(line=none, utf8_byte_offset=none)`\n\na token offset, useful as a key when cross referencing the `ast` and the\ntokenized source.\n\n#### `tokenize_rt.token(name, src, line=none, utf8_byte_offset=none)`\n\nconstruct a token\n\n- `name`: one of the token names listed in `token.tok_name` or\n  `escaped_nl` or `unimportant_ws`\n- `src`: token's source as text\n- `line`: the line number that this token appears on.\n- `utf8_byte_offset`: the utf8 byte offset that this token appears on in the\n  line.\n\n#### `tokenize_rt.token.offset`\n\nretrieves an `offset` for this token.\n\n### converting to and from `token` representations\n\n#### `tokenize_rt.src_to_tokens(text: str) -> list[token]`\n\n#### `tokenize_rt.tokens_to_src(iterable[token]) -> str`\n\n### additional tokens added by `tokenize-rt`\n\n#### `tokenize_rt.escaped_nl`\n\n#### `tokenize_rt.unimportant_ws`\n\n### helpers\n\n#### `tokenize_rt.non_coding_tokens`\n\na `frozenset` containing tokens which may appear between others while not\naffecting control flow or code:\n- `comment`\n- `escaped_nl`\n- `nl`\n- `unimportant_ws`\n\n#### `tokenize_rt.parse_string_literal(text: str) -> tuple[str, str]`\n\nparse a string literal into its prefix and string content\n\n```pycon\n>>> parse_string_literal('f\"foo\"')\n('f', '\"foo\"')\n```\n\n#### `tokenize_rt.reversed_enumerate(sequence[token]) -> iterator[tuple[int, token]]`\n\nyields `(index, token)` pairs.  useful for rewriting source.\n\n#### `tokenize_rt.rfind_string_parts(sequence[token], i) -> tuple[int, ...]`\n\nfind the indices of the string parts of a (joined) string literal\n\n- `i` should start at the end of the string literal\n- returns `()` (an empty tuple) for things which are not string literals\n\n```pycon\n>>> tokens = src_to_tokens('\"foo\" \"bar\".capitalize()')\n>>> rfind_string_parts(tokens, 2)\n(0, 2)\n>>> tokens = src_to_tokens('(\"foo\" \"bar\").capitalize()')\n>>> rfind_string_parts(tokens, 4)\n(1, 3)\n```\n\n## differences from `tokenize`\n\n- `tokenize-rt` adds `escaped_nl` for a backslash-escaped newline \"token\"\n- `tokenize-rt` adds `unimportant_ws` for whitespace (discarded in `tokenize`)\n- `tokenize-rt` normalizes string prefixes, even if they are not parsed -- for\n  instance, this means you'll see `token('string', \"f'foo'\", ...)` even in\n  python 2.\n- `tokenize-rt` normalizes python 2 long literals (`4l` / `4l`) and octal\n  literals (`0755`) in python 3 (for easier rewriting of python 2 code while\n  running python 3).\n\n## sample usage\n\n- https://github.com/asottile/add-trailing-comma\n- https://github.com/asottile/future-annotations\n- https://github.com/asottile/future-fstrings\n- https://github.com/asottile/pyupgrade\n- https://github.com/asottile/yesqa\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "tokenize-rt",
  "package_url": "https://pypi.org/project/tokenize-rt/",
  "project_url": "https://pypi.org/project/tokenize-rt/",
  "project_urls": {
    "Homepage": "https://github.com/asottile/tokenize-rt"
  },
  "release_url": "https://pypi.org/project/tokenize-rt/5.2.0/",
  "requires_dist": [],
  "requires_python": ">=3.8",
  "summary": "a wrapper around the stdlib `tokenize` which roundtrips.",
  "version": "5.2.0",
  "releases": [],
  "developers": [
    "anthony_sottile",
    "asottile@umich.edu"
  ],
  "kwds": "tokenize_rt tokenize tokenized tokens_to_src src_to_tokens",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_tokenize_rt",
  "homepage": "https://github.com/asottile/tokenize-rt",
  "release_count": 16,
  "dependency_ids": []
}