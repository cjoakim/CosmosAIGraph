{
  "classifiers": [
    "intended audience :: developers",
    "license :: osi approved :: apache software license",
    "natural language :: english",
    "programming language :: python",
    "programming language :: python :: 3.6"
  ],
  "description": "|codebuild|  |readthedocs|  |pypi|\n\n===================================\naws step functions data science sdk\n===================================\n\nthe aws step functions data science sdk is an open-source library that allows data\nscientists to easily create workflows that process and publish machine learning\nmodels using amazon sagemaker and aws step functions. you can create machine learning\nworkflows in python that orchestrate aws infrastructure at scale, without having\nto provision and integrate the aws services separately.\n\n* workflow - a sequence of steps designed to perform some work\n* step - a unit of work within a workflow\n* ml pipeline - a type of workflow used in data science to create and train machine learning models\n\nthe aws step functions data science sdk enables you to do the following.\n\n- easily construct and run machine learning workflows that use aws\n  infrastructure directly in python\n- instantiate common training pipelines\n- create standard machine learning workflows in a jupyter notebook from\n  templates\n\ntable of contents\n-----------------\n- `getting started with sample jupyter notebooks <#getting-started-with-sample-jupyter-notebooks>`__\n- `installing the aws step functions data science sdk <#installing-the-aws-step-functions-data-science-sdk>`__\n- `overview of sdk <#overview-of-sdk>`__\n- `contributing <#contributing>`__\n- `aws permissions <#aws-permissions>`__\n- `licensing <#licensing>`__\n- `verifying the signature <#verifying-the-signature>`__\n\ngetting started with sample jupyter notebooks\n---------------------------------------------\n\nthe best way to quickly review how the aws step functions data science sdk works\nis to review the related example notebooks. these notebooks provide code and\ndescriptions for creating and running workflows in aws step functions using\nthe aws step functions data science sdk.\n\nexample notebooks in sagemaker\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nin amazon sagemaker, example jupyter notebooks are available in the **example\nnotebooks** portion of a notebook instance. to run the example notebooks, do the following.\n\n1. either `create a notebook instance <https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html>`__ or `access an existing <https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-access-ws.html>`__ notebook instance.\n\n2. select the **sagemaker examples** tab.\n\n3. choose a notebook in the **step functions data science sdk** section and select **use**.\n\nfor more information, see `example notebooks <https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-nbexamples.html>`__\nin the amazon sagemaker documentation.\n\n\nrun example notebooks locally\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nto run the aws step functions data science sdk example notebooks locally, download\nthe sample notebooks and open them in a working jupyter instance.\n\n1. install jupyter: https://jupyter.readthedocs.io/en/latest/install.html\n\n2. download the following files from:\n   https://github.com/awslabs/amazon-sagemaker-examples/tree/master/step-functions-data-science-sdk.\n\n  * :code:`hello_world_workflow.ipynb`\n  * :code:`machine_learning_workflow_abalone.ipynb`\n  * :code:`training_pipeline_pytorch_mnist.ipynb`\n\n3. open the files in jupyter.\n\n\n\ninstalling the aws step functions data science sdk\n--------------------------------------------------\n\nthe aws step functions data science sdk is built to pypi and can be installed with\npip as follows.\n\n\n::\n\n        pip install stepfunctions\n\nyou can install from source by cloning this repository and running a pip install\ncommand in the root directory of the repository:\n\n::\n\n    git clone https://github.com/aws/aws-step-functions-data-science-sdk-python.git\n    cd aws-step-functions-data-science-sdk-python\n    pip install .\n\nsupported operating systems\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nthe aws step functions data science sdk supports unix/linux and mac.\n\nsupported python versions\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nthe aws step functions data science sdk is tested on:\n\n* python 3.6\n\noverview of sdk\n---------------\n\nthe aws step functions data science sdk provides a python api that enables you to\ncreate data science and machine learning workflows using aws step functions and\nsagemaker directly in your python code and jupyter notebooks.\n\nusing this sdk you can:\n\n1. create steps that accomplish tasks.\n2. chain those steps together into workflows.\n3. include retry, succeed, or fail steps.\n4. review a graphical representation and definition for your workflow.\n5. create a workflow in aws step functions.\n6. start and review executions in aws step functions.\n\nfor a detailed api reference of the aws step functions data science sdk,\nbe sure to view this documentation on\n`read the docs <https://aws-step-functions-data-science-sdk.readthedocs.io>`_.\n\n\naws step functions\n~~~~~~~~~~~~~~~~~~\n\naws step functions lets you coordinate multiple aws services into serverless\nworkflows so you can build and update apps quickly. using step functions, you\ncan design and run workflows that combine services such as amazon sagemaker, aws\nlambda, and amazon elastic container service (amazon ecs), into feature-rich\napplications. workflows are made up of a series of steps, with the output of one\nstep acting as input to the next.\n\nthe aws step functions data science sdk provides access to aws step functions so that\nyou can easily create and run machine learning and data science workflows\ndirectly in python, and inside your jupyter notebooks. workflows are created locally\nin python, but when they are ready for execution, the workflow is first uploaded\nto the aws step functions service for execution in the cloud.\n\nwhen you use the sdk to create, update, or execute workflows\nyou are talking to the step functions service in the cloud. your workflows\nlive in aws step functions and can be re-used.\n\nyou can execute a workflow as many times as you want, and you can optionally\nchange the input each time. each time you execute a workflow, it creates a new\nexecution instance in the cloud. you can inspect these executions with sdk\ncommands, or with the step functions management console. you can run more than\none execution at a time.\n\nusing this sdk you can create steps, chain them together to create a workflow,\ncreate that workflow in aws step functions, and execute the workflow in the\naws cloud.\n\n.. image:: doc/images/create.png\n    :width: 600\n    :alt: create a workflow in aws step functions\n\nonce you have created your workflow in aws step functions, you can execute that\nworkflow in step functions, in the aws cloud.\n\n.. image:: doc/images/execute.png\n    :width: 600\n    :alt: start a workflow in aws step functions\n\nstep functions creates workflows out of steps called `states <https://docs.aws.amazon.com/step-functions/latest/dg/concepts-states.html>`__,\nand expresses that workflow in the `amazon states language <https://docs.aws.amazon.com/step-functions/latest/dg/concepts-amazon-states-language.html>`__.\nwhen you create a workflow in the aws step functions data science sdk, it\ncreates a state machine representing your workflow and steps in aws step\nfunctions.\n\nfor more information about step functions concepts and use, see the step\nfunctions `documentation`_.\n\n.. _documentation: https://docs.aws.amazon.com/step-functions/index.html\n\nbuilding a workflow\n-------------------\n\nsteps\n~~~~~\n\nyou create steps using the sdk, and chain them together into sequential\nworkflows. then, you can create those workflows in aws step functions and\nexecute them in step functions directly from your python code. for example,\nthe following is how you define a pass step.\n\n.. code-block:: python\n\n    start_pass_state = pass(\n        state_id=\"mypassstate\"\n    )\n\nthe following is how you define a wait step.\n\n\n.. code-block:: python\n\n    wait_state = wait(\n        state_id=\"wait for 3 seconds\",\n        seconds=3\n    )\n\nthe following example shows how to define a lambda step,\nand then defines a `retry` and a `catch`.\n\n.. code-block:: python\n\n    lambda_state = lambdastep(\n        state_id=\"convert helloworld to base64\",\n        parameters={\n            \"functionname\": \"mylambda\", #replace with the name of your function\n            \"payload\": {\n            \"input\": \"helloworld\"\n            }\n        }\n    )\n\n    lambda_state.add_retry(retry(\n        error_equals=[\"states.taskfailed\"],\n        interval_seconds=15,\n        max_attempts=2,\n        backoff_rate=4.0\n    ))\n\n    lambda_state.add_catch(catch(\n        error_equals=[\"states.taskfailed\"],\n        next_step=fail(\"lambdataskfailed\")\n    ))\n\nworkflows\n~~~~~~~~~\n\nafter you define these steps, chain them together into a logical sequence.\n\n.. code-block:: python\n\n    workflow_definition=chain([start_pass_state, wait_state, lambda_state])\n\nonce the steps are chained together, you can define the workflow definition.\n\n.. code-block:: python\n\n     workflow = workflow(\n         name=\"myworkflow_v1234\",\n         definition=workflow_definition,\n         role=stepfunctions_execution_role\n     )\n\nvisualizing a workflow\n~~~~~~~~~~~~~~~~~~~~~~\n\nthe following generates a graphical representation of your workflow. please note that visualization currently only works in jupyter notebooks. visualization is not available in jupyterlab.\n\n.. code-block:: python\n\n  workflow.render_graph(portrait=false)\n\nreview a workflow definition\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nthe following renders the json of the `amazon states language\n<https://docs.aws.amazon.com/step-functions/latest/dg/concepts-amazon-states-language.html>`__\ndefinition of the workflow you created.\n\n.. code-block:: python\n\n  print(workflow.definition.to_json(pretty=true))\n\nrunning a workflow\n-------------------\n\ncreate workflow on aws step functions\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nthe following creates the workflow in aws step functions.\n\n.. code-block:: python\n\n  workflow.create()\n\nexecute the workflow\n~~~~~~~~~~~~~~~~~~~~\n\nthe following starts an execution of your workflow in aws step functions.\n\n.. code-block:: python\n\n  execution = workflow.execute(inputs={\n    \"ishelloworldexample\": true\n  })\n\nexport an aws cloudformation template\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nthe following generates an aws cloudformation template to deploy your workflow.\n\n.. code-block:: python\n\n  get_cloudformation_template()\n\nthe  generated template contains only the statemachine resource. to reuse\nthe cloudformation template in a different region, please make sure to update\nthe region specific aws resources (such as the lambda arn and training image)\nin the statemachine definition.\n\ncontributing\n------------\nwe welcome community contributions and pull requests. see\n`contributing.md <https://github.com/aws/aws-step-functions-data-science-sdk-python/blob/main/contributing.md>`__ for\ninformation on how to set up a development environment, run tests and submit code.\n\naws permissions\n---------------\nas a managed service, aws step functions performs operations on your behalf on\naws hardware that is managed by aws step functions.  aws step functions can\nperform only operations that the user permits.  you can read more about which\npermissions are necessary in the `aws documentation\n<https://docs.aws.amazon.com/step-functions/latest/dg/security.html>`__.\n\nthe aws step functions data science sdk should not require any additional permissions\naside from what is required for using .aws step functions.  however, if you are\nusing an iam role with a path in it, you should grant permission for\n``iam:getrole``.\n\nlicensing\n---------\naws step functions data science sdk is licensed under the apache 2.0 license. it is\ncopyright 2019 amazon.com, inc. or its affiliates. all rights reserved. the\nlicense is available at: http://aws.amazon.com/apache2.0/\n\nverifying the signature\n-----------------------\n\nthis section describes the recommended process of verifying the validity of the\naws data science workflows python sdk's compiled distributions on\n`pypi <https://pypi.org/project/stepfunctions/>`__.\n\nwhenever you download an application from the internet, we recommend that you\nauthenticate the identity of the software publisher and check that the\napplication is not altered or corrupted since it was published. this protects\nyou from installing a version of the application that contains a virus or other\nmalicious code.\n\nif after running the steps in this topic, you determine that the distribution\nfor the aws data science workflows python sdk is altered or corrupted, do not\ninstall the package. instead, contact aws support (https://aws.amazon.com/contact-us/).\n\naws data science workflows python sdk distributions on pypi are signed using\ngnupg, an open source implementation of the pretty good privacy (openpgp)\nstandard for secure digital signatures. gnupg (also known as gpg) provides\nauthentication and integrity checking through a digital signature. for more\ninformation about pgp and gnupg (gpg), see http://www.gnupg.org.\n\nthe first step is to establish trust with the software publisher. download the\npublic key of the software publisher, check that the owner of the public key is\nwho they claim to be, and then add the public key to your keyring. your keyring\nis a collection of known public keys. after you establish the authenticity of\nthe public key, you can use it to verify the signature of the application.\n\ntopics\n~~~~~~\n\n1. `installing the gpg tools <#installing-the-gpg-tools>`__\n2. `authenticating and importing the public key <#authenticating-and-importing-the-public-key>`__\n3. `verify the signature of the package <#verify-the-signature-of-the-package>`__\n\ninstalling the gpg tools\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nif your operating system is linux or unix, the gpg tools are likely already\ninstalled. to test whether the tools are installed on your system, type\n**gpg** at a command prompt. if the gpg tools are installed, you see a gpg\ncommand prompt. if the gpg tools are not installed, you see an error stating\nthat the command cannot be found. you can install the gnupg package from a\nrepository.\n\n**to install gpg tools on debian-based linux**\n\nfrom a terminal, run the following command: **apt-get install gnupg**\n\n**to install gpg tools on red hat\u2013based linux**\n\nfrom a terminal, run the following command: **yum install gnupg**\n\nauthenticating and importing the public key\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nthe next step in the process is to authenticate the aws data science workflows\npython sdk public key and add it as a trusted key in your gpg keyring.\n\nto authenticate and import the aws data science workflows python sdk public key\n\n1. copy the key from the following text and paste it into a file called\n`data_science_workflows.key`. make sure to include everything that follows:\n\n.. code-block:: text\n\n  -----begin pgp public key block-----\n\n  mqinbf27jxsbeac18loq7/smynwutjzdzosayzfpjt+3rn5ofld9vy559slb1aqv\n  ph+rpu35yor0gbr76nqzv6p2oicunvjmvvokxzud8nsv3gjcscdxn22ywvddfdx9\n  n0dmozo126kfikubwnsbzdxzgsgisku82+okjbdszyges7eoqcqievpubnak/pc5\n  j4sqydfhl2ijciwaw6yux4wemq1ysvvconio5j3+f1nzjzbvi9xwf+r2anx06ezb\n  fficx6kx5b8sz6s4ai0evft9yojtd+y6abs3e63wx9etahq5no26nffneve+pw3o\n  ftu7sq6hxx/ce+ssjalawv/3/1oiluz/icepgyvsl8uwkkulsnheimw2vzoe9ucw\n  9cyb7lgqmcd9o14kqy0+sets3edfh+onrub4rmkdt7nv5wfzgd4wpsyban1yljyx\n  xlyriopmzwurlsukmhzqsn48ulnwuvzvpplcviaotzqqbgfaewlw1fvv3awqaf7q\n  lnt0ebx5n71ljndmptrpticnxcvsnxt1uctk1mtzywumrxk0pdjzs06qplwehwmo\n  4a4bqcz/1avnxaauzshp7kzgpwg6kqocsbn3va/yhfdx/nbey3xg1ecdlfxmcrrv\n  d7xqpzgvazthbrior6anklmf72zmqxiyayrflllokjytncac8igo5baf2waraqab\n  tfbtdgvwznvuy3rpb25zlvb5dghvbi1trestu2lnbmluzya8c3rlcgz1bmn0aw9u\n  cy1kzxzlbg9wzxitzxhwzxjpzw5jzubhbwf6b24uy29tpokcvaqtaqgaphyhbmww\n  bxe3v509bl1rxwdredrjfkgjbqjduyv7ahsdbqkusssabqsjcaccbhukcqglagqw\n  agmbah4baheaaaojegdredrjfkgjq5ip/25lvdaa3itcicbp2/eu8kkuj437ozdr\n  +3z59z7p4mvispmezi4oob1lmgbh+mdhkgblrcsaj4xcisltkfkd4gp/cmsl14hb\n  x/oixexfxvtq4pmwucgl5ntsyabgb3paxgufnaxr2dv3mjfahsovuk5es4/kaj4a\n  5lra+1mwzzmdqhmtyuvtcliqpa/pxafkgl5g15ja5lfdyfq2zuv1bgqlkh7o24jw\n  a1kdb0asepkrh4gjhxaeogdjx2mcghejlbvch4ay7vgog6l+rjchnqsivx0tg9dz\n  ilc7rtr+1lx7jx8wdsysugekady6wgtjk9hbtafh8bl8sr2enoh1qzuin/yihxkr\n  jph/74hg71pjs4fwpbbbprdkc/g47mxmflurgpigcgkhepua1bbw30u0zzwwdhsf\n  isxp8hcqkr5gfhu+37tsc06pwihhdwgx4ktfetmnqkl03fth5lwnsig0hspuinwr\n  +ewn0jxb8dtjmzzbidhlxqx9u3hbedw2g2/ktsqv+mm1p1choegntzots3v9fqmy\n  txy7mkyltrdyu+sx5dnob309vpzbi4b3kbv6hcrjdnicjbvgl6c8whalm6+fu+68\n  rfrkw6wimwhyygdnv8bzdq4h+mate6ahteyutd+ztwpazfe1h0ngreerqju2vlzp\n  laacxhbqnjt+uqinbf27jxsbeac/pdjmwikjbdnompu/w0sosozrmvzs/kr89qei\n  ebt8o0rnfehr6iql5ak6kgedlwnzcoowqamo+vwgmrscwpt6nf9+hdkxczitoe22\n  71zkvjgvf+tx5khjzt8zqqbxvnk5cx/d7sr3kwlbhhyghls/kn2k9fhywbtsqtle\n  o9xvtboip+dohhhjjzhcboeynz2g2b8gnwe4cz75ogfncuhzxusr8y6enjx8wtby\n  /avxpvuiyrhbrxchans3uykzbhkh6w1cfkv6bb49fkykxh0n1zeooys6zxyf0x4n\n  tabycfofyq68kc17/pgmoxtr/ulqdeje0sfeyythkjdstdpa+wkkjjz5bscyq5hq\n  ewy6mvaickurexizynqrhrhb4p/0ba7exzmcryx1azpcqnamvqyjti5e+hsnoxnk\n  ab7jm2hhphcrgo4qvavr5dileokbm6qya1kvqoarw5hv8j8+r9ecn4kwz8qjblgo\n  y65q/b3mwqk0rva1w73bpwea/xlclrqqvrga/fb7dhtnpfn+bpaq3qrulinijatm\n  8c2/p1lz1nuwgrsssksmn3tlfff0lq9jtcbi7k11a082rib2l0lu+j8r07rgvqvz\n  4ulis1lklsp7ixh+zor712hkpqpnvlstehtxqhxztwak/ih7b9ukrl/1hjanhzbe\n  ubhddqaraqabiqi8bbgbcaamfieezbyfd7e/nt1uxvhfyosqoumuqakfal27jxsc\n  gwwfcrsxkwaacgkqyosqoumuqanjva//sdqzxf0zbge8o9kgfrm7bnexz8a6sxen\n  uroouask3isbgfaug+q7rq+vig9gdg74f5liwwckobct/z9tci/7p3qi0be0bm1j\n  ihdm5dxazacmluy6f0p3do3qe2ijnnjejvpm7xzt6tkju/sczqndqxg/cdn5+ezm\n  niatgdv6ugddv/2o0bxmyazt008t/qlr2u5desbt9h3bzl4ska6gjak2tojl0t61\n  1dzjfv/1ubeyrpfco6cslj9ueq+rohasvas4rl9hym3b2svzr8cmsp6lvdqla2qz\n  /nibd+gulofi3/pgvvs63ubfqsrgd5vvjxoirl2woe8lmyib5ujfffd8zdn6j+hq\n  c14vop89mefg57biqxfznzjfvnkl7t5i2g3x5o8stosncchqijtsh5c731kuvqxo\n  xyknfostioivkmyis/nwmwr6fiityyycwh5ycqag0r4slbhfevxdannubfpf6upo\n  ebklzp3iyu/kyanmnq+9+gimrprt/fcpm9rw1gfanuvbt9qjs+erq4dqjl/eaijz\n  cgqz+e5tznxdk9r2shc4zgwy88/2guhd8xh4fh5hbidjpmhutkh9xelq187va4jg\n  u0mbrydukmqiyuc6olzfjubvtmvkwapasbgtvaaowcftai33dz8bofjqlgob9udh\n  /vqojrxttmc=\n  =ovuh\n  -----end pgp public key block-----\n\n\n2. at a command prompt in the directory where you saved\n`data_science_workflows.key`, use the following command to import the aws data\nscience workflows python sdk public key into your keyring:\n\n.. code-block:: text\n\n  gpg --import data_science_workflows.key\n\nthe command returns results that are similar to the following:\n\n.. code-block:: text\n\n  gpg: key 60eb103ae314a809: public key \"stepfunctions-python-sdk-signing <stepfunctions-developer-experience [at] amazon.com>\" imported\n  gpg: total number processed: 1\n  gpg:               imported: 1\n\nmake a note of the key value; you need it in the next step. in the preceding\nexample, the key value is 60eb103ae314a809.\n\n3. verify the fingerprint by running the following command, replacing key-value\nwith the value from the preceding step:\n\n.. code-block:: text\n\n  gpg --fingerprint <key-value>\n\nthis command returns results similar to the following:\n\n.. code-block:: text\n\n  pub   rsa4096 2019-10-31 [sc] [expires: 2030-10-31] cc16 0577 b7bf 9d3d 6e5d\n  51c5 60eb 103a e314 a809 uid           [ unknown]\n  stepfunctions-python-sdk-signing\n  <stepfunctions-developer-experience [at] amazon.com> sub   rsa4096 2019-10-31 [e]\n  [expires: 2030-10-31]\n\nadditionally, the fingerprint string should be identical to cc16 0577 b7bf\n9d3d 6e5d  51c5 60eb 103a e314 a809, as shown in the preceding example.\ncompare the key fingerprint that is returned to the one published on this\npage. they should match. if they don't match, don't install the aws data\nscience workflows python sdk package, and contact aws support.\n\nverify the signature of the package\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nafter you install the gpg tools, authenticate and import the aws data science\nworkflows python sdk public key, and verify that the public key is trusted, you\nare ready to verify the signature of the package.\n\nto verify the package signature, do the following.\n\n1. download the detached signature for the package from pypi\n\n  go to the downloads section for the data science workflows python sdk\n  https://pypi.org/project/stepfunctions/#files on pypi, right-click on the sdk\n  distribution link, and choose \"copy link location/address\".\n\n  append the string \".asc\" to the end of the link you copied, and paste this\n  new link on your browser.\n\n  your browser will prompt you to download a file, which is the detatched\n  signature associated with the respective distribution. save the file on your\n  local machine.\n\n2. verify the signature by running the following command at a command prompt\nin the directory where you saved signature file and the aws data science\nworkflows python sdk installation file. both files must be present.\n\n.. code-block:: text\n\n  gpg --verify <path-to-detached-signature-file>\n\nthe output should look something like the following:\n\n.. code-block:: text\n\n  gpg: signature made thu 31 oct 12:14:53 2019 pdt\n  gpg:                using rsa key cc160577b7bf9d3d6e5d51c560eb103ae314a809\n  gpg: good signature from \"stepfunctions-python-sdk-signing <stepfunctions-developer-experience [at] amazon.com>\" [unknown]\n  gpg: warning: this key is not certified with a trusted signature!\n  gpg:          there is no indication that the signature belongs to the owner.\n  primary key fingerprint: cc16 0577 b7bf 9d3d 6e5d  51c5 60eb 103a e314 a809\n\nif the output contains the phrase good signature from \"aws data science\nworkflows python sdk <stepfunctions-developer-experience [at] amazon.com>\", it means\nthat the signature has successfully been verified, and you can proceed to run\nthe aws data science workflows python sdk package.\n\nif the output includes the phrase bad signature, check whether you performed the\nprocedure correctly. if you continue to get this response, don't run the\ninstallation file that you downloaded previously, and contact aws support.\n\nthe following are details about the warnings you might see:\n\n.. code-block:: text\n\n  warning: this key is not certified with a trusted signature! there is no\n  indication that the signature belongs to the owner. this refers to your\n  personal level of trust in your belief that you possess an authentic public\n  key for aws data science workflows python sdk. in an ideal world, you would\n  visit an aws office and receive the key in person. however, more often you\n  download it from a website. in this case, the website is an aws website.\n\n  gpg: no ultimately trusted keys found. this means that the specific key is not\n  \"ultimately trusted\" by you (or by other people whom you trust).\n\nfor more information, see http://www.gnupg.org.\n\n.. |codebuild| image:: https://codebuild.us-east-2.amazonaws.com/badges?uuid=eyjlbmnyexb0zwreyxrhijoiz2crzkxwn2lpthhbdzawouivzdluq2txqtryynznq3raq0dqykhsb2evt04xovriddbqywfoas8weklgu216outuc29pzfqvqjgrrdhrbwjoeejocfv3psisiml2ugfyyw1ldgvyu3blyyi6ilrqulzqd1zldgrqwkdvdwkilcjtyxrlcmlhbfnldfnlcmlhbci6mx0%3d&branch=master\n  :target: https://us-east-2.console.aws.amazon.com/codesuite/codebuild/projects/stepfunctionspythonsdk-unittests-private/history?region=us-east-2\n  :alt: unit tests build status\n\n.. |readthedocs| image:: https://readthedocs.org/projects/aws-step-functions-data-science-sdk/badge/?version=latest\n  :target: https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/?badge=latest\n  :alt: documentation status\n\n.. |pypi| image:: https://img.shields.io/pypi/v/stepfunctions\n  :target: https://pypi.org/project/stepfunctions/\n  :alt: pypi",
  "docs_url": null,
  "keywords": "ml amazon aws ai tensorflow mxnet",
  "license": "apache license 2.0",
  "name": "stepfunctions",
  "package_url": "https://pypi.org/project/stepfunctions/",
  "project_url": "https://pypi.org/project/stepfunctions/",
  "project_urls": {
    "Homepage": "https://github.com/aws/aws-step-functions-data-science-sdk-python"
  },
  "release_url": "https://pypi.org/project/stepfunctions/2.3.0/",
  "requires_dist": [],
  "requires_python": "",
  "summary": "open source library for developing data science workflows on aws step functions.",
  "version": "2.3.0",
  "releases": [],
  "developers": [
    "amazon_web_services"
  ],
  "kwds": "data_science_workflows tensorflow aws awslabs machine_learning_workflow_abalone",
  "license_kwds": "apache license 2.0",
  "libtype": "pypi",
  "id": "pypi_stepfunctions",
  "homepage": "https://github.com/aws/aws-step-functions-data-science-sdk-python",
  "release_count": 17,
  "dependency_ids": []
}