{
  "classifiers": [
    "development status :: 3 - alpha",
    "intended audience :: developers",
    "license :: osi approved :: apache software license",
    "operating system :: os independent",
    "programming language :: python",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.9",
    "topic :: internet"
  ],
  "description": "bigquery dataframes\n===================\n\nbigquery dataframes provides a pythonic dataframe and machine learning (ml) api\npowered by the bigquery engine.\n\n* ``bigframes.pandas`` provides a pandas-compatible api for analytics.\n* ``bigframes.ml`` provides a scikit-learn-like api for ml.\n\nbigquery dataframes is an open-source package. you can run\n``pip install --upgrade bigframes`` to install the latest version.\n\ndocumentation\n-------------\n\n* `bigquery dataframes source code (github) <https://github.com/googleapis/python-bigquery-dataframes>`_\n* `bigquery dataframes sample notebooks <https://github.com/googleapis/python-bigquery-dataframes/tree/main/notebooks>`_\n* `bigquery dataframes api reference <https://cloud.google.com/python/docs/reference/bigframes/latest>`_\n* `bigquery documentation <https://cloud.google.com/bigquery/docs/>`_\n\n\nquickstart\n----------\n\nprerequisites\n^^^^^^^^^^^^^\n\n* install the ``bigframes`` package.\n* create a google cloud project and billing account.\n* when running locally, authenticate with application default credentials. see\n  the `gcloud auth application-default login\n  <https://cloud.google.com/sdk/gcloud/reference/auth/application-default/login>`_\n  reference.\n\ncode sample\n^^^^^^^^^^^\n\nimport ``bigframes.pandas`` for a pandas-like interface. the ``read_gbq``\nmethod accepts either a fully-qualified table id or a sql query.\n\n.. code-block:: python\n\n  import bigframes.pandas as bpd\n\n  bpd.options.bigquery.project = your_gcp_project_id\n  df1 = bpd.read_gbq(\"project.dataset.table\")\n  df2 = bpd.read_gbq(\"select a, b, c, from `project.dataset.table`\")\n\n* `more code samples <https://github.com/googleapis/python-bigquery-dataframes/tree/main/samples/snippets>`_\n\n\nlocations\n---------\nbigquery dataframes uses a\n`bigquery session <https://cloud.google.com/bigquery/docs/sessions-intro>`_\ninternally to manage metadata on the service side. this session is tied to a\n`location <https://cloud.google.com/bigquery/docs/locations>`_ .\nbigquery dataframes uses the us multi-region as the default location, but you\ncan use ``session_options.location`` to set a different location. every query\nin a session is executed in the location where the session was created.\nbigquery dataframes\nauto-populates ``bf.options.bigquery.location`` if the user starts with\n``read_gbq/read_gbq_table/read_gbq_query()`` and specifies a table, either\ndirectly or in a sql statement.\n\nif you want to reset the location of the created dataframe or series objects,\nyou can close the session by executing ``bigframes.pandas.close_session()``.\nafter that, you can reuse ``bigframes.pandas.options.bigquery.location`` to\nspecify another location.\n\n\n``read_gbq()`` requires you to specify a location if the dataset you are\nquerying is not in the us multi-region. if you try to read a table from another\nlocation, you get a notfound exception.\n\nproject\n-------\nif ``bf.options.bigquery.project`` is not set, the ``$google_cloud_project``\nenvironment variable is used, which is set in the notebook runtime serving the\nbigquery studio/vertex notebooks.\n\nml capabilities\n---------------\n\nthe ml capabilities in bigquery dataframes let you preprocess data, and\nthen train models on that data. you can also chain these actions together to\ncreate data pipelines.\n\npreprocess data\n^^^^^^^^^^^^^^^^^^^^^^^^\n\ncreate transformers to prepare data for use in estimators (models) by\nusing the\n`bigframes.ml.preprocessing module <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.preprocessing>`_\nand the `bigframes.ml.compose module <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.compose>`_.\nbigquery dataframes offers the following transformations:\n\n* use the `kbinsdiscretizer class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.compose.columntransformer>`_\n  in the ``bigframes.ml.preprocessing`` module to bin continuous data into intervals.\n* use the `labelencoder class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.preprocessing.labelencoder>`_\n  in the ``bigframes.ml.preprocessing`` module to normalize the target labels as integer values.\n* use the `maxabsscaler class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.preprocessing.maxabsscaler>`_\n  in the ``bigframes.ml.preprocessing`` module to scale each feature to the range ``[-1, 1]`` by its maximum absolute value.\n* use the `minmaxscaler class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.preprocessing.minmaxscaler>`_\n  in the ``bigframes.ml.preprocessing`` module to standardize features by scaling each feature to the range ``[0, 1]``.\n* use the `standardscaler class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.preprocessing.standardscaler>`_\n  in the ``bigframes.ml.preprocessing`` module to standardize features by removing the mean and scaling to unit variance.\n* use the `onehotencoder class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.preprocessing.onehotencoder>`_\n  in the ``bigframes.ml.preprocessing`` module to transform categorical values into numeric format.\n* use the `columntransformer class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.compose.columntransformer>`_\n  in the ``bigframes.ml.compose`` module to apply transformers to dataframes columns.\n\n\ntrain models\n^^^^^^^^^^^^\n\ncreate estimators to train models in bigquery dataframes.\n\n**clustering models**\n\ncreate estimators for clustering models by using the\n`bigframes.ml.cluster module <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.cluster>`_.\n\n* use the `kmeans class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.cluster.kmeans>`_\n  to create k-means clustering models. use these models for\n  data segmentation. for example, identifying customer segments. k-means is an\n  unsupervised learning technique, so model training doesn't require labels or split\n  data for training or evaluation.\n\n**decomposition models**\n\ncreate estimators for decomposition models by using the `bigframes.ml.decomposition module <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.decomposition>`_.\n\n* use the `pca class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.decomposition.pca>`_\n  to create principal component analysis (pca) models. use these\n  models for computing  principal components and using them to perform a change of\n  basis on the data. this provides dimensionality reduction by projecting each data\n  point onto only the first few principal components to obtain lower-dimensional\n  data while preserving as much of the data's variation as possible.\n\n\n**ensemble models**\n\ncreate estimators for ensemble models by using the `bigframes.ml.ensemble module <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.ensemble>`_.\n\n* use the `randomforestclassifier class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.ensemble.randomforestclassifier>`_\n  to create random forest classifier models. use these models for constructing multiple\n  learning method decision trees for classification.\n* use the `randomforestregressor class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.ensemble.randomforestregressor>`_\n  to create random forest regression models. use\n  these models for constructing multiple learning method decision trees for regression.\n* use the `xgbclassifier class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.ensemble.xgbclassifier>`_\n  to create gradient boosted tree classifier models. use these models for additively\n  constructing multiple learning method decision trees for classification.\n* use the `xgbregressor class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.ensemble.xgbregressor>`_\n  to create gradient boosted tree regression models. use these models for additively\n  constructing multiple learning method decision trees for regression.\n\n\n**forecasting models**\n\ncreate estimators for forecasting models by using the `bigframes.ml.forecasting module <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.forecasting>`_.\n\n* use the `arimaplus class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.forecasting.arimaplus>`_\n  to create time series forecasting models.\n\n**imported models**\n\ncreate estimators for imported models by using the `bigframes.ml.imported module <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.imported>`_.\n\n* use the `onnxmodel class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.imported.onnxmodel>`_\n  to import open neural network exchange (onnx) models.\n* use the `tensorflowmodel class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.imported.tensorflowmodel>`_\n  to import tensorflow models.\n\n**linear models**\n\ncreate estimators for linear models by using the `bigframes.ml.linear_model module <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.linear_model>`_.\n\n* use the `linearregression class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.linear_model.linearregression>`_\n  to create linear regression models. use these models for forecasting. for example,\n  forecasting the sales of an item on a given day.\n* use the `logisticregression class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.linear_model.logisticregression>`_\n  to create logistic regression models. use these models for the classification of two\n  or more possible values such as whether an input is ``low-value``, ``medium-value``,\n  or ``high-value``.\n\n**large language models**\n\ncreate estimators for llms by using the `bigframes.ml.llm module <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.llm>`_.\n\n* use the `palm2textgenerator class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.llm.palm2textgenerator>`_ to create palm2 text generator models. use these models\n  for text generation tasks.\n* use the `palm2textembeddinggenerator class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.llm.palm2textembeddinggenerator>`_ to create palm2 text embedding generator models.\n  use these models for text embedding generation tasks.\n\n\ncreate pipelines\n^^^^^^^^^^^^^^^^\n\ncreate ml pipelines by using\n`bigframes.ml.pipeline module <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.pipeline>`_.\npipelines let you assemble several ml steps to be cross-validated together while setting\ndifferent parameters. this simplifies your code, and allows you to deploy data preprocessing\nsteps and an estimator together.\n\n* use the `pipeline class <https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.pipeline.pipeline>`_\n  to create a pipeline of transforms with a final estimator.\n\n\nml locations\n------------\n\n``bigframes.ml`` supports the same locations as bigquery ml. bigquery ml model\nprediction and other ml functions are supported in all bigquery regions. support\nfor model training varies by region. for more information, see\n`bigquery ml locations <https://cloud.google.com/bigquery/docs/locations#bqml-loc>`_.\n\n\ndata types\n----------\n\nbigquery dataframes supports the following numpy and pandas dtypes:\n\n* ``numpy.dtype(\"o\")``\n* ``pandas.booleandtype()``\n* ``pandas.float64dtype()``\n* ``pandas.int64dtype()``\n* ``pandas.stringdtype(storage=\"pyarrow\")``\n* ``pandas.arrowdtype(pa.date32())``\n* ``pandas.arrowdtype(pa.time64(\"us\"))``\n* ``pandas.arrowdtype(pa.timestamp(\"us\"))``\n* ``pandas.arrowdtype(pa.timestamp(\"us\", tz=\"utc\"))``\n\nbigquery dataframes doesn\u2019t support the following bigquery data types:\n\n* ``array``\n* ``numeric``\n* ``bignumeric``\n* ``interval``\n* ``struct``\n* ``json``\n\nall other bigquery data types display as the object type.\n\n\nremote functions\n----------------\n\nbigquery dataframes gives you the ability to turn your custom scalar functions\ninto `bigquery remote functions\n<https://cloud.google.com/bigquery/docs/remote-functions>`_ . creating a remote\nfunction in bigquery dataframes (see `code samples\n<https://cloud.google.com/bigquery/docs/remote-functions#bigquery-dataframes>`_)\ncreates a bigquery remote function, a `bigquery\nconnection\n<https://cloud.google.com/bigquery/docs/create-cloud-resource-connection>`_ ,\nand a `cloud functions (2nd gen) function\n<https://cloud.google.com/functions/docs/concepts/overview>`_ .\n\nbigquery connections are created in the same location as the bigquery\ndataframes session, using the name you provide in the custom function\ndefinition. to view and manage connections, do the following:\n\n1. go to `bigquery in the google cloud console <https://console.cloud.google.com/bigquery>`__.\n2. select the project in which you created the remote function.\n3. in the explorer pane, expand that project and then expand external connections.\n\nbigquery remote functions are created in the dataset you specify, or\nin a special type of `hidden dataset <https://cloud.google.com/bigquery/docs/datasets#hidden_datasets>`__\nreferred to as an anonymous dataset. to view and manage remote functions created\nin a user provided dataset, do the following:\n\n1. go to `bigquery in the google cloud console <https://console.cloud.google.com/bigquery>`__.\n2. select the project in which you created the remote function.\n3. in the explorer pane, expand that project, expand the dataset in which you\n   created the remote function, and then expand routines.\n\nto view and manage cloud functions functions, use the\n`functions <https://console.cloud.google.com/functions/list?env=gen2>`_\npage and use the project picker to select the project in which you\ncreated the function. for easy identification, the names of the functions\ncreated by bigquery dataframes are prefixed by ``bigframes``.\n\n**requirements**\n\nbigquery dataframes uses the ``gcloud`` command-line interface internally,\nso you must run ``gcloud auth login`` before using remote functions.\n\nto use bigquery dataframes remote functions, you must enable the following apis:\n\n* the bigquery api (bigquery.googleapis.com)\n* the bigquery connection api (bigqueryconnection.googleapis.com)\n* the cloud functions api (cloudfunctions.googleapis.com)\n* the cloud run api (run.googleapis.com)\n* the artifact registry api (artifactregistry.googleapis.com)\n* the cloud build api (cloudbuild.googleapis.com )\n* the cloud resource manager api (cloudresourcemanager.googleapis.com)\n\nto use bigquery dataframes remote functions, you must be granted the\nfollowing iam roles:\n\n* bigquery data editor (roles/bigquery.dataeditor)\n* bigquery connection admin (roles/bigquery.connectionadmin)\n* cloud functions developer (roles/cloudfunctions.developer)\n* service account user (roles/iam.serviceaccountuser) on the\n  `service account <https://cloud.google.com/functions/docs/reference/iam/roles#additional-configuration> `\n  ``project_number-compute@developer.gserviceaccount.com``\n* storage object viewer (roles/storage.objectviewer)\n* project iam admin (roles/resourcemanager.projectiamadmin)\n\n**limitations**\n\n* remote functions take about 90 seconds to become available when you first create them.\n* trivial changes in the notebook, such as inserting a new cell or renaming a variable,\n  might cause the remote function to be re-created, even if these changes are unrelated\n  to the remote function code.\n* bigquery dataframes does not differentiate any personal data you include in the remote\n  function code. the remote function code is serialized as an opaque box to deploy it as a\n  cloud functions function.\n* the cloud functions (2nd gen) functions, bigquery connections, and bigquery remote\n  functions created by bigquery dataframes persist in google cloud. if you don\u2019t want to\n  keep these resources, you must delete them separately using an appropriate cloud functions\n  or bigquery interface.\n* a project can have up to 1000 cloud functions (2nd gen) functions at a time. see cloud\n  functions quotas for all the limits.\n\n\nquotas and limits\n------------------\n\n`bigquery quotas <https://cloud.google.com/bigquery/quotas>`_\nincluding hardware, software, and network components.\n\n\nsession termination\n-------------------\n\neach bigquery dataframes dataframe or series object is tied to a bigquery\ndataframes session, which is in turn based on a bigquery session. bigquery\nsessions\n`auto-terminate <https://cloud.google.com/bigquery/docs/sessions-terminating#auto-terminate_a_session>`_\n; when this happens, you can\u2019t use previously\ncreated dataframe or series objects and must re-create them using a new\nbigquery dataframes session. you can do this by running\n``bigframes.pandas.close_session()`` and then re-running the bigquery\ndataframes expressions.\n\n\ndata processing location\n------------------------\n\nbigquery dataframes is designed for scale, which it achieves by keeping data\nand processing on the bigquery service. however, you can bring data into the\nmemory of your client machine by calling ``.to_pandas()`` on a dataframe or series\nobject. if you choose to do this, the memory limitation of your client machine\napplies.\n\n\nlicense\n-------\n\nbigquery dataframes is distributed with the `apache-2.0 license\n<https://github.com/googleapis/python-bigquery-dataframes/blob/main/license>`_.\n\nit also contains code derived from the following third-party packages:\n\n* `ibis <https://ibis-project.org/>`_\n* `pandas <https://pandas.pydata.org/>`_\n* `python <https://www.python.org/>`_\n* `scikit-learn <https://scikit-learn.org/>`_\n* `xgboost <https://xgboost.readthedocs.io/en/stable/>`_\n\nfor details, see the `third_party\n<https://github.com/googleapis/python-bigquery-dataframes/tree/main/third_party/bigframes_vendored>`_\ndirectory.\n\n\ncontact us\n----------\n\nfor further help and provide feedback, you can email us at `bigframes-feedback@google.com <https://mail.google.com/mail/?view=cm&fs=1&tf=1&to=bigframes-feedback@google.com>`_.\n",
  "docs_url": null,
  "keywords": "",
  "license": "apache 2.0",
  "name": "bigframes",
  "package_url": "https://pypi.org/project/bigframes/",
  "project_url": "https://pypi.org/project/bigframes/",
  "project_urls": {
    "Homepage": "https://github.com/googleapis/python-bigquery-dataframes"
  },
  "release_url": "https://pypi.org/project/bigframes/0.17.0/",
  "requires_dist": [
    "cloudpickle >=2.0.0",
    "fsspec >=2023.3.0",
    "gcsfs >=2023.3.0",
    "geopandas >=0.12.2",
    "google-auth <3.0dev,>2.14.1",
    "google-cloud-bigquery[bqstorage,pandas] >=3.10.0",
    "google-cloud-functions >=1.10.1",
    "google-cloud-bigquery-connection >=1.12.0",
    "google-cloud-iam >=2.12.1",
    "google-cloud-resource-manager >=1.10.3",
    "google-cloud-storage >=2.0.0",
    "ibis-framework[bigquery] <7.0.0dev,>=6.2.0",
    "pandas <2.1.4,>=1.5.0",
    "pydata-google-auth >=1.8.2",
    "requests >=2.27.1",
    "scikit-learn >=1.2.2",
    "sqlalchemy <3.0dev,>=1.4",
    "tabulate >=0.9",
    "ipywidgets >=7.7.1",
    "humanize >=4.6.0",
    "google-cloud-testutils ; extra == 'all'",
    "nox ; extra == 'all'",
    "pandas-gbq >=0.19.0 ; extra == 'all'",
    "pre-commit ; extra == 'all'",
    "pytest ; extra == 'all'",
    "pytest-mock ; extra == 'all'",
    "pytest ; extra == 'dev'",
    "pytest-mock ; extra == 'dev'",
    "pre-commit ; extra == 'dev'",
    "nox ; extra == 'dev'",
    "google-cloud-testutils ; extra == 'dev'",
    "pandas-gbq >=0.19.0 ; extra == 'tests'"
  ],
  "requires_python": ">=3.9",
  "summary": "bigquery dataframes -- scalable analytics and machine learning with bigquery",
  "version": "0.17.0",
  "releases": [],
  "developers": [
    "bigframes-feedback@google.com",
    "google_llc"
  ],
  "kwds": "bigquery dataframes bigqueryconnection pandas dataframe",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_bigframes",
  "homepage": "https://github.com/googleapis/python-bigquery-dataframes",
  "release_count": 20,
  "dependency_ids": [
    "pypi_cloudpickle",
    "pypi_fsspec",
    "pypi_gcsfs",
    "pypi_geopandas",
    "pypi_google_auth",
    "pypi_google_cloud_bigquery",
    "pypi_google_cloud_bigquery_connection",
    "pypi_google_cloud_functions",
    "pypi_google_cloud_iam",
    "pypi_google_cloud_resource_manager",
    "pypi_google_cloud_storage",
    "pypi_google_cloud_testutils",
    "pypi_humanize",
    "pypi_ibis_framework",
    "pypi_ipywidgets",
    "pypi_nox",
    "pypi_pandas",
    "pypi_pandas_gbq",
    "pypi_pre_commit",
    "pypi_pydata_google_auth",
    "pypi_pytest",
    "pypi_pytest_mock",
    "pypi_requests",
    "pypi_scikit_learn",
    "pypi_sqlalchemy",
    "pypi_tabulate"
  ]
}