{
  "classifiers": [],
  "description": "=====\nbig_o\n=====\n\nbig_o is a python module to estimate the time complexity of python code from\nits execution time.  it can be used to analyze how functions scale with inputs\nof increasing size.\n\nbig_o executes a python function for input of increasing size `n`, and measures\nits execution time. from the measurements, big_o fits a set of time complexity\nclasses and returns the best fitting class. this is an empirical way to\ncompute the asymptotic class of a function in `\"big-o\"\n<http://en.wikipedia.org/wiki/big_oh>`_.  notation. (strictly\nspeaking, we're empirically computing the big theta class.)\n\ninstallation\n------------\n\nto install the package use the command :code:`pip install big-o`\n\nusage\n-----\n\nfor concreteness, let's say we would like to compute the asymptotic behavior\nof a simple function that finds the maximum element in a list of positive\nintegers:\n\n    >>> def find_max(x):\n    ...     \"\"\"find the maximum element in a list of positive integers.\"\"\"\n    ...     max_ = 0\n    ...     for el in x:\n    ...         if el > max_:\n    ...             max_ = el\n    ...     return max_\n    ...\n\nto do this, we call `big_o.big_o` passing as argument the function and a\ndata generator that provides lists of random integers of length n:\n\n    >>> import big_o\n    >>> positive_int_generator = lambda n: big_o.datagen.integers(n, 0, 10000)\n    >>> best, others = big_o.big_o(find_max, positive_int_generator, n_repeats=100)\n    >>> print(best)\n    linear: time = -0.00035 + 2.7e-06*n (sec)\n\n`big_o` inferred that the asymptotic behavior of the `find_max` function is\nlinear, and returns an object containing the fitted coefficients for the\ncomplexity class. the second return argument, `others`, contains a dictionary\nof all fitted classes with the residuals from the fit as keys:\n\n    >>> for class_, residuals in others.items():\n    ...     print('{!s:<60s}    (res: {:.2g})'.format(class_, residuals))\n    ...\n    exponential: time = -5 * 4.6e-05^n (sec)                        (res: 15)\n    linear: time = -0.00035 + 2.7e-06*n (sec)                       (res: 6.3e-05)\n    quadratic: time = 0.046 + 2.4e-11*n^2 (sec)                     (res: 0.0056)\n    linearithmic: time = 0.0061 + 2.3e-07*n*log(n) (sec)            (res: 0.00016)\n    cubic: time = 0.067 + 2.3e-16*n^3 (sec)                         (res: 0.013)\n    logarithmic: time = -0.2 + 0.033*log(n) (sec)                   (res: 0.03)\n    constant: time = 0.13 (sec)                                     (res: 0.071)\n    polynomial: time = -13 * x^0.98 (sec)                           (res: 0.0056)\n\nsubmodules\n----------\n\n- `big_o.datagen`: this sub-module contains common data generators, including\n  an identity generator that simply returns n (`datagen.n_`), and a data\n  generator that returns a list of random integers of length n\n  (`datagen.integers`).\n\n- `big_o.complexities`: this sub-module defines the complexity classes to be\n  fit to the execution times. unless you want to define new classes, you don't\n  need to worry about it.\n\nstandard library examples\n-------------------------\n\nsorting a list in python is o(n*log(n)) (a.k.a. 'linearithmic'):\n\n    >>> big_o.big_o(sorted, lambda n: big_o.datagen.integers(n, 10000, 50000))\n    (<big_o.complexities.linearithmic object at 0x031da9d0>, ...)\n\ninserting elements at the beginning of a list is o(n):\n\n    >>> def insert_0(lst):\n    ...     lst.insert(0, 0)\n    ...\n    >>> print(big_o.big_o(insert_0, big_o.datagen.range_n, n_measures=100)[0])\n    linear: time = -4.2e-06 + 7.9e-10*n (sec)\n\ninserting elements at the beginning of a queue is o(1):\n\n    >>> from collections import deque\n    >>> def insert_0_queue(queue):\n    ...     queue.insert(0, 0)\n    ...\n    >>> def queue_generator(n):\n    ...      return deque(range(n))\n    ...\n    >>> print(big_o.big_o(insert_0_queue, queue_generator, n_measures=100)[0])\n    constant: time = 2.2e-06 (sec)\n\n`numpy` examples\n----------------\n\ncreating an array:\n\n- `numpy.zeros` is o(n), since it needs to initialize every element to 0:\n\n    >>> import numpy as np\n    >>> big_o.big_o(np.zeros, big_o.datagen.n_, max_n=100000, n_repeats=100)\n    (<class 'big_o.big_o.linear'>, ...)\n\n- `numpy.empty` instead just allocates the memory, and is thus o(1):\n\n    >>> big_o.big_o(np.empty, big_o.datagen.n_, max_n=100000, n_repeats=100)\n    (<class 'big_o.big_o.constant'> ...)\n\nadditional examples\n-------------------\n\nwe can compare the estimated time complexities of different fibonacci number\nimplementations. the naive implementation is exponential o(2^n). since this\nimplementation is very inefficient we'll reduce the maximum tested n:\n\n    >>> def fib_naive(n):\n    ...     if n < 0:\n    ...         return -1\n    ...     if n < 2:\n    ...         return n\n    ...     return fib_naive(n-1) + fib_naive(n-2)\n    ...\n    >>> print(big_o.big_o(fib_naive, big_o.datagen.n_, n_repeats=20, min_n=2, max_n=25)[0])\n    exponential: time = -11 * 0.47^n (sec)\n\na more efficient implementation to find fibonacci numbers involves using\ndynamic programming and is linear o(n):\n\n    >>> def fib_dp(n):\n    ...     if n < 0:\n    ...         return -1\n    ...     if n < 2:\n    ...         return n\n    ...     a = 0\n    ...     b = 1\n    ...     for i in range(2, n+1):\n    ...         a, b = b, a+b\n    ...     return b\n    ...\n    >>> print(big_o.big_o(fib_dp, big_o.datagen.n_, n_repeats=100, min_n=200, max_n=1000)[0])\n    linear: time = -1.8e-06 + 7.3e-06*n (sec)\n\nreport generation\n-----------------\n\nthis feature allows users to generate a report based on the outputs received from\ncalling the :code:`big-o` function.\nthe report defines the best time complexity along with the the others\nestimates and returns them as a string.\n\n    >>> best, others = big_o.big_o(heapify, data_generator_heapify, max_n=10**7)\n    >>> print(big_o.reports.big_o_report(best, others))\n    best : polynomial: time = 3.5e-06 * x^0.97 (sec)\n    constant: time = 0.13 (sec)                                     (res: 0.067)\n    linear: time = 0.0068 + 2.5e-06*n (sec)                         (res: 0.003)\n    quadratic: time = 0.053 + 2.2e-11*n^2 (sec)                     (res: 0.012)\n    cubic: time = 0.074 + 2.1e-16*n^3 (sec)                         (res: 0.02)\n    polynomial: time = 3.5e-06 * x^0.97 (sec)                       (res: 0.003)\n    logarithmic: time = -0.2 + 0.033*log(n) (sec)                   (res: 0.027)\n    linearithmic: time = 0.013 + 2.2e-07*n*log(n) (sec)             (res: 0.0035)\n    exponential: time = 0.007 * 1^n (sec)                           (res: 0.22)\n\nlicense\n-------\n\nbig_o is released under bsd-3. see license.txt .\n\ncopyright (c) 2011-2018, pietro berkes. all rights reserved.\n",
  "docs_url": null,
  "keywords": "",
  "license": "license.txt",
  "name": "big-o",
  "package_url": "https://pypi.org/project/big-O/",
  "project_url": "https://pypi.org/project/big-O/",
  "project_urls": {
    "Homepage": "https://github.com/pberkes/big_O"
  },
  "release_url": "https://pypi.org/project/big-O/0.11.0/",
  "requires_dist": [],
  "requires_python": "",
  "summary": "empirical estimation of time complexity from execution time",
  "version": "0.11.0",
  "releases": [],
  "developers": [
    "pietro.berkes@googlemail.com",
    "pietro_berkes"
  ],
  "kwds": "complexity big_o_report asymptotic big_o python",
  "license_kwds": "license.txt",
  "libtype": "pypi",
  "id": "pypi_big_o",
  "homepage": "https://github.com/pberkes/big_o",
  "release_count": 6,
  "dependency_ids": []
}