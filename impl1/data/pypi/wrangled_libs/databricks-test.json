{
  "classifiers": [
    "license :: osi approved :: mit license",
    "operating system :: os independent",
    "programming language :: python :: 3"
  ],
  "description": "# databricks_test\n\n## about\n\nan experimental unit test framework for databricks notebooks.\n\n_this open-source project is not developed by nor affiliated with databricks._\n\n## installing\n\n```\npip install databricks_test\n```\n\n## usage\n\nadd a cell at the beginning of your databricks notebook:\n\n```python\n# instrument for unit tests. this is only executed in local unit tests, not in databricks.\nif 'dbutils' not in locals():\n    import databricks_test\n    databricks_test.inject_variables()\n```\n\nthe `if` clause causes the inner code to be skipped when run in databricks.\ntherefore there is no need to install the `databricks_test` module on your databricks environment.\n\nadd your notebook into a code project, for example using [github version control in azure databricks](https://docs.microsoft.com/en-us/azure/databricks/notebooks/azure-devops-services-version-control).\n\nset up pytest in your code project (outside of databricks).\n\ncreate a test case with the following structure:\n\n```python\nimport databricks_test\n\ndef test_method():\n    with databricks_test.session() as dbrickstest:\n\n        # set up mocks on dbrickstest\n        # ...\n\n        # run notebook\n        dbrickstest.run_notebook(\"notebook_dir\", \"notebook_name_without_py_suffix\")\n\n        # test assertions\n        # ...\n```\n\nyou can set up [mocks](https://docs.python.org/dev/library/unittest.mock.html) on `dbrickstest`, for example:\n\n```python\ndbrickstest.dbutils.widgets.get.return_value = \"myvalue\"\n```\n\nsee samples below for more examples.\n\n## supported features\n\n* spark context injected into databricks notebooks: `spark`, `table`, `sql` etc.\n* pyspark with all spark features including reading and writing to disk, udfs and pandas udfs\n* databricks utilities (`dbutils`, `display`) with user-configurable mocks\n* mocking connectors such as azure storage, s3 and sql data warehouse\n\n## unsupported features\n\n* notebook formats other than `.py` (`.ipynb`, `.dbc`) are not supported\n* non-python cells such as `%scala` and `%sql` (those cells are skipped, as they are stored in `.py` notebooks as comments)\n* writing directly to `/dbfs` mount on local filesystem:\n  write to a local temporary file instead and use dbutils.fs.cp() to copy to dbfs, which you can intercept with a mock\n* databricks extensions to spark such as `spark.read.format(\"binaryfile\")`\n\n## sample test\n\nsample test case for an etl notebook reading csv and writing parquet.\n\n```python\nimport pandas as pd\nimport databricks_test\nfrom tempfile import temporarydirectory\n\nfrom pandas.testing import assert_frame_equal\n\ndef test_etl():\n    with databricks_test.session() as dbrickstest:\n        with temporarydirectory() as tmp_dir:\n            out_dir = f\"{tmp_dir}/out\"\n\n            # provide input and output location as widgets to notebook\n            switch = {\n                \"input\": \"tests/etl_input.csv\",\n                \"output\": out_dir,\n            }\n            dbrickstest.dbutils.widgets.get.side_effect = lambda x: switch.get(\n                x, \"\")\n\n            # run notebook\n            dbrickstest.run_notebook(\".\", \"etl_notebook\")\n\n            # notebook produces a parquet file (directory)\n            resultdf = pd.read_parquet(out_dir)\n\n        # compare produced parquet file and expected csv file\n        expecteddf = pd.read_csv(\"tests/etl_expected.csv\")\n        assert_frame_equal(expecteddf, resultdf, check_dtype=false)\n```\n\nin the notebook, we pass parameters using widgets.\nthis makes it easy to pass\na local file location in tests, and a remote url (such as azure storage or s3)\nin production.\n\n```python\n# databricks notebook source\n# this notebook processed the training dataset (imported by data factory)\n# and computes a cleaned dataset with additional features such as city.\nfrom pyspark.sql.types import structtype, structfield\nfrom pyspark.sql.types import doubletype, integertype\nfrom pyspark.sql.functions import col, pandas_udf, pandasudftype\n\n# command ----------\n\n# instrument for unit tests. this is only executed in local unit tests, not in databricks.\nif 'dbutils' not in locals():\n    import databricks_test\n    databricks_test.inject_variables()\n\n# command ----------\n\n# widgets for interactive development.\ndbutils.widgets.text(\"input\", \"\")\ndbutils.widgets.text(\"output\", \"\")\ndbutils.widgets.text(\"secretscope\", \"\")\ndbutils.widgets.text(\"secretname\", \"\")\ndbutils.widgets.text(\"keyname\", \"\")\n\n# command ----------\n\n# set up storage credentials\n\nspark.conf.set(\n    dbutils.widgets.get(\"keyname\"),\n    dbutils.secrets.get(\n        scope=dbutils.widgets.get(\"secretscope\"),\n        key=dbutils.widgets.get(\"secretname\")\n    ),\n)\n\n# command ----------\n\n# import csv files\nschema = structtype(\n    [\n        structfield(\"adouble\", doubletype(), nullable=false),\n        structfield(\"aninteger\", integertype(), nullable=false),\n    ]\n)\n\ndf = (\n    spark.read.format(\"csv\")\n    .options(header=\"true\", mode=\"failfast\")\n    .schema(schema)\n    .load(dbutils.widgets.get('input'))\n)\ndisplay(df)\n\n# command ----------\n\ndf.count()\n\n# command ----------\n\n# inputs and output are pandas.series of doubles\n@pandas_udf('integer', pandasudftype.scalar)\ndef square(x):\n    return x * x\n\n\n# command ----------\n\n# write out parquet data\n(df\n    .withcolumn(\"asquaredinteger\", square(col(\"aninteger\")))\n    .write\n    .parquet(dbutils.widgets.get('output'))\n )\n```\n\n\n## advanced mocking\n\nsample test case mocking pyspark classes for a notebook connecting to azure sql data warehouse.\n\n```python\nimport databricks_test\nimport pyspark\nimport pyspark.sql.functions as f\nfrom tempfile import temporarydirectory\nfrom pandas.testing import assert_frame_equal\nimport pandas as pd\n\n\ndef test_sqldw(monkeypatch):\n    with databricks_test.session() as dbrickstest, temporarydirectory() as tmp:\n\n        out_dir = f\"{tmp}/out\"\n\n        # mock sql dw loader, creating a spark dataframe instead\n        def mock_load(reader):\n            return (\n                dbrickstest.spark\n                .range(10)\n                .withcolumn(\"age\", f.col(\"id\") * 6)\n                .withcolumn(\"salary\", f.col(\"id\") * 10000)\n            )\n\n        monkeypatch.setattr(\n            pyspark.sql.readwriter.dataframereader, \"load\", mock_load)\n\n        # mock sql dw writer, writing to a local parquet file instead\n        def mock_save(writer):\n            monkeypatch.undo()\n            writer.format(\"parquet\")\n            writer.save(out_dir)\n\n        monkeypatch.setattr(\n            pyspark.sql.readwriter.dataframewriter, \"save\", mock_save)\n\n        # run notebook\n        dbrickstest.run_notebook(\".\", \"sqldw_notebook\")\n\n        # notebook produces a parquet file (directory)\n        resultdf = pd.read_parquet(out_dir)\n\n        # compare produced parquet file and expected csv file\n        expecteddf = pd.read_csv(\"tests/sqldw_expected.csv\")\n        assert_frame_equal(expecteddf, resultdf, check_dtype=false)\n```\n\n## issues\n\nplease report issues at [https://github.com/microsoft/dataops/issues](https://github.com/microsoft/dataops/issues).\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "databricks-test",
  "package_url": "https://pypi.org/project/databricks-test/",
  "project_url": "https://pypi.org/project/databricks-test/",
  "project_urls": {
    "Homepage": "https://github.com/microsoft/DataOps"
  },
  "release_url": "https://pypi.org/project/databricks-test/0.0.4/",
  "requires_dist": [],
  "requires_python": "",
  "summary": "unit testing and mocking for databricks",
  "version": "0.0.4",
  "releases": [],
  "developers": [
    "alexandre_gattiker",
    "algattik@microsoft.com"
  ],
  "kwds": "databricks_test databricks pyspark run_notebook notebook_name_without_py_suffix",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_databricks_test",
  "homepage": "https://github.com/microsoft/dataops",
  "release_count": 4,
  "dependency_ids": []
}