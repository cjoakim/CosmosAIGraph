{
  "classifiers": [
    "license :: osi approved :: mit license",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.12",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9"
  ],
  "description": "<h1 align=\"center\">\n        \ud83d\ude85 litellm\n    </h1>\n    <p align=\"center\">\n        <p align=\"center\">call all llm apis using the openai format [bedrock, huggingface, cohere, togetherai, azure, openai, etc.]\n        <br>\n    </p>\n<h4 align=\"center\"><a href=\"https://docs.litellm.ai/docs/simple_proxy\" target=\"_blank\">openai proxy server</a></h4>\n<h4 align=\"center\">\n    <a href=\"https://pypi.org/project/litellm/\" target=\"_blank\">\n        <img src=\"https://img.shields.io/pypi/v/litellm.svg\" alt=\"pypi version\">\n    </a>\n    <a href=\"https://dl.circleci.com/status-badge/redirect/gh/berriai/litellm/tree/main\" target=\"_blank\">\n        <img src=\"https://dl.circleci.com/status-badge/img/gh/berriai/litellm/tree/main.svg?style=svg\" alt=\"circleci\">\n    </a>\n    <a href=\"https://www.ycombinator.com/companies/berriai\">\n        <img src=\"https://img.shields.io/badge/y%20combinator-w23-orange?style=flat-square\" alt=\"y combinator w23\">\n    </a>\n    <a href=\"https://wa.link/huol9n\">\n        <img src=\"https://img.shields.io/static/v1?label=chat%20on&message=whatsapp&color=success&logo=whatsapp&style=flat-square\" alt=\"whatsapp\">\n    </a>\n    <a href=\"https://discord.gg/wupm9drgdw\">\n        <img src=\"https://img.shields.io/static/v1?label=chat%20on&message=discord&color=blue&logo=discord&style=flat-square\" alt=\"discord\">\n    </a>\n</h4>\n\nlitellm manages\n- translating inputs to the provider's `completion` and `embedding` endpoints\n- guarantees [consistent output](https://docs.litellm.ai/docs/completion/output), text responses will always be available at `['choices'][0]['message']['content']`\n- exception mapping - common exceptions across providers are mapped to the openai exception types.\n- load-balance across multiple deployments (e.g. azure/openai) - `router` **1k+ requests/second**\n\n# usage ([**docs**](https://docs.litellm.ai/docs/))\n\n> [!important]\n> litellm v1.0.0 now requires `openai>=1.0.0`. migration guide [here](https://docs.litellm.ai/docs/migration)\n\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/berriai/litellm/blob/main/cookbook/litellm_getting_started.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"open in colab\"/>\n</a>\n\n```\npip install litellm\n```\n\n```python\nfrom litellm import completion\nimport os\n\n## set env variables \nos.environ[\"openai_api_key\"] = \"your-openai-key\" \nos.environ[\"cohere_api_key\"] = \"your-cohere-key\" \n\nmessages = [{ \"content\": \"hello, how are you?\",\"role\": \"user\"}]\n\n# openai call\nresponse = completion(model=\"gpt-3.5-turbo\", messages=messages)\n\n# cohere call\nresponse = completion(model=\"command-nightly\", messages=messages)\nprint(response)\n```\n\n## async ([docs](https://docs.litellm.ai/docs/completion/stream#async-completion))\n\n```python\nfrom litellm import acompletion\nimport asyncio\n\nasync def test_get_response():\n    user_message = \"hello, how are you?\"\n    messages = [{\"content\": user_message, \"role\": \"user\"}]\n    response = await acompletion(model=\"gpt-3.5-turbo\", messages=messages)\n    return response\n\nresponse = asyncio.run(test_get_response())\nprint(response)\n```\n\n## streaming ([docs](https://docs.litellm.ai/docs/completion/stream))\nlitellm supports streaming the model response back, pass `stream=true` to get a streaming iterator in response.  \nstreaming is supported for all models (bedrock, huggingface, togetherai, azure, openai, etc.)\n```python\nfrom litellm import completion\nresponse = completion(model=\"gpt-3.5-turbo\", messages=messages, stream=true)\nfor part in response:\n    print(part.choices[0].delta.content or \"\")\n\n# claude 2\nresponse = completion('claude-2', messages, stream=true)\nfor part in response:\n    print(part.choices[0].delta.content or \"\")\n```\n\n## openai proxy - ([docs](https://docs.litellm.ai/docs/simple_proxy))\n\nlitellm proxy manages:\n* calling 100+ llms huggingface/bedrock/togetherai/etc. in the openai chatcompletions & completions format\n* load balancing - between multiple models + deployments of the same model litellm proxy can handle 1k+ requests/second during load tests\n* authentication & spend tracking virtual keys\n\n### step 1: start litellm proxy\n```shell\n$ litellm --model huggingface/bigcode/starcoder\n\n#info: proxy running on http://0.0.0.0:8000\n```\n\n### step 2: replace openai base\n```python\nimport openai # openai v1.0.0+\nclient = openai.openai(api_key=\"anything\",base_url=\"http://0.0.0.0:8000\") # set proxy to base_url\n# request sent to model set on litellm proxy, `litellm --model`\nresponse = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"this is a test request, write a short poem\"\n    }\n])\n\nprint(response)\n```\n\n## logging observability ([docs](https://docs.litellm.ai/docs/observability/callbacks))\nlitellm exposes pre defined callbacks to send data to langfuse, llmonitor, helicone, promptlayer, traceloop, slack\n```python\nfrom litellm import completion\n\n## set env variables for logging tools\nos.environ[\"langfuse_public_key\"] = \"\"\nos.environ[\"langfuse_secret_key\"] = \"\"\nos.environ[\"llmonitor_app_id\"] = \"your-llmonitor-app-id\"\n\nos.environ[\"openai_api_key\"]\n\n# set callbacks\nlitellm.success_callback = [\"langfuse\", \"llmonitor\"] # log input/output to langfuse, llmonitor, supabase\n\n#openai call\nresponse = completion(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"hi \ud83d\udc4b - i'm openai\"}])\n```\n\n## supported provider ([docs](https://docs.litellm.ai/docs/providers))\n| provider      | [completion](https://docs.litellm.ai/docs/#basic-usage) | [streaming](https://docs.litellm.ai/docs/completion/stream#streaming-responses)  | [async completion](https://docs.litellm.ai/docs/completion/stream#async-completion)  | [async streaming](https://docs.litellm.ai/docs/completion/stream#async-streaming)  |\n| ------------- | ------------- | ------------- | ------------- | ------------- |\n| [openai](https://docs.litellm.ai/docs/providers/openai)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [azure](https://docs.litellm.ai/docs/providers/azure)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [aws - sagemaker](https://docs.litellm.ai/docs/providers/aws_sagemaker)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [aws - bedrock](https://docs.litellm.ai/docs/providers/bedrock)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [cohere](https://docs.litellm.ai/docs/providers/cohere)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [anthropic](https://docs.litellm.ai/docs/providers/anthropic)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [huggingface](https://docs.litellm.ai/docs/providers/huggingface)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [replicate](https://docs.litellm.ai/docs/providers/replicate)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [together_ai](https://docs.litellm.ai/docs/providers/togetherai)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [openrouter](https://docs.litellm.ai/docs/providers/openrouter)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [google - vertex_ai](https://docs.litellm.ai/docs/providers/vertex)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [google - palm](https://docs.litellm.ai/docs/providers/palm)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [mistral ai api](https://docs.litellm.ai/docs/providers/mistral)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [ai21](https://docs.litellm.ai/docs/providers/ai21)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [baseten](https://docs.litellm.ai/docs/providers/baseten)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [vllm](https://docs.litellm.ai/docs/providers/vllm)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [nlp_cloud](https://docs.litellm.ai/docs/providers/nlp_cloud)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [aleph alpha](https://docs.litellm.ai/docs/providers/aleph_alpha)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [petals](https://docs.litellm.ai/docs/providers/petals)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [ollama](https://docs.litellm.ai/docs/providers/ollama)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [deepinfra](https://docs.litellm.ai/docs/providers/deepinfra)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [perplexity-ai](https://docs.litellm.ai/docs/providers/perplexity)  | \u2705 | \u2705 | \u2705 | \u2705 |\n| [anyscale](https://docs.litellm.ai/docs/providers/anyscale)  | \u2705 | \u2705 | \u2705 | \u2705 |\n\n[**read the docs**](https://docs.litellm.ai/docs/)\n\n## contributing\nto contribute: clone the repo locally -> make a change -> submit a pr with the change. \n\nhere's how to modify the repo locally: \nstep 1: clone the repo \n```\ngit clone https://github.com/berriai/litellm.git\n```\n\nstep 2: navigate into the project, and install dependencies: \n```\ncd litellm\npoetry install\n```\n\nstep 3: test your change:\n```\ncd litellm/tests # pwd: documents/litellm/litellm/tests\npytest .\n```\n\nstep 4: submit a pr with your changes! \ud83d\ude80\n- push your fork to your github repo \n- submit a pr from there \n\n# support / talk with founders\n- [schedule demo \ud83d\udc4b](https://calendly.com/d/4mp-gd3-k5k/berriai-1-1-onboarding-litellm-hosted-version)\n- [community discord \ud83d\udcad](https://discord.gg/wupm9drgdw)\n- our numbers \ud83d\udcde +1 (770) 8783-106 / \u202d+1 (412) 618-6238\u202c\n- our emails \u2709\ufe0f ishaan@berri.ai / krrish@berri.ai\n\n# why did we build this \n- **need for simplicity**: our code started to get extremely complicated managing & translating calls between azure, openai and cohere.\n\n# contributors\n\n<!-- all-contributors-list:start - do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- all-contributors-list:end -->\n\n<a href=\"https://github.com/berriai/litellm/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=berriai/litellm\" />\n</a>\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "litellm",
  "package_url": "https://pypi.org/project/litellm/",
  "project_url": "https://pypi.org/project/litellm/",
  "project_urls": null,
  "release_url": "https://pypi.org/project/litellm/1.15.7/",
  "requires_dist": [
    "aiohttp",
    "appdirs (>=1.4.4,<2.0.0)",
    "backoff ; extra == \"proxy\"",
    "certifi (>=2023.7.22,<2024.0.0)",
    "click",
    "fastapi (>=0.104.1,<0.105.0) ; extra == \"proxy\"",
    "importlib-metadata (>=6.8.0)",
    "jinja2 (>=3.1.2,<4.0.0)",
    "openai (>=1.0.0)",
    "python-dotenv (>=0.2.0)",
    "rq ; extra == \"proxy\"",
    "tiktoken (>=0.4.0)",
    "tokenizers",
    "uvicorn (>=0.24.0.post1,<0.25.0) ; extra == \"proxy\""
  ],
  "requires_python": ">=3.8,<4.0",
  "summary": "library to easily interface with llm api providers",
  "version": "1.15.7",
  "releases": [],
  "developers": [
    "berriai"
  ],
  "kwds": "litellm_getting_started openai proxy simple_proxy openai_api_key",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_litellm",
  "homepage": "",
  "release_count": 478,
  "dependency_ids": [
    "pypi_aiohttp",
    "pypi_appdirs",
    "pypi_backoff",
    "pypi_certifi",
    "pypi_click",
    "pypi_fastapi",
    "pypi_importlib_metadata",
    "pypi_jinja2",
    "pypi_openai",
    "pypi_python_dotenv",
    "pypi_rq",
    "pypi_tiktoken",
    "pypi_tokenizers",
    "pypi_uvicorn"
  ]
}