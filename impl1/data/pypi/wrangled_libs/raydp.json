{
  "classifiers": [
    "license :: osi approved :: apache software license",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8"
  ],
  "description": "# [raydp]()\nraydp provides simple apis for running spark\u00a0on\u00a0[ray](https://github.com/ray-project/ray) and integrating spark with ai libraries, making it simple to build distributed data and ai pipeline in a single python program.\n\n# introduction \n\n## problem statement\n\na large-scale ai workflow usually involves multiple systems, for example spark for data processing and pytorch or tensorflow for distributed training. a common setup is to use two separate clusters and stitch together multiple programs using glue code or a workflow orchestrator such as airflow or kubeflow. however, in many cases this adds costs in terms of system efficiency and operations. the setup overhead of the workflow tasks adds latency. data exchange among frameworks has to rely on external storage system which also adds latency. on operation side, managing two separate clusters introduces additional cost. writing the pipeline using workflow orchestrator usually is also more complex than writing a single python program. \n\n\n\n## solution with ray and raydp\nto solve the above challenges, more and more companies have adopted ray as a single substrate for data processing, model training, serving and more. ray makes it simple to build the data and ai pipeline in a single python program and scale from laptop to a cluster seamlessly. ray has built a rich ecosystem by providing high quality libraries and integrating with other popular ones. \n\nspark as a popular big data framework plays an important role in data and ai pipelines. raydp brings spark to the ray ecosystem by supporting running spark on top of ray. by using raydp, you can easily write pyspark code together with other ray libraries in the same python program which improves productivity and expressivity. raydp makes it simple to build distributed end-to-end data analytics and ai pipeline. raydp supports exchanging data between spark and other frameworks using ray's in-memory object to provide best performance.\n\n\n## who will use raydp\n* ml infrastructure team can build a modern ml platform on top of ray, utilize raydp to run spark on ray and unify with other ai components.\n* data scientists can use raydp to write pyspark code together with other ai libraries, scale from laptop to cloud seamlessly.\n* data engineers can use raydp to run on-demand spark job in cloud without a need to setup a spark cluster manually. the ray cluster launcher helps to start a ray cluster in cloud and raydp allows you to run spark in that cluster with auto scaling.\n\n## presentations\n* data + ai summit 2021: [build large-scale data analytics and ai pipeline using raydp](https://www.youtube.com/watch?v=b4ixqtxx2cs)\n* ray summit 2021: [raydp: build large-scale end-to-end data analytics and ai pipelines using spark and ray](https://www.youtube.com/watch?v=elsrr1geqg4)\n\n\n# architecture\nraydp provides simple apis for running spark on ray and apis for converting a spark dataframe to a ray dataset which can be consumed by xgboost, ray train, horovod on ray, etc. raydp also provides high level scikit-learn style estimator apis for distributed training with pytorch or tensorflow.\n\nraydp supports ray as a spark resource manager and runs spark executors in ray actors. the communication between spark executors still uses spark's internal protocol. \n\n![image](https://user-images.githubusercontent.com/9278199/199454034-5a87fb0b-069a-47dd-97ba-58e08edd4bb9.png)\n\n\n# quick start\n\n## installation\n\nyou can install latest raydp using pip. raydp requires ray and pyspark. please also make sure java is installed and java_home is set properly.\n\n```shell\npip install raydp\n```\n\nor you can install raydp nightly build:\n\n```shell\npip install --pre raydp\n```\n\nnotice: formerly used `raydp-nightly` will no longer be updated.\n\nif you'd like to build and install the latest master, use the following command:\n\n```shell\n./build.sh\npip install dist/raydp*.whl\n```\n\n## spark on ray\n\nraydp provides an api for starting a spark job on ray. to create a spark session, call the `raydp.init_spark` api. after that, you can use any spark api as you want. for example:\n\n```python\nimport ray\nimport raydp\n\n# connect to ray cluster\nray.init(address='auto')\n\n# create a spark cluster with specified resource requirements\nspark = raydp.init_spark(app_name='raydp example',\n                         num_executors=2,\n                         executor_cores=2,\n                         executor_memory='4gb')\n\n# normal data processesing with spark\ndf = spark.createdataframe([('look',), ('spark',), ('tutorial',), ('spark',), ('look', ), ('python', )], ['word'])\ndf.show()\nword_count = df.groupby('word').count()\nword_count.show()\n\n# stop the spark cluster\nraydp.stop_spark()\n```\n\nspark features such as dynamic resource allocation, spark-submit script, etc are also supported. please refer to [spark on ray](./doc/spark_on_ray.md) for more details.\n\n\n## spark + ai pipeline on ray\n\nraydp provides apis for converting a spark dataframe to a ray dataset which can be consumed by xgboost, ray train, horovod on ray, etc. raydp also provides high level scikit-learn style estimator apis for distributed training with pytorch or tensorflow. to get started with end-to-end spark + ai pipeline, the easiest way is to run the following tutorials on google collab. more examples are also available in the `examples` folder.\n* [spark + ray train tutorial on google collab](https://colab.research.google.com/github/oap-project/raydp/blob/master/tutorials/raytrain_example.ipynb)\n* [spark + torchestimator tutorial on google collab](https://colab.research.google.com/github/oap-project/raydp/blob/master/tutorials/pytorch_example.ipynb)\n\n\n***spark dataframe & ray dataset conversion***\n\nyou can use `ray.data.from_spark` and `ds.to_spark` to convert between spark dataframe and ray dataset.\n```python\nimport ray\nimport raydp\n\nray.init()\nspark = raydp.init_spark(app_name=\"raydp example\",\n                         num_executors=2,\n                         executor_cores=2,\n                         executor_memory=\"4gb\")\n\n# spark dataframe to ray dataset\ndf1 = spark.range(0, 1000)\nds1 = ray.data.from_spark(df1)\n\n# ray dataset to spark dataframe\nds2 = ray.data.from_items([{\"id\": i} for i in range(1000)])\ndf2 = ds2.to_spark(spark)\n```\n\nray dataset converted from spark dataframe this way will be no longer accessible after `raydp.stop_spark()`. if you want to access the data after spark is shutdown, please use `raydp.stop_spark(cleanup_data=false)`. \n\nplease refer to [spark+xgboost on ray](./examples/xgboost_ray_nyctaxi.py) for a full example.\n\n***estimator api***\n\nthe estimator apis allow you to train a deep neural network directly on a spark dataframe, leveraging ray\u2019s ability to scale out across the cluster. the estimator apis are wrappers of ray train and hide the complexity of converting a spark dataframe to a pytorch/tensorflow dataset and distributing the training. raydp provides `raydp.torch.torchestimator` for pytorch and `raydp.tf.tfestimator` for tensorflow. the following is an example of using torchestimator.\n\n```python\nimport ray\nimport raydp\nfrom raydp.torch import torchestimator\n\nray.init(address=\"auto\")\nspark = raydp.init_spark(app_name=\"raydp example\",\n                         num_executors=2,\n                         executor_cores=2,\n                         executor_memory=\"4gb\")\n\n# spark dataframe code \ndf = spark.read.parquet(\u2026) \ntrain_df = df.withcolumn(\u2026)\n\n# pytorch code \nmodel = torch.nn.sequential(torch.nn.linear(2, 1)) \noptimizer = torch.optim.adam(model.parameters())\n\nestimator = torchestimator(model=model, optimizer=optimizer, ...) \nestimator.fit_on_spark(train_df)\n\nraydp.stop_spark()\n```\nplease refer to [nyc taxi pytorch estimator](./examples/pytorch_nyctaxi.py) and [nyc taxi tensorflow estimator](./examples/tensorflow_nyctaxi.py) for full examples.\n\n***fault tolerance***\n\nthe ray dataset converted from spark dataframe like above is not fault-tolerant. this is because we implement it using `ray.put` combined with spark `mappartitions`. objects created by `ray.put` is not recoverable in ray.\n\nraydp now supports converting data in a way such that the resulting ray dataset is fault-tolerant. this feature is currently *experimental*. here is how to use it:\n```python\nimport ray\nimport raydp\n\nray.init(address=\"auto\")\n# set fault_tolerance_mode to true to enable the feature\n# this will connect pyspark driver to ray cluster\nspark = raydp.init_spark(app_name=\"raydp example\",\n                         num_executors=2,\n                         executor_cores=2,\n                         executor_memory=\"4gb\",\n                         fault_tolerance_mode=true)\n# df should be large enough so that result will be put into plasma\ndf = spark.range(100000)\n# use this api instead of ray.data.from_spark\nds = raydp.spark.from_spark_recoverable(df)\n# ds is now fault-tolerant.\n```\nnotice that `from_spark_recoverable` will persist the converted dataframe. you can provide the storage level through keyword parameter `storage_level`. in addition, this feature is not available in ray client mode. if you need to use ray client, please wrap your application in a ray actor, as described in the ray client chapter.\n\n\n## getting involved\nto report bugs or request new features, please open a github issue.\n\n\n",
  "docs_url": null,
  "keywords": "raydp spark ray distributed data-processing",
  "license": "apache 2.0",
  "name": "raydp",
  "package_url": "https://pypi.org/project/raydp/",
  "project_url": "https://pypi.org/project/raydp/",
  "project_urls": {
    "Homepage": "https://github.com/oap-project/raydp"
  },
  "release_url": "https://pypi.org/project/raydp/1.6.0/",
  "requires_dist": [
    "numpy",
    "pandas (>=1.1.4)",
    "psutil",
    "pyarrow (>=4.0.1)",
    "ray (>=2.1.0)",
    "pyspark (<=3.3.2,>=3.1.1)",
    "netifaces",
    "protobuf (<=3.20.3,>3.19.5)"
  ],
  "requires_python": ">=3.6",
  "summary": "raydp: distributed data processing on ray",
  "version": "1.6.0",
  "releases": [],
  "developers": [
    "raydp-dev@googlegroups.com",
    "raydp_developers"
  ],
  "kwds": "spark_on_ray ray pyspark clusters raydp",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_raydp",
  "homepage": "https://github.com/oap-project/raydp",
  "release_count": 51,
  "dependency_ids": [
    "pypi_netifaces",
    "pypi_numpy",
    "pypi_pandas",
    "pypi_protobuf",
    "pypi_psutil",
    "pypi_pyarrow",
    "pypi_pyspark",
    "pypi_ray"
  ]
}