{
  "classifiers": [
    "development status :: 3 - alpha",
    "intended audience :: developers",
    "intended audience :: science/research",
    "license :: osi approved :: mit license",
    "operating system :: os independent",
    "programming language :: python :: 3",
    "topic :: multimedia",
    "topic :: multimedia :: sound/audio",
    "topic :: scientific/engineering",
    "topic :: scientific/engineering :: artificial intelligence"
  ],
  "description": "![torch-audiomentations](images/torch_audiomentations_logo.png)\n---\n\n![build status](https://img.shields.io/github/workflow/status/asteroid-team/torch-audiomentations/ci)\n[![code coverage](https://img.shields.io/codecov/c/github/asteroid-team/torch-audiomentations/master.svg)](https://codecov.io/gh/asteroid-team/torch-audiomentations)\n[![code style: black](https://img.shields.io/badge/code%20style-black-black.svg)](https://github.com/ambv/black)\n[![doi](https://zenodo.org/badge/doi/10.5281/zenodo.6778064.svg)](https://doi.org/10.5281/zenodo.6778064)\n\naudio data augmentation in pytorch. inspired by [audiomentations](https://github.com/iver56/audiomentations).\n\n* supports cpu and gpu (cuda) - speed is a priority\n* supports batches of multichannel (or mono) audio\n* transforms extend `nn.module`, so they can be integrated as a part of a pytorch neural network model\n* most transforms are differentiable\n* three modes: `per_batch`, `per_example` and `per_channel`\n* cross-platform compatibility\n* permissive mit license\n* aiming for high test coverage\n\n# setup\n\n![python version support](https://img.shields.io/pypi/pyversions/torch-audiomentations)\n[![pypi version](https://img.shields.io/pypi/v/torch-audiomentations.svg?style=flat)](https://pypi.org/project/torch-audiomentations/)\n[![number of downloads from pypi per month](https://img.shields.io/pypi/dm/torch-audiomentations.svg?style=flat)](https://pypi.org/project/torch-audiomentations/)\n\n`pip install torch-audiomentations`\n\n# usage example\n\n```python\nimport torch\nfrom torch_audiomentations import compose, gain, polarityinversion\n\n\n# initialize augmentation callable\napply_augmentation = compose(\n    transforms=[\n        gain(\n            min_gain_in_db=-15.0,\n            max_gain_in_db=5.0,\n            p=0.5,\n        ),\n        polarityinversion(p=0.5)\n    ]\n)\n\ntorch_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# make an example tensor with white noise.\n# this tensor represents 8 audio snippets with 2 channels (stereo) and 2 s of 16 khz audio.\naudio_samples = torch.rand(size=(8, 2, 32000), dtype=torch.float32, device=torch_device) - 0.5\n\n# apply augmentation. this varies the gain and polarity of (some of)\n# the audio snippets in the batch independently.\nperturbed_audio_samples = apply_augmentation(audio_samples, sample_rate=16000)\n```\n\n# contribute\n\ncontributors welcome! \n[join the asteroid's slack](https://join.slack.com/t/asteroid-dev/shared_invite/zt-cn9y85t3-qnhxkd1et7qoyzu1ji5bca)\nto start discussing about `torch-audiomentations` with us.\n\n# motivation: speed\n\nwe don't want data augmentation to be a bottleneck in model training speed. here is a\ncomparison of the time it takes to run 1d convolution:\n\n![convolve execution times](images/convolve_exec_time_plot.png)\n\n# current state\n\ntorch-audiomentations is in an early development stage, so the apis are subject to change.\n\n# waveform transforms\n\nevery transform has `mode`, `p`, and `p_mode` -- the parameters that decide how the augmentation is performed.\n- `mode` decides how the randomization of the augmentation is grouped and applied.\n- `p` decides the on/off probability of applying the augmentation.   \n- `p_mode` decides how the on/off of the augmentation is applied.\n\nthis visualization shows how different combinations of `mode` and `p_mode` would perform an augmentation.    \n\n![explanation of mode, p and p_mode](images/visual_explanation_mode_etc.png)\n    \n\n## addbackgroundnoise\n\n_added in v0.5.0_\n\nadd background noise to the input audio.\n\n## addcolorednoise\n\n_added in v0.7.0_\n\nadd colored noise to the input audio.\n\n## applyimpulseresponse\n\n_added in v0.5.0_\n\nconvolve the given audio with impulse responses.\n\n## bandpassfilter\n\n_added in v0.9.0_\n\napply band-pass filtering to the input audio.\n\n## bandstopfilter\n\n_added in v0.10.0_\n\napply band-stop filtering to the input audio. also known as notch filter.\n\n## gain\n\n_added in v0.1.0_\n\nmultiply the audio by a random amplitude factor to reduce or increase the volume. this\ntechnique can help a model become somewhat invariant to the overall gain of the input audio.\n\nwarning: this transform can return samples outside the [-1, 1] range, which may lead to\nclipping or wrap distortion, depending on what you do with the audio in a later stage.\nsee also https://en.wikipedia.org/wiki/clipping_(audio)#digital_clipping\n\n## highpassfilter\n\n_added in v0.8.0_\n\napply high-pass filtering to the input audio.\n\n## identity\n\n_added in v0.11.0_\n\nthis transform returns the input unchanged. it can be used for simplifying the code\nin cases where data augmentation should be disabled.\n\n## lowpassfilter\n\n_added in v0.8.0_\n\napply low-pass filtering to the input audio.\n\n## peaknormalization\n\n_added in v0.2.0_\n\napply a constant amount of gain, so that highest signal level present in each audio snippet\nin the batch becomes 0 dbfs, i.e. the loudest level allowed if all samples must be between\n-1 and 1.\n\nthis transform has an alternative mode (apply_to=\"only_too_loud_sounds\") where it only\napplies to audio snippets that have extreme values outside the [-1, 1] range. this is useful\nfor avoiding digital clipping in audio that is too loud, while leaving other audio\nuntouched.\n\n## pitchshift\n\n_added in v0.9.0_\n\npitch-shift sounds up or down without changing the tempo.\n\n## polarityinversion\n\n_added in v0.1.0_\n\nflip the audio samples upside-down, reversing their polarity. in other words, multiply the\nwaveform by -1, so negative values become positive, and vice versa. the result will sound\nthe same compared to the original when played back in isolation. however, when mixed with\nother audio sources, the result may be different. this waveform inversion technique\nis sometimes used for audio cancellation or obtaining the difference between two waveforms.\nhowever, in the context of audio data augmentation, this transform can be useful when\ntraining phase-aware machine learning models.\n\n## shift\n\n_added in v0.5.0_\n\nshift the audio forwards or backwards, with or without rollover\n\n## shufflechannels\n\n_added in v0.6.0_\n\ngiven multichannel audio input (e.g. stereo), shuffle the channels, e.g. so left can become right and vice versa.\nthis transform can help combat positional bias in machine learning models that input multichannel waveforms.\n\nif the input audio is mono, this transform does nothing except emit a warning.\n\n## timeinversion\n\n_added in v0.10.0_\n\nreverse (invert) the audio along the time axis similar to random flip of\nan image in the visual domain. this can be relevant in the context of audio\nclassification. it was successfully applied in the paper\n[audioclip: extending clip to image, text and audio](https://arxiv.org/pdf/2106.13043.pdf)\n\n\n# changelog\n\n## unreleased\n\n### added\n\n* add new transforms: `mix`, `padding`, `randomcrop` and `spliceout`\n\n## [v0.11.0] - 2022-06-29\n\n### added\n\n* add new transform: `identity`\n* add api for processing of targets alongside inputs. some transforms experimentally\n  support this feature already.\n\n### changed\n\n* add `objectdict` output type as alternative to `torch.tensor`. this alternative is opt-in for\n  now (for backwards-compatibility), but note that the old output type (`torch.tensor`) is\n  deprecated and support for it will be removed in a future version.\n* allow specifying a file path, a folder path, a list of files or a list of folders to\n  `addbackgroundnoise` and `applyimpulseresponse`\n* require newer version of `torch-pitch-shift` to ensure support for torchaudio 0.11 in `pitchshift`\n\n### fixed\n\n* fix a bug where `bandpassfilter` didn't work on gpu\n\n## [v0.10.1] - 2022-03-24\n\n### added\n\n* add support for min snr == max snr in `addbackgroundnoise`\n* add support for librosa 0.9.0\n\n### fixed\n\n* fix a bug where loaded audio snippets were sometimes resampled to an incompatible\n length in `addbackgroundnoise`\n\n## [v0.10.0] - 2022-02-11\n\n### added\n\n* implement `oneof` and `someof` for applying one or more of a given set of transforms\n* implement new transforms: `bandstopfilter` and `timeinversion`\n\n### changed\n\n* put `ir_paths` in transform_parameters in `applyimpulseresponse` so it is possible\n to inspect what impulse responses were used. this also gives `freeze_parameters()`\n the expected behavior.\n\n### fixed\n\n* fix a bug where the actual bandwidth was twice as large as expected in\n `bandpassfilter`. the default values have been updated accordingly.\n if you were previously specifying `min_bandwidth_fraction` and/or `max_bandwidth_fraction`,\n you now need to double those numbers to get the same behavior as before.\n\n## [v0.9.1] - 2021-12-20\n\n### added\n\n* officially mark python>=3.9 as supported\n\n## [v0.9.0] - 2021-10-11\n\n### added\n\n* add parameter `compensate_for_propagation_delay` in `applyimpulseresponse`\n* implement `bandpassfilter`\n* implement `pitchshift`\n\n### removed\n\n* support for torchaudio<=0.6 has been removed\n\n## [v0.8.0] - 2021-06-15\n\n### added\n\n* implement `highpassfilter` and `lowpassfilter`\n\n### deprecated\n\n* support for torchaudio<=0.6 is deprecated and will be removed in the future\n\n### removed\n\n* support for pytorch<=1.6 has been removed\n\n## [v0.7.0] - 2021-04-16\n\n### added\n\n* implement `addcolorednoise`\n\n### deprecated\n\n* support for pytorch<=1.6 is deprecated and will be removed in the future\n\n## [v0.6.0] - 2021-02-22\n\n### added\n\n* implement `shufflechannels`\n\n## [v0.5.1] - 2020-12-18\n\n### fixed\n\n* fix a bug where `addbackgroundnoise` did not work on cuda\n* fix a bug where symlinked audio files/folders were not found when looking for audio files\n* use torch.fft.rfft instead of the torch.rfft (deprecated in pytorch 1.7) when possible. as a\nbonus, the change also improves performance in `applyimpulseresponse`.\n\n## [v0.5.0] - 2020-12-08\n\n### added\n\n* release `addbackgroundnoise` and `applyimpulseresponse`\n* implement `shift`\n\n### changed\n\n* make `sample_rate` optional. allow specifying `sample_rate` in `__init__` instead of `forward`. this means torchaudio transforms can be used in `compose` now.\n\n### removed\n\n* remove support for 1-dimensional and 2-dimensional audio tensors. only 3-dimensional audio\n tensors are supported now.\n\n### fixed\n\n* fix a bug where one could not use the `parameters` method of the `nn.module` subclass\n* fix a bug where files with uppercase filename extension were not found\n\n## [v0.4.0] - 2020-11-10\n\n### added\n\n* implement `compose` for applying multiple transforms\n* implement utility functions `from_dict` and `from_yaml` for loading data augmentation\nconfigurations from dict, json or yaml\n* officially support differentiability in most transforms\n\n## [v0.3.0] - 2020-10-27\n\n### added\n\n* add support for alternative modes `per_batch` and `per_channel`\n\n### changed\n\n* transforms now return the input unchanged when they are in eval mode\n\n## [v0.2.0] - 2020-10-19\n\n### added\n\n* implement `peaknormalization`\n* expose `convolve` in the api\n\n### changed\n\n* simplify api for using cuda tensors. the device is now inferred from the input tensor.\n\n## [v0.1.0] - 2020-10-12\n\n### added\n\n* initial release with `gain` and `polarityinversion`\n\n# development\n\n## setup\n\na gpu-enabled development environment for torch-audiomentations can be created with conda:\n\n* `conda env create`\n\n## run tests\n\n`pytest`\n\n## conventions\n\n* format python code with [black](https://github.com/psf/black)\n* use [google-style docstrings](https://google.github.io/styleguide/pyguide.html#381-docstrings)\n* use explicit relative imports, not absolute imports\n\n# acknowledgements\n\nthe development of torch-audiomentations is kindly backed by [nomono](https://nomono.co/).\n\nthanks to [all contributors](https://github.com/asteroid-team/torch-audiomentations/graphs/contributors) who help improving torch-audiomentations.\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "torch-audiomentations",
  "package_url": "https://pypi.org/project/torch-audiomentations/",
  "project_url": "https://pypi.org/project/torch-audiomentations/",
  "project_urls": {
    "Homepage": "https://github.com/asteroid-team/torch-audiomentations"
  },
  "release_url": "https://pypi.org/project/torch-audiomentations/0.11.0/",
  "requires_dist": [
    "julius (<0.3,>=0.2.3)",
    "librosa (>=0.6.0)",
    "torch (>=1.7.0)",
    "torchaudio (>=0.7.0)",
    "torch-pitch-shift (>=1.2.2)",
    "PyYAML ; extra == 'extras'"
  ],
  "requires_python": ">=3.6",
  "summary": "a pytorch library for audio data augmentation. inspired by audiomentations. useful for deep learning.",
  "version": "0.11.0",
  "releases": [],
  "developers": [
    "iver_jordal"
  ],
  "kwds": "torch_audiomentations torch_audiomentations_logo torchaudio audiomentations audioclip",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_torch_audiomentations",
  "homepage": "https://github.com/asteroid-team/torch-audiomentations",
  "release_count": 14,
  "dependency_ids": [
    "pypi_julius",
    "pypi_librosa",
    "pypi_pyyaml",
    "pypi_torch",
    "pypi_torch_pitch_shift",
    "pypi_torchaudio"
  ]
}