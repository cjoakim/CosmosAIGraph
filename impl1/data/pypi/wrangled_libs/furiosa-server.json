{
  "classifiers": [
    "development status :: 4 - beta",
    "environment :: console",
    "environment :: web environment",
    "intended audience :: developers",
    "intended audience :: system administrators",
    "license :: osi approved :: apache software license",
    "programming language :: python :: 3",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "# furiosa server (alpha)\n\nfuriosa model server is a framework for serving tflite/onnx models through a rest api, using furiosa npus.\n\nfuriosa model server api supoorts a rest and grpc interface, compliant with [kfserving's v2\ndataplane](https://github.com/kubeflow/kfserving/blob/master/docs/predict-api/v2/required_api.md)\nspecification and [triton's model repository](https://github.com/triton-inference-server/server/blob/main/docs/model_repository.md) specification.\n\n## features\n- [x] http rest api support\n- [x] multi-model support\n- [x] grpc support\n- [x] openapi specification support\n- [ ] compiler configuration support\n- [ ] input tensor adapter in python (e.g., converting jpeg, png image files to tensors)\n- [ ] authentication support\n\n# building for development\n**requirements**\n\n* python >= 3.8\n* libnpu\n* libnux\n\ninstall apt depdencies.\n```sh\nsudo apt install furiosa-libnpu-sim # or furiosa-libnpu-xrt if you have furiosa h/w\nsudo apt install furiosa-libnux\n```\n\ninstall python dependencies.\n\n```sh\npip install -e .\n```\n\nto build from source, generate required files from [grpc tools](https://grpc.io/docs/languages/python/quickstart/) and [datamodel-codegen](https://koxudaxi.github.io/datamodel-code-generator/). each step is needed to generate a grpc stub and [pydantic](https://pydantic-docs.helpmanual.io/datamodel_code_generator/) data class.\n\n**generate grpc api**\n```sh\nfor api in \"predict\" \"model_repository\"\ndo\n    python -m grpc_tools.protoc \\\n        -i\"./proto\" \\\n        --python_out=\"./furiosa/server/api/grpc/generated\" \\\n        --grpc_python_out=\"./furiosa/server/api/grpc/generated\" \\\n        --mypy_out=\"./furiosa/server/api/grpc/generated\" \\\n        \"./proto/$api.proto\"\ndone\n```\n\n**generate pydantic data type**\n```sh\nfor api in \"predict\" \"model_repository\"\ndo\n    datamodel-codegen \\\n    --output-model-type pydantic_v2.basemodel \\\n    --input \"./openapi/$api.yaml\" \\\n    --output \"./furiosa/server/types/$api.py\"\ndone\n```\n\n**testing**\n\n```sh\nfuriosa-server$ pytest --capture=no\n============================================================ test session starts =============================================================\nplatform linux -- python 3.9.6, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/ys/furiosa/cloud/furiosa-server\nplugins: asyncio-0.15.1\ncollected 10 items\n\ntests/test_server.py [1/6] \ud83d\udd0d   compiling from tflite to dfg\ndone in 0.006840319s\n[2/6] \ud83d\udd0d   compiling from dfg to ldfg\n\u25aa\u25aa\u25aa\u25aa\u25aa [1/3] splitting graph...done in 47.121174s\n\u25aa\u25aa\u25aa\u25aa\u25aa [2/3] lowering...done in 19.422386s\n\u25aa\u25aa\u25aa\u25aa\u25aa [3/3] precalculating operators...done in 0.27680752s\ndone in 66.82971s\n[3/6] \ud83d\udd0d   compiling from ldfg to cdfg\ndone in 0.000951856s\n[4/6] \ud83d\udd0d   compiling from cdfg to gir\ndone in 0.028555028s\n[5/6] \ud83d\udd0d   compiling from gir to lir\ndone in 0.01069514s\n[6/6] \ud83d\udd0d   compiling from lir to enf\ndone in 0.05054388s\n\u2728  finished in 66.980644s\n.........[1/6] \ud83d\udd0d   compiling from tflite to dfg\ndone in 0.005259287s\n[2/6] \ud83d\udd0d   compiling from dfg to ldfg\n\u25aa\u25aa\u25aa\u25aa\u25aa [1/3] splitting graph...done in 0.003461787s\n\u25aa\u25aa\u25aa\u25aa\u25aa [2/3] lowering...done in 7.16337s\n\u25aa\u25aa\u25aa\u25aa\u25aa [3/3] precalculating operators...done in 0.31032142s\ndone in 7.4865813s\n[3/6] \ud83d\udd0d   compiling from ldfg to cdfg\ndone in 0.001077142s\n[4/6] \ud83d\udd0d   compiling from cdfg to gir\ndone in 0.02613672s\n[5/6] \ud83d\udd0d   compiling from gir to lir\ndone in 0.012959026s\n[6/6] \ud83d\udd0d   compiling from lir to enf\ndone in 0.058442567s\n\u2728  finished in 7.642151s\n.\n\n======================================================= 10 passed in 76.17s (0:01:16) ========================================================\n\n```\n\n# installing\n**requirements**\n\n* python >= 3.8\n\ndownload the latest release from https://github.com/furiosa-ai/furiosa-server/releases.\n```\npip install furiosa_server-x.y.z-cp38-cp38-linux_x86_64.whl\n```\n\n## usages\n\n### command lines\n`furiosa-server` command has the following options.\nto print out the command line usage, you can run `furiosa-server --help` option.\n```sh\nusage: furiosa-server [options]\n\n  start serving models from furiosaai model server\n\noptions\n  --log-level                 [error|info|warn|debug|trace]    [default: loglevel.info]\n  --model-name                text                             model name [default: none]\n  --model-path                text                             path to a model file (tflite, onnx are supported)\n                                                               [default: none]\n  --model-version             text                             model version [default: default]\n  --host                      text                             ipv4 address to bind [default: 0.0.0.0]\n  --http-port                 integer                          http port to bind [default: 8080]\n  --model-config              filename                         path to a model config file [default: none]\n  --server-config             filename                         path to a server config file [default: none]\n  --install-completion        [bash|zsh|fish|powershell|pwsh]  install completion for the specified shell. [default: none]\n  --show-completion           [bash|zsh|fish|powershell|pwsh]  show completion for the specified shell, to copy it or\n                                                               customize the installation.\n                                                               [default: none]\n  --help                                                       show this message and exit.\n```\n\n### serving a single model\nto serve a single model, you will need only a couple of command line options.\nthe following is an example to start a model server with the specific model name and the model image file:\n\n```sh\n$ furiosa-server --model-name mnist --model-path samples/data/mnist_inception_v3_quant.tflite --model-version 1\nfind native library /home/ys/furiosa/compiler/npu-tools/target/x86_64-unknown-linux-gnu/release/\ninfo:furiosa.runtime._api.v1:loaded dynamic library /home/ys/furiosa/compiler/npu-tools/target/x86_64-unknown-linux-gnu/release/libnux.so (0.4.0-dev bdde0748b)\n[1/6] \ud83d\udd0d   compiling from tflite to dfg\ndone in 0.04330982s\n[2/6] \ud83d\udd0d   compiling from dfg to ldfg\n\u25aa\u25aa\u25aa\u25aa\u25aa [1/3] splitting graph...done in 38.590836s\n\u25aa\u25aa\u25aa\u25aa\u25aa [2/3] lowering...done in 26.293291s\n\u25aa\u25aa\u25aa\u25aa\u25aa [3/3] precalculating operators...done in 2.2485964s\ndone in 67.13952s\n[3/6] \ud83d\udd0d   compiling from ldfg to cdfg\ndone in 0.000349475s\n[4/6] \ud83d\udd0d   compiling from cdfg to gir\ndone in 0.07628228s\n[5/6] \ud83d\udd0d   compiling from gir to lir\ndone in 0.002296112s\n[6/6] \ud83d\udd0d   compiling from lir to enf\ndone in 0.06429358s\n\u2728  finished in 67.361084s\ninfo:     started server process [235857]\ninfo:     waiting for application startup.\ninfo:     application startup complete.\ninfo:     uvicorn running on http://0.0.0.0:8080 (press ctrl+c to quit)\n```\n\nyou can find and try apis via openapi: http://localhost:8080/docs#/\n\n### serving multiple models\nto serve multiple models, you need to write a model configuration file.\nthe following is an example file located at `samples/model_config_example.yml`:\n```yml\nmodel_config_list:\n  - name: mnist\n    path: \"samples/data/mnistnet_uint8_quant.tflite\"\n    version: 1\n    npu_device: npu0pe0\n    compiler_config:\n      keep_unsignedness: true\n      split_unit: 0\n  - name: ssd\n    path: \"samples/data/tflite/ssd512_mobilenet_v2_bdd_int_without_reshape.tflite\"\n    version: 1\n    npu_device: npu1\n```\n\nin a model configuration file, you can also specify a npu device name dedicated to serve a certain model,\nand a list of compiler configs as shown in the above example.\n\nif you write a model config file,\nyou can launch the model server with a specific model config file as follow:\n```sh\n$ furiosa-server --model-config samples/model_config_example.yaml\nfind native library /home/ys/furiosa/compiler/npu-tools/target/x86_64-unknown-linux-gnu/release/\ninfo:furiosa.runtime._api.v1:loaded dynamic library /home/ys/furiosa/compiler/npu-tools/target/x86_64-unknown-linux-gnu/release/libnux.so (0.4.0-dev bdde0748b)\n[1/6] \ud83d\udd0d   compiling from tflite to dfg\ndone in 0.000510351s\n[2/6] \ud83d\udd0d   compiling from dfg to ldfg\n\u25aa\u25aa\u25aa\u25aa\u25aa [1/3] splitting graph...done in 1.5242418s\n\u25aa\u25aa\u25aa\u25aa\u25aa [2/3] lowering...done in 0.41843188s\n\u25aa\u25aa\u25aa\u25aa\u25aa [3/3] precalculating operators...done in 0.00754911s\ndone in 1.9507353s\n[3/6] \ud83d\udd0d   compiling from ldfg to cdfg\ndone in 0.000069757s\n[4/6] \ud83d\udd0d   compiling from cdfg to gir\ndone in 0.005654631s\n[5/6] \ud83d\udd0d   compiling from gir to lir\ndone in 0.000294499s\n[6/6] \ud83d\udd0d   compiling from lir to enf\ndone in 0.003239762s\n\u2728  finished in 1.9631383s\n[1/6] \ud83d\udd0d   compiling from tflite to dfg\ndone in 0.010595854s\n[2/6] \ud83d\udd0d   compiling from dfg to ldfg\n\u25aa\u25aa\u25aa\u25aa\u25aa [1/3] splitting graph...done in 36.860104s\n\u25aa\u25aa\u25aa\u25aa\u25aa [2/3] lowering...done in 8.500944s\n\u25aa\u25aa\u25aa\u25aa\u25aa [3/3] precalculating operators...done in 1.2011535s\ndone in 46.564877s\n[3/6] \ud83d\udd0d   compiling from ldfg to cdfg\ndone in 0.000303809s\n[4/6] \ud83d\udd0d   compiling from cdfg to gir\ndone in 0.07403221s\n[5/6] \ud83d\udd0d   compiling from gir to lir\ndone in 0.001839668s\n[6/6] \ud83d\udd0d   compiling from lir to enf\ndone in 0.07413657s\n\u2728  finished in 46.771423s\ninfo:     started server process [245257]\ninfo:     waiting for application startup.\ninfo:     application startup complete.\ninfo:     uvicorn running on http://0.0.0.0:8080 (press ctrl+c to quit)\n```\n\n### submitting inference tasks\n\nthe following is an example of a request message. if you want to know the schema of the request message,\nplease refer to openapi specication.\n```\n{\"inputs\": [{\"name\": \"mnist\", \"datatype\": \"int32\", \"shape\": [1, 1, 28, 28], \"data\": ...}]}\n\n```\n\nyou can test one of mnist model with the following command:\n```\n$ curl -x post -h \"content-type: application/json\" \\\n-d \"@samples/mnist_input_sample_01.json\" \\\nhttp://localhost:8080/v2/models/mnist/versions/1/infer\n\n{\"model_name\":\"mnist\",\"model_version\":\"1\",\"id\":null,\"parameters\":null,\"outputs\":[{\"name\":\"0\",\"shape\":[1,10],\"datatype\":\"uint8\",\"parameters\":null,\"data\":[0,0,0,1,0,255,0,0,0,0]}]}%\n```\n\nalso, you can run a simple python code to request the prediction task to the furiosa-server. here is an example:\n```python\nimport requests\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nurl = 'http://localhost:8080/v2/models/mnist/versions/1/infer'\ndata = np.ndarray(x_train[0:1], dtype=np.uint8).flatten().tolist()\ntensor = {\n        'datatype': 'int32',\n        'shape': [1,1,28,28],\n        'data': data\n}\nrequest = {'inputs': [tensor] }\nresponse = requests.post(url, json=request)\nprint(response.json())\n```\n",
  "docs_url": null,
  "keywords": "",
  "license": "",
  "name": "furiosa-server",
  "package_url": "https://pypi.org/project/furiosa-server/",
  "project_url": "https://pypi.org/project/furiosa-server/",
  "project_urls": {
    "Bug Tracker": "https://github.com/furiosa-ai/furiosa-sdk/issues",
    "Documentation": "https://furiosa-ai.github.io/docs",
    "Home": "https://furiosa.ai",
    "Source Code": "https://github.com/furiosa-ai/furiosa-sdk"
  },
  "release_url": "https://pypi.org/project/furiosa-server/0.10.1/",
  "requires_dist": [
    "furiosa-runtime == 0.10.*",
    "fastapi",
    "httpx",
    "pydantic-settings",
    "grpcio-tools",
    "protobuf",
    "toml",
    "typer",
    "uvicorn",
    "openvino ; extra == \"openvino\"",
    "datamodel-code-generator ; extra == \"test\"",
    "mypy ; extra == \"test\"",
    "mypy-protobuf ; extra == \"test\"",
    "mypy-extensions ; extra == \"test\"",
    "pytest >=2.7.3 ; extra == \"test\"",
    "pytest ; extra == \"test\"",
    "pytest-asyncio ~= 0.17.2 ; extra == \"test\"",
    "pytest-cov ; extra == \"test\"",
    "requests ; extra == \"test\"",
    "ruff ; extra == \"test\"",
    "types-PyYAML ; extra == \"test\"",
    "types-protobuf ; extra == \"test\""
  ],
  "requires_python": "~=3.8",
  "summary": "furiosaai model server interacting furiosa npu.",
  "version": "0.10.1",
  "releases": [],
  "developers": [
    "pkg@furiosa.ai"
  ],
  "kwds": "furiosa_server triton furiosa grpc mnist_inception_v3_quant",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_furiosa_server",
  "homepage": "",
  "release_count": 18,
  "dependency_ids": [
    "pypi_datamodel_code_generator",
    "pypi_fastapi",
    "pypi_furiosa_runtime",
    "pypi_grpcio_tools",
    "pypi_httpx",
    "pypi_mypy",
    "pypi_mypy_extensions",
    "pypi_mypy_protobuf",
    "pypi_openvino",
    "pypi_protobuf",
    "pypi_pydantic_settings",
    "pypi_pytest",
    "pypi_pytest_asyncio",
    "pypi_pytest_cov",
    "pypi_requests",
    "pypi_ruff",
    "pypi_toml",
    "pypi_typer",
    "pypi_types_protobuf",
    "pypi_types_pyyaml",
    "pypi_uvicorn"
  ]
}