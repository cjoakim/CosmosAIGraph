{
  "classifiers": [
    "development status :: 5 - production/stable",
    "environment :: console",
    "intended audience :: developers",
    "intended audience :: education",
    "intended audience :: science/research",
    "license :: osi approved :: apache software license",
    "operating system :: os independent",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.11",
    "programming language :: python :: 3.7",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "programming language :: unix shell",
    "topic :: database :: database engines/servers",
    "topic :: internet :: www/http :: indexing/search",
    "topic :: multimedia :: video",
    "topic :: scientific/engineering",
    "topic :: scientific/engineering :: artificial intelligence",
    "topic :: scientific/engineering :: image recognition",
    "topic :: scientific/engineering :: mathematics",
    "topic :: software development",
    "topic :: software development :: libraries",
    "topic :: software development :: libraries :: python modules"
  ],
  "description": "<p align=\"center\">\n<a href=\"https://docs.jina.ai\"><img src=\"https://github.com/jina-ai/jina/blob/master/docs/_static/logo-light.svg?raw=true\" alt=\"jina logo: build multimodal ai services via cloud native technologies \u00b7 model serving \u00b7 generative ai \u00b7 neural search \u00b7 cloud native\" width=\"150px\"></a>\n</p>\n\n<p align=\"center\">\n<b>build multimodal ai applications with cloud-native technologies</b>\n</p>\n\n<p align=center>\n<a href=\"https://pypi.org/project/jina/\"><img alt=\"pypi\" src=\"https://img.shields.io/pypi/v/jina?label=release&style=flat-square\"></a>\n<!--<a href=\"https://codecov.io/gh/jina-ai/jina\"><img alt=\"codecov branch\" src=\"https://img.shields.io/codecov/c/github/jina-ai/jina/master?&logo=codecov&logocolor=white&style=flat-square\"></a>-->\n<a href=\"https://discord.jina.ai\"><img src=\"https://img.shields.io/discord/1106542220112302130?logo=discord&logocolor=white&style=flat-square\"></a>\n<a href=\"https://pypistats.org/packages/jina\"><img alt=\"pypi - downloads from official pypistats\" src=\"https://img.shields.io/pypi/dm/jina?style=flat-square\"></a>\n<a href=\"https://github.com/jina-ai/jina/actions/workflows/cd.yml\"><img alt=\"github cd status\" src=\"https://github.com/jina-ai/jina/actions/workflows/cd.yml/badge.svg\"></a>\n</p>\n\n<!-- start jina-description -->\n\njina lets you build multimodal [**ai services**](#build-ai-models) and [**pipelines**](#build-a-pipeline) that communicate via grpc, http and websockets, then scale them up and deploy to production. you can focus on your logic and algorithms, without worrying about the infrastructure complexity.\n\n![](./.github/images/build-deploy.png)\n\njina provides a smooth pythonic experience for serving ml models transitioning from local deployment to advanced orchestration frameworks like docker-compose, kubernetes, or jina ai cloud. jina makes advanced solution engineering and cloud-native technologies accessible to every developer.\n\n- build and serve models for any [data type](https://docs.docarray.org/data_types/first_steps/) and any mainstream [deep learning framework](https://docarray.org/docarray/how_to/multimodal_training_and_serving/).\n- design high-performance services, with [easy scaling](https://docs.jina.ai/concepts/orchestration/scale-out/), duplex client-server streaming, batching, [dynamic batching](https://docs.jina.ai/concepts/serving/executor/dynamic-batching/), async/non-blocking data processing and any [protocol](https://docs.jina.ai/concepts/serving/gateway/#set-protocol-in-python).\n- serve [llm models while streaming their output](https://github.com/jina-ai/jina#streaming-for-llms).\n- docker container integration via [executor hub](https://cloud.jina.ai), opentelemetry/prometheus observability.\n- streamlined cpu/gpu hosting via [jina ai cloud](https://cloud.jina.ai).\n- deploy to your own cloud or system with our [kubernetes](https://docs.jina.ai/cloud-nativeness/k8s/) and [docker compose](https://docs.jina.ai/cloud-nativeness/docker-compose/) integration.\n\n<details>\n    <summary><strong>wait, how is jina different from fastapi?</strong></summary>\njina's value proposition may seem quite similar to that of fastapi. however, there are several fundamental differences:\n\n **data structure and communication protocols**\n  - fastapi communication relies on pydantic and jina relies on [docarray](https://github.com/docarray/docarray) allowing jina to support multiple protocols\n  to expose its services. the support for grpc protocol is specially useful for data intensive applications as for embedding services\n  where the embeddings and tensors can be more efficiently serialized.\n\n **advanced orchestration and scaling capabilities**\n  - jina allows you to easily containerize and orchestrate your services and models, providing concurrency and scalability.\n  - jina lets you deploy applications formed from multiple microservices that can be containerized and scaled independently.\n\n **journey to the cloud**\n  - jina provides a smooth transition from local development (using [docarray](https://github.com/docarray/docarray)) to local serving using [deployment](https://docs.jina.ai/concepts/orchestration/deployment/) and [flow](https://docs.jina.ai/concepts/orchestration/flow/)\n  to having production-ready services by using kubernetes capacity to orchestrate the lifetime of containers.\n  - by using [jina ai cloud](https://cloud.jina.ai) you have access to scalable and serverless deployments of your applications in one command.\n</details>\n\n<!-- end jina-description -->\n\n## [documentation](https://docs.jina.ai)\n\n## install \n\n```bash\npip install jina\n```\n\nfind more install options on [apple silicon](https://docs.jina.ai/get-started/install/apple-silicon-m1-m2/)/[windows](https://docs.jina.ai/get-started/install/windows/).\n\n## get started\n\n### basic concepts\n\njina has three fundamental layers:\n\n- data layer: [**basedoc**](https://docarray.docs.org/) and [**doclist**](https://docarray.docs.org/) (from [docarray](https://github.com/docarray/docarray)) are the input/output formats in jina.\n- serving layer: an [**executor**](https://docs.jina.ai/concepts/serving/executor/) is a python class that transforms and processes documents. by simply wrapping your models into an executor, you allow them to be served and scaled by jina. [**gateway**](https://docs.jina.ai/concepts/serving/gateway/) is the service making sure connecting all executors inside a flow.\n- orchestration layer:  [**deployment**](https://docs.jina.ai/concepts/orchestration/deployment) serves a single executor, while a [**flow**](https://docs.jina.ai/concepts/orchestration/flow/) serves executors chained into a pipeline.\n\n\n[the full glossary is explained here](https://docs.jina.ai/concepts/preliminaries/#).\n\n### serve ai models\n<!-- start build-ai-services -->\n\nlet's build a fast, reliable and scalable grpc-based ai service. in jina we call this an **[executor](https://docs.jina.ai/concepts/serving/executor/)**. our simple executor will wrap the [stablelm](https://huggingface.co/stabilityai/stablelm-base-alpha-3b) llm from stability ai. we'll then use a **deployment** to serve it.\n\n![](./.github/images/deployment-diagram.png)\n\n> **note**\n> a deployment serves just one executor. to combine multiple executors into a pipeline and serve that, use a [flow](#build-a-pipeline).\n\nlet's implement the service's logic:\n\n<table>\n<tr>\n<th><code>executor.py</code></th> \n<tr>\n<td>\n\n```python\nfrom jina import executor, requests\nfrom docarray import doclist, basedoc\n\nfrom transformers import pipeline\n\n\nclass prompt(basedoc):\n    text: str\n\n\nclass generation(basedoc):\n    prompt: str\n    text: str\n\n\nclass stablelm(executor):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.generator = pipeline(\n            'text-generation', model='stabilityai/stablelm-base-alpha-3b'\n        )\n\n    @requests\n    def generate(self, docs: doclist[prompt], **kwargs) -> doclist[generation]:\n        generations = doclist[generation]()\n        prompts = docs.text\n        llm_outputs = self.generator(prompts)\n        for prompt, output in zip(prompts, llm_outputs):\n            generations.append(generation(prompt=prompt, text=output))\n        return generations\n```\n\n</td>\n</tr>\n</table>\n\nthen we deploy it with either the python api or yaml:\n<div class=\"table-wrapper\">\n<table>\n<tr>\n<th> python api: <code>deployment.py</code> </th> \n<th> yaml: <code>deployment.yml</code> </th>\n</tr>\n<tr>\n<td>\n\n```python\nfrom jina import deployment\nfrom executor import stablelm\n\ndep = deployment(uses=stablelm, timeout_ready=-1, port=12345)\n\nwith dep:\n    dep.block()\n```\n\n</td>\n<td>\n\n```yaml\njtype: deployment\nwith:\n  uses: stablelm\n  py_modules:\n    - executor.py\n  timeout_ready: -1\n  port: 12345\n```\n\nand run the yaml deployment with the cli: `jina deployment --uses deployment.yml`\n\n</td>\n</tr>\n</table>\n</div>\n\nuse [jina client](https://docs.jina.ai/concepts/client/) to make requests to the service:\n\n```python\nfrom jina import client\nfrom docarray import doclist, basedoc\n\n\nclass prompt(basedoc):\n    text: str\n\n\nclass generation(basedoc):\n    prompt: str\n    text: str\n\n\nprompt = prompt(\n    text='suggest an interesting image generation prompt for a mona lisa variant'\n)\n\nclient = client(port=12345)  # use port from output above\nresponse = client.post(on='/', inputs=[prompt], return_type=doclist[generation])\n\nprint(response[0].text)\n```\n\n```text\na steampunk version of the mona lisa, incorporating mechanical gears, brass elements, and victorian era clothing details\n```\n\n<!-- end build-ai-services -->\n\n> **note**\n> in a notebook, you can't use `deployment.block()` and then make requests to the client. please refer to the colab link above for reproducible jupyter notebook code snippets.\n\n### build a pipeline\n\n<!-- start build-pipelines -->\n\nsometimes you want to chain microservices together into a pipeline. that's where a [flow](https://docs.jina.ai/concepts/orchestration/flow/) comes in.\n\na flow is a [dag](https://en.wikipedia.org/wiki/directed_acyclic_graph) pipeline, composed of a set of steps, it orchestrates a set of [executors](https://docs.jina.ai/concepts/serving/executor/) and a [gateway](https://docs.jina.ai/concepts/serving/gateway/) to offer an end-to-end service.\n\n> **note**\n> if you just want to serve a single executor, you can use a [deployment](#build-ai--ml-services).\n\nfor instance, let's combine [our stablelm language model](#build-ai--ml-services) with a stable diffusion image generation model. chaining these services together into a [flow](https://docs.jina.ai/concepts/orchestration/flow/) will give us a service that will generate images based on a prompt generated by the llm.\n\n\n<table>\n<tr>\n<th><code>text_to_image.py</code></th> \n<tr>\n<td>\n\n```python\nimport numpy as np\nfrom jina import executor, requests\nfrom docarray import basedoc, doclist\nfrom docarray.documents import imagedoc\n\n\nclass generation(basedoc):\n    prompt: str\n    text: str\n\n\nclass texttoimage(executor):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        from diffusers import stablediffusionpipeline\n        import torch\n\n        self.pipe = stablediffusionpipeline.from_pretrained(\n            \"compvis/stable-diffusion-v1-4\", torch_dtype=torch.float16\n        ).to(\"cuda\")\n\n    @requests\n    def generate_image(self, docs: doclist[generation], **kwargs) -> doclist[imagedoc]:\n        result = doclist[imagedoc]()\n        images = self.pipe(\n            docs.text\n        ).images  # image here is in [pil format](https://pillow.readthedocs.io/en/stable/)\n        result.tensor = np.array(images)\n        return result\n```\n\n</td>\n</tr>\n</table>\n\n\n![](./.github/images/flow-diagram.png)\n\nbuild the flow with either python or yaml:\n\n<div class=\"table-wrapper\">\n<table>\n<tr>\n<th> python api: <code>flow.py</code> </th> \n<th> yaml: <code>flow.yml</code> </th>\n</tr>\n<tr>\n<td>\n\n```python\nfrom jina import flow\nfrom executor import stablelm\nfrom text_to_image import texttoimage\n\nflow = (\n    flow(port=12345)\n    .add(uses=stablelm, timeout_ready=-1)\n    .add(uses=texttoimage, timeout_ready=-1)\n)\n\nwith flow:\n    flow.block()\n```\n\n</td>\n<td>\n\n```yaml\njtype: flow\nwith:\n    port: 12345\nexecutors:\n  - uses: stablelm\n    timeout_ready: -1\n    py_modules:\n      - executor.py\n  - uses: texttoimage\n    timeout_ready: -1\n    py_modules:\n      - text_to_image.py\n```\n\nthen run the yaml flow with the cli: `jina flow --uses flow.yml`\n\n</td>\n</tr>\n</table>\n</div>\n\nthen, use [jina client](https://docs.jina.ai/concepts/client/) to make requests to the flow:\n\n```python\nfrom jina import client\nfrom docarray import doclist, basedoc\nfrom docarray.documents import imagedoc\n\n\nclass prompt(basedoc):\n    text: str\n\n\nprompt = prompt(\n    text='suggest an interesting image generation prompt for a mona lisa variant'\n)\n\nclient = client(port=12345)  # use port from output above\nresponse = client.post(on='/', inputs=[prompt], return_type=doclist[imagedoc])\n\nresponse[0].display()\n```\n\n![](./.github/images/mona-lisa.png)\n\n<!-- end build-pipelines -->\n\n### easy scalability and concurrency\n\nwhy not just use standard python to build that service and pipeline? jina accelerates time to market of your application by making it more scalable and cloud-native. jina also handles the infrastructure complexity in production and other day-2 operations so that you can focus on the data application itself.\n\nincrease your application's throughput with scalability features out of the box, like [replicas](https://docs.jina.ai/concepts/orchestration/scale-out/#replicate-executors), [shards](https://docs.jina.ai/concepts/orchestration/scale-out/#customize-polling-behaviors) and [dynamic batching](https://docs.jina.ai/concepts/serving/executor/dynamic-batching/).\n\nlet's scale a stable diffusion executor deployment with replicas and dynamic batching:\n\n![](./.github/images/scaled-deployment.png)\n\n* create two replicas, with [a gpu assigned for each](https://docs.jina.ai/concepts/orchestration/scale-out/#replicate-on-multiple-gpus).\n* enable dynamic batching to process incoming parallel requests together with the same model inference.\n\n\n<div class=\"table-wrapper\">\n<table>\n<tr>\n<th> normal deployment </th> \n<th> scaled deployment </th>\n</tr>\n<tr>\n<td>\n\n```yaml\njtype: deployment\nwith:\n  uses: texttoimage\n  timeout_ready: -1\n  py_modules:\n    - text_to_image.py\n```\n\n</td>\n<td>\n\n```yaml\njtype: deployment\nwith:\n  uses: texttoimage\n  timeout_ready: -1\n  py_modules:\n    - text_to_image.py\n  env:\n   cuda_visible_devices: rr\n  replicas: 2\n  uses_dynamic_batching: # configure dynamic batching\n    /default:\n      preferred_batch_size: 10\n      timeout: 200\n```\n\n</td>\n</tr>\n</table>\n</div>\n\nassuming your machine has two gpus, using the scaled deployment yaml will give better throughput compared to the normal deployment.\n\nthese features apply to both [deployment yaml](https://docs.jina.ai/concepts/orchestration/yaml-spec/#example-yaml) and [flow yaml](https://docs.jina.ai/concepts/orchestration/yaml-spec/#example-yaml). thanks to the yaml syntax, you can inject deployment configurations regardless of executor code.\n\n## deploy to the cloud\n\n### containerize your executor\n\nin order to deploy your solutions to the cloud, you need to containerize your services. jina provides the [executor hub](https://docs.jina.ai/concepts/serving/executor/hub/create-hub-executor/), the perfect tool\nto streamline this process taking a lot of the troubles with you. it also lets you share these executors publicly or privately.\n\nyou just need to structure your executor in a folder:\n\n```shell script\ntexttoimage/\n\u251c\u2500\u2500 executor.py\n\u251c\u2500\u2500 config.yml\n\u251c\u2500\u2500 requirements.txt\n```\n<div class=\"table-wrapper\">\n<table>\n<tr>\n<th> <code>config.yml</code> </th>\n<th> <code>requirements.txt</code> </th>\n</tr>\n<tr>\n<td>\n\n```yaml\njtype: texttoimage\npy_modules:\n  - executor.py\nmetas:\n  name: texttoimage\n  description: text to image generation executor based on stablediffusion\n  url:\n  keywords: []\n```\n\n</td>\n<td>\n\n```requirements.txt\ndiffusers\naccelerate\ntransformers\n```\n\n</td>\n</tr>\n</table>\n</div>\n\n\nthen push the executor to the hub by doing: `jina hub push texttoimage`.\n\nthis will give you a url that you can use in your `deployment` and `flow` to use the pushed executors containers.\n\n\n```yaml\njtype: flow\nwith:\n    port: 12345\nexecutors:\n  - uses: jinai+docker://<user-id>/stablelm\n  - uses: jinai+docker://<user-id>/texttoimage\n```\n\n\n### get on the fast lane to cloud-native\n\nusing kubernetes with jina is easy:\n\n```bash\njina export kubernetes flow.yml ./my-k8s\nkubectl apply -r -f my-k8s\n```\n\nand so is docker compose:\n\n```bash\njina export docker-compose flow.yml docker-compose.yml\ndocker-compose up\n```\n\n> **note**\n> you can also export deployment yaml to [kubernetes](https://docs.jina.ai/concepts/serving/executor/serve/#serve-via-kubernetes) and [docker compose](https://docs.jina.ai/concepts/serving/executor/serve/#serve-via-docker-compose).\n\nthat's not all. we also support [opentelemetry, prometheus, and jaeger](https://docs.jina.ai/cloud-nativeness/opentelemetry/).\n\nwhat cloud-native technology is still challenging to you? [tell us](https://github.com/jina-ai/jina/issues) and we'll handle the complexity and make it easy for you.\n\n### deploy to jcloud\n\nyou can also deploy a flow to jcloud, where you can easily enjoy autoscaling, monitoring and more with a single command. \n\nfirst, turn the `flow.yml` file into a [jcloud-compatible yaml](https://docs.jina.ai/yaml-spec/) by specifying resource requirements and using containerized hub executors.\n\nthen, use `jina cloud deploy` command to deploy to the cloud:\n\n```shell\nwget https://raw.githubusercontent.com/jina-ai/jina/master/.github/getting-started/jcloud-flow.yml\njina cloud deploy jcloud-flow.yml\n```\n\n> **warning**\n>\n> make sure to delete/clean up the flow once you are done with this tutorial to save resources and credits.\n\nread more about [deploying flows to jcloud](https://docs.jina.ai/concepts/jcloud/#deploy).\n\n### streaming for llms\n<!-- start llm-streaming-intro -->\nlarge language models can power a wide range of applications from chatbots to assistants and intelligent systems.\nhowever, these models can be heavy and slow and your users want systems that are both intelligent _and_ fast!\n\nlarge language models work by turning your questions into tokens and then generating new token one at a \ntime until it decides that generation should stop.\nthis means you want to **stream** the output tokens generated by a large language model to the client. \nin this tutorial, we will discuss how to achieve this with streaming endpoints in jina.\n<!-- end llm-streaming-intro -->\n\n#### service schemas\n<!-- start llm-streaming-schemas -->\nthe first step is to define the streaming service schemas, as you would do in any other service framework.\nthe input to the service is the prompt and the maximum number of tokens to generate, while the output is simply the \ntoken id:\n```python\nfrom docarray import basedoc\n\n\nclass promptdocument(basedoc):\n    prompt: str\n    max_tokens: int\n\n\nclass modeloutputdocument(basedoc):\n    token_id: int\n    generated_text: str\n```\n<!-- end llm-streaming-schemas -->\n\n#### service initialization\n<!-- start llm-streaming-init -->\nour service depends on a large language model. as an example, we will use the `gpt2` model. this is how you would load \nsuch a model in your executor\n```python\nfrom jina import executor, requests\nfrom transformers import gpt2tokenizer, gpt2lmheadmodel\nimport torch\n\ntokenizer = gpt2tokenizer.from_pretrained('gpt2')\n\n\nclass tokenstreamingexecutor(executor):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.model = gpt2lmheadmodel.from_pretrained('gpt2')\n```\n<!-- end llm-streaming-init -->\n\n\n#### implement the streaming endpoint\n<!-- start llm-streaming-endpoint -->\nour streaming endpoint accepts a `promptdocument` as input and streams `modeloutputdocument`s. to stream a document back to \nthe client, use the `yield` keyword in the endpoint implementation. therefore, we use the model to generate \nup to `max_tokens` tokens and yield them until the generation stops: \n```python\nclass tokenstreamingexecutor(executor):\n    ...\n\n    @requests(on='/stream')\n    async def task(self, doc: promptdocument, **kwargs) -> modeloutputdocument:\n        input = tokenizer(doc.prompt, return_tensors='pt')\n        input_len = input['input_ids'].shape[1]\n        for _ in range(doc.max_tokens):\n            output = self.model.generate(**input, max_new_tokens=1)\n            if output[0][-1] == tokenizer.eos_token_id:\n                break\n            yield modeloutputdocument(\n                token_id=output[0][-1],\n                generated_text=tokenizer.decode(\n                    output[0][input_len:], skip_special_tokens=true\n                ),\n            )\n            input = {\n                'input_ids': output,\n                'attention_mask': torch.ones(1, len(output[0])),\n            }\n```\n\nlearn more about [streaming endpoints](https://docs.jina.ai/concepts/serving/executor/add-endpoints/#streaming-endpoints) from the `executor` documentation.\n<!-- end llm-streaming-endpoint -->\n\n\n#### serve and send requests\n<!-- start llm-streaming-serve -->\n\nthe final step is to serve the executor and send requests using the client.\nto serve the executor using grpc:\n```python\nfrom jina import deployment\n\nwith deployment(uses=tokenstreamingexecutor, port=12345, protocol='grpc') as dep:\n    dep.block()\n```\n\nto send requests from a client:\n```python\nimport asyncio\nfrom jina import client\n\n\nasync def main():\n    client = client(port=12345, protocol='grpc', asyncio=true)\n    async for doc in client.stream_doc(\n        on='/stream',\n        inputs=promptdocument(prompt='what is the capital of france ?', max_tokens=10),\n        return_type=modeloutputdocument,\n    ):\n        print(doc.generated_text)\n\n\nasyncio.run(main())\n```\n\n```text\nthe\nthe capital\nthe capital of\nthe capital of france\nthe capital of france is\nthe capital of france is paris\nthe capital of france is paris.\n```\n\n<!-- end llm-streaming-serve -->\n\n<!-- start support-pitch -->\n\n## support\n\n- join our [discord community](https://discord.jina.ai) and chat with other community members about ideas.\n- subscribe to the latest video tutorials on our [youtube channel](https://youtube.com/c/jina-ai)\n\n## join us\n\njina is backed by [jina ai](https://jina.ai) and licensed under [apache-2.0](./license).\n\n<!-- end support-pitch -->\n",
  "docs_url": null,
  "keywords": "jina cloud-native cross-modal multimodal neural-search query search index elastic neural-network encoding embedding serving docker container image video audio deep-learning mlops",
  "license": "apache 2.0",
  "name": "jina",
  "package_url": "https://pypi.org/project/jina/",
  "project_url": "https://pypi.org/project/jina/",
  "project_urls": {
    "Documentation": "https://docs.jina.ai",
    "Download": "https://github.com/jina-ai/jina/tags",
    "Homepage": "https://github.com/jina-ai/jina/",
    "Source": "https://github.com/jina-ai/jina/",
    "Tracker": "https://github.com/jina-ai/jina/issues"
  },
  "release_url": "https://pypi.org/project/jina/3.23.2/",
  "requires_dist": [
    "numpy",
    "protobuf >=3.19.0",
    "urllib3 <2.0.0,>=1.25.9",
    "fastapi >=0.76.0",
    "jcloud >=0.0.35",
    "opentelemetry-sdk <1.20.0,>=1.14.0",
    "opentelemetry-api >=1.12.0",
    "jina-hubble-sdk >=0.30.4",
    "docker",
    "pyyaml >=5.3.1",
    "python-multipart",
    "aiofiles",
    "grpcio <=1.57.0,>=1.46.0",
    "opentelemetry-exporter-otlp >=1.12.0",
    "pathspec",
    "pydantic <2.0.0",
    "opentelemetry-instrumentation-fastapi >=0.33b0",
    "prometheus-client >=0.12.0",
    "aiohttp",
    "packaging >=20.0",
    "opentelemetry-instrumentation-grpc >=0.35b0",
    "grpcio-reflection <=1.57.0,>=1.46.0",
    "opentelemetry-exporter-prometheus >=0.33b0",
    "docarray >=0.16.4",
    "uvicorn[standard] <=0.23.1",
    "opentelemetry-exporter-otlp-proto-grpc >=1.13.0",
    "opentelemetry-instrumentation-aiohttp-client >=0.33b0",
    "grpcio-health-checking <=1.57.0,>=1.46.0",
    "websockets",
    "requests",
    "filelock",
    "uvloop ; platform_system != \"Windows\"",
    "Pillow ; extra == 'pillow'",
    "aiofiles ; extra == 'aiofiles'",
    "aiohttp ; extra == 'aiohttp'",
    "numpy ; extra == 'all'",
    "requests-mock ; extra == 'all'",
    "protobuf >=3.19.0 ; extra == 'all'",
    "strawberry-graphql >=0.96.0 ; extra == 'all'",
    "urllib3 <2.0.0,>=1.25.9 ; extra == 'all'",
    "fastapi >=0.76.0 ; extra == 'all'",
    "jcloud >=0.0.35 ; extra == 'all'",
    "opentelemetry-sdk <1.20.0,>=1.14.0 ; extra == 'all'",
    "scipy >=1.6.1 ; extra == 'all'",
    "opentelemetry-api >=1.12.0 ; extra == 'all'",
    "jina-hubble-sdk >=0.30.4 ; extra == 'all'",
    "docker ; extra == 'all'",
    "sgqlc ; extra == 'all'",
    "pyyaml >=5.3.1 ; extra == 'all'",
    "python-multipart ; extra == 'all'",
    "prometheus-api-client >=0.5.1 ; extra == 'all'",
    "coverage ==6.2 ; extra == 'all'",
    "pytest-reraise ; extra == 'all'",
    "pytest-timeout ; extra == 'all'",
    "aiofiles ; extra == 'all'",
    "grpcio <=1.57.0,>=1.46.0 ; extra == 'all'",
    "pytest-repeat ; extra == 'all'",
    "pytest-custom-exit-code ; extra == 'all'",
    "opentelemetry-exporter-otlp >=1.12.0 ; extra == 'all'",
    "pathspec ; extra == 'all'",
    "pydantic <2.0.0 ; extra == 'all'",
    "opentelemetry-instrumentation-fastapi >=0.33b0 ; extra == 'all'",
    "torch ; extra == 'all'",
    "prometheus-client >=0.12.0 ; extra == 'all'",
    "flaky ; extra == 'all'",
    "aiohttp ; extra == 'all'",
    "packaging >=20.0 ; extra == 'all'",
    "portforward <0.4.3,>=0.2.4 ; extra == 'all'",
    "tensorflow >=2.0 ; extra == 'all'",
    "pytest-asyncio <0.23.0 ; extra == 'all'",
    "opentelemetry-instrumentation-grpc >=0.35b0 ; extra == 'all'",
    "Pillow ; extra == 'all'",
    "grpcio-reflection <=1.57.0,>=1.46.0 ; extra == 'all'",
    "black ==22.3.0 ; extra == 'all'",
    "psutil ; extra == 'all'",
    "opentelemetry-exporter-prometheus >=0.33b0 ; extra == 'all'",
    "jsonschema ; extra == 'all'",
    "mock ; extra == 'all'",
    "docarray >=0.16.4 ; extra == 'all'",
    "uvicorn[standard] <=0.23.1 ; extra == 'all'",
    "kubernetes >=18.20.0 ; extra == 'all'",
    "filelock ; extra == 'all'",
    "pytest-kind ==22.11.1 ; extra == 'all'",
    "opentelemetry-exporter-otlp-proto-grpc >=1.13.0 ; extra == 'all'",
    "grpcio-health-checking <=1.57.0,>=1.46.0 ; extra == 'all'",
    "opentelemetry-instrumentation-aiohttp-client >=0.33b0 ; extra == 'all'",
    "websockets ; extra == 'all'",
    "opentelemetry-test-utils >=0.33b0 ; extra == 'all'",
    "pytest-lazy-fixture ; extra == 'all'",
    "pytest-mock ; extra == 'all'",
    "pytest ; extra == 'all'",
    "pytest-cov ==3.0.0 ; extra == 'all'",
    "requests ; extra == 'all'",
    "bs4 ; extra == 'all'",
    "watchfiles >=0.18.0 ; extra == 'all'",
    "uvloop ; (platform_system != \"Windows\") and extra == 'all'",
    "black ==22.3.0 ; extra == 'black'",
    "bs4 ; extra == 'bs4'",
    "sgqlc ; extra == 'cicd'",
    "strawberry-graphql >=0.96.0 ; extra == 'cicd'",
    "jsonschema ; extra == 'cicd'",
    "torch ; extra == 'cicd'",
    "bs4 ; extra == 'cicd'",
    "portforward <0.4.3,>=0.2.4 ; extra == 'cicd'",
    "tensorflow >=2.0 ; extra == 'cicd'",
    "numpy ; extra == 'core'",
    "grpcio-health-checking <=1.57.0,>=1.46.0 ; extra == 'core'",
    "jina-hubble-sdk >=0.30.4 ; extra == 'core'",
    "protobuf >=3.19.0 ; extra == 'core'",
    "pyyaml >=5.3.1 ; extra == 'core'",
    "opentelemetry-instrumentation-grpc >=0.35b0 ; extra == 'core'",
    "grpcio-reflection <=1.57.0,>=1.46.0 ; extra == 'core'",
    "urllib3 <2.0.0,>=1.25.9 ; extra == 'core'",
    "grpcio <=1.57.0,>=1.46.0 ; extra == 'core'",
    "docarray >=0.16.4 ; extra == 'core'",
    "pydantic <2.0.0 ; extra == 'core'",
    "jcloud >=0.0.35 ; extra == 'core'",
    "packaging >=20.0 ; extra == 'core'",
    "opentelemetry-api >=1.12.0 ; extra == 'core'",
    "coverage ==6.2 ; extra == 'coverage'",
    "strawberry-graphql >=0.96.0 ; extra == 'devel'",
    "fastapi >=0.76.0 ; extra == 'devel'",
    "opentelemetry-sdk <1.20.0,>=1.14.0 ; extra == 'devel'",
    "docker ; extra == 'devel'",
    "sgqlc ; extra == 'devel'",
    "python-multipart ; extra == 'devel'",
    "aiofiles ; extra == 'devel'",
    "opentelemetry-exporter-otlp >=1.12.0 ; extra == 'devel'",
    "pathspec ; extra == 'devel'",
    "opentelemetry-instrumentation-fastapi >=0.33b0 ; extra == 'devel'",
    "prometheus-client >=0.12.0 ; extra == 'devel'",
    "aiohttp ; extra == 'devel'",
    "opentelemetry-exporter-prometheus >=0.33b0 ; extra == 'devel'",
    "uvicorn[standard] <=0.23.1 ; extra == 'devel'",
    "opentelemetry-exporter-otlp-proto-grpc >=1.13.0 ; extra == 'devel'",
    "opentelemetry-instrumentation-aiohttp-client >=0.33b0 ; extra == 'devel'",
    "websockets ; extra == 'devel'",
    "requests ; extra == 'devel'",
    "filelock ; extra == 'devel'",
    "watchfiles >=0.18.0 ; extra == 'devel'",
    "uvloop ; (platform_system != \"Windows\") and extra == 'devel'",
    "docarray >=0.16.4 ; extra == 'docarray'",
    "docker ; extra == 'docker'",
    "fastapi >=0.76.0 ; extra == 'fastapi'",
    "filelock ; extra == 'filelock'",
    "flaky ; extra == 'flaky'",
    "grpcio <=1.57.0,>=1.46.0 ; extra == 'grpcio'",
    "grpcio-health-checking <=1.57.0,>=1.46.0 ; extra == 'grpcio-health-checking'",
    "grpcio-reflection <=1.57.0,>=1.46.0 ; extra == 'grpcio-reflection'",
    "jcloud >=0.0.35 ; extra == 'jcloud'",
    "jina-hubble-sdk >=0.30.4 ; extra == 'jina-hubble-sdk'",
    "jsonschema ; extra == 'jsonschema'",
    "kubernetes >=18.20.0 ; extra == 'kubernetes'",
    "mock ; extra == 'mock'",
    "numpy ; extra == 'numpy'",
    "opentelemetry-api >=1.12.0 ; extra == 'opentelemetry-api'",
    "opentelemetry-exporter-otlp >=1.12.0 ; extra == 'opentelemetry-exporter-otlp'",
    "opentelemetry-exporter-otlp-proto-grpc >=1.13.0 ; extra == 'opentelemetry-exporter-otlp-proto-grpc'",
    "opentelemetry-exporter-prometheus >=0.33b0 ; extra == 'opentelemetry-exporter-prometheus'",
    "opentelemetry-instrumentation-aiohttp-client >=0.33b0 ; extra == 'opentelemetry-instrumentation-aiohttp-client'",
    "opentelemetry-instrumentation-fastapi >=0.33b0 ; extra == 'opentelemetry-instrumentation-fastapi'",
    "opentelemetry-instrumentation-grpc >=0.35b0 ; extra == 'opentelemetry-instrumentation-grpc'",
    "opentelemetry-sdk <1.20.0,>=1.14.0 ; extra == 'opentelemetry-sdk'",
    "opentelemetry-test-utils >=0.33b0 ; extra == 'opentelemetry-test-utils'",
    "packaging >=20.0 ; extra == 'packaging'",
    "pathspec ; extra == 'pathspec'",
    "opentelemetry-exporter-otlp-proto-grpc >=1.13.0 ; extra == 'perf'",
    "opentelemetry-instrumentation-aiohttp-client >=0.33b0 ; extra == 'perf'",
    "uvloop ; extra == 'perf'",
    "opentelemetry-exporter-prometheus >=0.33b0 ; extra == 'perf'",
    "opentelemetry-exporter-otlp >=1.12.0 ; extra == 'perf'",
    "opentelemetry-instrumentation-fastapi >=0.33b0 ; extra == 'perf'",
    "prometheus-client >=0.12.0 ; extra == 'perf'",
    "opentelemetry-sdk <1.20.0,>=1.14.0 ; extra == 'perf'",
    "portforward <0.4.3,>=0.2.4 ; extra == 'portforward'",
    "prometheus-api-client >=0.5.1 ; extra == 'prometheus-api-client'",
    "prometheus-client >=0.12.0 ; extra == 'prometheus_client'",
    "protobuf >=3.19.0 ; extra == 'protobuf'",
    "psutil ; extra == 'psutil'",
    "pydantic <2.0.0 ; extra == 'pydantic'",
    "pytest ; extra == 'pytest'",
    "pytest-asyncio <0.23.0 ; extra == 'pytest-asyncio'",
    "pytest-cov ==3.0.0 ; extra == 'pytest-cov'",
    "pytest-custom-exit-code ; extra == 'pytest-custom_exit_code'",
    "pytest-kind ==22.11.1 ; extra == 'pytest-kind'",
    "pytest-lazy-fixture ; extra == 'pytest-lazy-fixture'",
    "pytest-mock ; extra == 'pytest-mock'",
    "pytest-repeat ; extra == 'pytest-repeat'",
    "pytest-reraise ; extra == 'pytest-reraise'",
    "pytest-timeout ; extra == 'pytest-timeout'",
    "python-multipart ; extra == 'python-multipart'",
    "pyyaml >=5.3.1 ; extra == 'pyyaml'",
    "requests ; extra == 'requests'",
    "requests-mock ; extra == 'requests-mock'",
    "scipy >=1.6.1 ; extra == 'scipy'",
    "sgqlc ; extra == 'sgqlc'",
    "opentelemetry-instrumentation-aiohttp-client >=0.33b0 ; extra == 'standard'",
    "docker ; extra == 'standard'",
    "websockets ; extra == 'standard'",
    "python-multipart ; extra == 'standard'",
    "uvloop ; extra == 'standard'",
    "requests ; extra == 'standard'",
    "aiofiles ; extra == 'standard'",
    "fastapi >=0.76.0 ; extra == 'standard'",
    "opentelemetry-exporter-prometheus >=0.33b0 ; extra == 'standard'",
    "opentelemetry-exporter-otlp >=1.12.0 ; extra == 'standard'",
    "pathspec ; extra == 'standard'",
    "uvicorn[standard] <=0.23.1 ; extra == 'standard'",
    "opentelemetry-instrumentation-fastapi >=0.33b0 ; extra == 'standard'",
    "prometheus-client >=0.12.0 ; extra == 'standard'",
    "opentelemetry-sdk <1.20.0,>=1.14.0 ; extra == 'standard'",
    "filelock ; extra == 'standard'",
    "aiohttp ; extra == 'standard'",
    "opentelemetry-exporter-otlp-proto-grpc >=1.13.0 ; extra == 'standrad'",
    "strawberry-graphql >=0.96.0 ; extra == 'strawberry-graphql'",
    "tensorflow >=2.0 ; extra == 'tensorflow'",
    "requests-mock ; extra == 'test'",
    "scipy >=1.6.1 ; extra == 'test'",
    "prometheus-api-client >=0.5.1 ; extra == 'test'",
    "coverage ==6.2 ; extra == 'test'",
    "pytest-reraise ; extra == 'test'",
    "pytest-timeout ; extra == 'test'",
    "pytest-repeat ; extra == 'test'",
    "pytest-custom-exit-code ; extra == 'test'",
    "flaky ; extra == 'test'",
    "pytest-asyncio <0.23.0 ; extra == 'test'",
    "Pillow ; extra == 'test'",
    "black ==22.3.0 ; extra == 'test'",
    "psutil ; extra == 'test'",
    "mock ; extra == 'test'",
    "kubernetes >=18.20.0 ; extra == 'test'",
    "pytest-kind ==22.11.1 ; extra == 'test'",
    "opentelemetry-test-utils >=0.33b0 ; extra == 'test'",
    "pytest-lazy-fixture ; extra == 'test'",
    "pytest-mock ; extra == 'test'",
    "pytest ; extra == 'test'",
    "pytest-cov ==3.0.0 ; extra == 'test'",
    "torch ; extra == 'torch'",
    "urllib3 <2.0.0,>=1.25.9 ; extra == 'urllib3'",
    "uvicorn[standard] <=0.23.1 ; extra == 'uvicorn_standard_'",
    "uvloop ; extra == 'uvloop'",
    "watchfiles >=0.18.0 ; extra == 'watchfiles'",
    "websockets ; extra == 'websockets'"
  ],
  "requires_python": "",
  "summary": "multimodal ai services & pipelines with cloud-native stack: grpc, kubernetes, docker, opentelemetry, prometheus, jaeger, etc.",
  "version": "3.23.2",
  "releases": [],
  "developers": [
    "hello@jina.ai",
    "jina_ai"
  ],
  "kwds": "jina multimodal multimodal_training_and_serving jinai cloud",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_jina",
  "homepage": "https://github.com/jina-ai/jina/",
  "release_count": 2398,
  "dependency_ids": [
    "pypi_aiofiles",
    "pypi_aiohttp",
    "pypi_black",
    "pypi_bs4",
    "pypi_coverage",
    "pypi_docarray",
    "pypi_docker",
    "pypi_fastapi",
    "pypi_filelock",
    "pypi_flaky",
    "pypi_grpcio",
    "pypi_grpcio_health_checking",
    "pypi_grpcio_reflection",
    "pypi_jcloud",
    "pypi_jina_hubble_sdk",
    "pypi_jsonschema",
    "pypi_kubernetes",
    "pypi_mock",
    "pypi_numpy",
    "pypi_opentelemetry_api",
    "pypi_opentelemetry_exporter_otlp",
    "pypi_opentelemetry_exporter_otlp_proto_grpc",
    "pypi_opentelemetry_exporter_prometheus",
    "pypi_opentelemetry_instrumentation_aiohttp_client",
    "pypi_opentelemetry_instrumentation_fastapi",
    "pypi_opentelemetry_instrumentation_grpc",
    "pypi_opentelemetry_sdk",
    "pypi_opentelemetry_test_utils",
    "pypi_packaging",
    "pypi_pathspec",
    "pypi_pillow",
    "pypi_portforward",
    "pypi_prometheus_api_client",
    "pypi_prometheus_client",
    "pypi_protobuf",
    "pypi_psutil",
    "pypi_pydantic",
    "pypi_pytest",
    "pypi_pytest_asyncio",
    "pypi_pytest_cov",
    "pypi_pytest_custom_exit_code",
    "pypi_pytest_kind",
    "pypi_pytest_lazy_fixture",
    "pypi_pytest_mock",
    "pypi_pytest_repeat",
    "pypi_pytest_reraise",
    "pypi_pytest_timeout",
    "pypi_python_multipart",
    "pypi_pyyaml",
    "pypi_requests",
    "pypi_requests_mock",
    "pypi_scipy",
    "pypi_sgqlc",
    "pypi_strawberry_graphql",
    "pypi_tensorflow",
    "pypi_torch",
    "pypi_urllib3",
    "pypi_uvicorn",
    "pypi_uvloop",
    "pypi_watchfiles",
    "pypi_websockets"
  ]
}