{
  "classifiers": [],
  "description": "# matrix compression library\n\nthis document describes an experimental api that facilitates matrix compression\nof a neural network's weight tensors. the api helps inject the necessary\ntensorflow operations into the training graph so the model can be compressed\nwhile it is being trained.\n\nfull documentation can be found\n[here](https://drive.google.com/file/d/1843anpkx_rznpuh9ameshgakmisvdpjy/view).\n\n## table of contents\n\n1.  [library overview](#library-overview)\n2.  [model creation](#model-creation)\n3.  [hyperparameters for compression](#hyperparameters)\n    -   [smoothed compression](#smoothed-compression)\n4.  [adding compression ops to the training graph](#adding-compression-ops)\n5.  [example](#example)\n\n### library overview <a name=\"library-overview\"></a>\n\n1.  **matrixcompressorinterface** - used to implement any matrix compression\n    algorithm in the method\n2.  **compressionopinterface** - used to create a tensorflow operator-like\n    object that injects any matrix compression method dynamically into a\n    tensorflow layer.\n3.  **applycompression** - convenience wrapper class that can be used directly\n    or extended for novel compression operator types; used to repeatedly invoke\n    the compression operator to different layers in a model.\n4.  **compressionwrapper** - wrapper module used to create the proper\n    applycompression implementation for the compression_option (method) of\n    choice.\n\n### model creation <a name=\"model-creation\"></a>\n\nthe first step involves creating an applycompression object, with the desired\ncompression parameters. this object then is used to compress the model weights\nand use these compressed weights during the forward execution of the graph.\nmatrices are compressed to the rank specified in the compression parameters,\nprovided at the start. to apply the compression, the weight tensor of the layer\nshould be wrapped with the compression object's 'apply_compression' method,\nprovided in\n[compression_op.py](https://github.com/google-research/google-research/tree/master/graph_compression/compression_lib/compression_op.py).for\nan example, see the [section below](#adding-compression-ops).\n\n### hyperparameters for compression <a name=\"hyperparameters\"></a>\n\nthe pruning library allows for specification of the following hyper parameters:\n\nhyperparameter         | type    | default           | description\n:--------------------- | :-----: | :---------------: | :----------\nname                   | string  | model_compression | name of the compression specification. used for adding summaries and ops under a common tensorflow name_scope.\nalpha_decrement_value  | float   | 0.01              | real number by which alpha is decremented at each update.\nbegin_compression_step | integer | 0                 | global step at which to begin compression.\nend_compression_step   | integer | -1                | global step at which to terminate compression. defaults to -1 implying compression continues till the training stops.\ncompression_frequency  | integer | 10                | intervals at which compression is applied and compression parameters updated.\ncompression_option     | integer | 0                 | indicates what type of factorization/compression to use (see the list below for the algorithm options).\nrank                   | integer | 100               | factorization rank (r), where if a = bc. see definition below of how rank (r) is used to compute final weights matrix dimensions.\nupdate_option          | integer | 0                 | indicates how update logic is being run: 0 - use tensorflow operations for updates; 1 - use python functions for updates.\nuse_tpu                | boolean | false             | **experimental flag** - training using tpus\n\n#### compression methods & algorithms (compression_option param)\n\n1.  low rank approximation\n2.  simhash\n3.  dictionary learning\n4.  kmeans quantization\n\n#### decomposed matrix dimensions\n\nthe hyperparameter rank (r) is used to compute the new ranks as such: (rank of\na) * (100 / r) + 1. for simhash compression, the value r provided should be the\nratio value you would like divided by 8 (i.e. 300 / 8 -> same as using r = 300\nin the equation above). this is because simhash compression represents values as\nbits (rather than bytes) therefore the true rank is the size of the array\ndivided by 8.\n\n#### computing compression ratio\n\nif the original weights were m-by-n and the compressed decomposition b\\*c is\n(m-by-k)\\*(k-by-n), then the compression ratio is (m\\*k + k\\*n) / (m\\*n).\n\n#### smoothed compression <a name=\"smoothed-compression\"></a>\n\na gradually increasing alpha value is used to smooth the compression from\nstart_step to end_step. this way the model gradually moves from the full weights\nmatrix to a compressed one. for example, in the low-rank approximation scheme,\nthe weight matrix that is used in the training process is w = (alpha) * a + (1 -\nalpha) * bc. this alpha value is decremented over time from alpha = 1 to alpha =\n0, using the alpha_decrement_value at intervals of compression_frequency.\n\n### adding compression ops to the training graph <a name=\"adding-compression-op\"></a>\n\n```python\n# parse compression hyperparameters\ncompression_hparams = compression.compressionop.get_default_hparams().parse(\n      hparams)\n\n# create a compression object using the compression hyperparameters\ncompression_obj = compression_wrapper.get_apply_compression(\n    compression_hparams, global_step=global_step)\n\n# somewhere in the model, compute the compressed weights\nlocal = tf.nn.relu(\n         tf.matmul(reshape, compression_obj.apply_compression(weights, scope)) +\n         biases,\n         name=scope.name)\n\nall_update_op = [apply_gradient_op, ...] # all existing model updates\n# run compression update steps with all the other updates. example below is\n# assuming update_option=0.\nall_update_op.append(compression_obj.all_update_op())\n\nwith tf.control_dependencies(all_update_op):\n  train_op = tf.no_op(name='train')\n```\n\nensure that `global_step` is being incremented, otherwise compression will not\nwork!\n\n#### example usage <a name=\"example\"></a>\n\nas an example, the cifar10 model provided in tensorflow\u2019s\n[advanced convolutional neural networks](https://www.tensorflow.org/tutorials/images/deep_cnn)\n(see page for more details) has been modified to incorporate the compression\nlibrary:\n\n*   [cifar10_compression.py](https://github.com/google-research/google-research/tree/master/graph_compression/compression_lib/examples/cifar10/cifar10_compression.py)\n    creates the deep cnn and adds the weight compression to the fully-connected\n    layers.\n*   [cifar10_train.py](https://github.com/google-research/google-research/tree/master/graph_compression/compression_lib/examples/cifar10/cifar10_train.py)\n    creates the compression object and provides it to the training graph\n    (described above) to use.\n\nto train the compression version of cifar10 (make sure you're working in a\nproperly configured virtualenv - as setup using the\n[run.sh](https://github.com/google-research/google-research/tree/master/graph_compression/run.sh)\nscript):\n\n```shell\n$ python cifar10_train.py --compression_hparams=name=cifar10_compression,alpha_decrement_value=0.005,begin_compression_step=40000,end_compression_step=100000,compression_frequency=100,compression_option=1,use_tpu=true,update_option=0,rank=200 --max_steps 120000\n```\n\neval:\n\n```shell\n$ python cifar10_eval.py --compression_hparams=name=cifar10_compression,alpha_decrement_value=0.005,begin_compression_step=40000,end_compression_step=100000,compression_frequency=100,compression_option=1,use_tpu=true,update_option=0,rank=200 --run_once\n```\n\neager execution example.\n\nan eager execution example is provided at compression_lib/examples/mnist_eager_mode/mnist_compression.py. to train the model, run:\n\n```shell\n$python mnist_compression.py\n```\n\n\n\nauthors: rina panigrahy (corresponding author -- email: rinap@google.com),\nlucine oganesian, sudeshna roy, xin wang, with support from: badih ghazi for helpful contributions, rasmus pagh (pagh@google.com,\n[doc](https://drive.google.com/file/d/10twvnhexdwdq8dypelv18rq92zutszp9/view?usp=sharing))\nfor the simhash code, zoya svitkina for code reviews, and suyog gupta for\nconsultations.\n\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "apache 2.0",
  "name": "graph-compression-google-research",
  "package_url": "https://pypi.org/project/graph-compression-google-research/",
  "project_url": "https://pypi.org/project/graph-compression-google-research/",
  "project_urls": null,
  "release_url": "https://pypi.org/project/graph-compression-google-research/0.0.4/",
  "requires_dist": [],
  "requires_python": ">=3.6",
  "summary": "matrix compression for neural networks.",
  "version": "0.0.4",
  "releases": [],
  "developers": [
    "google",
    "rinap@google.com"
  ],
  "kwds": "tensorflow model_compression graph_compression get_apply_compression apply_compression",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_graph_compression_google_research",
  "homepage": "",
  "release_count": 4,
  "dependency_ids": []
}