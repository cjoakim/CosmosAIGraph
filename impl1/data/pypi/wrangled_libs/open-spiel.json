{
  "classifiers": [],
  "description": "\n# openspiel: a framework for reinforcement learning in games\n\n[![documentation status](https://readthedocs.org/projects/openspiel/badge/?version=latest)](https://openspiel.readthedocs.io/en/latest/?badge=latest)\n![build_and_test](https://github.com/deepmind/open_spiel/workflows/build_and_test/badge.svg)\n\nopenspiel is a collection of environments and algorithms for research in general\nreinforcement learning and search/planning in games. openspiel supports n-player\n(single- and multi- agent) zero-sum, cooperative and general-sum, one-shot and\nsequential, strictly turn-taking and simultaneous-move, perfect and imperfect\ninformation games, as well as traditional multiagent environments such as\n(partially- and fully- observable) grid worlds and social dilemmas. openspiel\nalso includes tools to analyze learning dynamics and other common evaluation\nmetrics. games are represented as procedural extensive-form games, with some\nnatural extensions. the core api and games are implemented in c++ and exposed to\npython. algorithms and tools are written both in c++ and python.\n\nto try openspiel in google colaboratory, please refer to `open_spiel/colabs` subdirectory or start [here](https://colab.research.google.com/github/deepmind/open_spiel/blob/master/open_spiel/colabs/install_open_spiel.ipynb).\n\n<p align=\"center\">\n  <img src=\"docs/_static/openspielb.png\" alt=\"openspiel visual asset\">\n</p>\n\n# index\n\nplease choose among the following options:\n\n*   [installing openspiel](docs/install.md)\n*   [introduction to openspiel](docs/intro.md)\n*   [api overview and first example](docs/concepts.md)\n*   [api reference](docs/api_reference.md)\n*   [overview of implemented games](docs/games.md)\n*   [overview of implemented algorithms](docs/algorithms.md)\n*   [developer guide](docs/developer_guide.md)\n*   [using openspiel as a c++ library](docs/library.md)\n*   [guidelines and contributing](docs/contributing.md)\n*   [authors](docs/authors.md)\n\nfor a longer introduction to the core concepts, formalisms, and terminology,\nincluding an overview of the algorithms and some results, please see\n[openspiel: a framework for reinforcement learning in games](https://arxiv.org/abs/1908.09453).\n\nfor an overview of openspiel and example uses of the core api, please check out\nour tutorials:\n\n*   [motivation, core api, brief intro to replictor dynamics and imperfect\n    information games](https://www.youtube.com/watch?v=8ncpqtpwlfq) by marc\n    lanctot.\n    [(slides)](http://mlanctot.info/files/openspiel_tutorial_ku_leuven_2022.pdf)\n    [(colab)](https://colab.research.google.com/github/deepmind/open_spiel/blob/master/open_spiel/colabs/openspieltutorial.ipynb)\n*   [motivation, core api, implementing cfr and reinforce on kuhn poker, leduc\n    poker, and goofspiel](https://www.youtube.com/watch?v=o6jnhoguxco) by edward\n    lockhart.\n    [(slides)](http://mlanctot.info/files/open_spiel_tutorial-mar2021-comarl.pdf)\n    [(colab)](https://colab.research.google.com/github/deepmind/open_spiel/blob/master/open_spiel/colabs/cfr_and_reinforce.ipynb)\n\nif you use openspiel in your research, please cite the paper using the following\nbibtex:\n\n```bibtex\n@article{lanctotetal2019openspiel,\n  title     = {{openspiel}: a framework for reinforcement learning in games},\n  author    = {marc lanctot and edward lockhart and jean-baptiste lespiau and\n               vinicius zambaldi and satyaki upadhyay and julien p\\'{e}rolat and\n               sriram srinivasan and finbarr timbers and karl tuyls and\n               shayegan omidshafiei and daniel hennes and dustin morrill and\n               paul muller and timo ewalds and ryan faulkner and j\\'{a}nos kram\\'{a}r\n               and bart de vylder and brennan saeta and james bradbury and david ding\n               and sebastian borgeaud and matthew lai and julian schrittwieser and\n               thomas anthony and edward hughes and ivo danihelka and jonah ryan-davis},\n  year      = {2019},\n  eprint    = {1908.09453},\n  archiveprefix = {arxiv},\n  primaryclass = {cs.lg},\n  journal   = {corr},\n  volume    = {abs/1908.09453},\n  url       = {http://arxiv.org/abs/1908.09453},\n}\n```\n\n## versioning\n\nwe use [semantic versioning](https://semver.org/).\n\n",
  "docs_url": null,
  "keywords": "",
  "license": "apache 2.0",
  "name": "open-spiel",
  "package_url": "https://pypi.org/project/open-spiel/",
  "project_url": "https://pypi.org/project/open-spiel/",
  "project_urls": {
    "Homepage": "https://github.com/deepmind/open_spiel"
  },
  "release_url": "https://pypi.org/project/open-spiel/1.4/",
  "requires_dist": [
    "pip >=20.0.2",
    "attrs >=19.3.0",
    "absl-py >=0.10.0",
    "numpy >=1.21.5",
    "scipy >=1.10.1",
    "ml-collections >=0.1.1"
  ],
  "requires_python": ">=3.8",
  "summary": "a framework for reinforcement learning in games",
  "version": "1.4",
  "releases": [],
  "developers": [
    "open_spiel@google.com",
    "the_openspiel_authors"
  ],
  "kwds": "openspiel games reinforcement openspielb deepmind",
  "license_kwds": "apache 2.0",
  "libtype": "pypi",
  "id": "pypi_open_spiel",
  "homepage": "https://github.com/deepmind/open_spiel",
  "release_count": 17,
  "dependency_ids": [
    "pypi_absl_py",
    "pypi_attrs",
    "pypi_ml_collections",
    "pypi_numpy",
    "pypi_pip",
    "pypi_scipy"
  ]
}