{
  "classifiers": [
    "development status :: 4 - beta",
    "intended audience :: science/research",
    "license :: osi approved :: mit license",
    "natural language :: english",
    "programming language :: python :: 3.10",
    "programming language :: python :: 3.8",
    "programming language :: python :: 3.9",
    "topic :: scientific/engineering"
  ],
  "description": "using `pyannote.audio` open-source toolkit in production?\nmake the most of it thanks to our [consulting services](https://herve.niderb.fr/consulting.html).\n\n# `pyannote.audio` speaker diarization toolkit\n\n`pyannote.audio` is an open-source toolkit written in python for speaker diarization. based on [pytorch](pytorch.org) machine learning framework, it comes with state-of-the-art [pretrained models and pipelines](https://hf.co/pyannote), that can be further finetuned to your own data for even better performance.\n\n<p align=\"center\">\n <a href=\"https://www.youtube.com/watch?v=37r_r82lfwa\"><img src=\"https://img.youtube.com/vi/37r_r82lfwa/0.jpg\"></a>\n</p>\n\n## tl;dr\n\n1. install [`pyannote.audio`](https://github.com/pyannote/pyannote-audio) with `pip install pyannote.audio`\n2. accept [`pyannote/segmentation-3.0`](https://hf.co/pyannote/segmentation-3.0) user conditions\n3. accept [`pyannote/speaker-diarization-3.1`](https://hf.co/pyannote/speaker-diarization-3.1) user conditions\n4. create access token at [`hf.co/settings/tokens`](https://hf.co/settings/tokens).\n\n```python\nfrom pyannote.audio import pipeline\npipeline = pipeline.from_pretrained(\n    \"pyannote/speaker-diarization-3.1\",\n    use_auth_token=\"huggingface_access_token_goes_here\")\n\n# send pipeline to gpu (when available)\nimport torch\npipeline.to(torch.device(\"cuda\"))\n\n# apply pretrained pipeline\ndiarization = pipeline(\"audio.wav\")\n\n# print the result\nfor turn, _, speaker in diarization.itertracks(yield_label=true):\n    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\n# start=0.2s stop=1.5s speaker_0\n# start=1.8s stop=3.9s speaker_1\n# start=4.2s stop=5.7s speaker_0\n# ...\n```\n\n## highlights\n\n- :hugs: pretrained [pipelines](https://hf.co/models?other=pyannote-audio-pipeline) (and [models](https://hf.co/models?other=pyannote-audio-model)) on [:hugs: model hub](https://huggingface.co/pyannote)\n- :exploding_head: state-of-the-art performance (see [benchmark](#benchmark))\n- :snake: python-first api\n- :zap: multi-gpu training with [pytorch-lightning](https://pytorchlightning.ai/)\n\n## documentation\n\n- [changelog](changelog.md)\n- [frequently asked questions](faq.md)\n- models\n  - available tasks explained\n  - [applying a pretrained model](tutorials/applying_a_model.ipynb)\n  - [training, fine-tuning, and transfer learning](tutorials/training_a_model.ipynb)\n- pipelines\n  - available pipelines explained\n  - [applying a pretrained pipeline](tutorials/applying_a_pipeline.ipynb)\n  - [adapting a pretrained pipeline to your own data](tutorials/adapting_pretrained_pipeline.ipynb)\n  - [training a pipeline](tutorials/voice_activity_detection.ipynb)\n- contributing\n  - [adding a new model](tutorials/add_your_own_model.ipynb)\n  - [adding a new task](tutorials/add_your_own_task.ipynb)\n  - adding a new pipeline\n  - sharing pretrained models and pipelines\n- blog\n  - 2022-12-02 > [\"how i reached 1st place at ego4d 2022, 1st place at albayzin 2022, and 6th place at voxsrc 2022 speaker diarization challenges\"](tutorials/adapting_pretrained_pipeline.ipynb)\n  - 2022-10-23 > [\"one speaker segmentation model to rule them all\"](https://herve.niderb.fr/fastpages/2022/10/23/one-speaker-segmentation-model-to-rule-them-all)\n  - 2021-08-05 > [\"streaming voice activity detection with pyannote.audio\"](https://herve.niderb.fr/fastpages/2021/08/05/streaming-voice-activity-detection-with-pyannote.html)\n- videos\n  - [introduction to speaker diarization](https://umotion.univ-lemans.fr/video/9513-speech-segmentation-and-speaker-diarization/) / jsalt 2023 summer school / 90 min\n  - [speaker segmentation model](https://www.youtube.com/watch?v=wdh2rvkjymy) / interspeech 2021 / 3 min\n  - [first releaase of pyannote.audio](https://www.youtube.com/watch?v=37r_r82lfwa) / icassp 2020 / 8 min\n\n## benchmark\n\nout of the box, `pyannote.audio` speaker diarization [pipeline](https://hf.co/pyannote/speaker-diarization-3.1) v3.1 is expected to be much better (and faster) than v2.x.\nthose numbers are diarization error rates (in %):\n\n| benchmark              | [v2.1](https://hf.co/pyannote/speaker-diarization-2.1) | [v3.1](https://hf.co/pyannote/speaker-diarization-3.1) | [premium](https://forms.gle/ekhn7h2zta68smmx8) |\n| ---------------------- | ------------------------------------------------------ | ------------------------------------------------------ | ---------------------------------------------- |\n| aishell-4              | 14.1                                                   | 12.3                                                   | 11.9                                           |\n| alimeeting (channel 1) | 27.4                                                   | 24.5                                                   | 22.5                                           |\n| ami (ihm)              | 18.9                                                   | 18.8                                                   | 16.6                                           |\n| ami (sdm)              | 27.1                                                   | 22.6                                                   | 20.9                                           |\n| ava-avd                | 66.3                                                   | 50.0                                                   | 39.8                                           |\n| callhome (part 2)      | 31.6                                                   | 28.4                                                   | 22.2                                           |\n| dihard 3 (full)        | 26.9                                                   | 21.4                                                   | 17.2                                           |\n| ego4d (dev.)           | 61.5                                                   | 51.2                                                   | 43.8                                           |\n| msdwild                | 32.8                                                   | 25.4                                                   | 19.8                                           |\n| repere (phase2)        | 8.2                                                    | 7.8                                                    | 7.6                                            |\n| voxconverse (v0.3)     | 11.2                                                   | 11.2                                                   | 9.4                                            |\n\n[diarization error rate](http://pyannote.github.io/pyannote-metrics/reference.html#diarization) (in %)\n\n## citations\n\nif you use `pyannote.audio` please use the following citations:\n\n```bibtex\n@inproceedings{plaquet23,\n  author={alexis plaquet and herv\u00e9 bredin},\n  title={{powerset multi-class cross entropy loss for neural speaker diarization}},\n  year=2023,\n  booktitle={proc. interspeech 2023},\n}\n```\n\n```bibtex\n@inproceedings{bredin23,\n  author={herv\u00e9 bredin},\n  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},\n  year=2023,\n  booktitle={proc. interspeech 2023},\n}\n```\n\n## development\n\nthe commands below will setup pre-commit hooks and packages needed for developing the `pyannote.audio` library.\n\n```bash\npip install -e .[dev,testing]\npre-commit install\n```\n\n## test\n\n```bash\npytest\n```\n",
  "docs_url": null,
  "keywords": "",
  "license": "mit",
  "name": "pyannote.audio",
  "package_url": "https://pypi.org/project/pyannote.audio/",
  "project_url": "https://pypi.org/project/pyannote.audio/",
  "project_urls": {
    "Homepage": "https://github.com/pyannote/pyannote-audio"
  },
  "release_url": "https://pypi.org/project/pyannote.audio/3.1.1/",
  "requires_dist": [
    "asteroid-filterbanks >=0.4",
    "einops >=0.6.0",
    "huggingface-hub >=0.13.0",
    "lightning >=2.0.1",
    "omegaconf <3.0,>=2.1",
    "pyannote.core >=5.0.0",
    "pyannote.database >=5.0.1",
    "pyannote.metrics >=3.2",
    "pyannote.pipeline >=3.0.1",
    "pytorch-metric-learning >=2.1.0",
    "rich >=12.0.0",
    "semver >=3.0.0",
    "soundfile >=0.12.1",
    "speechbrain >=0.5.14",
    "tensorboardX >=2.6",
    "torch >=2.0.0",
    "torch-audiomentations >=0.11.0",
    "torchaudio >=2.0.0",
    "torchmetrics >=0.11.0",
    "hydra-core <1.2,>=1.1 ; extra == 'cli'",
    "typer <0.5.0,>=0.4.0 ; extra == 'cli'",
    "pre-commit >=2.7 ; extra == 'dev'",
    "recommonmark >=0.6 ; extra == 'dev'",
    "black >=22.3.0 ; extra == 'dev'",
    "pytest >=6.0 ; extra == 'testing'",
    "pytest-cov >=2.10 ; extra == 'testing'",
    "jupyter ; extra == 'testing'",
    "papermill ; extra == 'testing'"
  ],
  "requires_python": ">=3.7",
  "summary": "neural building blocks for speaker diarization",
  "version": "3.1.1",
  "releases": [],
  "developers": [
    "herve.bredin@irit.fr"
  ],
  "kwds": "pyannote voice_activity_detection audio speaker_0 pytorch",
  "license_kwds": "mit",
  "libtype": "pypi",
  "id": "pypi_pyannote.audio",
  "homepage": "https://github.com/pyannote/pyannote-audio",
  "release_count": 10,
  "dependency_ids": [
    "pypi_asteroid_filterbanks",
    "pypi_black",
    "pypi_einops",
    "pypi_huggingface_hub",
    "pypi_hydra_core",
    "pypi_jupyter",
    "pypi_lightning",
    "pypi_omegaconf",
    "pypi_papermill",
    "pypi_pre_commit",
    "pypi_pyannote.core",
    "pypi_pyannote.database",
    "pypi_pyannote.metrics",
    "pypi_pyannote.pipeline",
    "pypi_pytest",
    "pypi_pytest_cov",
    "pypi_pytorch_metric_learning",
    "pypi_recommonmark",
    "pypi_rich",
    "pypi_semver",
    "pypi_soundfile",
    "pypi_speechbrain",
    "pypi_tensorboardx",
    "pypi_torch",
    "pypi_torch_audiomentations",
    "pypi_torchaudio",
    "pypi_torchmetrics",
    "pypi_typer"
  ]
}