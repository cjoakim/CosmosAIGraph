{
  "classifiers": [
    "development status :: 3 - alpha",
    "intended audience :: developers",
    "programming language :: python :: 2.7",
    "programming language :: python :: 3",
    "programming language :: python :: 3.4",
    "programming language :: python :: 3.5",
    "programming language :: python :: 3.6",
    "programming language :: python :: 3.7"
  ],
  "description": "# set similarity search\n\n[![python package](https://github.com/ekzhu/setsimilaritysearch/actions/workflows/python-package.yml/badge.svg)](https://github.com/ekzhu/setsimilaritysearch/actions/workflows/python-package.yml)\n\nefficient set similarity search algorithms in python.\nfor even better performance see the\n[go implementation](https://github.com/ekzhu/go-set-similarity-search).\n\n## what is set similarity search?\n\nlet's say we have a database of users and the books they have read.\nassume that we want to recommend \"friends\" for each user,\nand the \"friends\" must have read very similar set of books\nas the user have. we can model this as a set similarity search problem,\nby representing each user's books as a set:\n\n```\nalice: {\"anna karenina\", \"war and peace\", \"the chameleon\", ...}\nbob: {\"lolita\", \"the metamorphosis\", \"the judgement\", ...}\njoey: {\"anna karenina\", \"the chameleon\" ...}\n```\n\na popular way to measure the similarity between two sets is\n[jaccard similarity](https://en.wikipedia.org/wiki/jaccard_index), which\ngives a fractional score between 0 and 1.0.\n\nthere are two versions of set similarity search problem,\nboth can be defined given a collection of sets, a\nsimilarity function and a threshold:\n\n1. *all-pairs:* find all pairs of sets that have\nsimilarities greater than (or equal to) the threshold;\n2. *query:* given a query set, from the collection  of sets, find all that\nhave similarities greater than (or equal to) the threshold with respect to\nthe query set.\n\nboth versions of the problem can be very computationally expensive\nas the collection can be large and the set sizes can be large.\nthe simple brute-force algorithm is o(n^2) for (1) and o(n) for (2).\n\nthis package includes a python implementation of the \"all-pair-binary\"\nalgorithm in\n[scaling up all pairs similarity search](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/32781.pdf)\npaper, with additional position filter optimization.\nthis algorithm still has the same worst-case complexity as the brute-force\nalgorithm, however, by taking advantage of skewness in empirical\ndistributions of set sizes and frequencies, it often runs much faster\n(even better than [minhash lsh](https://ekzhu.github.io/datasketch/lsh.html)).\n\n## benchmarks\n\nrun *all-pairs* on 3.5 ghz intel core i7, using similarity function `jaccard`\nand similarity threshold 0.5.\nthe running time of [`datasketch.minhashlsh`](https://ekzhu.github.io/datasketch/lsh.html) is also shown below for\ncomparison (`num_perm=32`).\n\n| dataset | input sets | avg. size | `setsimilaritysearch` runtime | `datasketch` runtime | `datasketch` accuracy |\n|---------|--------------|--------------|---------|------|--|\n| [pokec social network (relationships)](https://snap.stanford.edu/data/soc-pokec.html): from-nodes are set ids; to-nodes are elements | 1432693 | 27.31 | 10m49s | 11m4s | precision: 0.73; recall: 0.67 |\n| [livejournal](https://snap.stanford.edu/data/soc-livejournal1.html): from-nodes are set ids; to-nodes are elements | 4308452 | 16.01 | 28m51s | 31m58s | precision: 0.79; recall: 0.74|\n\nalthough `datasketch.minhashlsh` is an approximate algorithm, and i am using `num_perm=32` which is quite low, it is still\na bit slower than the exact algorithm `setsimilaritysearch`.\nthe time for\ncreating `datasketch.minhash` is also included in the end-to-end time, while\nin practice this time can be saved through pre-computation. however, for\n*ad hoc* computation of *all-pairs*, `setsimilaritysearch` is still\nthe better choice, especially when sets are small and fit in memory.\n\nrun *query* on 3.5 ghz intel core i7, using similarity function `jaccard`\nand similarity threshold 0.5.\nthe query sets are sampled from the dataset itself.\nthe running time of [`datasketch.minhashlsh`](https://ekzhu.github.io/datasketch/lsh.html) is also shown below for\ncomparison (`num_perm=32`).\n\n| dataset | indexed sets | query sets | avg. size | `setsimilaritysearch` indexing & querying time | `datasketch` indexing & querying time | `datasketch` accuracy |\n|--|--|--|--|--|--|--|\n| [pokec social network (relationships)](https://snap.stanford.edu/data/soc-pokec.html): from-nodes are set ids; to-nodes are elements | 1432693 | 10k | 27.31 | indexing: 1m7s; querying (90pct): 2.3ms | indexing: 9m23s; querying (90pct): 0.72ms | precision: 0.90; recall: 0.88 |\n| [livejournal](https://snap.stanford.edu/data/soc-livejournal1.html): from-nodes are set ids; to-nodes are elements | 4308452 | 10k | 16.01 | indexing: 2m32s; querying (90pct): 1.6ms | indexing: 30m58s; querying (90pct): 2.1ms | precision: 0.85; recall: 0.78|\n\nthe indexing time for `datasketch.minhashlsh`, including the time for\ncreating `datasketch.minhash`, is much worse than `setsimilaritysearch` --\nnearly 10x and 15x. therefore `setsimilaritysearch` is much better for\n*ad hoc* computation of the *query* problem. for the scenario in which the same\nsearch index is reused for many *query* problems, `datasketch.minhashlsh` is\nfaster than `setsimilaritysearch` when the set sizes are large. this is\neasy to understand: the size of `datasketch.minhash` is constant, wheres\na set can be arbitrarily large, so the query time for large sets is faster\nwhen sketch is used. however, when the set sizes become smaller, the sketch\nlooses its advantage.\n\n## install\n\n### pip\n\n```bash\npip install -u setsimilaritysearch\n```\n\n### conda\n\n```bash\nconda install -c conda-forge setsimilaritysearch\n```\n\n## library usage\n\nfor *all-pairs*, it takes an input of a list of sets, and output pairs that\nmeet the similarity threshold.\n\n```python\nfrom setsimilaritysearch import all_pairs\n\n# the input sets must be a python list of iterables (i.e., lists or sets).\nsets = [[1,2,3], [3,4,5], [2,3,4], [5,6,7]]\n# all_pairs returns an iterable of tuples.\npairs = all_pairs(sets, similarity_func_name=\"jaccard\", \n        similarity_threshold=0.1)\nlist(pairs)\n# [(1, 0, 0.2), (2, 0, 0.5), (2, 1, 0.5), (3, 1, 0.2)]\n# each tuple is (<index of the first set>, <index of the second set>, <similarity>).\n# the indexes are the list indexes of the input sets.\n```\n\nfor *query*, it takes an input of a list of sets, and builds a search index\nthat can compute any number of queries. currently the search index only\nsupports a static collection of sets with no updates.\n\n```python\nfrom setsimilaritysearch import searchindex\n\n# the input sets must be a python list of iterables (i.e., lists or sets).\nsets = [[1,2,3], [3,4,5], [2,3,4], [5,6,7]]\n# the search index cannot be updated.\nindex = searchindex(sets, similarity_func_name=\"jaccard\", \n    similarity_threshold=0.1)\n# the query function takes input a set.\nresults = index.query([5,3,4])\nresults\n# [(1, 1.0), (0, 0.2), (2, 0.5), (3, 0.2)]\n# each tuple is (<index of the found set>, <similarity>).\n# the index is the list index of the sets in the search index.\n```\n\nsupported similarity functions (more to come):\n\n* [jaccard](https://en.wikipedia.org/wiki/jaccard_index): intersection size divided by union size; set `similarity_func_name=\"jaccard\"`.\n* [cosine](https://en.wikipedia.org/wiki/cosine_similarity): intersection size divided by square root of the product of sizes; set `similarity_func_name=\"cosine\"`.\n* [containment](https://ekzhu.github.io/datasketch/lshensemble.html#containment): intersection size divided by the size of the first set (or query set); set `similarity_func_name=\"containment\"`.\n\n## command line usage\n\nyou can also use the command line program `all_pairs.py`.\nthe input must be **one or two** files with each line a **unique** `setid token`\ntuple.\nfor example:\n\n```\n# line starts with # will be ignored.\n# each line is <set id> <token (i.e. set element)>, separate by a whitespace or tab.\n# every line must be unique.\n1 a\n1 b\n1 c\n1 d\n2 a\n2 b\n2 c\n3 d\n3 e\n```\n\nwhen one input file is given, it computes *all-pairs*; when two input files\nare given, it computes *query* by building a search index on the first\ncollection and querying with sets from the second collection -- effectively\ncomputes cross-collection pairs.\n\nexample usage (*all-pairs*):\n\n```bash\nall_pairs.py --input-sets testdata/example_input.txt \\\n    --output-pairs testdata/example_output.txt \\\n    --similarity-func jaccard \\\n    --similarity-threshold 0.1\n```\n",
  "docs_url": null,
  "keywords": "set similarity search all pairs",
  "license": "",
  "name": "setsimilaritysearch",
  "package_url": "https://pypi.org/project/SetSimilaritySearch/",
  "project_url": "https://pypi.org/project/SetSimilaritySearch/",
  "project_urls": {
    "Homepage": "https://github.com/ekzhu/SetSimilaritySearch"
  },
  "release_url": "https://pypi.org/project/SetSimilaritySearch/1.0.1/",
  "requires_dist": [
    "numpy",
    "coverage ; extra == 'test'",
    "nose ; extra == 'test'"
  ],
  "requires_python": "",
  "summary": "a python library of set similarity search algorithms",
  "version": "1.0.1",
  "releases": [],
  "developers": [
    "ekzhu@cs.toronto.edu",
    "eric_zhu"
  ],
  "kwds": "setsimilaritysearch similarity_threshold similarity similarity_func_name sets",
  "license_kwds": "",
  "libtype": "pypi",
  "id": "pypi_setsimilaritysearch",
  "homepage": "https://github.com/ekzhu/setsimilaritysearch",
  "release_count": 9,
  "dependency_ids": [
    "pypi_coverage",
    "pypi_nose",
    "pypi_numpy"
  ]
}