{
  "libtype": "pypi",
  "libname": "vector-quantize-pytorch",
  "url": "https://github.com/lucidrains/vector-quantizer-pytorch",
  "html": "<!DOCTYPE html><html  lang=\"en\"    data-color-mode=\"auto\" data-light-theme=\"light\" data-dark-theme=\"dark\"  data-a11y-animated-images=\"system\" data-a11y-link-underlines=\"true\"  >  <head>    <meta charset=\"utf-8\">  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">  <link rel=\"dns-prefetch\" href=\"https://avatars.githubusercontent.com\">  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">  <link rel=\"dns-prefetch\" href=\"https://user-images.githubusercontent.com/\">  <link rel=\"preconnect\" href=\"https://github.githubassets.com\" crossorigin>  <link rel=\"preconnect\" href=\"https://avatars.githubusercontent.com\">    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/light-0eace2597ca3.css\" /><link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/dark-a167e256da9c.css\" /><link data-color-theme=\"dark_dimmed\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/dark_dimmed-d11f2cf8009b.css\" /><link data-color-theme=\"dark_high_contrast\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/dark_high_contrast-ea7373db06c8.css\" /><link data-color-theme=\"dark_colorblind\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/dark_colorblind-afa99dcf40f7.css\" /><link data-color-theme=\"light_colorblind\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/light_colorblind-af6c685139ba.css\" /><link data-color-theme=\"light_high_contrast\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/light_high_contrast-578cdbc8a5a9.css\" /><link data-color-theme=\"light_tritanopia\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/light_tritanopia-5cb699a7e247.css\" /><link data-color-theme=\"dark_tritanopia\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/dark_tritanopia-9b32204967c6.css\" />    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/primer-primitives-2ef2a46b27ee.css\" />    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/primer-711f412bb361.css\" />    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/global-4803cd254267.css\" />    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/github-f4d857cbc96a.css\" />  <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/repository-6247ca238fd4.css\" /><link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/code-6d7b4ef0ea51.css\" />    <script type=\"application/json\" id=\"client-env\">{\"locale\":\"en\",\"featureFlags\":[\"code_vulnerability_scanning\",\"copilot_conversational_ux_history_refs\",\"copilot_chat_attach_knowledge\",\"copilot_chat_knowledge_base_copy\",\"copilot_smell_icebreaker_ux\",\"copilot_implicit_context\",\"docset_management_ui\",\"copilot_chat_settings\",\"failbot_handle_non_errors\",\"geojson_azure_maps\",\"image_metric_tracking\",\"marketing_forms_api_integration_contact_request\",\"marketing_pages_search_explore_provider\",\"turbo_experiment_risky\",\"sample_network_conn_type\",\"no_character_key_shortcuts_in_inputs\",\"custom_inp\",\"remove_child_patch\"]}</script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/wp-runtime-47578fb192fd.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_dompurify_dist_purify_js-6890e890956f.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_stacktrace-parser_dist_stack-trace-parser_esm_js-node_modules_github_bro-a4c183-79f9611c275b.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_hydro-analytics-client_dist_analytics-client_js-node_modules_gith-6a10dd-e66ebda625fb.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_failbot_failbot_ts-479802999bcc.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/environment-fe7570f3bc38.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_selector-observer_dist_index_esm_js-9f960d9b217c.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_behaviors_dist_esm_focus-zone_js-086f7a27bac0.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_relative-time-element_dist_index_js-c76945c5961a.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_delegated-events_dist_index_js-node_modules_github_details-dialog-elemen-29dc30-a2a71f11a507.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_auto-complete-element_dist_index_js-12366198e7a5.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_text-expander-element_dist_index_js-8a621df59e80.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_filter-input-element_dist_index_js-node_modules_github_remote-inp-b7d8f4-654130b7cde5.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_file-attachment-element_dist_index_js-node_modules_primer_view-co-5dccdf-e5e2b9fa3c0c.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/github-elements-e4eda4896b4e.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/element-registry-b99c9d8fad1d.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_catalyst_lib_index_js-node_modules_github_hydro-analytics-client_-978abc0-add939c751ce.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_lit-html_lit-html_js-5b376145beff.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_mini-throttle_dist_index_js-node_modules_github_alive-client_dist-bf5aa2-1b562c29ab8e.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_morphdom_dist_morphdom-esm_js-5bff297a06de.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_turbo_dist_turbo_es2017-esm_js-c91f4ad18b62.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_color-convert_index_js-72c9fbde5ad4.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_scroll-anchoring_dist_scro-231ccf-aa129238d13b.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_behaviors_dist_esm_dimensions_js-node_modules_github_jtml_lib_index_js-95b84ee6bc34.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_session-resume_dist_index_js-node_modules_primer_behaviors_dist_e-da6ec6-3f39339c9d98.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_paste-markdown_dist_index_esm_js-node_modules_github_quote-select-67e0dc-1aa35af077a4.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_updatable-content_ts-ee3fc84d7fb0.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_behaviors_task-list_ts-app_assets_modules_github_onfocus_ts-app_ass-421cec-9de4213015af.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_sticky-scroll-into-view_ts-94209c43e6af.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_behaviors_ajax-error_ts-app_assets_modules_github_behaviors_include-467754-f9bd433e9591.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_behaviors_commenting_edit_ts-app_assets_modules_github_behaviors_ht-83c235-9285faa0e011.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_blob-anchor_ts-app_assets_modules_github_filter-sort_ts-app_assets_-c96432-da3733f430b8.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/behaviors-1fb9e5061509.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_delegated-events_dist_index_js-node_modules_github_catalyst_lib_index_js-d0256ebff5cd.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/notifications-global-352d84c6cc82.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_virtualized-list_es_index_js-node_modules_github_template-parts_lib_index_js-878844713bc9.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_delegated-events_dist_inde-c537341-c7f6a41a084c.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_ref-selector_ts-b593b93f23f5.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/codespaces-1a8626dd714a.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_filter-input-element_dist_index_js-node_modules_github_mini-throt-08ab15-3e0517baca99.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_file-attachment-element_dist_index_js-node_modules_github_mini-th-55cf52-e14cb4b719b4.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/repositories-69068e0899f9.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/code-menu-614feb194539.js\"></script>    <title>GitHub - lucidrains/vector-quantize-pytorch: Vector (and Scalar) Quantization, in Pytorch</title>  <meta name=\"route-pattern\" content=\"/:user_id/:repository\" data-turbo-transient>  <meta name=\"route-controller\" content=\"files\" data-turbo-transient>  <meta name=\"route-action\" content=\"disambiguate\" data-turbo-transient>      <meta name=\"current-catalog-service-hash\" content=\"82c569b93da5c18ed649ebd4c2c79437db4611a6a1373e805a3cb001c64130b7\">  <meta name=\"request-id\" content=\"CBC9:082D:BF4830:119E564:65E793E4\" data-pjax-transient=\"true\"/><meta name=\"html-safe-nonce\" content=\"4086f392ce8343ef4df19b5cf2cf7a80a8bc37f07bf5f3b2640cbcf794e3ca03\" data-pjax-transient=\"true\"/><meta name=\"visitor-payload\" content=\"eyJyZWZlcnJlciI6IiIsInJlcXVlc3RfaWQiOiJDQkM5OjA4MkQ6QkY0ODMwOjExOUU1NjQ6NjVFNzkzRTQiLCJ2aXNpdG9yX2lkIjoiMjk2MjA2NTQyNDc4NzE0OTc5NiIsInJlZ2lvbl9lZGdlIjoiaWFkIiwicmVnaW9uX3JlbmRlciI6ImlhZCJ9\" data-pjax-transient=\"true\"/><meta name=\"visitor-hmac\" content=\"b5e1a273e31cbb1bcb344d0d8fa96bd6505ba85d616650e69f48d5447ff895fe\" data-pjax-transient=\"true\"/>    <meta name=\"hovercard-subject-tag\" content=\"repository:271090649\" data-turbo-transient>  <meta name=\"github-keyboard-shortcuts\" content=\"repository,copilot\" data-turbo-transient=\"true\" />    <meta name=\"selected-link\" value=\"repo_source\" data-turbo-transient>  <link rel=\"assets\" href=\"https://github.githubassets.com/\">    <meta name=\"google-site-verification\" content=\"c1kuD-K2HIVF635lypcsWPoD4kilo5-jA_wBFyT4uMY\">  <meta name=\"google-site-verification\" content=\"KT5gs8h0wvaagLKAVWq8bbeNwnZZK1r1XQysX3xurLU\">  <meta name=\"google-site-verification\" content=\"ZzhVyEFwb7w3e0-uOTltm8Jsck2F5StVihD0exw2fsA\">  <meta name=\"google-site-verification\" content=\"GXs5KoUUkNCoaAZn7wPN-t01Pywp9M3sEjnt_3_ZWPc\">  <meta name=\"google-site-verification\" content=\"Apib7-x98H0j5cPqHWwSMm6dNU4GmODRoqxLiDzdx9I\"><meta name=\"octolytics-url\" content=\"https://collector.github.com/github/collect\" />  <meta name=\"analytics-location\" content=\"/&lt;user-name&gt;/&lt;repo-name&gt;\" data-turbo-transient=\"true\" />        <meta name=\"user-login\" content=\"\">      <meta name=\"viewport\" content=\"width=device-width\">          <meta name=\"description\" content=\"Vector (and Scalar) Quantization, in Pytorch. Contribute to lucidrains/vector-quantize-pytorch development by creating an account on GitHub.\">      <link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"/opensearch.xml\" title=\"GitHub\">    <link rel=\"fluid-icon\" href=\"https://github.com/fluidicon.png\" title=\"GitHub\">    <meta property=\"fb:app_id\" content=\"1401488693436528\">    <meta name=\"apple-itunes-app\" content=\"app-id=1477376905, app-argument=https://github.com/lucidrains/vector-quantize-pytorch\" />      <meta name=\"twitter:image:src\" content=\"https://opengraph.githubassets.com/63e6f9cc044617178cc803d91307fb9c2dfad78ca5da66c6bd3b139b7ac95033/lucidrains/vector-quantize-pytorch\" /><meta name=\"twitter:site\" content=\"@github\" /><meta name=\"twitter:card\" content=\"summary_large_image\" /><meta name=\"twitter:title\" content=\"GitHub - lucidrains/vector-quantize-pytorch: Vector (and Scalar) Quantization, in Pytorch\" /><meta name=\"twitter:description\" content=\"Vector (and Scalar) Quantization, in Pytorch. Contribute to lucidrains/vector-quantize-pytorch development by creating an account on GitHub.\" />      <meta property=\"og:image\" content=\"https://opengraph.githubassets.com/63e6f9cc044617178cc803d91307fb9c2dfad78ca5da66c6bd3b139b7ac95033/lucidrains/vector-quantize-pytorch\" /><meta property=\"og:image:alt\" content=\"Vector (and Scalar) Quantization, in Pytorch. Contribute to lucidrains/vector-quantize-pytorch development by creating an account on GitHub.\" /><meta property=\"og:image:width\" content=\"1200\" /><meta property=\"og:image:height\" content=\"600\" /><meta property=\"og:site_name\" content=\"GitHub\" /><meta property=\"og:type\" content=\"object\" /><meta property=\"og:title\" content=\"GitHub - lucidrains/vector-quantize-pytorch: Vector (and Scalar) Quantization, in Pytorch\" /><meta property=\"og:url\" content=\"https://github.com/lucidrains/vector-quantize-pytorch\" /><meta property=\"og:description\" content=\"Vector (and Scalar) Quantization, in Pytorch. Contribute to lucidrains/vector-quantize-pytorch development by creating an account on GitHub.\" />              <meta name=\"hostname\" content=\"github.com\">        <meta name=\"expected-hostname\" content=\"github.com\">  <meta http-equiv=\"x-pjax-version\" content=\"b9fa4cafade57d606c6dcfafff1d08bd597980af7b9837ed473fdf0cdea8a3bc\" data-turbo-track=\"reload\">  <meta http-equiv=\"x-pjax-csp-version\" content=\"5dcfbec3488c5fd5a334e287ce6a17058b7d4beb91db2d4d184e4d55bbf1d7d7\" data-turbo-track=\"reload\">  <meta http-equiv=\"x-pjax-css-version\" content=\"d33c7c2fcff40783f3002896023f41e2c17ec62b12ddbe7434e2001d743fb853\" data-turbo-track=\"reload\">  <meta http-equiv=\"x-pjax-js-version\" content=\"4ba4a7cc07194c8d5f24291dea4fbc790ffd83ba40beacaf8d0117187b571b4d\" data-turbo-track=\"reload\">  <meta name=\"turbo-cache-control\" content=\"no-preview\" data-turbo-transient=\"\">      <meta data-hydrostats=\"publish\">  <meta name=\"go-import\" content=\"github.com/lucidrains/vector-quantize-pytorch git https://github.com/lucidrains/vector-quantize-pytorch.git\">  <meta name=\"octolytics-dimension-user_id\" content=\"108653\" /><meta name=\"octolytics-dimension-user_login\" content=\"lucidrains\" /><meta name=\"octolytics-dimension-repository_id\" content=\"271090649\" /><meta name=\"octolytics-dimension-repository_nwo\" content=\"lucidrains/vector-quantize-pytorch\" /><meta name=\"octolytics-dimension-repository_public\" content=\"true\" /><meta name=\"octolytics-dimension-repository_is_fork\" content=\"false\" /><meta name=\"octolytics-dimension-repository_network_root_id\" content=\"271090649\" /><meta name=\"octolytics-dimension-repository_network_root_nwo\" content=\"lucidrains/vector-quantize-pytorch\" />    <link rel=\"canonical\" href=\"https://github.com/lucidrains/vector-quantize-pytorch\" data-turbo-transient>  <meta name=\"turbo-body-classes\" content=\"logged-out env-production page-responsive\">  <meta name=\"browser-stats-url\" content=\"https://api.github.com/_private/browser/stats\">  <meta name=\"browser-errors-url\" content=\"https://api.github.com/_private/browser/errors\">  <link rel=\"mask-icon\" href=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" color=\"#000000\">  <link rel=\"alternate icon\" class=\"js-site-favicon\" type=\"image/png\" href=\"https://github.githubassets.com/favicons/favicon.png\">  <link rel=\"icon\" class=\"js-site-favicon\" type=\"image/svg+xml\" href=\"https://github.githubassets.com/favicons/favicon.svg\"><meta name=\"theme-color\" content=\"#1e2327\"><meta name=\"color-scheme\" content=\"light dark\" />  <link rel=\"manifest\" href=\"/manifest.json\" crossOrigin=\"use-credentials\">  </head>  <body class=\"logged-out env-production page-responsive\" style=\"word-wrap: break-word;\">    <div data-turbo-body class=\"logged-out env-production page-responsive\" style=\"word-wrap: break-word;\">          <div class=\"position-relative js-header-wrapper \">      <a href=\"#start-of-content\" class=\"px-2 py-4 color-bg-accent-emphasis color-fg-on-emphasis show-on-focus js-skip-to-content\">Skip to content</a>      <span data-view-component=\"true\" class=\"progress-pjax-loader Progress position-fixed width-full\">    <span style=\"width: 0%;\" data-view-component=\"true\" class=\"Progress-item progress-pjax-loader-bar left-0 top-0 color-bg-accent-emphasis\"></span></span>              <script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Button_IconButton_js-node_modules_primer_react_lib--23bcad-a89698f38643.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/keyboard-shortcuts-dialog-a23eda2bcf8d.js\"></script><react-partial  partial-name=\"keyboard-shortcuts-dialog\"  data-ssr=\"false\">    <script type=\"application/json\" data-target=\"react-partial.embeddedData\">{\"props\":{}}</script>  <div data-target=\"react-partial.reactRoot\"></div></react-partial>                          <script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_delegated-events_dist_inde-94fd67-99519581d0f8.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/sessions-585a7232e50a.js\"></script><header class=\"Header-old header-logged-out js-details-container Details position-relative f4 py-3\" role=\"banner\" data-color-mode=light data-light-theme=light data-dark-theme=dark>  <button type=\"button\" class=\"Header-backdrop d-lg-none border-0 position-fixed top-0 left-0 width-full height-full js-details-target\" aria-label=\"Toggle navigation\">    <span class=\"d-none\">Toggle navigation</span>  </button>  <div class=\" d-flex flex-column flex-lg-row flex-items-center p-responsive height-full position-relative z-1\">    <div class=\"d-flex flex-justify-between flex-items-center width-full width-lg-auto\">      <a class=\"mr-lg-3 color-fg-inherit flex-order-2\" href=\"https://github.com/\" aria-label=\"Homepage\" data-ga-click=\"(Logged out) Header, go to homepage, icon:logo-wordmark\">        <svg height=\"32\" aria-hidden=\"true\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"32\" data-view-component=\"true\" class=\"octicon octicon-mark-github\">    <path d=\"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z\"></path></svg>      </a>      <div class=\"flex-1\">        <a href=\"/login?return_to=https%3A%2F%2Fgithub.com%2Flucidrains%2Fvector-quantize-pytorch\"          class=\"d-inline-block d-lg-none flex-order-1 f5 no-underline border color-border-default rounded-2 px-2 py-1 color-fg-inherit\"          data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/vector-quantize-pytorch&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"e098aedcc7adde0b1d3fca9179e1b427144554eda2a9f4fa2c99e6691399b2d2\"          data-ga-click=\"(Logged out) Header, clicked Sign in, text:sign-in\">          Sign in        </a>      </div>      <div class=\"flex-1 flex-order-2 text-right\">        <button aria-label=\"Toggle navigation\" aria-expanded=\"false\" type=\"button\" data-view-component=\"true\" class=\"js-details-target Button--link Button--medium Button d-lg-none color-fg-inherit p-1\">  <span class=\"Button-content\">    <span class=\"Button-label\"><div class=\"HeaderMenu-toggle-bar rounded my-1\"></div>            <div class=\"HeaderMenu-toggle-bar rounded my-1\"></div>            <div class=\"HeaderMenu-toggle-bar rounded my-1\"></div></span>  </span></button>      </div>    </div>    <div class=\"HeaderMenu--logged-out p-responsive height-fit position-lg-relative d-lg-flex flex-column flex-auto pt-7 pb-4 top-0\">      <div class=\"header-menu-wrapper d-flex flex-column flex-self-end flex-lg-row flex-justify-between flex-auto p-3 p-lg-0 rounded rounded-lg-0 mt-3 mt-lg-0\">          <nav class=\"mt-0 px-3 px-lg-0 mb-3 mb-lg-0\" aria-label=\"Global\">            <ul class=\"d-lg-flex list-style-none\">                <li class=\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\">      <button type=\"button\" class=\"HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target\" aria-expanded=\"false\">        Product        <svg opacity=\"0.5\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-chevron-down HeaderMenu-icon ml-1\">    <path d=\"M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z\"></path></svg>      </button>      <div class=\"HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 d-lg-flex dropdown-menu-wide\">          <div class=\"px-lg-4 border-lg-right mb-4 mb-lg-0 pr-lg-7\">            <ul class=\"list-style-none f5\" >                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}\" href=\"/features/actions\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-workflow color-fg-subtle mr-3\">    <path d=\"M1 3a2 2 0 0 1 2-2h6.5a2 2 0 0 1 2 2v6.5a2 2 0 0 1-2 2H7v4.063C7 16.355 7.644 17 8.438 17H12.5v-2.5a2 2 0 0 1 2-2H21a2 2 0 0 1 2 2V21a2 2 0 0 1-2 2h-6.5a2 2 0 0 1-2-2v-2.5H8.437A2.939 2.939 0 0 1 5.5 15.562V11.5H3a2 2 0 0 1-2-2Zm2-.5a.5.5 0 0 0-.5.5v6.5a.5.5 0 0 0 .5.5h6.5a.5.5 0 0 0 .5-.5V3a.5.5 0 0 0-.5-.5ZM14.5 14a.5.5 0 0 0-.5.5V21a.5.5 0 0 0 .5.5H21a.5.5 0 0 0 .5-.5v-6.5a.5.5 0 0 0-.5-.5Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Actions</div>        Automate any workflow      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}\" href=\"/features/packages\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-package color-fg-subtle mr-3\">    <path d=\"M12.876.64V.639l8.25 4.763c.541.313.875.89.875 1.515v9.525a1.75 1.75 0 0 1-.875 1.516l-8.25 4.762a1.748 1.748 0 0 1-1.75 0l-8.25-4.763a1.75 1.75 0 0 1-.875-1.515V6.917c0-.625.334-1.202.875-1.515L11.126.64a1.748 1.748 0 0 1 1.75 0Zm-1 1.298L4.251 6.34l7.75 4.474 7.75-4.474-7.625-4.402a.248.248 0 0 0-.25 0Zm.875 19.123 7.625-4.402a.25.25 0 0 0 .125-.216V7.639l-7.75 4.474ZM3.501 7.64v8.803c0 .09.048.172.125.216l7.625 4.402v-8.947Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Packages</div>        Host and manage packages      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}\" href=\"/features/security\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-shield-check color-fg-subtle mr-3\">    <path d=\"M16.53 9.78a.75.75 0 0 0-1.06-1.06L11 13.19l-1.97-1.97a.75.75 0 0 0-1.06 1.06l2.5 2.5a.75.75 0 0 0 1.06 0l5-5Z\"></path><path d=\"m12.54.637 8.25 2.675A1.75 1.75 0 0 1 22 4.976V10c0 6.19-3.771 10.704-9.401 12.83a1.704 1.704 0 0 1-1.198 0C5.77 20.705 2 16.19 2 10V4.976c0-.758.489-1.43 1.21-1.664L11.46.637a1.748 1.748 0 0 1 1.08 0Zm-.617 1.426-8.25 2.676a.249.249 0 0 0-.173.237V10c0 5.46 3.28 9.483 8.43 11.426a.199.199 0 0 0 .14 0C17.22 19.483 20.5 15.461 20.5 10V4.976a.25.25 0 0 0-.173-.237l-8.25-2.676a.253.253 0 0 0-.154 0Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Security</div>        Find and fix vulnerabilities      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}\" href=\"/features/codespaces\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-codespaces color-fg-subtle mr-3\">    <path d=\"M3.5 3.75C3.5 2.784 4.284 2 5.25 2h13.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 18.75 13H5.25a1.75 1.75 0 0 1-1.75-1.75Zm-2 12c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v4a1.75 1.75 0 0 1-1.75 1.75H3.25a1.75 1.75 0 0 1-1.75-1.75ZM5.25 3.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h13.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Zm-2 12a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h17.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25Z\"></path><path d=\"M10 17.75a.75.75 0 0 1 .75-.75h6.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Codespaces</div>        Instant dev environments      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}\" href=\"/features/copilot\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-copilot color-fg-subtle mr-3\">    <path d=\"M23.922 16.992c-.861 1.495-5.859 5.023-11.922 5.023-6.063 0-11.061-3.528-11.922-5.023A.641.641 0 0 1 0 16.736v-2.869a.841.841 0 0 1 .053-.22c.372-.935 1.347-2.292 2.605-2.656.167-.429.414-1.055.644-1.517a10.195 10.195 0 0 1-.052-1.086c0-1.331.282-2.499 1.132-3.368.397-.406.89-.717 1.474-.952 1.399-1.136 3.392-2.093 6.122-2.093 2.731 0 4.767.957 6.166 2.093.584.235 1.077.546 1.474.952.85.869 1.132 2.037 1.132 3.368 0 .368-.014.733-.052 1.086.23.462.477 1.088.644 1.517 1.258.364 2.233 1.721 2.605 2.656a.832.832 0 0 1 .053.22v2.869a.641.641 0 0 1-.078.256ZM12.172 11h-.344a4.323 4.323 0 0 1-.355.508C10.703 12.455 9.555 13 7.965 13c-1.725 0-2.989-.359-3.782-1.259a2.005 2.005 0 0 1-.085-.104L4 11.741v6.585c1.435.779 4.514 2.179 8 2.179 3.486 0 6.565-1.4 8-2.179v-6.585l-.098-.104s-.033.045-.085.104c-.793.9-2.057 1.259-3.782 1.259-1.59 0-2.738-.545-3.508-1.492a4.323 4.323 0 0 1-.355-.508h-.016.016Zm.641-2.935c.136 1.057.403 1.913.878 2.497.442.544 1.134.938 2.344.938 1.573 0 2.292-.337 2.657-.751.384-.435.558-1.15.558-2.361 0-1.14-.243-1.847-.705-2.319-.477-.488-1.319-.862-2.824-1.025-1.487-.161-2.192.138-2.533.529-.269.307-.437.808-.438 1.578v.021c0 .265.021.562.063.893Zm-1.626 0c.042-.331.063-.628.063-.894v-.02c-.001-.77-.169-1.271-.438-1.578-.341-.391-1.046-.69-2.533-.529-1.505.163-2.347.537-2.824 1.025-.462.472-.705 1.179-.705 2.319 0 1.211.175 1.926.558 2.361.365.414 1.084.751 2.657.751 1.21 0 1.902-.394 2.344-.938.475-.584.742-1.44.878-2.497Z\"></path><path d=\"M14.5 14.25a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Zm-5 0a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Copilot</div>        Write better code with AI      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}\" href=\"/features/code-review\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-code-review color-fg-subtle mr-3\">    <path d=\"M10.3 6.74a.75.75 0 0 1-.04 1.06l-2.908 2.7 2.908 2.7a.75.75 0 1 1-1.02 1.1l-3.5-3.25a.75.75 0 0 1 0-1.1l3.5-3.25a.75.75 0 0 1 1.06.04Zm3.44 1.06a.75.75 0 1 1 1.02-1.1l3.5 3.25a.75.75 0 0 1 0 1.1l-3.5 3.25a.75.75 0 1 1-1.02-1.1l2.908-2.7-2.908-2.7Z\"></path><path d=\"M1.5 4.25c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v12.5a1.75 1.75 0 0 1-1.75 1.75h-9.69l-3.573 3.573A1.458 1.458 0 0 1 5 21.043V18.5H3.25a1.75 1.75 0 0 1-1.75-1.75ZM3.25 4a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h2.5a.75.75 0 0 1 .75.75v3.19l3.72-3.72a.749.749 0 0 1 .53-.22h10a.25.25 0 0 0 .25-.25V4.25a.25.25 0 0 0-.25-.25Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Code review</div>        Manage code changes      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}\" href=\"/features/issues\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-issue-opened color-fg-subtle mr-3\">    <path d=\"M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1ZM2.5 12a9.5 9.5 0 0 0 9.5 9.5 9.5 9.5 0 0 0 9.5-9.5A9.5 9.5 0 0 0 12 2.5 9.5 9.5 0 0 0 2.5 12Zm9.5 2a2 2 0 1 1-.001-3.999A2 2 0 0 1 12 14Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Issues</div>        Plan and track work      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}\" href=\"/features/discussions\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-comment-discussion color-fg-subtle mr-3\">    <path d=\"M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 14.25 14H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 15.543V14H1.75A1.75 1.75 0 0 1 0 12.25v-9.5C0 1.784.784 1 1.75 1ZM1.5 2.75v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Z\"></path><path d=\"M22.5 8.75a.25.25 0 0 0-.25-.25h-3.5a.75.75 0 0 1 0-1.5h3.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 22.25 20H21v1.543a1.457 1.457 0 0 1-2.487 1.03L15.939 20H10.75A1.75 1.75 0 0 1 9 18.25v-1.465a.75.75 0 0 1 1.5 0v1.465c0 .138.112.25.25.25h5.5a.75.75 0 0 1 .53.22l2.72 2.72v-2.19a.75.75 0 0 1 .75-.75h2a.25.25 0 0 0 .25-.25v-9.5Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Discussions</div>        Collaborate outside of code      </div>    </a></li>            </ul>          </div>          <div class=\"px-lg-4\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"product-explore-heading\">Explore</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"product-explore-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to All features&quot;,&quot;label&quot;:&quot;ref_cta:All features;&quot;}\" href=\"/features\">      All features    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Documentation&quot;,&quot;label&quot;:&quot;ref_cta:Documentation;&quot;}\" href=\"https://docs.github.com\">      Documentation    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to GitHub Skills&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Skills;&quot;}\" href=\"https://skills.github.com/\">      GitHub Skills    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Blog&quot;,&quot;label&quot;:&quot;ref_cta:Blog;&quot;}\" href=\"https://github.blog\">      Blog    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>            </ul>          </div>      </div></li>                <li class=\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\">      <button type=\"button\" class=\"HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target\" aria-expanded=\"false\">        Solutions        <svg opacity=\"0.5\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-chevron-down HeaderMenu-icon ml-1\">    <path d=\"M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z\"></path></svg>      </button>      <div class=\"HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 px-lg-4\">          <div class=\"border-bottom pb-3 mb-3\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"solutions-for-heading\">For</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"solutions-for-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Enterprise&quot;,&quot;label&quot;:&quot;ref_cta:Enterprise;&quot;}\" href=\"/enterprise\">      Enterprise    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Teams&quot;,&quot;label&quot;:&quot;ref_cta:Teams;&quot;}\" href=\"/team\">      Teams    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Startups&quot;,&quot;label&quot;:&quot;ref_cta:Startups;&quot;}\" href=\"/enterprise/startups\">      Startups    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Education&quot;,&quot;label&quot;:&quot;ref_cta:Education;&quot;}\" href=\"https://education.github.com\">      Education    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>            </ul>          </div>          <div class=\"border-bottom pb-3 mb-3\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"solutions-by-solution-heading\">By Solution</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"solutions-by-solution-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to CI/CD &amp;amp; Automation&quot;,&quot;label&quot;:&quot;ref_cta:CI/CD &amp;amp; Automation;&quot;}\" href=\"/solutions/ci-cd/\">      CI/CD &amp; Automation    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to DevOps&quot;,&quot;label&quot;:&quot;ref_cta:DevOps;&quot;}\" href=\"/solutions/devops/\">      DevOps    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to DevSecOps&quot;,&quot;label&quot;:&quot;ref_cta:DevSecOps;&quot;}\" href=\"https://resources.github.com/devops/fundamentals/devsecops/\">      DevSecOps    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>            </ul>          </div>          <div class=\"\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"solutions-resources-heading\">Resources</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"solutions-resources-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Learning Pathways&quot;,&quot;label&quot;:&quot;ref_cta:Learning Pathways;&quot;}\" href=\"https://resources.github.com/learn/pathways/\">      Learning Pathways    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to White papers, Ebooks, Webinars&quot;,&quot;label&quot;:&quot;ref_cta:White papers, Ebooks, Webinars;&quot;}\" href=\"https://resources.github.com/\">      White papers, Ebooks, Webinars    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Customer Stories&quot;,&quot;label&quot;:&quot;ref_cta:Customer Stories;&quot;}\" href=\"/customer-stories\">      Customer Stories    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Partners&quot;,&quot;label&quot;:&quot;ref_cta:Partners;&quot;}\" href=\"https://partner.github.com/\">      Partners    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>            </ul>          </div>      </div></li>                <li class=\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\">      <button type=\"button\" class=\"HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target\" aria-expanded=\"false\">        Open Source        <svg opacity=\"0.5\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-chevron-down HeaderMenu-icon ml-1\">    <path d=\"M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z\"></path></svg>      </button>      <div class=\"HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 px-lg-4\">          <div class=\"border-bottom pb-3 mb-3\">            <ul class=\"list-style-none f5\" >                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}\" href=\"/sponsors\">            <div>        <div class=\"color-fg-default h4\">GitHub Sponsors</div>        Fund open source developers      </div>    </a></li>            </ul>          </div>          <div class=\"border-bottom pb-3 mb-3\">            <ul class=\"list-style-none f5\" >                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}\" href=\"/readme\">            <div>        <div class=\"color-fg-default h4\">The ReadME Project</div>        GitHub community articles      </div>    </a></li>            </ul>          </div>          <div class=\"\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"open-source-repositories-heading\">Repositories</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"open-source-repositories-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Topics&quot;,&quot;label&quot;:&quot;ref_cta:Topics;&quot;}\" href=\"/topics\">      Topics    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Trending&quot;,&quot;label&quot;:&quot;ref_cta:Trending;&quot;}\" href=\"/trending\">      Trending    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Collections&quot;,&quot;label&quot;:&quot;ref_cta:Collections;&quot;}\" href=\"/collections\">      Collections    </a></li>            </ul>          </div>      </div></li>                <li class=\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\">    <a class=\"HeaderMenu-link no-underline px-0 px-lg-2 py-3 py-lg-2 d-block d-lg-inline-block\" data-analytics-event=\"{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}\" href=\"/pricing\">Pricing</a></li>            </ul>          </nav>        <div class=\"d-lg-flex flex-items-center mb-3 mb-lg-0 text-center text-lg-left ml-3\" style=\"\">                <qbsearch-input class=\"search-input\" data-scope=\"repo:lucidrains/vector-quantize-pytorch\" data-custom-scopes-path=\"/search/custom_scopes\" data-delete-custom-scopes-csrf=\"FWdxvWuZY1u9K73EB4p2QhWX109uisYQZQBay3xSUnKX3sGG4sQ51tY81yZDogckytkPUK3q1DTK2fDQbwMU1w\" data-max-custom-scopes=\"10\" data-header-redesign-enabled=\"false\" data-initial-value=\"\" data-blackbird-suggestions-path=\"/search/suggestions\" data-jump-to-suggestions-path=\"/_graphql/GetSuggestedNavigationDestinations\" data-current-repository=\"lucidrains/vector-quantize-pytorch\" data-current-org=\"\" data-current-owner=\"lucidrains\" data-logged-in=\"false\" data-copilot-chat-enabled=\"false\" data-blackbird-indexed-repo-csrf=\"<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=SoWpJ%2BL2hKJ0EhDyCxvo360hZXx98eqLs%2FK49z%2B%2Fm%2BDjvZEMa3DWlxNUyA7TGLbiciS7oPa8N7jPdprvY5MQ4D40hVrq928gI1dC%2Bw7qRIyNefsHjL8ySRQjndJ9N7I%2BBJMnGVFI4tw%2F%2F%2FSVnM7Jyo5Mxz%2BVhjVkY9CVO3HYFr5yCel%2BD8ydiI%2Fs%2FVam4XHyg2XYCZfV08V7gI4FILkzGxw3Hl5xv8jjq%2F8mFbn9ACsxOXzhrv1diEW7Q7%2FwZzgswEaLja4ttNxeESVCoNCP3YJJ%2FVKuJhmSapMU91ZDJhHzJAN%2Fw%2Fu8iaHXQY99SYuID0N%2FI3p7xkWR5QNpVEBLvvVRH9iHx0E1GaH3yQmwbipZYL5zQNj1btQRU6pF%2BJR%2BFFjDyWxoNAoN5VlrQ9LWNdNfhqaS3yGfaVOaeDzk2Fp553uqEKFCAwIhjnGdhkAltBZR7fbiwgDXCWsyiZVkjcwzCTDdBeBU4C2PJ6%2FDvyKWGMVq6mORMpE0TvU%2FYyK6Iebcg%2BQ%2BP2KH5TtZ6BIyVduYdRcTnbTFqjCHAimjWZU6WuQMGd0%3D--GCdklgpvaG2E8OqE--oIGcVNCAoBOdayBr08M6kw%3D%3D&quot; />\">  <div    class=\"search-input-container search-with-dialog position-relative d-flex flex-row flex-items-center mr-4 rounded\"    data-action=\"click:qbsearch-input#searchInputContainerClicked\"  >      <button        type=\"button\"        class=\"header-search-button placeholder  input-button form-control d-flex flex-1 flex-self-stretch flex-items-center no-wrap width-full py-0 pl-2 pr-0 text-left border-0 box-shadow-none\"        data-target=\"qbsearch-input.inputButton\"        placeholder=\"Search or jump to...\"        data-hotkey=s,/        autocapitalize=\"off\"        data-action=\"click:qbsearch-input#handleExpand\"      >        <div class=\"mr-2 color-fg-muted\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-search\">    <path d=\"M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z\"></path></svg>        </div>        <span class=\"flex-1\" data-target=\"qbsearch-input.inputButtonText\">Search or jump to...</span>          <div class=\"d-flex\" data-target=\"qbsearch-input.hotkeyIndicator\">            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"22\" height=\"20\" aria-hidden=\"true\" class=\"mr-1\"><path fill=\"none\" stroke=\"#979A9C\" opacity=\".4\" d=\"M3.5.5h12c1.7 0 3 1.3 3 3v13c0 1.7-1.3 3-3 3h-12c-1.7 0-3-1.3-3-3v-13c0-1.7 1.3-3 3-3z\"></path><path fill=\"#979A9C\" d=\"M11.8 6L8 15.1h-.9L10.8 6h1z\"></path></svg>          </div>      </button>    <input type=\"hidden\" name=\"type\" class=\"js-site-search-type-field\">    <div class=\"Overlay--hidden \" data-modal-dialog-overlay>  <modal-dialog data-action=\"close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose\" data-target=\"qbsearch-input.searchSuggestionsDialog\" role=\"dialog\" id=\"search-suggestions-dialog\" aria-modal=\"true\" aria-labelledby=\"search-suggestions-dialog-header\" data-view-component=\"true\" class=\"Overlay Overlay--width-large Overlay--height-auto\">      <h1 id=\"search-suggestions-dialog-header\" class=\"sr-only\">Search code, repositories, users, issues, pull requests...</h1>    <div class=\"Overlay-body Overlay-body--paddingNone\">                <div data-view-component=\"true\">        <div class=\"search-suggestions position-fixed width-full color-shadow-large border color-fg-default color-bg-default overflow-hidden d-flex flex-column query-builder-container\"          style=\"border-radius: 12px;\"          data-target=\"qbsearch-input.queryBuilderContainer\"          hidden        >          <!-- '\"` --><!-- </textarea></xmp> --></option></form><form id=\"query-builder-test-form\" action=\"\" accept-charset=\"UTF-8\" method=\"get\">  <query-builder data-target=\"qbsearch-input.queryBuilder\" id=\"query-builder-query-builder-test\" data-filter-key=\":\" data-view-component=\"true\" class=\"QueryBuilder search-query-builder\">    <div class=\"FormControl FormControl--fullWidth\">      <label id=\"query-builder-test-label\" for=\"query-builder-test\" class=\"FormControl-label sr-only\">        Search      </label>      <div        class=\"QueryBuilder-StyledInput width-fit \"        data-target=\"query-builder.styledInput\"      >          <span id=\"query-builder-test-leadingvisual-wrap\" class=\"FormControl-input-leadingVisualWrap QueryBuilder-leadingVisualWrap\">            <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-search FormControl-input-leadingVisual\">    <path d=\"M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z\"></path></svg>          </span>        <div data-target=\"query-builder.styledInputContainer\" class=\"QueryBuilder-StyledInputContainer\">          <div            aria-hidden=\"true\"            class=\"QueryBuilder-StyledInputContent\"            data-target=\"query-builder.styledInputContent\"          ></div>          <div class=\"QueryBuilder-InputWrapper\">            <div aria-hidden=\"true\" class=\"QueryBuilder-Sizer\" data-target=\"query-builder.sizer\"></div>            <input id=\"query-builder-test\" name=\"query-builder-test\" value=\"\" autocomplete=\"off\" type=\"text\" role=\"combobox\" spellcheck=\"false\" aria-expanded=\"false\" aria-describedby=\"validation-cbaf08ae-5c93-481c-a182-adf3a4afc79d\" data-target=\"query-builder.input\" data-action=\"          input:query-builder#inputChange          blur:query-builder#inputBlur          keydown:query-builder#inputKeydown          focus:query-builder#inputFocus        \" data-view-component=\"true\" class=\"FormControl-input QueryBuilder-Input FormControl-medium\" />          </div>        </div>          <span class=\"sr-only\" id=\"query-builder-test-clear\">Clear</span>          <button role=\"button\" id=\"query-builder-test-clear-button\" aria-labelledby=\"query-builder-test-clear query-builder-test-label\" data-target=\"query-builder.clearButton\" data-action=\"                click:query-builder#clear                focus:query-builder#clearButtonFocus                blur:query-builder#clearButtonBlur              \" variant=\"small\" hidden=\"hidden\" type=\"button\" data-view-component=\"true\" class=\"Button Button--iconOnly Button--invisible Button--medium mr-1 px-2 py-0 d-flex flex-items-center rounded-1 color-fg-muted\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x-circle-fill Button-visual\">    <path d=\"M2.343 13.657A8 8 0 1 1 13.658 2.343 8 8 0 0 1 2.343 13.657ZM6.03 4.97a.751.751 0 0 0-1.042.018.751.751 0 0 0-.018 1.042L6.94 8 4.97 9.97a.749.749 0 0 0 .326 1.275.749.749 0 0 0 .734-.215L8 9.06l1.97 1.97a.749.749 0 0 0 1.275-.326.749.749 0 0 0-.215-.734L9.06 8l1.97-1.97a.749.749 0 0 0-.326-1.275.749.749 0 0 0-.734.215L8 6.94Z\"></path></svg></button>      </div>      <template id=\"search-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-search\">    <path d=\"M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z\"></path></svg></template><template id=\"code-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code\">    <path d=\"m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z\"></path></svg></template><template id=\"file-code-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-file-code\">    <path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0 1 14.25 15h-9a.75.75 0 0 1 0-1.5h9a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 10 4.25V1.5H5.75a.25.25 0 0 0-.25.25v2.5a.75.75 0 0 1-1.5 0Zm1.72 4.97a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734l1.47-1.47-1.47-1.47a.75.75 0 0 1 0-1.06ZM3.28 7.78 1.81 9.25l1.47 1.47a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018l-2-2a.75.75 0 0 1 0-1.06l2-2a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Zm8.22-6.218V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg></template><template id=\"history-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-history\">    <path d=\"m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z\"></path></svg></template><template id=\"repo-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo\">    <path d=\"M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z\"></path></svg></template><template id=\"bookmark-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-bookmark\">    <path d=\"M3 2.75C3 1.784 3.784 1 4.75 1h6.5c.966 0 1.75.784 1.75 1.75v11.5a.75.75 0 0 1-1.227.579L8 11.722l-3.773 3.107A.751.751 0 0 1 3 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.91l3.023-2.489a.75.75 0 0 1 .954 0l3.023 2.49V2.75a.25.25 0 0 0-.25-.25Z\"></path></svg></template><template id=\"plus-circle-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-plus-circle\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm7.25-3.25v2.5h2.5a.75.75 0 0 1 0 1.5h-2.5v2.5a.75.75 0 0 1-1.5 0v-2.5h-2.5a.75.75 0 0 1 0-1.5h2.5v-2.5a.75.75 0 0 1 1.5 0Z\"></path></svg></template><template id=\"circle-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-dot-fill\">    <path d=\"M8 4a4 4 0 1 1 0 8 4 4 0 0 1 0-8Z\"></path></svg></template><template id=\"trash-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-trash\">    <path d=\"M11 1.75V3h2.25a.75.75 0 0 1 0 1.5H2.75a.75.75 0 0 1 0-1.5H5V1.75C5 .784 5.784 0 6.75 0h2.5C10.216 0 11 .784 11 1.75ZM4.496 6.675l.66 6.6a.25.25 0 0 0 .249.225h5.19a.25.25 0 0 0 .249-.225l.66-6.6a.75.75 0 0 1 1.492.149l-.66 6.6A1.748 1.748 0 0 1 10.595 15h-5.19a1.75 1.75 0 0 1-1.741-1.575l-.66-6.6a.75.75 0 1 1 1.492-.15ZM6.5 1.75V3h3V1.75a.25.25 0 0 0-.25-.25h-2.5a.25.25 0 0 0-.25.25Z\"></path></svg></template><template id=\"team-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-people\">    <path d=\"M2 5.5a3.5 3.5 0 1 1 5.898 2.549 5.508 5.508 0 0 1 3.034 4.084.75.75 0 1 1-1.482.235 4 4 0 0 0-7.9 0 .75.75 0 0 1-1.482-.236A5.507 5.507 0 0 1 3.102 8.05 3.493 3.493 0 0 1 2 5.5ZM11 4a3.001 3.001 0 0 1 2.22 5.018 5.01 5.01 0 0 1 2.56 3.012.749.749 0 0 1-.885.954.752.752 0 0 1-.549-.514 3.507 3.507 0 0 0-2.522-2.372.75.75 0 0 1-.574-.73v-.352a.75.75 0 0 1 .416-.672A1.5 1.5 0 0 0 11 5.5.75.75 0 0 1 11 4Zm-5.5-.5a2 2 0 1 0-.001 3.999A2 2 0 0 0 5.5 3.5Z\"></path></svg></template><template id=\"project-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-project\">    <path d=\"M1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0ZM1.5 1.75v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25ZM11.75 3a.75.75 0 0 1 .75.75v7.5a.75.75 0 0 1-1.5 0v-7.5a.75.75 0 0 1 .75-.75Zm-8.25.75a.75.75 0 0 1 1.5 0v5.5a.75.75 0 0 1-1.5 0ZM8 3a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 3Z\"></path></svg></template><template id=\"pencil-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-pencil\">    <path d=\"M11.013 1.427a1.75 1.75 0 0 1 2.474 0l1.086 1.086a1.75 1.75 0 0 1 0 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 0 1-.927-.928l.929-3.25c.081-.286.235-.547.445-.758l8.61-8.61Zm.176 4.823L9.75 4.81l-6.286 6.287a.253.253 0 0 0-.064.108l-.558 1.953 1.953-.558a.253.253 0 0 0 .108-.064Zm1.238-3.763a.25.25 0 0 0-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 0 0 0-.354Z\"></path></svg></template><template id=\"copilot-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copilot\">    <path d=\"M7.998 15.035c-4.562 0-7.873-2.914-7.998-3.749V9.338c.085-.628.677-1.686 1.588-2.065.013-.07.024-.143.036-.218.029-.183.06-.384.126-.612-.201-.508-.254-1.084-.254-1.656 0-.87.128-1.769.693-2.484.579-.733 1.494-1.124 2.724-1.261 1.206-.134 2.262.034 2.944.765.05.053.096.108.139.165.044-.057.094-.112.143-.165.682-.731 1.738-.899 2.944-.765 1.23.137 2.145.528 2.724 1.261.566.715.693 1.614.693 2.484 0 .572-.053 1.148-.254 1.656.066.228.098.429.126.612.012.076.024.148.037.218.924.385 1.522 1.471 1.591 2.095v1.872c0 .766-3.351 3.795-8.002 3.795Zm0-1.485c2.28 0 4.584-1.11 5.002-1.433V7.862l-.023-.116c-.49.21-1.075.291-1.727.291-1.146 0-2.059-.327-2.71-.991A3.222 3.222 0 0 1 8 6.303a3.24 3.24 0 0 1-.544.743c-.65.664-1.563.991-2.71.991-.652 0-1.236-.081-1.727-.291l-.023.116v4.255c.419.323 2.722 1.433 5.002 1.433ZM6.762 2.83c-.193-.206-.637-.413-1.682-.297-1.019.113-1.479.404-1.713.7-.247.312-.369.789-.369 1.554 0 .793.129 1.171.308 1.371.162.181.519.379 1.442.379.853 0 1.339-.235 1.638-.54.315-.322.527-.827.617-1.553.117-.935-.037-1.395-.241-1.614Zm4.155-.297c-1.044-.116-1.488.091-1.681.297-.204.219-.359.679-.242 1.614.091.726.303 1.231.618 1.553.299.305.784.54 1.638.54.922 0 1.28-.198 1.442-.379.179-.2.308-.578.308-1.371 0-.765-.123-1.242-.37-1.554-.233-.296-.693-.587-1.713-.7Z\"></path><path d=\"M6.25 9.037a.75.75 0 0 1 .75.75v1.501a.75.75 0 0 1-1.5 0V9.787a.75.75 0 0 1 .75-.75Zm4.25.75v1.501a.75.75 0 0 1-1.5 0V9.787a.75.75 0 0 1 1.5 0Z\"></path></svg></template><template id=\"workflow-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-workflow\">    <path d=\"M0 1.75C0 .784.784 0 1.75 0h3.5C6.216 0 7 .784 7 1.75v3.5A1.75 1.75 0 0 1 5.25 7H4v4a1 1 0 0 0 1 1h4v-1.25C9 9.784 9.784 9 10.75 9h3.5c.966 0 1.75.784 1.75 1.75v3.5A1.75 1.75 0 0 1 14.25 16h-3.5A1.75 1.75 0 0 1 9 14.25v-.75H5A2.5 2.5 0 0 1 2.5 11V7h-.75A1.75 1.75 0 0 1 0 5.25Zm1.75-.25a.25.25 0 0 0-.25.25v3.5c0 .138.112.25.25.25h3.5a.25.25 0 0 0 .25-.25v-3.5a.25.25 0 0 0-.25-.25Zm9 9a.25.25 0 0 0-.25.25v3.5c0 .138.112.25.25.25h3.5a.25.25 0 0 0 .25-.25v-3.5a.25.25 0 0 0-.25-.25Z\"></path></svg></template><template id=\"book-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-book\">    <path d=\"M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z\"></path></svg></template><template id=\"code-review-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code-review\">    <path d=\"M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 13H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25v-8.5C0 1.784.784 1 1.75 1ZM1.5 2.75v8.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-8.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Zm5.28 1.72a.75.75 0 0 1 0 1.06L5.31 7l1.47 1.47a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018l-2-2a.75.75 0 0 1 0-1.06l2-2a.75.75 0 0 1 1.06 0Zm2.44 0a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L10.69 7 9.22 5.53a.75.75 0 0 1 0-1.06Z\"></path></svg></template><template id=\"codespaces-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-codespaces\">    <path d=\"M0 11.25c0-.966.784-1.75 1.75-1.75h12.5c.966 0 1.75.784 1.75 1.75v3A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25Zm2-9.5C2 .784 2.784 0 3.75 0h8.5C13.216 0 14 .784 14 1.75v5a1.75 1.75 0 0 1-1.75 1.75h-8.5A1.75 1.75 0 0 1 2 6.75Zm1.75-.25a.25.25 0 0 0-.25.25v5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-5a.25.25 0 0 0-.25-.25Zm-2 9.5a.25.25 0 0 0-.25.25v3c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-3a.25.25 0 0 0-.25-.25Z\"></path><path d=\"M7 12.75a.75.75 0 0 1 .75-.75h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z\"></path></svg></template><template id=\"comment-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-comment\">    <path d=\"M1 2.75C1 1.784 1.784 1 2.75 1h10.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 13.25 12H9.06l-2.573 2.573A1.458 1.458 0 0 1 4 13.543V12H2.75A1.75 1.75 0 0 1 1 10.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h4.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path></svg></template><template id=\"comment-discussion-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-comment-discussion\">    <path d=\"M1.75 1h8.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 10.25 10H7.061l-2.574 2.573A1.458 1.458 0 0 1 2 11.543V10h-.25A1.75 1.75 0 0 1 0 8.25v-5.5C0 1.784.784 1 1.75 1ZM1.5 2.75v5.5c0 .138.112.25.25.25h1a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h3.5a.25.25 0 0 0 .25-.25v-5.5a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25Zm13 2a.25.25 0 0 0-.25-.25h-.5a.75.75 0 0 1 0-1.5h.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 14.25 12H14v1.543a1.458 1.458 0 0 1-2.487 1.03L9.22 12.28a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l2.22 2.22v-2.19a.75.75 0 0 1 .75-.75h1a.25.25 0 0 0 .25-.25Z\"></path></svg></template><template id=\"organization-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-organization\">    <path d=\"M1.75 16A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0h8.5C11.216 0 12 .784 12 1.75v12.5c0 .085-.006.168-.018.25h2.268a.25.25 0 0 0 .25-.25V8.285a.25.25 0 0 0-.111-.208l-1.055-.703a.749.749 0 1 1 .832-1.248l1.055.703c.487.325.779.871.779 1.456v5.965A1.75 1.75 0 0 1 14.25 16h-3.5a.766.766 0 0 1-.197-.026c-.099.017-.2.026-.303.026h-3a.75.75 0 0 1-.75-.75V14h-1v1.25a.75.75 0 0 1-.75.75Zm-.25-1.75c0 .138.112.25.25.25H4v-1.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 .75.75v1.25h2.25a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25ZM3.75 6h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5ZM3 3.75A.75.75 0 0 1 3.75 3h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 3.75Zm4 3A.75.75 0 0 1 7.75 6h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 7 6.75ZM7.75 3h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5ZM3 9.75A.75.75 0 0 1 3.75 9h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 9.75ZM7.75 9h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5Z\"></path></svg></template><template id=\"rocket-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-rocket\">    <path d=\"M14.064 0h.186C15.216 0 16 .784 16 1.75v.186a8.752 8.752 0 0 1-2.564 6.186l-.458.459c-.314.314-.641.616-.979.904v3.207c0 .608-.315 1.172-.833 1.49l-2.774 1.707a.749.749 0 0 1-1.11-.418l-.954-3.102a1.214 1.214 0 0 1-.145-.125L3.754 9.816a1.218 1.218 0 0 1-.124-.145L.528 8.717a.749.749 0 0 1-.418-1.11l1.71-2.774A1.748 1.748 0 0 1 3.31 4h3.204c.288-.338.59-.665.904-.979l.459-.458A8.749 8.749 0 0 1 14.064 0ZM8.938 3.623h-.002l-.458.458c-.76.76-1.437 1.598-2.02 2.5l-1.5 2.317 2.143 2.143 2.317-1.5c.902-.583 1.74-1.26 2.499-2.02l.459-.458a7.25 7.25 0 0 0 2.123-5.127V1.75a.25.25 0 0 0-.25-.25h-.186a7.249 7.249 0 0 0-5.125 2.123ZM3.56 14.56c-.732.732-2.334 1.045-3.005 1.148a.234.234 0 0 1-.201-.064.234.234 0 0 1-.064-.201c.103-.671.416-2.273 1.15-3.003a1.502 1.502 0 1 1 2.12 2.12Zm6.94-3.935c-.088.06-.177.118-.266.175l-2.35 1.521.548 1.783 1.949-1.2a.25.25 0 0 0 .119-.213ZM3.678 8.116 5.2 5.766c.058-.09.117-.178.176-.266H3.309a.25.25 0 0 0-.213.119l-1.2 1.95ZM12 5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg></template><template id=\"shield-check-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-shield-check\">    <path d=\"m8.533.133 5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667l5.25-1.68a1.748 1.748 0 0 1 1.066 0Zm-.61 1.429.001.001-5.25 1.68a.251.251 0 0 0-.174.237V7c0 1.36.275 2.666 1.057 3.859.784 1.194 2.121 2.342 4.366 3.298a.196.196 0 0 0 .154 0c2.245-.957 3.582-2.103 4.366-3.297C13.225 9.666 13.5 8.358 13.5 7V3.48a.25.25 0 0 0-.174-.238l-5.25-1.68a.25.25 0 0 0-.153 0ZM11.28 6.28l-3.5 3.5a.75.75 0 0 1-1.06 0l-1.5-1.5a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l.97.97 2.97-2.97a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z\"></path></svg></template><template id=\"heart-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-heart\">    <path d=\"m8 14.25.345.666a.75.75 0 0 1-.69 0l-.008-.004-.018-.01a7.152 7.152 0 0 1-.31-.17 22.055 22.055 0 0 1-3.434-2.414C2.045 10.731 0 8.35 0 5.5 0 2.836 2.086 1 4.25 1 5.797 1 7.153 1.802 8 3.02 8.847 1.802 10.203 1 11.75 1 13.914 1 16 2.836 16 5.5c0 2.85-2.045 5.231-3.885 6.818a22.066 22.066 0 0 1-3.744 2.584l-.018.01-.006.003h-.002ZM4.25 2.5c-1.336 0-2.75 1.164-2.75 3 0 2.15 1.58 4.144 3.365 5.682A20.58 20.58 0 0 0 8 13.393a20.58 20.58 0 0 0 3.135-2.211C12.92 9.644 14.5 7.65 14.5 5.5c0-1.836-1.414-3-2.75-3-1.373 0-2.609.986-3.029 2.456a.749.749 0 0 1-1.442 0C6.859 3.486 5.623 2.5 4.25 2.5Z\"></path></svg></template><template id=\"server-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-server\">    <path d=\"M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v4c0 .372-.116.717-.314 1 .198.283.314.628.314 1v4a1.75 1.75 0 0 1-1.75 1.75H1.75A1.75 1.75 0 0 1 0 12.75v-4c0-.358.109-.707.314-1a1.739 1.739 0 0 1-.314-1v-4C0 1.784.784 1 1.75 1ZM1.5 2.75v4c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Zm.25 5.75a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25ZM7 4.75A.75.75 0 0 1 7.75 4h4.5a.75.75 0 0 1 0 1.5h-4.5A.75.75 0 0 1 7 4.75ZM7.75 10h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM3 4.75A.75.75 0 0 1 3.75 4h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 4.75ZM3.75 10h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5Z\"></path></svg></template><template id=\"globe-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-globe\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM5.78 8.75a9.64 9.64 0 0 0 1.363 4.177c.255.426.542.832.857 1.215.245-.296.551-.705.857-1.215A9.64 9.64 0 0 0 10.22 8.75Zm4.44-1.5a9.64 9.64 0 0 0-1.363-4.177c-.307-.51-.612-.919-.857-1.215a9.927 9.927 0 0 0-.857 1.215A9.64 9.64 0 0 0 5.78 7.25Zm-5.944 1.5H1.543a6.507 6.507 0 0 0 4.666 5.5c-.123-.181-.24-.365-.352-.552-.715-1.192-1.437-2.874-1.581-4.948Zm-2.733-1.5h2.733c.144-2.074.866-3.756 1.58-4.948.12-.197.237-.381.353-.552a6.507 6.507 0 0 0-4.666 5.5Zm10.181 1.5c-.144 2.074-.866 3.756-1.58 4.948-.12.197-.237.381-.353.552a6.507 6.507 0 0 0 4.666-5.5Zm2.733-1.5a6.507 6.507 0 0 0-4.666-5.5c.123.181.24.365.353.552.714 1.192 1.436 2.874 1.58 4.948Z\"></path></svg></template><template id=\"issue-opened-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-issue-opened\">    <path d=\"M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path><path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z\"></path></svg></template><template id=\"device-mobile-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-device-mobile\">    <path d=\"M3.75 0h8.5C13.216 0 14 .784 14 1.75v12.5A1.75 1.75 0 0 1 12.25 16h-8.5A1.75 1.75 0 0 1 2 14.25V1.75C2 .784 2.784 0 3.75 0ZM3.5 1.75v12.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25ZM8 13a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z\"></path></svg></template><template id=\"package-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-package\">    <path d=\"m8.878.392 5.25 3.045c.54.314.872.89.872 1.514v6.098a1.75 1.75 0 0 1-.872 1.514l-5.25 3.045a1.75 1.75 0 0 1-1.756 0l-5.25-3.045A1.75 1.75 0 0 1 1 11.049V4.951c0-.624.332-1.201.872-1.514L7.122.392a1.75 1.75 0 0 1 1.756 0ZM7.875 1.69l-4.63 2.685L8 7.133l4.755-2.758-4.63-2.685a.248.248 0 0 0-.25 0ZM2.5 5.677v5.372c0 .09.047.171.125.216l4.625 2.683V8.432Zm6.25 8.271 4.625-2.683a.25.25 0 0 0 .125-.216V5.677L8.75 8.432Z\"></path></svg></template><template id=\"credit-card-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-credit-card\">    <path d=\"M10.75 9a.75.75 0 0 0 0 1.5h1.5a.75.75 0 0 0 0-1.5h-1.5Z\"></path><path d=\"M0 3.75C0 2.784.784 2 1.75 2h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 14H1.75A1.75 1.75 0 0 1 0 12.25ZM14.5 6.5h-13v5.75c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25Zm0-2.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25V5h13Z\"></path></svg></template><template id=\"play-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-play\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z\"></path></svg></template><template id=\"gift-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-gift\">    <path d=\"M2 2.75A2.75 2.75 0 0 1 4.75 0c.983 0 1.873.42 2.57 1.232.268.318.497.668.68 1.042.183-.375.411-.725.68-1.044C9.376.42 10.266 0 11.25 0a2.75 2.75 0 0 1 2.45 4h.55c.966 0 1.75.784 1.75 1.75v2c0 .698-.409 1.301-1 1.582v4.918A1.75 1.75 0 0 1 13.25 16H2.75A1.75 1.75 0 0 1 1 14.25V9.332C.409 9.05 0 8.448 0 7.75v-2C0 4.784.784 4 1.75 4h.55c-.192-.375-.3-.8-.3-1.25ZM7.25 9.5H2.5v4.75c0 .138.112.25.25.25h4.5Zm1.5 0v5h4.5a.25.25 0 0 0 .25-.25V9.5Zm0-4V8h5.5a.25.25 0 0 0 .25-.25v-2a.25.25 0 0 0-.25-.25Zm-7 0a.25.25 0 0 0-.25.25v2c0 .138.112.25.25.25h5.5V5.5h-5.5Zm3-4a1.25 1.25 0 0 0 0 2.5h2.309c-.233-.818-.542-1.401-.878-1.793-.43-.502-.915-.707-1.431-.707ZM8.941 4h2.309a1.25 1.25 0 0 0 0-2.5c-.516 0-1 .205-1.43.707-.337.392-.646.975-.879 1.793Z\"></path></svg></template><template id=\"code-square-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code-square\">    <path d=\"M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25Zm7.47 3.97a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L10.69 8 9.22 6.53a.75.75 0 0 1 0-1.06ZM6.78 6.53 5.31 8l1.47 1.47a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215l-2-2a.75.75 0 0 1 0-1.06l2-2a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z\"></path></svg></template><template id=\"device-desktop-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-device-desktop\">    <path d=\"M14.25 1c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 14.25 12h-3.727c.099 1.041.52 1.872 1.292 2.757A.752.752 0 0 1 11.25 16h-6.5a.75.75 0 0 1-.565-1.243c.772-.885 1.192-1.716 1.292-2.757H1.75A1.75 1.75 0 0 1 0 10.25v-7.5C0 1.784.784 1 1.75 1ZM1.75 2.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25ZM9.018 12H6.982a5.72 5.72 0 0 1-.765 2.5h3.566a5.72 5.72 0 0 1-.765-2.5Z\"></path></svg></template>        <div class=\"position-relative\">                <ul                  role=\"listbox\"                  class=\"ActionListWrap QueryBuilder-ListWrap\"                  aria-label=\"Suggestions\"                  data-action=\"                    combobox-commit:query-builder#comboboxCommit                    mousedown:query-builder#resultsMousedown                  \"                  data-target=\"query-builder.resultsList\"                  data-persist-list=false                  id=\"query-builder-test-results\"                ></ul>        </div>      <div class=\"FormControl-inlineValidation\" id=\"validation-cbaf08ae-5c93-481c-a182-adf3a4afc79d\" hidden=\"hidden\">        <span class=\"FormControl-inlineValidation--visual\">          <svg aria-hidden=\"true\" height=\"12\" viewBox=\"0 0 12 12\" version=\"1.1\" width=\"12\" data-view-component=\"true\" class=\"octicon octicon-alert-fill\">    <path d=\"M4.855.708c.5-.896 1.79-.896 2.29 0l4.675 8.351a1.312 1.312 0 0 1-1.146 1.954H1.33A1.313 1.313 0 0 1 .183 9.058ZM7 7V3H5v4Zm-1 3a1 1 0 1 0 0-2 1 1 0 0 0 0 2Z\"></path></svg>        </span>        <span></span></div>    </div>    <div data-target=\"query-builder.screenReaderFeedback\" aria-live=\"polite\" aria-atomic=\"true\" class=\"sr-only\"></div></query-builder></form>          <div class=\"d-flex flex-row color-fg-muted px-3 text-small color-bg-default search-feedback-prompt\">            <a target=\"_blank\" href=\"https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax\" data-view-component=\"true\" class=\"Link color-fg-accent text-normal ml-2\">              Search syntax tips</a>            <div class=\"d-flex flex-1\"></div>          </div>        </div></div>    </div></modal-dialog></div>  </div>  <div data-action=\"click:qbsearch-input#retract\" class=\"dark-backdrop position-fixed\" hidden data-target=\"qbsearch-input.darkBackdrop\"></div>  <div class=\"color-fg-default\">    <dialog-helper>  <dialog data-target=\"qbsearch-input.feedbackDialog\" data-action=\"close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose\" id=\"feedback-dialog\" aria-modal=\"true\" aria-labelledby=\"feedback-dialog-title\" aria-describedby=\"feedback-dialog-description\" data-view-component=\"true\" class=\"Overlay Overlay-whenNarrow Overlay--size-medium Overlay--motion-scaleFade\">    <div data-view-component=\"true\" class=\"Overlay-header\">  <div class=\"Overlay-headerContentWrap\">    <div class=\"Overlay-titleWrap\">      <h1 class=\"Overlay-title \" id=\"feedback-dialog-title\">        Provide feedback      </h1>    </div>    <div class=\"Overlay-actionWrap\">      <button data-close-dialog-id=\"feedback-dialog\" aria-label=\"Close\" type=\"button\" data-view-component=\"true\" class=\"close-button Overlay-closeButton\"><svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg></button>    </div>  </div></div>      <scrollable-region data-labelled-by=\"feedback-dialog-title\">        <div data-view-component=\"true\" class=\"Overlay-body\">        <!-- '\"` --><!-- </textarea></xmp> --></option></form><form id=\"code-search-feedback-form\" data-turbo=\"false\" action=\"/search/feedback\" accept-charset=\"UTF-8\" method=\"post\"><input type=\"hidden\" data-csrf=\"true\" name=\"authenticity_token\" value=\"DDTwVEikSat6+n0RlyHSGXQwxywYThx/zBh7PYDFxbLDwh1oRQ5N9pmcfXlS8jSFNGSwpb0+ENot2e/3SPX5ew==\" />          <p>We read every piece of feedback, and take your input very seriously.</p>          <textarea name=\"feedback\" class=\"form-control width-full mb-2\" style=\"height: 120px\" id=\"feedback\"></textarea>          <input name=\"include_email\" id=\"include_email\" aria-label=\"Include my email address so I can be contacted\" class=\"form-control mr-2\" type=\"checkbox\">          <label for=\"include_email\" style=\"font-weight: normal\">Include my email address so I can be contacted</label></form></div>      </scrollable-region>      <div data-view-component=\"true\" class=\"Overlay-footer Overlay-footer--alignEnd\">          <button data-close-dialog-id=\"feedback-dialog\" type=\"button\" data-view-component=\"true\" class=\"btn\">    Cancel</button>          <button form=\"code-search-feedback-form\" data-action=\"click:qbsearch-input#submitFeedback\" type=\"submit\" data-view-component=\"true\" class=\"btn-primary btn\">    Submit feedback</button></div></dialog></dialog-helper>    <custom-scopes data-target=\"qbsearch-input.customScopesManager\">    <dialog-helper>  <dialog data-target=\"custom-scopes.customScopesModalDialog\" data-action=\"close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose\" id=\"custom-scopes-dialog\" aria-modal=\"true\" aria-labelledby=\"custom-scopes-dialog-title\" aria-describedby=\"custom-scopes-dialog-description\" data-view-component=\"true\" class=\"Overlay Overlay-whenNarrow Overlay--size-medium Overlay--motion-scaleFade\">    <div data-view-component=\"true\" class=\"Overlay-header Overlay-header--divided\">  <div class=\"Overlay-headerContentWrap\">    <div class=\"Overlay-titleWrap\">      <h1 class=\"Overlay-title \" id=\"custom-scopes-dialog-title\">        Saved searches      </h1>        <h2 id=\"custom-scopes-dialog-description\" class=\"Overlay-description\">Use saved searches to filter your results more quickly</h2>    </div>    <div class=\"Overlay-actionWrap\">      <button data-close-dialog-id=\"custom-scopes-dialog\" aria-label=\"Close\" type=\"button\" data-view-component=\"true\" class=\"close-button Overlay-closeButton\"><svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg></button>    </div>  </div></div>      <scrollable-region data-labelled-by=\"custom-scopes-dialog-title\">        <div data-view-component=\"true\" class=\"Overlay-body\">        <div data-target=\"custom-scopes.customScopesModalDialogFlash\"></div>        <div hidden class=\"create-custom-scope-form\" data-target=\"custom-scopes.createCustomScopeForm\">        <!-- '\"` --><!-- </textarea></xmp> --></option></form><form id=\"custom-scopes-dialog-form\" data-turbo=\"false\" action=\"/search/custom_scopes\" accept-charset=\"UTF-8\" method=\"post\"><input type=\"hidden\" data-csrf=\"true\" name=\"authenticity_token\" value=\"j1hA6g6M9l98HcMjxtO1Ix+xI5BfkLHhvevDx3Z6HHeGWhnHCyIARltCLx4yXUbHRpZKGBUJY5NRfwxZsSML3Q==\" />          <div data-target=\"custom-scopes.customScopesModalDialogFlash\"></div>          <input type=\"hidden\" id=\"custom_scope_id\" name=\"custom_scope_id\" data-target=\"custom-scopes.customScopesIdField\">          <div class=\"form-group\">            <label for=\"custom_scope_name\">Name</label>            <auto-check src=\"/search/custom_scopes/check_name\" required>              <input                type=\"text\"                name=\"custom_scope_name\"                id=\"custom_scope_name\"                data-target=\"custom-scopes.customScopesNameField\"                class=\"form-control\"                autocomplete=\"off\"                placeholder=\"github-ruby\"                required                maxlength=\"50\">              <input type=\"hidden\" data-csrf=\"true\" value=\"OGAVdj3rNZQCxh4sIj5NakLa62f8BJxbff2WZjMTJ0kPYopEE/JJEoBOrXBGwrL2rHKudOGzhGAewko2XUE45g==\" />            </auto-check>          </div>          <div class=\"form-group\">            <label for=\"custom_scope_query\">Query</label>            <input              type=\"text\"              name=\"custom_scope_query\"              id=\"custom_scope_query\"              data-target=\"custom-scopes.customScopesQueryField\"              class=\"form-control\"              autocomplete=\"off\"              placeholder=\"(repo:mona/a OR repo:mona/b) AND lang:python\"              required              maxlength=\"500\">          </div>          <p class=\"text-small color-fg-muted\">            To see all available qualifiers, see our <a class=\"Link--inTextBlock\" href=\"https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax\">documentation</a>.          </p></form>        </div>        <div data-target=\"custom-scopes.manageCustomScopesForm\">          <div data-target=\"custom-scopes.list\"></div>        </div></div>      </scrollable-region>      <div data-view-component=\"true\" class=\"Overlay-footer Overlay-footer--alignEnd Overlay-footer--divided\">          <button data-action=\"click:custom-scopes#customScopesCancel\" type=\"button\" data-view-component=\"true\" class=\"btn\">    Cancel</button>          <button form=\"custom-scopes-dialog-form\" data-action=\"click:custom-scopes#customScopesSubmit\" data-target=\"custom-scopes.customScopesSubmitButton\" type=\"submit\" data-view-component=\"true\" class=\"btn-primary btn\">    Create saved search</button></div></dialog></dialog-helper>    </custom-scopes>  </div></qbsearch-input><input type=\"hidden\" data-csrf=\"true\" class=\"js-data-jump-to-suggestions-path-csrf\" value=\"GkZSrCAE5Z1FuWYFd4QwdRI/+Pmq++gD8X+wnx/fhIe46oYySYwrZVrcwblb+UY58RJXHX8GNIoW3Iy/iywBPg==\" />          <div class=\"position-relative mr-lg-3 d-lg-inline-block\">            <a href=\"/login?return_to=https%3A%2F%2Fgithub.com%2Flucidrains%2Fvector-quantize-pytorch\"              class=\"HeaderMenu-link HeaderMenu-link--sign-in flex-shrink-0 no-underline d-block d-lg-inline-block border border-lg-0 rounded rounded-lg-0 p-2 p-lg-0\"              data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/vector-quantize-pytorch&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"e098aedcc7adde0b1d3fca9179e1b427144554eda2a9f4fa2c99e6691399b2d2\"              data-ga-click=\"(Logged out) Header, clicked Sign in, text:sign-in\">              Sign in            </a>          </div>            <a href=\"/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=lucidrains%2Fvector-quantize-pytorch\"              class=\"HeaderMenu-link HeaderMenu-link--sign-up flex-shrink-0 d-none d-lg-inline-block no-underline border color-border-default rounded px-2 py-1\"              data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/vector-quantize-pytorch&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"e098aedcc7adde0b1d3fca9179e1b427144554eda2a9f4fa2c99e6691399b2d2\"              data-analytics-event=\"{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/&lt;user-name&gt;/&lt;repo-name&gt;;ref_cta:Sign up;ref_loc:header logged out&quot;}\"            >              Sign up            </a>        </div>      </div>    </div>  </div></header>      <div hidden=\"hidden\" data-view-component=\"true\" class=\"js-stale-session-flash stale-session-flash flash flash-warn flash-full mb-3\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-alert\">    <path d=\"M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>        <span class=\"js-stale-session-flash-signed-in\" hidden>You signed in with another tab or window. <a class=\"Link--inTextBlock\" href=\"\">Reload</a> to refresh your session.</span>        <span class=\"js-stale-session-flash-signed-out\" hidden>You signed out in another tab or window. <a class=\"Link--inTextBlock\" href=\"\">Reload</a> to refresh your session.</span>        <span class=\"js-stale-session-flash-switched\" hidden>You switched accounts on another tab or window. <a class=\"Link--inTextBlock\" href=\"\">Reload</a> to refresh your session.</span>    <button id=\"icon-button-540730d2-3f64-4e8f-bbf1-bfb76c2d3d0d\" aria-labelledby=\"tooltip-a919456b-10ce-44c6-8edc-b0856a0187e6\" type=\"button\" data-view-component=\"true\" class=\"Button Button--iconOnly Button--invisible Button--medium flash-close js-flash-close\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x Button-visual\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg></button><tool-tip id=\"tooltip-a919456b-10ce-44c6-8edc-b0856a0187e6\" for=\"icon-button-540730d2-3f64-4e8f-bbf1-bfb76c2d3d0d\" popover=\"manual\" data-direction=\"s\" data-type=\"label\" data-view-component=\"true\" class=\"sr-only position-absolute\">Dismiss alert</tool-tip>  </div>    </div>  <div id=\"start-of-content\" class=\"show-on-focus\"></div>    <div id=\"js-flash-container\" data-turbo-replace>  <template class=\"js-flash-template\">    <div class=\"flash flash-full   {{ className }}\">  <div class=\"px-2\" >    <button autofocus class=\"flash-close js-flash-close\" type=\"button\" aria-label=\"Dismiss this message\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg>    </button>    <div aria-atomic=\"true\" role=\"alert\" class=\"js-flash-alert\">            <div>{{ message }}</div>    </div>  </div></div>  </template></div>        <include-fragment class=\"js-notification-shelf-include-fragment\" data-base-src=\"https://github.com/notifications/beta/shelf\"></include-fragment>  <div    class=\"application-main \"    data-commit-hovercards-enabled    data-discussion-hovercards-enabled    data-issue-and-pr-hovercards-enabled  >        <div itemscope itemtype=\"http://schema.org/SoftwareSourceCode\" class=\"\">    <main id=\"js-repo-pjax-container\" >                <div id=\"repository-container-header\"  class=\"pt-3 hide-full-screen\" style=\"background-color: var(--page-header-bgColor, var(--color-page-header-bg));\" data-turbo-replace>      <div class=\"d-flex flex-wrap flex-justify-end mb-3  px-3 px-md-4 px-lg-5\" style=\"gap: 1rem;\">        <div class=\"flex-auto min-width-0 width-fit mr-3\">              <div class=\" d-flex flex-wrap flex-items-center wb-break-word f3 text-normal\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo color-fg-muted mr-2\">    <path d=\"M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z\"></path></svg>        <span class=\"author flex-self-stretch\" itemprop=\"author\">      <a class=\"url fn\" rel=\"author\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/lucidrains/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"/lucidrains\">        lucidrains</a>    </span>    <span class=\"mx-1 flex-self-stretch color-fg-muted\">/</span>    <strong itemprop=\"name\" class=\"mr-2 flex-self-stretch\">      <a data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" href=\"/lucidrains/vector-quantize-pytorch\">vector-quantize-pytorch</a>    </strong>    <span></span><span class=\"Label Label--secondary v-align-middle mr-1\">Public</span>  </div>        </div>        <div id=\"repository-details-container\" data-turbo-replace>            <ul class=\"pagehead-actions flex-shrink-0 d-none d-md-inline\" style=\"padding: 2px 0;\">            <li>            <a href=\"/login?return_to=%2Flucidrains%2Fvector-quantize-pytorch\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/vector-quantize-pytorch&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"01d484d69f5b39d2aa423164cbb75752326bb5fc7b272a8977ca741706153630\" aria-label=\"You must be signed in to change notification settings\" data-view-component=\"true\" class=\"tooltipped tooltipped-s btn-sm btn\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-bell mr-2\">    <path d=\"M8 16a2 2 0 0 0 1.985-1.75c.017-.137-.097-.25-.235-.25h-3.5c-.138 0-.252.113-.235.25A2 2 0 0 0 8 16ZM3 5a5 5 0 0 1 10 0v2.947c0 .05.015.098.042.139l1.703 2.555A1.519 1.519 0 0 1 13.482 13H2.518a1.516 1.516 0 0 1-1.263-2.36l1.703-2.554A.255.255 0 0 0 3 7.947Zm5-3.5A3.5 3.5 0 0 0 4.5 5v2.947c0 .346-.102.683-.294.97l-1.703 2.556a.017.017 0 0 0-.003.01l.001.006c0 .002.002.004.004.006l.006.004.007.001h10.964l.007-.001.006-.004.004-.006.001-.007a.017.017 0 0 0-.003-.01l-1.703-2.554a1.745 1.745 0 0 1-.294-.97V5A3.5 3.5 0 0 0 8 1.5Z\"></path></svg>Notifications</a>  </li>  <li>          <a icon=\"repo-forked\" id=\"fork-button\" href=\"/login?return_to=%2Flucidrains%2Fvector-quantize-pytorch\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;repo details fork button&quot;,&quot;repository_id&quot;:271090649,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/vector-quantize-pytorch&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"8cce4f338641f66b612c43c1ef411362c4d4b41713a79e8b206923e645c1bb55\" data-view-component=\"true\" class=\"btn-sm btn\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo-forked mr-2\">    <path d=\"M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z\"></path></svg>Fork    <span id=\"repo-network-counter\" data-pjax-replace=\"true\" data-turbo-replace=\"true\" title=\"151\" data-view-component=\"true\" class=\"Counter\">151</span></a>  </li>  <li>        <div data-view-component=\"true\" class=\"BtnGroup d-flex\">        <a href=\"/login?return_to=%2Flucidrains%2Fvector-quantize-pytorch\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:271090649,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/vector-quantize-pytorch&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"ff302262790c19567f66df16d0aeec95f7aa56f2e25f2311cce3d64dacb03ad0\" aria-label=\"You must be signed in to star a repository\" data-view-component=\"true\" class=\"tooltipped tooltipped-s btn-sm btn BtnGroup-item\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-star v-align-text-bottom d-inline-block mr-2\">    <path d=\"M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z\"></path></svg><span data-view-component=\"true\" class=\"d-inline\">          Star</span>          <span id=\"repo-stars-counter-star\" aria-label=\"1734 users starred this repository\" data-singular-suffix=\"user starred this repository\" data-plural-suffix=\"users starred this repository\" data-turbo-replace=\"true\" title=\"1,734\" data-view-component=\"true\" class=\"Counter js-social-count\">1.7k</span></a>        <button aria-label=\"You must be signed in to add this repository to a list\" type=\"button\" disabled=\"disabled\" data-view-component=\"true\" class=\"btn-sm btn BtnGroup-item px-2\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-triangle-down\">    <path d=\"m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z\"></path></svg></button></div>  </li>    <li>            </li></ul>        </div>      </div>        <div id=\"responsive-meta-container\" data-turbo-replace>      <div class=\"d-block d-md-none mb-2 px-3 px-md-4 px-lg-5\">      <p class=\"f4 mb-3 \">        Vector (and Scalar) Quantization, in Pytorch      </p>          <h3 class=\"sr-only\">License</h3>  <div class=\"mb-2\">    <a href=\"/lucidrains/vector-quantize-pytorch/blob/master/LICENSE\"      class=\"Link--muted\"            data-analytics-event=\"{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:license&quot;}\"    >      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-law mr-2\">    <path d=\"M8.75.75V2h.985c.304 0 .603.08.867.231l1.29.736c.038.022.08.033.124.033h2.234a.75.75 0 0 1 0 1.5h-.427l2.111 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.006.005-.01.01-.045.04c-.21.176-.441.327-.686.45C14.556 10.78 13.88 11 13 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L12.178 4.5h-.162c-.305 0-.604-.079-.868-.231l-1.29-.736a.245.245 0 0 0-.124-.033H8.75V13h2.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1 0-1.5h2.5V3.5h-.984a.245.245 0 0 0-.124.033l-1.289.737c-.265.15-.564.23-.869.23h-.162l2.112 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.016.015-.045.04c-.21.176-.441.327-.686.45C4.556 10.78 3.88 11 3 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L2.178 4.5H1.75a.75.75 0 0 1 0-1.5h2.234a.249.249 0 0 0 .125-.033l1.288-.737c.265-.15.564-.23.869-.23h.984V.75a.75.75 0 0 1 1.5 0Zm2.945 8.477c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L13 6.327Zm-10 0c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L3 6.327Z\"></path></svg>     MIT license    </a>  </div>    <div class=\"mb-3\">        <a class=\"Link--secondary no-underline mr-3\" href=\"/lucidrains/vector-quantize-pytorch/stargazers\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-star mr-1\">    <path d=\"M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z\"></path></svg>          <span class=\"text-bold\">1.7k</span>          stars</a>        <a class=\"Link--secondary no-underline mr-3\" href=\"/lucidrains/vector-quantize-pytorch/forks\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo-forked mr-1\">    <path d=\"M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z\"></path></svg>          <span class=\"text-bold\">151</span>          forks</a>          <a class=\"Link--secondary no-underline mr-3 d-inline-block\" href=\"/lucidrains/vector-quantize-pytorch/branches\">            <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-git-branch mr-1\">    <path d=\"M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z\"></path></svg>            <span>Branches</span></a>          <a class=\"Link--secondary no-underline d-inline-block\" href=\"/lucidrains/vector-quantize-pytorch/tags\">            <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-tag mr-1\">    <path d=\"M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z\"></path></svg>            <span>Tags</span></a>        <a class=\"Link--secondary no-underline d-inline-block\" href=\"/lucidrains/vector-quantize-pytorch/activity\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-pulse mr-1\">    <path d=\"M6 2c.306 0 .582.187.696.471L10 10.731l1.304-3.26A.751.751 0 0 1 12 7h3.25a.75.75 0 0 1 0 1.5h-2.742l-1.812 4.528a.751.751 0 0 1-1.392 0L6 4.77 4.696 8.03A.75.75 0 0 1 4 8.5H.75a.75.75 0 0 1 0-1.5h2.742l1.812-4.529A.751.751 0 0 1 6 2Z\"></path></svg>          <span>Activity</span></a>    </div>      <div class=\"d-flex flex-wrap gap-2\">        <div class=\"flex-1\">            <div data-view-component=\"true\" class=\"BtnGroup d-flex\">        <a href=\"/login?return_to=%2Flucidrains%2Fvector-quantize-pytorch\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:271090649,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/vector-quantize-pytorch&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"ff302262790c19567f66df16d0aeec95f7aa56f2e25f2311cce3d64dacb03ad0\" aria-label=\"You must be signed in to star a repository\" data-view-component=\"true\" class=\"tooltipped tooltipped-s btn-sm btn btn-block BtnGroup-item\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-star v-align-text-bottom d-inline-block mr-2\">    <path d=\"M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z\"></path></svg><span data-view-component=\"true\" class=\"d-inline\">          Star</span></a>        <button aria-label=\"You must be signed in to add this repository to a list\" type=\"button\" disabled=\"disabled\" data-view-component=\"true\" class=\"btn-sm btn BtnGroup-item px-2\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-triangle-down\">    <path d=\"m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z\"></path></svg></button></div>        </div>        <div class=\"flex-1\">                <a href=\"/login?return_to=%2Flucidrains%2Fvector-quantize-pytorch\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/vector-quantize-pytorch&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"01d484d69f5b39d2aa423164cbb75752326bb5fc7b272a8977ca741706153630\" aria-label=\"You must be signed in to change notification settings\" data-view-component=\"true\" class=\"tooltipped tooltipped-s btn-sm btn btn-block\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-bell mr-2\">    <path d=\"M8 16a2 2 0 0 0 1.985-1.75c.017-.137-.097-.25-.235-.25h-3.5c-.138 0-.252.113-.235.25A2 2 0 0 0 8 16ZM3 5a5 5 0 0 1 10 0v2.947c0 .05.015.098.042.139l1.703 2.555A1.519 1.519 0 0 1 13.482 13H2.518a1.516 1.516 0 0 1-1.263-2.36l1.703-2.554A.255.255 0 0 0 3 7.947Zm5-3.5A3.5 3.5 0 0 0 4.5 5v2.947c0 .346-.102.683-.294.97l-1.703 2.556a.017.017 0 0 0-.003.01l.001.006c0 .002.002.004.004.006l.006.004.007.001h10.964l.007-.001.006-.004.004-.006.001-.007a.017.017 0 0 0-.003-.01l-1.703-2.554a1.745 1.745 0 0 1-.294-.97V5A3.5 3.5 0 0 0 8 1.5Z\"></path></svg>Notifications</a>        </div>          <span>                      </span>      </div>  </div></div>          <nav data-pjax=\"#js-repo-pjax-container\" aria-label=\"Repository\" data-view-component=\"true\" class=\"js-repo-nav js-sidenav-container-pjax js-responsive-underlinenav overflow-hidden UnderlineNav px-3 px-md-4 px-lg-5\">  <ul data-view-component=\"true\" class=\"UnderlineNav-body list-style-none\">      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"code-tab\" href=\"/lucidrains/vector-quantize-pytorch\" data-tab-item=\"i0code-tab\" data-selected-links=\"repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches repo_packages repo_deployments repo_attestations /lucidrains/vector-quantize-pytorch\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g c\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Code&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" aria-current=\"page\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item selected\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z\"></path></svg>        <span data-content=\"Code\">Code</span>          <span id=\"code-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"Not available\" data-view-component=\"true\" class=\"Counter\"></span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"issues-tab\" href=\"/lucidrains/vector-quantize-pytorch/issues\" data-tab-item=\"i1issues-tab\" data-selected-links=\"repo_issues repo_labels repo_milestones /lucidrains/vector-quantize-pytorch/issues\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g i\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Issues&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-issue-opened UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path><path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z\"></path></svg>        <span data-content=\"Issues\">Issues</span>          <span id=\"issues-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"30\" data-view-component=\"true\" class=\"Counter\">30</span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"pull-requests-tab\" href=\"/lucidrains/vector-quantize-pytorch/pulls\" data-tab-item=\"i2pull-requests-tab\" data-selected-links=\"repo_pulls checks /lucidrains/vector-quantize-pytorch/pulls\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g p\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Pull requests&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-git-pull-request UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M1.5 3.25a2.25 2.25 0 1 1 3 2.122v5.256a2.251 2.251 0 1 1-1.5 0V5.372A2.25 2.25 0 0 1 1.5 3.25Zm5.677-.177L9.573.677A.25.25 0 0 1 10 .854V2.5h1A2.5 2.5 0 0 1 13.5 5v5.628a2.251 2.251 0 1 1-1.5 0V5a1 1 0 0 0-1-1h-1v1.646a.25.25 0 0 1-.427.177L7.177 3.427a.25.25 0 0 1 0-.354ZM3.75 2.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm0 9.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm8.25.75a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Z\"></path></svg>        <span data-content=\"Pull requests\">Pull requests</span>          <span id=\"pull-requests-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"1\" data-view-component=\"true\" class=\"Counter\">1</span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"discussions-tab\" href=\"/lucidrains/vector-quantize-pytorch/discussions\" data-tab-item=\"i3discussions-tab\" data-selected-links=\"repo_discussions /lucidrains/vector-quantize-pytorch/discussions\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g g\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Discussions&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-comment-discussion UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M1.75 1h8.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 10.25 10H7.061l-2.574 2.573A1.458 1.458 0 0 1 2 11.543V10h-.25A1.75 1.75 0 0 1 0 8.25v-5.5C0 1.784.784 1 1.75 1ZM1.5 2.75v5.5c0 .138.112.25.25.25h1a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h3.5a.25.25 0 0 0 .25-.25v-5.5a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25Zm13 2a.25.25 0 0 0-.25-.25h-.5a.75.75 0 0 1 0-1.5h.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 14.25 12H14v1.543a1.458 1.458 0 0 1-2.487 1.03L9.22 12.28a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l2.22 2.22v-2.19a.75.75 0 0 1 .75-.75h1a.25.25 0 0 0 .25-.25Z\"></path></svg>        <span data-content=\"Discussions\">Discussions</span>          <span id=\"discussions-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"Not available\" data-view-component=\"true\" class=\"Counter\"></span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"actions-tab\" href=\"/lucidrains/vector-quantize-pytorch/actions\" data-tab-item=\"i4actions-tab\" data-selected-links=\"repo_actions /lucidrains/vector-quantize-pytorch/actions\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g a\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Actions&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-play UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z\"></path></svg>        <span data-content=\"Actions\">Actions</span>          <span id=\"actions-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"Not available\" data-view-component=\"true\" class=\"Counter\"></span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"projects-tab\" href=\"/lucidrains/vector-quantize-pytorch/projects\" data-tab-item=\"i5projects-tab\" data-selected-links=\"repo_projects new_repo_project repo_project /lucidrains/vector-quantize-pytorch/projects\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g b\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Projects&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-table UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25ZM6.5 6.5v8h7.75a.25.25 0 0 0 .25-.25V6.5Zm8-1.5V1.75a.25.25 0 0 0-.25-.25H6.5V5Zm-13 1.5v7.75c0 .138.112.25.25.25H5v-8ZM5 5V1.5H1.75a.25.25 0 0 0-.25.25V5Z\"></path></svg>        <span data-content=\"Projects\">Projects</span>          <span id=\"projects-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"0\" hidden=\"hidden\" data-view-component=\"true\" class=\"Counter\">0</span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"security-tab\" href=\"/lucidrains/vector-quantize-pytorch/security\" data-tab-item=\"i6security-tab\" data-selected-links=\"security overview alerts policy token_scanning code_scanning /lucidrains/vector-quantize-pytorch/security\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g s\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Security&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-shield UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M7.467.133a1.748 1.748 0 0 1 1.066 0l5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667Zm.61 1.429a.25.25 0 0 0-.153 0l-5.25 1.68a.25.25 0 0 0-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.196.196 0 0 0 .154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.251.251 0 0 0-.174-.237l-5.25-1.68ZM8.75 4.75v3a.75.75 0 0 1-1.5 0v-3a.75.75 0 0 1 1.5 0ZM9 10.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>        <span data-content=\"Security\">Security</span>          <include-fragment src=\"/lucidrains/vector-quantize-pytorch/security/overall-count\" accept=\"text/fragment+html\"></include-fragment>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"insights-tab\" href=\"/lucidrains/vector-quantize-pytorch/pulse\" data-tab-item=\"i7insights-tab\" data-selected-links=\"repo_graphs repo_contributors dependency_graph dependabot_updates pulse people community /lucidrains/vector-quantize-pytorch/pulse\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Insights&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-graph UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M1.5 1.75V13.5h13.75a.75.75 0 0 1 0 1.5H.75a.75.75 0 0 1-.75-.75V1.75a.75.75 0 0 1 1.5 0Zm14.28 2.53-5.25 5.25a.75.75 0 0 1-1.06 0L7 7.06 4.28 9.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.25-3.25a.75.75 0 0 1 1.06 0L10 7.94l4.72-4.72a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z\"></path></svg>        <span data-content=\"Insights\">Insights</span>          <span id=\"insights-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"Not available\" data-view-component=\"true\" class=\"Counter\"></span>    </a></li></ul>    <div style=\"visibility:hidden;\" data-view-component=\"true\" class=\"UnderlineNav-actions js-responsive-underlinenav-overflow position-absolute pr-3 pr-md-4 pr-lg-5 right-0\">      <action-menu data-select-variant=\"none\" data-view-component=\"true\">  <focus-group direction=\"vertical\" mnemonics retain>    <button id=\"action-menu-11d4e1ad-25df-42b6-9931-e4c97aba2b77-button\" popovertarget=\"action-menu-11d4e1ad-25df-42b6-9931-e4c97aba2b77-overlay\" aria-controls=\"action-menu-11d4e1ad-25df-42b6-9931-e4c97aba2b77-list\" aria-haspopup=\"true\" aria-labelledby=\"tooltip-aa2edb0e-dcd8-479a-987b-d3f06a165da5\" type=\"button\" data-view-component=\"true\" class=\"Button Button--iconOnly Button--secondary Button--medium UnderlineNav-item\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-kebab-horizontal Button-visual\">    <path d=\"M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path></svg></button><tool-tip id=\"tooltip-aa2edb0e-dcd8-479a-987b-d3f06a165da5\" for=\"action-menu-11d4e1ad-25df-42b6-9931-e4c97aba2b77-button\" popover=\"manual\" data-direction=\"s\" data-type=\"label\" data-view-component=\"true\" class=\"sr-only position-absolute\">Additional navigation options</tool-tip><anchored-position id=\"action-menu-11d4e1ad-25df-42b6-9931-e4c97aba2b77-overlay\" anchor=\"action-menu-11d4e1ad-25df-42b6-9931-e4c97aba2b77-button\" align=\"start\" side=\"outside-bottom\" anchor-offset=\"normal\" popover=\"auto\" data-view-component=\"true\">  <div data-view-component=\"true\" class=\"Overlay Overlay--size-auto\">          <div data-view-component=\"true\" class=\"Overlay-body Overlay-body--paddingNone\">          <div data-view-component=\"true\">  <ul aria-labelledby=\"action-menu-11d4e1ad-25df-42b6-9931-e4c97aba2b77-button\" id=\"action-menu-11d4e1ad-25df-42b6-9931-e4c97aba2b77-list\" role=\"menu\" data-view-component=\"true\" class=\"ActionListWrap--inset ActionListWrap\">      <li hidden=\"hidden\" data-menu-item=\"i0code-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-ba923a23-761d-4b5e-a6c5-fe9f6142c9c0\" href=\"/lucidrains/vector-quantize-pytorch\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code\">    <path d=\"m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Code</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i1issues-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-95347755-ff8a-4534-a3fc-2beaec3fb5a2\" href=\"/lucidrains/vector-quantize-pytorch/issues\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-issue-opened\">    <path d=\"M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path><path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Issues</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i2pull-requests-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-e14c25f6-8d4a-4b3b-ad7c-d8d44c617db2\" href=\"/lucidrains/vector-quantize-pytorch/pulls\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-git-pull-request\">    <path d=\"M1.5 3.25a2.25 2.25 0 1 1 3 2.122v5.256a2.251 2.251 0 1 1-1.5 0V5.372A2.25 2.25 0 0 1 1.5 3.25Zm5.677-.177L9.573.677A.25.25 0 0 1 10 .854V2.5h1A2.5 2.5 0 0 1 13.5 5v5.628a2.251 2.251 0 1 1-1.5 0V5a1 1 0 0 0-1-1h-1v1.646a.25.25 0 0 1-.427.177L7.177 3.427a.25.25 0 0 1 0-.354ZM3.75 2.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm0 9.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm8.25.75a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Pull requests</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i3discussions-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-0816fe4d-b668-4cad-923a-463c1f2e7776\" href=\"/lucidrains/vector-quantize-pytorch/discussions\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-comment-discussion\">    <path d=\"M1.75 1h8.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 10.25 10H7.061l-2.574 2.573A1.458 1.458 0 0 1 2 11.543V10h-.25A1.75 1.75 0 0 1 0 8.25v-5.5C0 1.784.784 1 1.75 1ZM1.5 2.75v5.5c0 .138.112.25.25.25h1a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h3.5a.25.25 0 0 0 .25-.25v-5.5a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25Zm13 2a.25.25 0 0 0-.25-.25h-.5a.75.75 0 0 1 0-1.5h.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 14.25 12H14v1.543a1.458 1.458 0 0 1-2.487 1.03L9.22 12.28a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l2.22 2.22v-2.19a.75.75 0 0 1 .75-.75h1a.25.25 0 0 0 .25-.25Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Discussions</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i4actions-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-882caaab-e14e-4082-9ea0-4a137c92d64e\" href=\"/lucidrains/vector-quantize-pytorch/actions\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-play\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Actions</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i5projects-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-19b1c223-30cf-4628-98f6-a47624a8f341\" href=\"/lucidrains/vector-quantize-pytorch/projects\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-table\">    <path d=\"M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25ZM6.5 6.5v8h7.75a.25.25 0 0 0 .25-.25V6.5Zm8-1.5V1.75a.25.25 0 0 0-.25-.25H6.5V5Zm-13 1.5v7.75c0 .138.112.25.25.25H5v-8ZM5 5V1.5H1.75a.25.25 0 0 0-.25.25V5Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Projects</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i6security-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-a9ff575d-4910-4694-b3bf-cebf4a5b1db4\" href=\"/lucidrains/vector-quantize-pytorch/security\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-shield\">    <path d=\"M7.467.133a1.748 1.748 0 0 1 1.066 0l5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667Zm.61 1.429a.25.25 0 0 0-.153 0l-5.25 1.68a.25.25 0 0 0-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.196.196 0 0 0 .154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.251.251 0 0 0-.174-.237l-5.25-1.68ZM8.75 4.75v3a.75.75 0 0 1-1.5 0v-3a.75.75 0 0 1 1.5 0ZM9 10.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Security</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i7insights-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-a26b7889-253b-4e72-b4af-750e7236b00a\" href=\"/lucidrains/vector-quantize-pytorch/pulse\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-graph\">    <path d=\"M1.5 1.75V13.5h13.75a.75.75 0 0 1 0 1.5H.75a.75.75 0 0 1-.75-.75V1.75a.75.75 0 0 1 1.5 0Zm14.28 2.53-5.25 5.25a.75.75 0 0 1-1.06 0L7 7.06 4.28 9.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.25-3.25a.75.75 0 0 1 1.06 0L10 7.94l4.72-4.72a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Insights</span></a>    </li></ul>  </div></div>      </div></anchored-position>  </focus-group></action-menu></div></nav>  </div>  <turbo-frame id=\"repo-content-turbo-frame\" target=\"_top\" data-turbo-action=\"advance\" class=\"\">    <div id=\"repo-content-pjax-container\" class=\"repository-content \" >                <h1 class='sr-only'>lucidrains/vector-quantize-pytorch</h1>  <div class=\"clearfix container-xl px-md-4 px-lg-5 px-3\">    <div>  <div id=\"spoof-warning\" class=\"mt-0 pb-3\" hidden aria-hidden>  <div data-view-component=\"true\" class=\"flash flash-warn mt-0 clearfix\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-alert float-left mt-1\">    <path d=\"M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>      <div class=\"overflow-hidden\">This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.</div>  </div></div>  <include-fragment src=\"/lucidrains/vector-quantize-pytorch/spoofed_commit_check/6102e37efefefb673ebc8bec3abb02d5030dd933\" data-test-selector=\"spoofed-commit-check\"></include-fragment>  <div style=\"max-width: 100%\" data-view-component=\"true\" class=\"Layout Layout--flowRow-until-md react-repos-overview-margin Layout--sidebarPosition-end Layout--sidebarPosition-flowRow-end\">  <div data-view-component=\"true\" class=\"Layout-main\">        <script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/react-lib-1fbfc5be2c18.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_octicons-react_dist_index_esm_js-node_modules_primer_react_lib-es-2e8e7c-adc8451a70cf.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Box_Box_js-8f8c5e2a2cbf.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Button_Button_js-67fe00b5266a.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_ActionList_index_js-2dd4d13d3ae6.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Overlay_Overlay_js-node_modules_primer_react_lib-es-fa1130-829932cf63db.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Text_Text_js-node_modules_primer_react_lib-esm_Text-85a14b-236dc9716ad0.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_ActionMenu_ActionMenu_js-eaf74522e470.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_react-router-dom_dist_index_js-3b41341d50fe.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Dialog_js-node_modules_primer_react_lib-esm_Label_L-857e1c-77794958a54a.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_UnderlineNav_index_js-89fa5806aa3c.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_AvatarStack_AvatarStack_js-node_modules_primer_reac-e445e7-175b51e43dcc.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_react-core_create-browser-history_ts-ui_packages_react-core_AppContextProvider_ts-809ab9-bf008735d0bb.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_paths_index_ts-7137b25aa38b.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_ref-selector_RefSelector_tsx-dbbdef4348e2.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_commit-attribution_index_ts-ui_packages_commit-checks-status_index_ts-ui_packages-ffbe33-4c4ddf7d268d.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_react-code-view_components_directory_DirectoryContent_index_ts-app_assets_-1fd1f5-c96303590595.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/repos-overview-523b8f59ec33.js\"></script><react-partial  partial-name=\"repos-overview\"  data-ssr=\"true\">    <script type=\"application/json\" data-target=\"react-partial.embeddedData\">{\"props\":{\"initialPayload\":{\"allShortcutsEnabled\":false,\"path\":\"/\",\"repo\":{\"id\":271090649,\"defaultBranch\":\"master\",\"name\":\"vector-quantize-pytorch\",\"ownerLogin\":\"lucidrains\",\"currentUserCanPush\":false,\"isFork\":false,\"isEmpty\":false,\"createdAt\":\"2020-06-09T19:23:00.000Z\",\"ownerAvatar\":\"https://avatars.githubusercontent.com/u/108653?v=4\",\"public\":true,\"private\":false,\"isOrgOwned\":false},\"currentUser\":null,\"refInfo\":{\"name\":\"master\",\"listCacheKey\":\"v0:1708264098.0\",\"canEdit\":false,\"refType\":\"branch\",\"currentOid\":\"6102e37efefefb673ebc8bec3abb02d5030dd933\"},\"tree\":{\"items\":[{\"name\":\".github/workflows\",\"path\":\".github/workflows\",\"contentType\":\"directory\",\"hasSimplifiedPath\":true},{\"name\":\"examples\",\"path\":\"examples\",\"contentType\":\"directory\"},{\"name\":\"images\",\"path\":\"images\",\"contentType\":\"directory\"},{\"name\":\"vector_quantize_pytorch\",\"path\":\"vector_quantize_pytorch\",\"contentType\":\"directory\"},{\"name\":\".gitignore\",\"path\":\".gitignore\",\"contentType\":\"file\"},{\"name\":\"LICENSE\",\"path\":\"LICENSE\",\"contentType\":\"file\"},{\"name\":\"README.md\",\"path\":\"README.md\",\"contentType\":\"file\"},{\"name\":\"setup.py\",\"path\":\"setup.py\",\"contentType\":\"file\"}],\"templateDirectorySuggestionUrl\":null,\"readme\":null,\"totalCount\":8,\"showBranchInfobar\":false},\"fileTree\":null,\"fileTreeProcessingTime\":null,\"foldersToFetch\":[],\"treeExpanded\":false,\"symbolsExpanded\":false,\"isOverview\":true,\"overview\":{\"banners\":{\"shouldRecommendReadme\":false,\"isPersonalRepo\":false,\"showUseActionBanner\":false,\"actionSlug\":null,\"actionId\":null,\"showProtectBranchBanner\":false,\"recentlyTouchedDataChannel\":null,\"publishBannersInfo\":{\"dismissActionNoticePath\":\"/settings/dismiss-notice/publish_action_from_repo\",\"releasePath\":\"/lucidrains/vector-quantize-pytorch/releases/new?marketplace=true\",\"showPublishActionBanner\":false},\"interactionLimitBanner\":null,\"showInvitationBanner\":false,\"inviterName\":null},\"codeButton\":{\"contactPath\":\"/contact\",\"isEnterprise\":false,\"local\":{\"protocolInfo\":{\"httpAvailable\":true,\"sshAvailable\":null,\"httpUrl\":\"https://github.com/lucidrains/vector-quantize-pytorch.git\",\"showCloneWarning\":null,\"sshUrl\":null,\"sshCertificatesRequired\":null,\"sshCertificatesAvailable\":null,\"ghCliUrl\":\"gh repo clone lucidrains/vector-quantize-pytorch\",\"defaultProtocol\":\"http\",\"newSshKeyUrl\":\"/settings/ssh/new\",\"setProtocolPath\":\"/users/set_protocol\"},\"platformInfo\":{\"cloneUrl\":\"https://desktop.github.com\",\"showVisualStudioCloneButton\":false,\"visualStudioCloneUrl\":\"https://windows.github.com\",\"showXcodeCloneButton\":false,\"xcodeCloneUrl\":\"https://developer.apple.com\",\"zipballUrl\":\"/lucidrains/vector-quantize-pytorch/archive/refs/heads/master.zip\"}},\"newCodespacePath\":\"/codespaces/new?hide_repo_select=true\\u0026repo=271090649\"},\"popovers\":{\"rename\":null,\"renamedParentRepo\":null},\"commitCount\":\"268\",\"overviewFiles\":[{\"displayName\":\"README.md\",\"repoName\":\"vector-quantize-pytorch\",\"refName\":\"master\",\"path\":\"README.md\",\"preferredFileType\":\"readme\",\"tabName\":\"README\",\"richText\":\"\\u003carticle class=\\\"markdown-body entry-content container-lg\\\" itemprop=\\\"text\\\"\\u003e\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vector-quantize-pytorch/blob/master/images/vq.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vector-quantize-pytorch/raw/master/images/vq.png\\\" width=\\\"500px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eVector Quantization - Pytorch\\u003c/h2\\u003e\\u003ca id=\\\"user-content-vector-quantization---pytorch\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Vector Quantization - Pytorch\\\" href=\\\"#vector-quantization---pytorch\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eA vector quantization library originally transcribed from Deepmind's tensorflow implementation, made conveniently into a package. It uses exponential moving averages to update the dictionary.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eVQ has been successfully used by Deepmind and OpenAI for high quality generation of images (VQ-VAE-2) and music (Jukebox).\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eInstall\\u003c/h2\\u003e\\u003ca id=\\\"user-content-install\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Install\\\" href=\\\"#install\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"$ pip install vector-quantize-pytorch\\\"\\u003e\\u003cpre\\u003e$ pip install vector-quantize-pytorch\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eUsage\\u003c/h2\\u003e\\u003ca id=\\\"user-content-usage\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Usage\\\" href=\\\"#usage\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import VectorQuantize\\n\\nvq = VectorQuantize(\\n    dim = 256,\\n    codebook_size = 512,     # codebook size\\n    decay = 0.8,             # the exponential moving average decay, lower means the dictionary will change faster\\n    commitment_weight = 1.   # the weight on the commitment loss\\n)\\n\\nx = torch.randn(1, 1024, 256)\\nquantized, indices, commit_loss = vq(x) # (1, 1024, 256), (1, 1024), (1)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eVectorQuantize\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003evq\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eVectorQuantize\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecodebook_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,     \\u003cspan class=\\\"pl-c\\\"\\u003e# codebook size\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edecay\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.8\\u003c/span\\u003e,             \\u003cspan class=\\\"pl-c\\\"\\u003e# the exponential moving average decay, lower means the dictionary will change faster\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecommitment_weight\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1.\\u003c/span\\u003e   \\u003cspan class=\\\"pl-c\\\"\\u003e# the weight on the commitment loss\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ecommit_loss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003evq\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1024, 256), (1, 1024), (1)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eResidual VQ\\u003c/h2\\u003e\\u003ca id=\\\"user-content-residual-vq\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Residual VQ\\\" href=\\\"#residual-vq\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis \\u003ca href=\\\"https://arxiv.org/abs/2107.03312\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e proposes to use multiple vector quantizers to recursively quantize the residuals of the waveform. You can use this with the \\u003ccode\\u003eResidualVQ\\u003c/code\\u003e class and one extra initialization parameter.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import ResidualVQ\\n\\nresidual_vq = ResidualVQ(\\n    dim = 256,\\n    num_quantizers = 8,      # specify number of quantizers\\n    codebook_size = 1024,    # codebook size\\n)\\n\\nx = torch.randn(1, 1024, 256)\\n\\nquantized, indices, commit_loss = residual_vq(x)\\n\\n# (1, 1024, 256), (1, 1024, 8), (1, 8)\\n# (batch, seq, dim), (batch, seq, quantizer), (batch, quantizer)\\n\\n# if you need all the codes across the quantization layers, just pass return_all_codes = True\\n\\nquantized, indices, commit_loss, all_codes = residual_vq(x, return_all_codes = True)\\n\\n# *_, (8, 1, 1024, 256)\\n# all_codes - (quantizer, batch, seq, dim)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eResidualVQ\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eresidual_vq\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eResidualVQ\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_quantizers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# specify number of quantizers\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecodebook_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,    \\u003cspan class=\\\"pl-c\\\"\\u003e# codebook size\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ecommit_loss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003eresidual_vq\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1024, 256), (1, 1024, 8), (1, 8)\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (batch, seq, dim), (batch, seq, quantizer), (batch, quantizer)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# if you need all the codes across the quantization layers, just pass return_all_codes = True\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ecommit_loss\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eall_codes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003eresidual_vq\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ereturn_all_codes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# *_, (8, 1, 1024, 256)\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# all_codes - (quantizer, batch, seq, dim)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eFurthermore, \\u003ca href=\\\"https://arxiv.org/abs/2203.01941\\\" rel=\\\"nofollow\\\"\\u003ethis paper\\u003c/a\\u003e uses Residual-VQ to construct the RQ-VAE, for generating high resolution images with more compressed codes.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThey make two modifications. The first is to share the codebook across all quantizers. The second is to stochastically sample the codes rather than always taking the closest match. You can use both of these features with two extra keyword arguments.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import ResidualVQ\\n\\nresidual_vq = ResidualVQ(\\n    dim = 256,\\n    num_quantizers = 8,\\n    codebook_size = 1024,\\n    stochastic_sample_codes = True,\\n    sample_codebook_temp = 0.1,         # temperature for stochastically sampling codes, 0 would be equivalent to non-stochastic\\n    shared_codebook = True              # whether to share the codebooks for all quantizers or not\\n)\\n\\nx = torch.randn(1, 1024, 256)\\nquantized, indices, commit_loss = residual_vq(x)\\n\\n# (1, 1024, 256), (8, 1, 1024), (8, 1)\\n# (batch, seq, dim), (quantizer, batch, seq), (quantizer, batch)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eResidualVQ\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eresidual_vq\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eResidualVQ\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_quantizers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecodebook_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003estochastic_sample_codes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003esample_codebook_temp\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,         \\u003cspan class=\\\"pl-c\\\"\\u003e# temperature for stochastically sampling codes, 0 would be equivalent to non-stochastic\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eshared_codebook\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e              \\u003cspan class=\\\"pl-c\\\"\\u003e# whether to share the codebooks for all quantizers or not\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ecommit_loss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003eresidual_vq\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1024, 256), (8, 1, 1024), (8, 1)\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (batch, seq, dim), (quantizer, batch, seq), (quantizer, batch)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2305.02765\\\" rel=\\\"nofollow\\\"\\u003eA recent paper\\u003c/a\\u003e further proposes to do residual VQ on groups of the feature dimension, showing equivalent results to Encodec while using far fewer codebooks. You can use it by importing \\u003ccode\\u003eGroupedResidualVQ\\u003c/code\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import GroupedResidualVQ\\n\\nresidual_vq = GroupedResidualVQ(\\n    dim = 256,\\n    num_quantizers = 8,      # specify number of quantizers\\n    groups = 2,\\n    codebook_size = 1024,    # codebook size\\n)\\n\\nx = torch.randn(1, 1024, 256)\\n\\nquantized, indices, commit_loss = residual_vq(x)\\n\\n# (1, 1024, 256), (2, 1, 1024, 8), (2, 1, 8)\\n# (batch, seq, dim), (groups, batch, seq, quantizer), (groups, batch, quantizer)\\n\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eGroupedResidualVQ\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eresidual_vq\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eGroupedResidualVQ\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_quantizers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# specify number of quantizers\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003egroups\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecodebook_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,    \\u003cspan class=\\\"pl-c\\\"\\u003e# codebook size\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ecommit_loss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003eresidual_vq\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1024, 256), (2, 1, 1024, 8), (2, 1, 8)\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (batch, seq, dim), (groups, batch, seq, quantizer), (groups, batch, quantizer)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eInitialization\\u003c/h2\\u003e\\u003ca id=\\\"user-content-initialization\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Initialization\\\" href=\\\"#initialization\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe SoundStream paper proposes that the codebook should be initialized by the kmeans centroids of the first batch. You can easily turn on this feature with one flag \\u003ccode\\u003ekmeans_init = True\\u003c/code\\u003e, for either \\u003ccode\\u003eVectorQuantize\\u003c/code\\u003e or \\u003ccode\\u003eResidualVQ\\u003c/code\\u003e class\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import ResidualVQ\\n\\nresidual_vq = ResidualVQ(\\n    dim = 256,\\n    codebook_size = 256,\\n    num_quantizers = 4,\\n    kmeans_init = True,   # set to True\\n    kmeans_iters = 10     # number of kmeans iterations to calculate the centroids for the codebook on init\\n)\\n\\nx = torch.randn(1, 1024, 256)\\nquantized, indices, commit_loss = residual_vq(x)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eResidualVQ\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eresidual_vq\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eResidualVQ\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecodebook_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_quantizers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ekmeans_init\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e,   \\u003cspan class=\\\"pl-c\\\"\\u003e# set to True\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ekmeans_iters\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e10\\u003c/span\\u003e     \\u003cspan class=\\\"pl-c\\\"\\u003e# number of kmeans iterations to calculate the centroids for the codebook on init\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ecommit_loss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003eresidual_vq\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eIncreasing codebook usage\\u003c/h2\\u003e\\u003ca id=\\\"user-content-increasing-codebook-usage\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Increasing codebook usage\\\" href=\\\"#increasing-codebook-usage\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis repository will contain a few techniques from various papers to combat \\\"dead\\\" codebook entries, which is a common problem when using vector quantizers.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eLower codebook dimension\\u003c/h3\\u003e\\u003ca id=\\\"user-content-lower-codebook-dimension\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Lower codebook dimension\\\" href=\\\"#lower-codebook-dimension\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe \\u003ca href=\\\"https://openreview.net/forum?id=pfNyExj7z2\\\" rel=\\\"nofollow\\\"\\u003eImproved VQGAN paper\\u003c/a\\u003e proposes to have the codebook kept in a lower dimension. The encoder values are projected down before being projected back to high dimensional after quantization. You can set this with the \\u003ccode\\u003ecodebook_dim\\u003c/code\\u003e hyperparameter.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import VectorQuantize\\n\\nvq = VectorQuantize(\\n    dim = 256,\\n    codebook_size = 256,\\n    codebook_dim = 16      # paper proposes setting this to 32 or as low as 8 to increase codebook usage\\n)\\n\\nx = torch.randn(1, 1024, 256)\\nquantized, indices, commit_loss = vq(x)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eVectorQuantize\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003evq\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eVectorQuantize\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecodebook_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecodebook_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e      \\u003cspan class=\\\"pl-c\\\"\\u003e# paper proposes setting this to 32 or as low as 8 to increase codebook usage\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ecommit_loss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003evq\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eCosine similarity\\u003c/h3\\u003e\\u003ca id=\\\"user-content-cosine-similarity\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Cosine similarity\\\" href=\\\"#cosine-similarity\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe \\u003ca href=\\\"https://openreview.net/forum?id=pfNyExj7z2\\\" rel=\\\"nofollow\\\"\\u003eImproved VQGAN paper\\u003c/a\\u003e also proposes to l2 normalize the codes and the encoded vectors, which boils down to using cosine similarity for the distance. They claim enforcing the vectors on a sphere leads to improvements in code usage and downstream reconstruction. You can turn this on by setting \\u003ccode\\u003euse_cosine_sim = True\\u003c/code\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import VectorQuantize\\n\\nvq = VectorQuantize(\\n    dim = 256,\\n    codebook_size = 256,\\n    use_cosine_sim = True   # set this to True\\n)\\n\\nx = torch.randn(1, 1024, 256)\\nquantized, indices, commit_loss = vq(x)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eVectorQuantize\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003evq\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eVectorQuantize\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecodebook_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003euse_cosine_sim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e   \\u003cspan class=\\\"pl-c\\\"\\u003e# set this to True\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ecommit_loss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003evq\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eExpiring stale codes\\u003c/h3\\u003e\\u003ca id=\\\"user-content-expiring-stale-codes\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Expiring stale codes\\\" href=\\\"#expiring-stale-codes\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eFinally, the SoundStream paper has a scheme where they replace codes that have hits below a certain threshold with randomly selected vector from the current batch. You can set this threshold with \\u003ccode\\u003ethreshold_ema_dead_code\\u003c/code\\u003e keyword.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import VectorQuantize\\n\\nvq = VectorQuantize(\\n    dim = 256,\\n    codebook_size = 512,\\n    threshold_ema_dead_code = 2  # should actively replace any codes that have an exponential moving average cluster size less than 2\\n)\\n\\nx = torch.randn(1, 1024, 256)\\nquantized, indices, commit_loss = vq(x)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eVectorQuantize\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003evq\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eVectorQuantize\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecodebook_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ethreshold_ema_dead_code\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c\\\"\\u003e# should actively replace any codes that have an exponential moving average cluster size less than 2\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ecommit_loss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003evq\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eOrthogonal regularization loss\\u003c/h3\\u003e\\u003ca id=\\\"user-content-orthogonal-regularization-loss\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Orthogonal regularization loss\\\" href=\\\"#orthogonal-regularization-loss\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eVQ-VAE / VQ-GAN is quickly gaining popularity. A \\u003ca href=\\\"https://arxiv.org/abs/2112.00384\\\" rel=\\\"nofollow\\\"\\u003erecent paper\\u003c/a\\u003e proposes that when using vector quantization on images, enforcing the codebook to be orthogonal leads to translation equivariance of the discretized codes, leading to large improvements in downstream text to image generation tasks.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use this feature by simply setting the \\u003ccode\\u003eorthogonal_reg_weight\\u003c/code\\u003e to be greater than \\u003ccode\\u003e0\\u003c/code\\u003e, in which case the orthogonal regularization will be added to the auxiliary loss outputted by the module.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import VectorQuantize\\n\\nvq = VectorQuantize(\\n    dim = 256,\\n    codebook_size = 256,\\n    accept_image_fmap = True,                   # set this true to be able to pass in an image feature map\\n    orthogonal_reg_weight = 10,                 # in paper, they recommended a value of 10\\n    orthogonal_reg_max_codes = 128,             # this would randomly sample from the codebook for the orthogonal regularization loss, for limiting memory usage\\n    orthogonal_reg_active_codes_only = False    # set this to True if you have a very large codebook, and would only like to enforce the loss on the activated codes per batch\\n)\\n\\nimg_fmap = torch.randn(1, 256, 32, 32)\\nquantized, indices, loss = vq(img_fmap) # (1, 256, 32, 32), (1, 32, 32), (1,)\\n# loss now contains the orthogonal regularization loss with the weight as assigned\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eVectorQuantize\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003evq\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eVectorQuantize\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecodebook_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eaccept_image_fmap\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e,                   \\u003cspan class=\\\"pl-c\\\"\\u003e# set this true to be able to pass in an image feature map\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eorthogonal_reg_weight\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e10\\u003c/span\\u003e,                 \\u003cspan class=\\\"pl-c\\\"\\u003e# in paper, they recommended a value of 10\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eorthogonal_reg_max_codes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e,             \\u003cspan class=\\\"pl-c\\\"\\u003e# this would randomly sample from the codebook for the orthogonal regularization loss, for limiting memory usage\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eorthogonal_reg_active_codes_only\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eFalse\\u003c/span\\u003e    \\u003cspan class=\\\"pl-c\\\"\\u003e# set this to True if you have a very large codebook, and would only like to enforce the loss on the activated codes per batch\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg_fmap\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003evq\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg_fmap\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 256, 32, 32), (1, 32, 32), (1,)\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# loss now contains the orthogonal regularization loss with the weight as assigned\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eMulti-headed VQ\\u003c/h3\\u003e\\u003ca id=\\\"user-content-multi-headed-vq\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Multi-headed VQ\\\" href=\\\"#multi-headed-vq\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThere has been a number of papers that proposes variants of discrete latent representations with a multi-headed approach (multiple codes per feature). I have decided to offer one variant where the same codebook is used to vector quantize across the input dimension \\u003ccode\\u003ehead\\u003c/code\\u003e times.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can also use a more proven approach (memcodes) from \\u003ca href=\\\"https://github.com/lucidrains/nwt-pytorch\\\"\\u003eNWT paper\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import VectorQuantize\\n\\nvq = VectorQuantize(\\n    dim = 256,\\n    codebook_dim = 32,                  # a number of papers have shown smaller codebook dimension to be acceptable\\n    heads = 8,                          # number of heads to vector quantize, codebook shared across all heads\\n    separate_codebook_per_head = True,  # whether to have a separate codebook per head. False would mean 1 shared codebook\\n    codebook_size = 8196,\\n    accept_image_fmap = True\\n)\\n\\nimg_fmap = torch.randn(1, 256, 32, 32)\\nquantized, indices, loss = vq(img_fmap) # (1, 256, 32, 32), (1, 32, 32, 8), (1,)\\n\\n# indices shape - (batch, height, width, heads)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eVectorQuantize\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003evq\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eVectorQuantize\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecodebook_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,                  \\u003cspan class=\\\"pl-c\\\"\\u003e# a number of papers have shown smaller codebook dimension to be acceptable\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,                          \\u003cspan class=\\\"pl-c\\\"\\u003e# number of heads to vector quantize, codebook shared across all heads\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eseparate_codebook_per_head\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e,  \\u003cspan class=\\\"pl-c\\\"\\u003e# whether to have a separate codebook per head. False would mean 1 shared codebook\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecodebook_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8196\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eaccept_image_fmap\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg_fmap\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003evq\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg_fmap\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 256, 32, 32), (1, 32, 32, 8), (1,)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# indices shape - (batch, height, width, heads)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eRandom Projection Quantizer\\u003c/h3\\u003e\\u003ca id=\\\"user-content-random-projection-quantizer\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Random Projection Quantizer\\\" href=\\\"#random-projection-quantizer\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2202.01855\\\" rel=\\\"nofollow\\\"\\u003eThis paper\\u003c/a\\u003e first proposed to use a random projection quantizer for masked speech modeling, where signals are projected with a randomly initialized matrix and then matched with a random initialized codebook. One therefore does not need to learn the quantizer. This technique was used by Google's \\u003ca href=\\\"https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html\\\" rel=\\\"nofollow\\\"\\u003eUniversal Speech Model\\u003c/a\\u003e to achieve SOTA for speech-to-text modeling.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eUSM further proposes to use multiple codebook, and the masked speech modeling with a multi-softmax objective. You can do this easily by setting \\u003ccode\\u003enum_codebooks\\u003c/code\\u003e to be greater than 1\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import RandomProjectionQuantizer\\n\\nquantizer = RandomProjectionQuantizer(\\n    dim = 512,               # input dimensions\\n    num_codebooks = 16,      # in USM, they used up to 16 for 5% gain\\n    codebook_dim = 256,      # codebook dimension\\n    codebook_size = 1024     # codebook size\\n)\\n\\nx = torch.randn(1, 1024, 512)\\nindices = quantizer(x) # (1, 1024, 16) - (batch, seq, num_codebooks)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eRandomProjectionQuantizer\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantizer\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eRandomProjectionQuantizer\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,               \\u003cspan class=\\\"pl-c\\\"\\u003e# input dimensions\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_codebooks\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# in USM, they used up to 16 for 5% gain\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecodebook_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# codebook dimension\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecodebook_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e     \\u003cspan class=\\\"pl-c\\\"\\u003e# codebook size\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003equantizer\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1024, 16) - (batch, seq, num_codebooks)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis repository should also automatically synchronizing the codebooks in a multi-process setting. If somehow it isn't, please open an issue. You can override whether to synchronize codebooks or not by setting \\u003ccode\\u003esync_codebook = True | False\\u003c/code\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eFinite Scalar Quantization\\u003c/h3\\u003e\\u003ca id=\\\"user-content-finite-scalar-quantization\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Finite Scalar Quantization\\\" href=\\\"#finite-scalar-quantization\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vector-quantize-pytorch/blob/master/images/fsq.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vector-quantize-pytorch/raw/master/images/fsq.png\\\" width=\\\"500px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003ctable\\u003e\\n\\u003cthead\\u003e\\n\\u003ctr\\u003e\\n\\u003cth\\u003e\\u003c/th\\u003e\\n\\u003cth\\u003eVQ\\u003c/th\\u003e\\n\\u003cth\\u003eFSQ\\u003c/th\\u003e\\n\\u003c/tr\\u003e\\n\\u003c/thead\\u003e\\n\\u003ctbody\\u003e\\n\\u003ctr\\u003e\\n\\u003ctd\\u003eQuantization\\u003c/td\\u003e\\n\\u003ctd\\u003eargmin_c || z-c ||\\u003c/td\\u003e\\n\\u003ctd\\u003eround(f(z))\\u003c/td\\u003e\\n\\u003c/tr\\u003e\\n\\u003ctr\\u003e\\n\\u003ctd\\u003eGradients\\u003c/td\\u003e\\n\\u003ctd\\u003eStraight Through Estimation (STE)\\u003c/td\\u003e\\n\\u003ctd\\u003eSTE\\u003c/td\\u003e\\n\\u003c/tr\\u003e\\n\\u003ctr\\u003e\\n\\u003ctd\\u003eAuxiliary Losses\\u003c/td\\u003e\\n\\u003ctd\\u003eCommitment, codebook, entropy loss, ...\\u003c/td\\u003e\\n\\u003ctd\\u003eN/A\\u003c/td\\u003e\\n\\u003c/tr\\u003e\\n\\u003ctr\\u003e\\n\\u003ctd\\u003eTricks\\u003c/td\\u003e\\n\\u003ctd\\u003eEMA on codebook, codebook splitting, projections, ...\\u003c/td\\u003e\\n\\u003ctd\\u003eN/A\\u003c/td\\u003e\\n\\u003c/tr\\u003e\\n\\u003ctr\\u003e\\n\\u003ctd\\u003eParameters\\u003c/td\\u003e\\n\\u003ctd\\u003eCodebook\\u003c/td\\u003e\\n\\u003ctd\\u003eN/A\\u003c/td\\u003e\\n\\u003c/tr\\u003e\\n\\u003c/tbody\\u003e\\n\\u003c/table\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2309.15505\\\" rel=\\\"nofollow\\\"\\u003eThis\\u003c/a\\u003e work out of Google Deepmind aims to vastly simplify the way vector quantization is done for generative modeling, removing the need for commitment losses, EMA updating of the codebook, as well as tackle the issues with codebook collapse or insufficient utilization. They simply round each scalar into discrete levels with straight through gradients; the codes become uniform points in a hypercube.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThanks goes out to \\u003ca href=\\\"https://github.com/sekstini\\\"\\u003e@sekstini\\u003c/a\\u003e for porting over this implementation in record time!\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import FSQ\\n\\nlevels = [8,5,5,5] # see 4.1 and A.4.1 in the paper\\nquantizer = FSQ(levels)\\n\\nx = torch.randn(1, 1024, 4) # 4 since there are 4 levels\\nxhat, indices = quantizer(x)\\n\\nprint(xhat.shape)    # (1, 1024, 4) - (batch, seq, dim)\\nprint(indices.shape) # (1, 1024)    - (batch, seq)\\n\\nassert xhat.shape == x.shape\\nassert torch.all(xhat == quantizer.indices_to_codes(indices))\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eFSQ\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003elevels\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e [\\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\u003cspan class=\\\"pl-c1\\\"\\u003e5\\u003c/span\\u003e,\\u003cspan class=\\\"pl-c1\\\"\\u003e5\\u003c/span\\u003e,\\u003cspan class=\\\"pl-c1\\\"\\u003e5\\u003c/span\\u003e] \\u003cspan class=\\\"pl-c\\\"\\u003e# see 4.1 and A.4.1 in the paper\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantizer\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eFSQ\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003elevels\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# 4 since there are 4 levels\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003exhat\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003equantizer\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-en\\\"\\u003eprint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003exhat\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e)    \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1024, 4) - (batch, seq, dim)\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-en\\\"\\u003eprint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1024)    - (batch, seq)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003eassert\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003exhat\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e==\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003eassert\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eall\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003exhat\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e==\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003equantizer\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eindices_to_codes\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e))\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eAn improvised Residual FSQ, for an attempt to improve audio encoding.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eCredit goes to \\u003ca href=\\\"https://github.com/sekstini\\\"\\u003e@sekstini\\u003c/a\\u003e for originally incepting the idea \\u003ca href=\\\"https://github.com/lucidrains/vector-quantize-pytorch/pull/74#issuecomment-1742048597\\\" data-hovercard-type=\\\"pull_request\\\" data-hovercard-url=\\\"/lucidrains/vector-quantize-pytorch/pull/74/hovercard\\\"\\u003ehere\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import ResidualFSQ\\n\\nresidual_fsq = ResidualFSQ(\\n    dim = 256,\\n    levels = [8, 5, 5, 3],\\n    num_quantizers = 8\\n)\\n\\nx = torch.randn(1, 1024, 256)\\n\\nresidual_fsq.eval()\\n\\nquantized, indices = residual_fsq(x)\\n\\n# (1, 1024, 256), (1, 1024, 8), (8)\\n# (batch, seq, dim), (batch, seq, quantizers), (quantizers)\\n\\nquantized_out = residual_fsq.get_output_from_indices(indices)\\n\\n# (8, 1, 1024, 8)\\n# (residual layers, batch, seq, quantizers)\\n\\nassert torch.all(quantized == quantized_out)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eResidualFSQ\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eresidual_fsq\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eResidualFSQ\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elevels\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e [\\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e5\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e5\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e],\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_quantizers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eresidual_fsq\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eeval\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003eresidual_fsq\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1024, 256), (1, 1024, 8), (8)\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (batch, seq, dim), (batch, seq, quantizers), (quantizers)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized_out\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eresidual_fsq\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eget_output_from_indices\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (8, 1, 1024, 8)\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (residual layers, batch, seq, quantizers)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003eassert\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eall\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e==\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003equantized_out\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eLookup Free Quantization\\u003c/h3\\u003e\\u003ca id=\\\"user-content-lookup-free-quantization\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Lookup Free Quantization\\\" href=\\\"#lookup-free-quantization\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vector-quantize-pytorch/blob/master/images/lfq.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vector-quantize-pytorch/raw/master/images/lfq.png\\\" width=\\\"450px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe research team behind \\u003ca href=\\\"https://arxiv.org/abs/2212.05199\\\" rel=\\\"nofollow\\\"\\u003eMagViT\\u003c/a\\u003e has released new SOTA results for generative video modeling. A core change between v1 and v2 include a new type of quantization, look-up free quantization (LFQ), which eliminates the codebook and embedding lookup entirely.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis paper presents a simple LFQ quantizer of using independent binary latents. Other implementations of LFQ exist. However, the team shows that MAGVIT-v2 with LFQ significantly improves on the ImageNet benchmark. The differences between LFQ and 2-level FSQ includes entropy regularizations as well as maintained commitment loss.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eDeveloping a more advanced method of LFQ quantization without codebook-lookup could revolutionize generative modeling.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use it simply as follows. Will be dogfooded at \\u003ca href=\\\"https://github.com/lucidrains/magvit2-pytorch\\\"\\u003eMagViT2 pytorch port\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import LFQ\\n\\n# you can specify either dim or codebook_size\\n# if both specified, will be validated against each other\\n\\nquantizer = LFQ(\\n    codebook_size = 65536,      # codebook size, must be a power of 2\\n    dim = 16,                   # this is the input feature dimension, defaults to log2(codebook_size) if not defined\\n    entropy_loss_weight = 0.1,  # how much weight to place on entropy loss\\n    diversity_gamma = 1.        # within entropy loss, how much weight to give to diversity of codes, taken from https://arxiv.org/abs/1911.05894\\n)\\n\\nimage_feats = torch.randn(1, 16, 32, 32)\\n\\nquantized, indices, entropy_aux_loss = quantizer(image_feats, inv_temperature=100.)  # you may want to experiment with temperature\\n\\n# (1, 16, 32, 32), (1, 32, 32), (1,)\\n\\nassert image_feats.shape == quantized.shape\\nassert (quantized == quantizer.indices_to_codes(indices)).all()\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eLFQ\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# you can specify either dim or codebook_size\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# if both specified, will be validated against each other\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantizer\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eLFQ\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecodebook_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e65536\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# codebook size, must be a power of 2\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,                   \\u003cspan class=\\\"pl-c\\\"\\u003e# this is the input feature dimension, defaults to log2(codebook_size) if not defined\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eentropy_loss_weight\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,  \\u003cspan class=\\\"pl-c\\\"\\u003e# how much weight to place on entropy loss\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ediversity_gamma\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1.\\u003c/span\\u003e        \\u003cspan class=\\\"pl-c\\\"\\u003e# within entropy loss, how much weight to give to diversity of codes, taken from https://arxiv.org/abs/1911.05894\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimage_feats\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eentropy_aux_loss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003equantizer\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimage_feats\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003einv_temperature\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e100.\\u003c/span\\u003e)  \\u003cspan class=\\\"pl-c\\\"\\u003e# you may want to experiment with temperature\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 16, 32, 32), (1, 32, 32), (1,)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003eassert\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_feats\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e==\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003eassert\\u003c/span\\u003e (\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e==\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003equantizer\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eindices_to_codes\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e)).\\u003cspan class=\\\"pl-en\\\"\\u003eall\\u003c/span\\u003e()\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can also pass in video features as \\u003ccode\\u003e(batch, feat, time, height, width)\\u003c/code\\u003e or sequences as \\u003ccode\\u003e(batch, seq, feat)\\u003c/code\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"\\nseq = torch.randn(1, 32, 16)\\nquantized, *_ = quantizer(seq)\\n\\nassert seq.shape == quantized.shape\\n\\nvideo_feats = torch.randn(1, 16, 10, 32, 32)\\nquantized, *_ = quantizer(video_feats)\\n\\nassert video_feats.shape == quantized.shape\\n\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-s1\\\"\\u003eseq\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e*\\u003c/span\\u003e\\u003cspan class=\\\"pl-s1\\\"\\u003e_\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003equantizer\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eseq\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003eassert\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eseq\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e==\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003evideo_feats\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e10\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e*\\u003c/span\\u003e\\u003cspan class=\\\"pl-s1\\\"\\u003e_\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003equantizer\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003evideo_feats\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003eassert\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evideo_feats\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e==\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eOr support multiple codebooks\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import LFQ\\n\\nquantizer = LFQ(\\n    codebook_size = 4096,\\n    dim = 16,\\n    num_codebooks = 4  # 4 codebooks, total codebook dimension is log2(4096) * 4\\n)\\n\\nimage_feats = torch.randn(1, 16, 32, 32)\\n\\nquantized, indices, entropy_aux_loss = quantizer(image_feats)\\n\\n# (1, 16, 32, 32), (1, 32, 32, 4), (1,)\\n\\nassert image_feats.shape == quantized.shape\\nassert (quantized == quantizer.indices_to_codes(indices)).all()\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eLFQ\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantizer\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eLFQ\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecodebook_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4096\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_codebooks\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c\\\"\\u003e# 4 codebooks, total codebook dimension is log2(4096) * 4\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimage_feats\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eentropy_aux_loss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003equantizer\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimage_feats\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 16, 32, 32), (1, 32, 32, 4), (1,)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003eassert\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_feats\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e==\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003eassert\\u003c/span\\u003e (\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e==\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003equantizer\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eindices_to_codes\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e)).\\u003cspan class=\\\"pl-en\\\"\\u003eall\\u003c/span\\u003e()\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eAn improvised Residual LFQ, to see if it can lead to an improvement for audio compression.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import ResidualLFQ\\n\\nresidual_lfq = ResidualLFQ(\\n    dim = 256,\\n    codebook_size = 256,\\n    num_quantizers = 8\\n)\\n\\nx = torch.randn(1, 1024, 256)\\n\\nresidual_lfq.eval()\\n\\nquantized, indices, commit_loss = residual_lfq(x)\\n\\n# (1, 1024, 256), (1, 1024, 8), (8)\\n# (batch, seq, dim), (batch, seq, quantizers), (quantizers)\\n\\nquantized_out = residual_lfq.get_output_from_indices(indices)\\n\\n# (8, 1, 1024, 8)\\n# (residual layers, batch, seq, quantizers)\\n\\nassert torch.all(quantized == quantized_out)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eResidualLFQ\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eresidual_lfq\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eResidualLFQ\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecodebook_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_quantizers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eresidual_lfq\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eeval\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ecommit_loss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003eresidual_lfq\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ex\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1024, 256), (1, 1024, 8), (8)\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (batch, seq, dim), (batch, seq, quantizers), (quantizers)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized_out\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eresidual_lfq\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eget_output_from_indices\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (8, 1, 1024, 8)\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (residual layers, batch, seq, quantizers)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003eassert\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eall\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e==\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003equantized_out\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eLatent Quantization\\u003c/h3\\u003e\\u003ca id=\\\"user-content-latent-quantization\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Latent Quantization\\\" href=\\\"#latent-quantization\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eDisentanglement is essential for representation learning as it promotes interpretability, generalization, improved learning, and robustness. It aligns with the goal of capturing meaningful and independent features of the data, facilitating more effective use of learned representations across various applications. For better disentanglement, the challenge is to disentangle underlying variations in a dataset without explicit ground truth information. This work introduces a key inductive bias aimed at encoding and decoding within an organized latent space. The strategy incorporated encompasses discretizing the latent space by assigning discrete code vectors through the utilization of an individual learnable scalar codebook for each dimension. This methodology enables their models to surpass robust prior methods effectively.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eBe aware they had to use a very high weight decay for the results in this paper.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import LatentQuantize\\n\\n# you can specify either dim or codebook_size\\n# if both specified, will be validated against each other\\n\\nquantizer = LatentQuantize(\\n    levels = [5, 5, 8],      # number of levels per codebook dimension\\n    dim = 16,                   # input dim\\n    commitment_loss_weight=0.1,  \\n    quantization_loss_weight=0.1,\\n)\\n\\nimage_feats = torch.randn(1, 16, 32, 32)\\n\\nquantized, indices, loss = quantizer(image_feats)\\n\\n# (1, 16, 32, 32), (1, 32, 32), (1,)\\n\\nassert image_feats.shape == quantized.shape\\nassert (quantized == quantizer.indices_to_codes(indices)).all()\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eLatentQuantize\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# you can specify either dim or codebook_size\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# if both specified, will be validated against each other\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantizer\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eLatentQuantize\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elevels\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e [\\u003cspan class=\\\"pl-c1\\\"\\u003e5\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e5\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e],      \\u003cspan class=\\\"pl-c\\\"\\u003e# number of levels per codebook dimension\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,                   \\u003cspan class=\\\"pl-c\\\"\\u003e# input dim\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecommitment_loss_weight\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,  \\n    \\u003cspan class=\\\"pl-s1\\\"\\u003equantization_loss_weight\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimage_feats\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003equantizer\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimage_feats\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 16, 32, 32), (1, 32, 32), (1,)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003eassert\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_feats\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e==\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003eassert\\u003c/span\\u003e (\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e==\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003equantizer\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eindices_to_codes\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e)).\\u003cspan class=\\\"pl-en\\\"\\u003eall\\u003c/span\\u003e()\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can also pass in video features as \\u003ccode\\u003e(batch, feat, time, height, width)\\u003c/code\\u003e or sequences as \\u003ccode\\u003e(batch, seq, feat)\\u003c/code\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"\\nseq = torch.randn(1, 32, 16)\\nquantized, *_ = quantizer(seq)\\n\\nassert seq.shape == quantized.shape\\n\\nvideo_feats = torch.randn(1, 16, 10, 32, 32)\\nquantized, *_ = quantizer(video_feats)\\n\\nassert video_feats.shape == quantized.shape\\n\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-s1\\\"\\u003eseq\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e*\\u003c/span\\u003e\\u003cspan class=\\\"pl-s1\\\"\\u003e_\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003equantizer\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eseq\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003eassert\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eseq\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e==\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003evideo_feats\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e10\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e*\\u003c/span\\u003e\\u003cspan class=\\\"pl-s1\\\"\\u003e_\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003equantizer\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003evideo_feats\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003eassert\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evideo_feats\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e==\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003equantized\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eOr support multiple codebooks\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vector_quantize_pytorch import LatentQuantize\\n\\nlevels = [4, 8, 16]\\ndim = 9\\nnum_codebooks = 3\\n\\nmodel = LatentQuantize(levels, dim, num_codebooks=num_codebooks)\\n\\ninput_tensor = torch.randn(2, 3, dim)\\noutput_tensor, indices, loss = model(input_tensor)\\n\\nassert output_tensor.shape == input_tensor.shape\\nassert indices.shape == (2, 3, num_codebooks)\\nassert loss.item() \\u0026gt;= 0\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evector_quantize_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eLatentQuantize\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003elevels\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e [\\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e]\\n\\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e9\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003enum_codebooks\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eLatentQuantize\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003elevels\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003enum_codebooks\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-s1\\\"\\u003enum_codebooks\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003einput_tensor\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eoutput_tensor\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003einput_tensor\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003eassert\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eoutput_tensor\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e==\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003einput_tensor\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003eassert\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eindices\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eshape\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e==\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003enum_codebooks\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-k\\\"\\u003eassert\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eitem\\u003c/span\\u003e() \\u003cspan class=\\\"pl-c1\\\"\\u003e\\u0026gt;=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eCitations\\u003c/h2\\u003e\\u003ca id=\\\"user-content-citations\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Citations\\\" href=\\\"#citations\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{oord2018neural,\\n    title   = {Neural Discrete Representation Learning},\\n    author  = {Aaron van den Oord and Oriol Vinyals and Koray Kavukcuoglu},\\n    year    = {2018},\\n    eprint  = {1711.00937},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.LG}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eoord2018neural\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eNeural Discrete Representation Learning\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAaron van den Oord and Oriol Vinyals and Koray Kavukcuoglu\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2018\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e1711.00937\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.LG\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{zeghidour2021soundstream,\\n    title   = {SoundStream: An End-to-End Neural Audio Codec},\\n    author  = {Neil Zeghidour and Alejandro Luebs and Ahmed Omran and Jan Skoglund and Marco Tagliasacchi},\\n    year    = {2021},\\n    eprint  = {2107.03312},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.SD}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ezeghidour2021soundstream\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eSoundStream: An End-to-End Neural Audio Codec\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eNeil Zeghidour and Alejandro Luebs and Ahmed Omran and Jan Skoglund and Marco Tagliasacchi\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2107.03312\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.SD\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{anonymous2022vectorquantized,\\n    title   = {Vector-quantized Image Modeling with Improved {VQGAN}},\\n    author  = {Anonymous},\\n    booktitle = {Submitted to The Tenth International Conference on Learning Representations },\\n    year    = {2022},\\n    url     = {https://openreview.net/forum?id=pfNyExj7z2},\\n    note    = {under review}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eanonymous2022vectorquantized\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eVector-quantized Image Modeling with Improved {VQGAN}\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAnonymous\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ebooktitle\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eSubmitted to The Tenth International Conference on Learning Representations \\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://openreview.net/forum?id=pfNyExj7z2\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003enote\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eunder review\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@unknown{unknown,\\n    author  = {Lee, Doyup and Kim, Chiheon and Kim, Saehoon and Cho, Minsu and Han, Wook-Shin},\\n    year    = {2022},\\n    month   = {03},\\n    title   = {Autoregressive Image Generation using Residual Quantization}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@unknown\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eunknown\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eLee, Doyup and Kim, Chiheon and Kim, Saehoon and Cho, Minsu and Han, Wook-Shin\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003emonth\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e03\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAutoregressive Image Generation using Residual Quantization\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{Defossez2022HighFN,\\n    title   = {High Fidelity Neural Audio Compression},\\n    author  = {Alexandre D'efossez and Jade Copet and Gabriel Synnaeve and Yossi Adi},\\n    journal = {ArXiv},\\n    year    = {2022},\\n    volume  = {abs/2210.13438}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eDefossez2022HighFN\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eHigh Fidelity Neural Audio Compression\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAlexandre D'efossez and Jade Copet and Gabriel Synnaeve and Yossi Adi\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eArXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003evolume\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eabs/2210.13438\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Chiu2022SelfsupervisedLW,\\n    title   = {Self-supervised Learning with Random-projection Quantizer for Speech Recognition},\\n    author  = {Chung-Cheng Chiu and James Qin and Yu Zhang and Jiahui Yu and Yonghui Wu},\\n    booktitle = {International Conference on Machine Learning},\\n    year    = {2022}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eChiu2022SelfsupervisedLW\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eSelf-supervised Learning with Random-projection Quantizer for Speech Recognition\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eChung-Cheng Chiu and James Qin and Yu Zhang and Jiahui Yu and Yonghui Wu\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ebooktitle\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eInternational Conference on Machine Learning\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Zhang2023GoogleUS,\\n    title   = {Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages},\\n    author  = {Yu Zhang and Wei Han and James Qin and Yongqiang Wang and Ankur Bapna and Zhehuai Chen and Nanxin Chen and Bo Li and Vera Axelrod and Gary Wang and Zhong Meng and Ke Hu and Andrew Rosenberg and Rohit Prabhavalkar and Daniel S. Park and Parisa Haghani and Jason Riesa and Ginger Perng and Hagen Soltau and Trevor Strohman and Bhuvana Ramabhadran and Tara N. Sainath and Pedro J. Moreno and Chung-Cheng Chiu and Johan Schalkwyk and Franccoise Beaufays and Yonghui Wu},\\n    year    = {2023}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eZhang2023GoogleUS\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eGoogle USM: Scaling Automatic Speech Recognition Beyond 100 Languages\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eYu Zhang and Wei Han and James Qin and Yongqiang Wang and Ankur Bapna and Zhehuai Chen and Nanxin Chen and Bo Li and Vera Axelrod and Gary Wang and Zhong Meng and Ke Hu and Andrew Rosenberg and Rohit Prabhavalkar and Daniel S. Park and Parisa Haghani and Jason Riesa and Ginger Perng and Hagen Soltau and Trevor Strohman and Bhuvana Ramabhadran and Tara N. Sainath and Pedro J. Moreno and Chung-Cheng Chiu and Johan Schalkwyk and Franccoise Beaufays and Yonghui Wu\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Shen2023NaturalSpeech2L,\\n    title   = {NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers},\\n    author  = {Kai Shen and Zeqian Ju and Xu Tan and Yanqing Liu and Yichong Leng and Lei He and Tao Qin and Sheng Zhao and Jiang Bian},\\n    year    = {2023}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eShen2023NaturalSpeech2L\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eNaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eKai Shen and Zeqian Ju and Xu Tan and Yanqing Liu and Yichong Leng and Lei He and Tao Qin and Sheng Zhao and Jiang Bian\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Yang2023HiFiCodecGV,\\n    title   = {HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec},\\n    author  = {Dongchao Yang and Songxiang Liu and Rongjie Huang and Jinchuan Tian and Chao Weng and Yuexian Zou},\\n    year    = {2023}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eYang2023HiFiCodecGV\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eHiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eDongchao Yang and Songxiang Liu and Rongjie Huang and Jinchuan Tian and Chao Weng and Yuexian Zou\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{Liu2023BridgingDA,\\n    title   = {Bridging Discrete and Backpropagation: Straight-Through and Beyond},\\n    author  = {Liyuan Liu and Chengyu Dong and Xiaodong Liu and Bin Yu and Jianfeng Gao},\\n    journal = {ArXiv},\\n    year    = {2023},\\n    volume  = {abs/2304.08612}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eLiu2023BridgingDA\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eBridging Discrete and Backpropagation: Straight-Through and Beyond\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eLiyuan Liu and Chengyu Dong and Xiaodong Liu and Bin Yu and Jianfeng Gao\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eArXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003evolume\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eabs/2304.08612\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{huh2023improvedvqste,\\n    title   = {Straightening Out the Straight-Through Estimator: Overcoming Optimization Challenges in Vector Quantized Networks},\\n    author  = {Huh, Minyoung and Cheung, Brian and Agrawal, Pulkit and Isola, Phillip},\\n    booktitle = {International Conference on Machine Learning},\\n    year    = {2023},\\n    organization = {PMLR}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ehuh2023improvedvqste\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eStraightening Out the Straight-Through Estimator: Overcoming Optimization Challenges in Vector Quantized Networks\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eHuh, Minyoung and Cheung, Brian and Agrawal, Pulkit and Isola, Phillip\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ebooktitle\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eInternational Conference on Machine Learning\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eorganization\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ePMLR\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{rogozhnikov2022einops,\\n    title   = {Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation},\\n    author  = {Alex Rogozhnikov},\\n    booktitle = {International Conference on Learning Representations},\\n    year    = {2022},\\n    url     = {https://openreview.net/forum?id=oapKSVM2bcj}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003erogozhnikov2022einops\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eEinops: Clear and Reliable Tensor Manipulations with Einstein-like Notation\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAlex Rogozhnikov\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ebooktitle\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eInternational Conference on Learning Representations\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://openreview.net/forum?id=oapKSVM2bcj\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{shin2021translationequivariant,\\n    title   = {Translation-equivariant Image Quantizer for Bi-directional Image-Text Generation},\\n    author  = {Woncheol Shin and Gyubok Lee and Jiyoung Lee and Joonseok Lee and Edward Choi},\\n    year    = {2021},\\n    eprint  = {2112.00384},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eshin2021translationequivariant\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eTranslation-equivariant Image Quantizer for Bi-directional Image-Text Generation\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eWoncheol Shin and Gyubok Lee and Jiyoung Lee and Joonseok Lee and Edward Choi\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2112.00384\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{mentzer2023finite,\\n    title   = {Finite Scalar Quantization: VQ-VAE Made Simple},\\n    author  = {Fabian Mentzer and David Minnen and Eirikur Agustsson and Michael Tschannen},\\n    year    = {2023},\\n    eprint  = {2309.15505},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ementzer2023finite\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eFinite Scalar Quantization: VQ-VAE Made Simple\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eFabian Mentzer and David Minnen and Eirikur Agustsson and Michael Tschannen\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2309.15505\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{yu2023language,\\n    title   = {Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation},\\n    author  = {Lijun Yu and Jos\u00e9 Lezama and Nitesh B. Gundavarapu and Luca Versari and Kihyuk Sohn and David Minnen and Yong Cheng and Agrim Gupta and Xiuye Gu and Alexander G. Hauptmann and Boqing Gong and Ming-Hsuan Yang and Irfan Essa and David A. Ross and Lu Jiang},\\n    year    = {2023},\\n    eprint  = {2310.05737},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eyu2023language\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eLanguage Model Beats Diffusion -- Tokenizer is Key to Visual Generation\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eLijun Yu and Jos\u00e9 Lezama and Nitesh B. Gundavarapu and Luca Versari and Kihyuk Sohn and David Minnen and Yong Cheng and Agrim Gupta and Xiuye Gu and Alexander G. Hauptmann and Boqing Gong and Ming-Hsuan Yang and Irfan Essa and David A. Ross and Lu Jiang\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2310.05737\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{hsu2023disentanglement,\\n    title   = {Disentanglement via Latent Quantization}, \\n    author  = {Kyle Hsu and Will Dorrell and James C. R. Whittington and Jiajun Wu and Chelsea Finn},\\n    year    = {2023},\\n    eprint  = {2305.18378},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.LG}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ehsu2023disentanglement\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eDisentanglement via Latent Quantization\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eKyle Hsu and Will Dorrell and James C. R. Whittington and Jiajun Wu and Chelsea Finn\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2305.18378\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.LG\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003c/article\\u003e\",\"loaded\":true,\"timedOut\":false,\"errorMessage\":null,\"headerInfo\":{\"toc\":[{\"level\":2,\"text\":\"Vector Quantization - Pytorch\",\"anchor\":\"vector-quantization---pytorch\",\"htmlText\":\"Vector Quantization - Pytorch\"},{\"level\":2,\"text\":\"Install\",\"anchor\":\"install\",\"htmlText\":\"Install\"},{\"level\":2,\"text\":\"Usage\",\"anchor\":\"usage\",\"htmlText\":\"Usage\"},{\"level\":2,\"text\":\"Residual VQ\",\"anchor\":\"residual-vq\",\"htmlText\":\"Residual VQ\"},{\"level\":2,\"text\":\"Initialization\",\"anchor\":\"initialization\",\"htmlText\":\"Initialization\"},{\"level\":2,\"text\":\"Increasing codebook usage\",\"anchor\":\"increasing-codebook-usage\",\"htmlText\":\"Increasing codebook usage\"},{\"level\":3,\"text\":\"Lower codebook dimension\",\"anchor\":\"lower-codebook-dimension\",\"htmlText\":\"Lower codebook dimension\"},{\"level\":3,\"text\":\"Cosine similarity\",\"anchor\":\"cosine-similarity\",\"htmlText\":\"Cosine similarity\"},{\"level\":3,\"text\":\"Expiring stale codes\",\"anchor\":\"expiring-stale-codes\",\"htmlText\":\"Expiring stale codes\"},{\"level\":3,\"text\":\"Orthogonal regularization loss\",\"anchor\":\"orthogonal-regularization-loss\",\"htmlText\":\"Orthogonal regularization loss\"},{\"level\":3,\"text\":\"Multi-headed VQ\",\"anchor\":\"multi-headed-vq\",\"htmlText\":\"Multi-headed VQ\"},{\"level\":3,\"text\":\"Random Projection Quantizer\",\"anchor\":\"random-projection-quantizer\",\"htmlText\":\"Random Projection Quantizer\"},{\"level\":3,\"text\":\"Finite Scalar Quantization\",\"anchor\":\"finite-scalar-quantization\",\"htmlText\":\"Finite Scalar Quantization\"},{\"level\":3,\"text\":\"Lookup Free Quantization\",\"anchor\":\"lookup-free-quantization\",\"htmlText\":\"Lookup Free Quantization\"},{\"level\":3,\"text\":\"Latent Quantization\",\"anchor\":\"latent-quantization\",\"htmlText\":\"Latent Quantization\"},{\"level\":2,\"text\":\"Citations\",\"anchor\":\"citations\",\"htmlText\":\"Citations\"}],\"siteNavLoginPath\":\"/login?return_to=https%3A%2F%2Fgithub.com%2Flucidrains%2Fvector-quantize-pytorch\"}},{\"displayName\":\"LICENSE\",\"repoName\":\"vector-quantize-pytorch\",\"refName\":\"master\",\"path\":\"LICENSE\",\"preferredFileType\":\"license\",\"tabName\":\"MIT\",\"richText\":null,\"loaded\":false,\"timedOut\":false,\"errorMessage\":null,\"headerInfo\":{\"toc\":null,\"siteNavLoginPath\":\"/login?return_to=https%3A%2F%2Fgithub.com%2Flucidrains%2Fvector-quantize-pytorch\"}}],\"overviewFilesProcessingTime\":146.469173}},\"appPayload\":{\"helpUrl\":\"https://docs.github.com\",\"findFileWorkerPath\":\"/assets-cdn/worker/find-file-worker-32bb159cc57c.js\",\"findInFileWorkerPath\":\"/assets-cdn/worker/find-in-file-worker-c6704d501c10.js\",\"githubDevUrl\":null,\"enabled_features\":{\"code_nav_ui_events\":false,\"copilot_conversational_ux\":false,\"copilot_conversational_ux_embedding_update\":false,\"copilot_popover_file_editor_header\":false,\"copilot_smell_icebreaker_ux\":true,\"copilot_workspace\":false,\"codeview_firefox_inert\":true}}}}</script>  <div data-target=\"react-partial.reactRoot\"><style data-styled=\"true\" data-styled-version=\"5.3.6\">.cgQnMS{font-weight:600;font-size:32px;margin:0;}/*!sc*/data-styled.g1[id=\"Heading__StyledHeading-sc-1c1dgg0-0\"]{content:\"cgQnMS,\"}/*!sc*/.izjvBm{margin-top:16px;margin-bottom:16px;}/*!sc*/.rPQgy{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/.eUMEDg{margin-bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;row-gap:16px;}/*!sc*/.eLcVee{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;padding-bottom:16px;padding-top:8px;}/*!sc*/.hsfLlq{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;gap:8px;}/*!sc*/@media screen and (max-width:320px){.hsfLlq{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}}/*!sc*/.gpKoUz{position:relative;}/*!sc*/@media screen and (max-width:380px){.gpKoUz .ref-selector-button-text-container{max-width:80px;}}/*!sc*/@media screen and (max-width:320px){.gpKoUz{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}.gpKoUz .overview-ref-selector{width:100%;}.gpKoUz .overview-ref-selector > span{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;}.gpKoUz .overview-ref-selector > span > span[data-component=\"text\"]{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}}/*!sc*/.kkrdEu{-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}/*!sc*/.bKgizp{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;}/*!sc*/.iPGYsi{margin-right:4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.dKmYfk{font-size:14px;min-width:0;max-width:125px;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}/*!sc*/.trpoQ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;pointer-events:none;}/*!sc*/.laYubZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (max-width:1079px){.laYubZ{display:none;}}/*!sc*/.swnaL{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (min-width:1080px){.swnaL{display:none;}}/*!sc*/@media screen and (max-width:543px){.swnaL{display:none;}}/*!sc*/.bWpuBf{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-left:8px;gap:8px;}/*!sc*/.grHjNb{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:8px;}/*!sc*/@media screen and (max-width:543px){.grHjNb{display:none;}}/*!sc*/.dXTsqj{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (max-width:1011px){.dXTsqj{display:none;}}/*!sc*/.dCOrmu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (min-width:1012px){.dCOrmu{display:none;}}/*!sc*/@media screen and (max-width:544px){.bVvbgP{display:none;}}/*!sc*/.bNDvfp{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (min-width:544px){.bNDvfp{display:none;}}/*!sc*/.yfPnm{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;gap:16px;}/*!sc*/.cAQuiW{width:100%;border-collapse:separate;border-spacing:0;border:1px solid;border-color:var(--borderColor-default,var(--color-border-default,#d0d7de));border-radius:6px;table-layout:fixed;overflow:unset;}/*!sc*/.iiUlLN{height:0px;line-height:0px;}/*!sc*/.iiUlLN tr{height:0px;font-size:0px;}/*!sc*/.jmggSN{padding:16px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));font-size:12px;text-align:left;height:40px;}/*!sc*/.jmggSN th{padding-left:16px;background-color:var(--bgColor-muted,var(--color-canvas-subtle,#f6f8fa));}/*!sc*/.kvYunM{width:100%;border-top-left-radius:6px;}/*!sc*/@media screen and (min-width:544px){.kvYunM{display:none;}}/*!sc*/.hrLuxA{width:40%;border-top-left-radius:6px;}/*!sc*/@media screen and (max-width:543px){.hrLuxA{display:none;}}/*!sc*/@media screen and (max-width:543px){.ePjhhA{display:none;}}/*!sc*/.cuEKae{text-align:right;padding-right:16px;width:136px;border-top-right-radius:6px;}/*!sc*/.jEbBOT{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));font-size:12px;height:40px;}/*!sc*/.bTxCvM{background-color:var(--bgColor-muted,var(--color-canvas-subtle,#f6f8fa));padding:4px;border-top-left-radius:6px;border-top-right-radius:6px;}/*!sc*/.eYedVD{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;gap:8px;min-width:273px;padding-right:8px;padding-left:16px;padding-top:8px;padding-bottom:8px;}/*!sc*/.jGfYmh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:8px;}/*!sc*/.lhFvfi{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/.bqgLjk{display:inherit;}/*!sc*/@media screen and (min-width:544px){.bqgLjk{display:none;}}/*!sc*/@media screen and (min-width:768px){.bqgLjk{display:none;}}/*!sc*/.epsqEd{text-align:center;vertical-align:center;height:40px;border-top:1px solid;border-color:var(--borderColor-default,var(--color-border-default,#d0d7de));}/*!sc*/.ldpruc{border-top:1px solid var(--borderColor-default,var(--color-border-default));cursor:pointer;}/*!sc*/.ehcSsh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;gap:16px;}/*!sc*/.iGmlUb{border:1px solid;border-color:var(--borderColor-default,var(--color-border-default,#d0d7de));border-radius:6px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}/*!sc*/@media screen and (max-width:543px){.iGmlUb{margin-left:-16px;margin-right:-16px;max-width:calc(100% + 32px);}}/*!sc*/@media screen and (min-width:544px){.iGmlUb{max-width:100%;}}/*!sc*/.iRQGXA{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;border-bottom:1px solid;border-bottom-color:var(--borderColor-default,var(--color-border-default,#d0d7de));-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-right:8px;position:-webkit-sticky;position:sticky;top:0;background-color:var(--bgColor-default,var(--color-canvas-default,#ffffff));z-index:1;border-top-left-radius:6px;border-top-right-radius:6px;}/*!sc*/.dvTdPK{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-left:8px;padding-right:8px;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;border-bottom:none;border-bottom-color:var(--borderColor-muted,var(--color-border-muted,hsla(210,18%,87%,1)));align:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-height:48px;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;max-width:100%;}/*!sc*/.gwuIGu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/.kOxwQs{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;margin-right:8px;}/*!sc*/.kOgeFj{font-weight:600;}/*!sc*/.bJMeLZ{padding:32px;overflow:auto;}/*!sc*/data-styled.g2[id=\"Box-sc-g0xbh4-0\"]{content:\"izjvBm,rPQgy,eUMEDg,eLcVee,hsfLlq,gpKoUz,kkrdEu,bKgizp,iPGYsi,dKmYfk,trpoQ,laYubZ,swnaL,bWpuBf,grHjNb,dXTsqj,dCOrmu,bVvbgP,bNDvfp,yfPnm,cAQuiW,iiUlLN,jmggSN,kvYunM,hrLuxA,ePjhhA,cuEKae,jEbBOT,bTxCvM,eYedVD,jGfYmh,lhFvfi,bqgLjk,epsqEd,ldpruc,ehcSsh,iGmlUb,iRQGXA,dvTdPK,gwuIGu,kOxwQs,kOgeFj,bJMeLZ,\"}/*!sc*/.bOMzPg{min-width:0;}/*!sc*/.eUGNHp{font-weight:600;}/*!sc*/.dALsKK{color:var(--fgColor-default,var(--color-fg-default,#1F2328));}/*!sc*/data-styled.g6[id=\"Text-sc-17v1xeu-0\"]{content:\"bOMzPg,eUGNHp,dALsKK,\"}/*!sc*/.dheQRw{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));-webkit-text-decoration:none;text-decoration:none;}/*!sc*/[data-a11y-link-underlines='true'] .Link__StyledLink-sc-14289xe-0[data-inline='true']{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.dheQRw:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.dheQRw:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/.vLMkZ{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));-webkit-text-decoration:none;text-decoration:none;position:relative;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;color:var(--fgColor-default,var(--color-fg-default,#1F2328));text-align:center;-webkit-text-decoration:none;text-decoration:none;line-height:calc(20/14);border-radius:6px;font-size:14px;padding-left:8px;padding-right:8px;padding-top:calc((2rem - 1.25rem) / 2);padding-bottom:calc((2rem - 1.25rem) / 2);}/*!sc*/[data-a11y-link-underlines='true'] .Link__StyledLink-sc-14289xe-0[data-inline='true']{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.vLMkZ:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.vLMkZ:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/.vLMkZ span[data-component=\"icon\"]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/@media (hover:hover){.vLMkZ:hover{background-color:var(--bgColor-neutral-muted,var(--color-neutral-muted,rgba(175,184,193,0.2)));-webkit-transition:background .12s ease-out;transition:background .12s ease-out;-webkit-text-decoration:none;text-decoration:none;}}/*!sc*/.vLMkZ:focus{outline:2px solid transparent;}/*!sc*/.vLMkZ:focus{box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.vLMkZ:focus:not(:focus-visible){box-shadow:none;}/*!sc*/.vLMkZ:focus-visible{outline:2px solid transparent;box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.vLMkZ span[data-content]::before{content:attr(data-content);display:block;height:0;font-weight:600;visibility:hidden;white-space:nowrap;}/*!sc*/.vLMkZ::after{position:absolute;right:50%;bottom:calc(50% - 25px);width:100%;height:2px;content:\"\";background-color:var(--underlineNav-borderColor-active,var(--color-primer-border-active,#fd8c73));border-radius:0;-webkit-transform:translate(50%,-50%);-ms-transform:translate(50%,-50%);transform:translate(50%,-50%);}/*!sc*/@media (forced-colors:active){.vLMkZ::after{background-color:LinkText;}}/*!sc*/.bhqztV{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));-webkit-text-decoration:none;text-decoration:none;position:relative;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;color:var(--fgColor-default,var(--color-fg-default,#1F2328));text-align:center;-webkit-text-decoration:none;text-decoration:none;line-height:calc(20/14);border-radius:6px;font-size:14px;padding-left:8px;padding-right:8px;padding-top:calc((2rem - 1.25rem) / 2);padding-bottom:calc((2rem - 1.25rem) / 2);}/*!sc*/[data-a11y-link-underlines='true'] .Link__StyledLink-sc-14289xe-0[data-inline='true']{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.bhqztV:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.bhqztV:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/.bhqztV span[data-component=\"icon\"]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/@media (hover:hover){.bhqztV:hover{background-color:var(--bgColor-neutral-muted,var(--color-neutral-muted,rgba(175,184,193,0.2)));-webkit-transition:background .12s ease-out;transition:background .12s ease-out;-webkit-text-decoration:none;text-decoration:none;}}/*!sc*/.bhqztV:focus{outline:2px solid transparent;}/*!sc*/.bhqztV:focus{box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.bhqztV:focus:not(:focus-visible){box-shadow:none;}/*!sc*/.bhqztV:focus-visible{outline:2px solid transparent;box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.bhqztV span[data-content]::before{content:attr(data-content);display:block;height:0;font-weight:600;visibility:hidden;white-space:nowrap;}/*!sc*/.bhqztV::after{position:absolute;right:50%;bottom:calc(50% - 25px);width:100%;height:2px;content:\"\";background-color:transparent;border-radius:0;-webkit-transform:translate(50%,-50%);-ms-transform:translate(50%,-50%);transform:translate(50%,-50%);}/*!sc*/@media (forced-colors:active){.bhqztV::after{background-color:transparent;}}/*!sc*/data-styled.g8[id=\"Link__StyledLink-sc-14289xe-0\"]{content:\"dheQRw,vLMkZ,bhqztV,\"}/*!sc*/.izDscS{border-radius:6px;border:1px solid;border-color:var(--button-default-borderColor-rest,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:var(--button-default-bgColor-rest,var(--color-btn-bg,#f6f8fa));box-shadow:var(--button-default-shadow-resting,var(--color-btn-shadow,0 1px 0 rgba(31,35,40,0.04))),var(--button-default-shadow-inset,var(--color-btn-inset-shadow,inset 0 1px 0 rgba(255,255,255,0.25)));}/*!sc*/.izDscS:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.izDscS:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.izDscS:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.izDscS[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.izDscS[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.izDscS:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.izDscS:active{-webkit-transition:none;transition:none;}/*!sc*/.izDscS[data-inactive]{cursor:auto;}/*!sc*/.izDscS:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));border-color:var(--button-default-borderColor-disabled,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));}/*!sc*/.izDscS:disabled [data-component=ButtonCounter]{color:inherit;}/*!sc*/@media (forced-colors:active){.izDscS:focus{outline:solid 1px transparent;}}/*!sc*/.izDscS [data-component=ButtonCounter]{font-size:12px;background-color:var(--buttonCounter-default-bgColor-rest,var(--color-btn-counter-bg,rgba(31,35,40,0.08)));}/*!sc*/.izDscS[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.izDscS[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.izDscS[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.izDscS[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.izDscS[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.izDscS[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.izDscS[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.izDscS[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.izDscS[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.izDscS[data-block=\"block\"]{width:100%;}/*!sc*/.izDscS[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.izDscS[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.izDscS [data-component=\"leadingVisual\"]{grid-area:leadingVisual;}/*!sc*/.izDscS [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.izDscS [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.izDscS [data-component=\"trailingAction\"]{margin-right:-4px;}/*!sc*/.izDscS [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.izDscS [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.izDscS:hover:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-hover,var(--color-btn-hover-bg,#f3f4f6));border-color:var(--button-default-borderColor-hover,var(--button-default-borderColor-hover,var(--color-btn-hover-border,rgba(31,35,40,0.15))));}/*!sc*/.izDscS:active:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/.izDscS[aria-expanded=true]{background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/.izDscS [data-component=\"leadingVisual\"],.izDscS [data-component=\"trailingVisual\"],.izDscS [data-component=\"trailingAction\"]{color:var(--button-color,var(--fgColor-muted,var(--color-fg-muted,#656d76)));}/*!sc*/.izDscS{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/.izDscS svg{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.izDscS > span{width:inherit;}/*!sc*/.cuOWTR{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:transparent;box-shadow:none;}/*!sc*/.cuOWTR:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.cuOWTR:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.cuOWTR:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.cuOWTR[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.cuOWTR[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.cuOWTR:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.cuOWTR:active{-webkit-transition:none;transition:none;}/*!sc*/.cuOWTR[data-inactive]{cursor:auto;}/*!sc*/.cuOWTR:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.cuOWTR:disabled [data-component=ButtonCounter],.cuOWTR:disabled [data-component=\"leadingVisual\"],.cuOWTR:disabled [data-component=\"trailingAction\"]{color:inherit;}/*!sc*/@media (forced-colors:active){.cuOWTR:focus{outline:solid 1px transparent;}}/*!sc*/.cuOWTR [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.cuOWTR[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.cuOWTR[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.cuOWTR[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.cuOWTR[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.cuOWTR[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.cuOWTR[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.cuOWTR[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.cuOWTR[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.cuOWTR[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.cuOWTR[data-block=\"block\"]{width:100%;}/*!sc*/.cuOWTR[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.cuOWTR[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.cuOWTR [data-component=\"leadingVisual\"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.cuOWTR [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.cuOWTR [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.cuOWTR [data-component=\"trailingAction\"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.cuOWTR [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.cuOWTR [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.cuOWTR:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));}/*!sc*/.cuOWTR:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));}/*!sc*/.cuOWTR[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/.cuOWTR[data-component=\"IconButton\"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.cuOWTR[data-no-visuals]{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.cuOWTR:has([data-component=\"ButtonCounter\"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/.cuOWTR:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.cuOWTR:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/.cuOWTR{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));padding-left:4px;padding-right:4px;}/*!sc*/.cuOWTR span[data-component=\"leadingVisual\"]{margin-right:4px !important;}/*!sc*/.tDSzd{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:transparent;box-shadow:none;}/*!sc*/.tDSzd:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.tDSzd:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.tDSzd:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.tDSzd[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.tDSzd[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.tDSzd:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.tDSzd:active{-webkit-transition:none;transition:none;}/*!sc*/.tDSzd[data-inactive]{cursor:auto;}/*!sc*/.tDSzd:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.tDSzd:disabled [data-component=ButtonCounter],.tDSzd:disabled [data-component=\"leadingVisual\"],.tDSzd:disabled [data-component=\"trailingAction\"]{color:inherit;}/*!sc*/@media (forced-colors:active){.tDSzd:focus{outline:solid 1px transparent;}}/*!sc*/.tDSzd [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.tDSzd[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.tDSzd[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.tDSzd[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.tDSzd[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.tDSzd[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.tDSzd[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.tDSzd[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.tDSzd[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.tDSzd[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.tDSzd[data-block=\"block\"]{width:100%;}/*!sc*/.tDSzd[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.tDSzd[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.tDSzd [data-component=\"leadingVisual\"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.tDSzd [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.tDSzd [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.tDSzd [data-component=\"trailingAction\"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.tDSzd [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.tDSzd [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.tDSzd:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));}/*!sc*/.tDSzd:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));}/*!sc*/.tDSzd[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/.tDSzd[data-component=\"IconButton\"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.tDSzd[data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.tDSzd:has([data-component=\"ButtonCounter\"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/.tDSzd:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.tDSzd:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/.ftZGca{border-radius:6px;border:1px solid;border-color:var(--button-default-borderColor-rest,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:var(--button-default-bgColor-rest,var(--color-btn-bg,#f6f8fa));box-shadow:var(--button-default-shadow-resting,var(--color-btn-shadow,0 1px 0 rgba(31,35,40,0.04))),var(--button-default-shadow-inset,var(--color-btn-inset-shadow,inset 0 1px 0 rgba(255,255,255,0.25)));}/*!sc*/.ftZGca:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.ftZGca:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.ftZGca:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.ftZGca[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.ftZGca[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.ftZGca:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.ftZGca:active{-webkit-transition:none;transition:none;}/*!sc*/.ftZGca[data-inactive]{cursor:auto;}/*!sc*/.ftZGca:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));border-color:var(--button-default-borderColor-disabled,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));}/*!sc*/.ftZGca:disabled [data-component=ButtonCounter]{color:inherit;}/*!sc*/@media (forced-colors:active){.ftZGca:focus{outline:solid 1px transparent;}}/*!sc*/.ftZGca [data-component=ButtonCounter]{font-size:12px;background-color:var(--buttonCounter-default-bgColor-rest,var(--color-btn-counter-bg,rgba(31,35,40,0.08)));}/*!sc*/.ftZGca[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.ftZGca[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.ftZGca[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.ftZGca[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.ftZGca[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.ftZGca[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.ftZGca[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.ftZGca[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.ftZGca[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.ftZGca[data-block=\"block\"]{width:100%;}/*!sc*/.ftZGca[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.ftZGca[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.ftZGca [data-component=\"leadingVisual\"]{grid-area:leadingVisual;}/*!sc*/.ftZGca [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.ftZGca [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.ftZGca [data-component=\"trailingAction\"]{margin-right:-4px;}/*!sc*/.ftZGca [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.ftZGca [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.ftZGca:hover:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-hover,var(--color-btn-hover-bg,#f3f4f6));border-color:var(--button-default-borderColor-hover,var(--button-default-borderColor-hover,var(--color-btn-hover-border,rgba(31,35,40,0.15))));}/*!sc*/.ftZGca:active:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/.ftZGca[aria-expanded=true]{background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/.ftZGca [data-component=\"leadingVisual\"],.ftZGca [data-component=\"trailingVisual\"],.ftZGca [data-component=\"trailingAction\"]{color:var(--button-color,var(--fgColor-muted,var(--color-fg-muted,#656d76)));}/*!sc*/.gYvpXq{border-radius:6px;border:1px solid;border-color:var(--button-primary-borderColor-rest,var(--color-btn-primary-border,rgba(31,35,40,0.15)));font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-primary-fgColor-rest,var(--color-btn-primary-text,#ffffff));background-color:var(--button-primary-bgColor-rest,var(--color-btn-primary-bg,#1f883d));box-shadow:var(--shadow-resting-small,var(--color-btn-primary-shadow,0 1px 0 rgba(31,35,40,0.1)));}/*!sc*/.gYvpXq:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.gYvpXq:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.gYvpXq:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.gYvpXq[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.gYvpXq[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.gYvpXq:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.gYvpXq:active{-webkit-transition:none;transition:none;}/*!sc*/.gYvpXq[data-inactive]{cursor:auto;}/*!sc*/.gYvpXq:disabled{cursor:not-allowed;box-shadow:none;color:var(--button-primary-fgColor-disabled,var(--color-btn-primary-disabled-text,rgba(255,255,255,0.8)));background-color:var(--button-primary-bgColor-disabled,var(--color-btn-primary-disabled-bg,#94d3a2));border-color:var(--button-primary-borderColor-disabled,var(--color-btn-primary-disabled-border,rgba(31,35,40,0.15)));}/*!sc*/.gYvpXq:disabled [data-component=ButtonCounter]{color:inherit;}/*!sc*/@media (forced-colors:active){.gYvpXq:focus{outline:solid 1px transparent;}}/*!sc*/.gYvpXq [data-component=ButtonCounter]{font-size:12px;background-color:var(--buttonCounter-primary-bgColor-rest,var(--color-btn-primary-counter-bg,rgba(0,45,17,0.2)));color:var(--button-primary-fgColor-rest,var(--color-btn-primary-text,#ffffff));}/*!sc*/.gYvpXq[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.gYvpXq[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.gYvpXq[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.gYvpXq[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.gYvpXq[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.gYvpXq[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.gYvpXq[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.gYvpXq[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.gYvpXq[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.gYvpXq[data-block=\"block\"]{width:100%;}/*!sc*/.gYvpXq[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.gYvpXq[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.gYvpXq [data-component=\"leadingVisual\"]{grid-area:leadingVisual;}/*!sc*/.gYvpXq [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.gYvpXq [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.gYvpXq [data-component=\"trailingAction\"]{margin-right:-4px;}/*!sc*/.gYvpXq [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.gYvpXq [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.gYvpXq:hover:not([disabled]):not([data-inactive]){color:btn.primary.hoverText;background-color:var(--button-primary-bgColor-hover,var(--color-btn-primary-hover-bg,#1a7f37));}/*!sc*/.gYvpXq:focus:not([disabled]){box-shadow:inset 0 0 0 3px;}/*!sc*/.gYvpXq:focus-visible:not([disabled]){box-shadow:inset 0 0 0 3px;}/*!sc*/.gYvpXq:active:not([disabled]):not([data-inactive]){background-color:var(--button-primary-bgColor-active,var(--color-btn-primary-selected-bg,hsla(137,66%,28%,1)));box-shadow:var(--button-primary-shadow-selected,var(--color-btn-primary-selected-shadow,inset 0 1px 0 rgba(0,45,17,0.2)));}/*!sc*/.gYvpXq[aria-expanded=true]{background-color:var(--button-primary-bgColor-active,var(--color-btn-primary-selected-bg,hsla(137,66%,28%,1)));box-shadow:var(--button-primary-shadow-selected,var(--color-btn-primary-selected-shadow,inset 0 1px 0 rgba(0,45,17,0.2)));}/*!sc*/.gYvpXq svg{color:fg.primary;}/*!sc*/.fAkXQN{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--fgColor-default,var(--color-fg-default,#1F2328));background-color:transparent;box-shadow:none;}/*!sc*/.fAkXQN:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.fAkXQN:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.fAkXQN:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.fAkXQN[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.fAkXQN[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.fAkXQN:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.fAkXQN:active{-webkit-transition:none;transition:none;}/*!sc*/.fAkXQN[data-inactive]{cursor:auto;}/*!sc*/.fAkXQN:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.fAkXQN:disabled [data-component=ButtonCounter],.fAkXQN:disabled [data-component=\"leadingVisual\"],.fAkXQN:disabled [data-component=\"trailingAction\"]{color:inherit;}/*!sc*/@media (forced-colors:active){.fAkXQN:focus{outline:solid 1px transparent;}}/*!sc*/.fAkXQN [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.fAkXQN[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.fAkXQN[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.fAkXQN[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.fAkXQN[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.fAkXQN[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.fAkXQN[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.fAkXQN[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.fAkXQN[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.fAkXQN[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.fAkXQN[data-block=\"block\"]{width:100%;}/*!sc*/.fAkXQN[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.fAkXQN[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.fAkXQN [data-component=\"leadingVisual\"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.fAkXQN [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.fAkXQN [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.fAkXQN [data-component=\"trailingAction\"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.fAkXQN [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.fAkXQN [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.fAkXQN:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.fAkXQN:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.fAkXQN[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/.fAkXQN[data-component=\"IconButton\"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.fAkXQN[data-no-visuals]{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.fAkXQN:has([data-component=\"ButtonCounter\"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/.fAkXQN:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.fAkXQN:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/.fAkXQN:focus:not([disabled]){-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.jPraEl{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:transparent;box-shadow:none;}/*!sc*/.jPraEl:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.jPraEl:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.jPraEl:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.jPraEl[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.jPraEl[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.jPraEl:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.jPraEl:active{-webkit-transition:none;transition:none;}/*!sc*/.jPraEl[data-inactive]{cursor:auto;}/*!sc*/.jPraEl:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.jPraEl:disabled [data-component=ButtonCounter],.jPraEl:disabled [data-component=\"leadingVisual\"],.jPraEl:disabled [data-component=\"trailingAction\"]{color:inherit;}/*!sc*/@media (forced-colors:active){.jPraEl:focus{outline:solid 1px transparent;}}/*!sc*/.jPraEl [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.jPraEl[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.jPraEl[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.jPraEl[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.jPraEl[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.jPraEl[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.jPraEl[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.jPraEl[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.jPraEl[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.jPraEl[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.jPraEl[data-block=\"block\"]{width:100%;}/*!sc*/.jPraEl[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.jPraEl[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.jPraEl [data-component=\"leadingVisual\"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.jPraEl [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.jPraEl [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.jPraEl [data-component=\"trailingAction\"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.jPraEl [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.jPraEl [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.jPraEl:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));}/*!sc*/.jPraEl:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));}/*!sc*/.jPraEl[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/.jPraEl[data-component=\"IconButton\"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.jPraEl[data-no-visuals]{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.jPraEl:has([data-component=\"ButtonCounter\"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/.jPraEl:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.jPraEl:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/.jPraEl{color:var(--fgColor-muted,var(--color-fg-subtle,#6e7781));padding-left:8px;padding-right:8px;}/*!sc*/data-styled.g9[id=\"types__StyledButton-sc-ws60qy-0\"]{content:\"izDscS,cuOWTR,tDSzd,ftZGca,gYvpXq,fAkXQN,jPraEl,\"}/*!sc*/.rTZSs{position:absolute;width:1px;height:1px;padding:0;margin:-1px;overflow:hidden;-webkit-clip:rect(0,0,0,0);clip:rect(0,0,0,0);white-space:nowrap;border-width:0;}/*!sc*/data-styled.g10[id=\"_VisuallyHidden__VisuallyHidden-sc-11jhm7a-0\"]{content:\"rTZSs,\"}/*!sc*/.fUpWeN{display:inline-block;overflow:hidden;text-overflow:ellipsis;vertical-align:top;white-space:nowrap;max-width:125px;max-width:100%;}/*!sc*/data-styled.g15[id=\"Truncate__StyledTruncate-sc-23o1d2-0\"]{content:\"fUpWeN,\"}/*!sc*/.dMjscx{position:relative;display:inline-block;}/*!sc*/.dMjscx::before{position:absolute;z-index:1000001;display:none;width:0px;height:0px;color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));pointer-events:none;content:'';border:6px solid transparent;opacity:0;}/*!sc*/.dMjscx::after{position:absolute;z-index:1000000;display:none;padding:0.5em 0.75em;font:normal normal 11px/1.5 -apple-system,BlinkMacSystemFont,\"Segoe UI\",\"Noto Sans\",Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\";-webkit-font-smoothing:subpixel-antialiased;color:var(--fgColor-onEmphasis,var(--color-fg-on-emphasis,#ffffff));text-align:center;-webkit-text-decoration:none;text-decoration:none;text-shadow:none;text-transform:none;-webkit-letter-spacing:normal;-moz-letter-spacing:normal;-ms-letter-spacing:normal;letter-spacing:normal;word-wrap:break-word;white-space:pre;pointer-events:none;content:attr(aria-label);background:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));border-radius:6px;opacity:0;}/*!sc*/@-webkit-keyframes tooltip-appear{from{opacity:0;}to{opacity:1;}}/*!sc*/@keyframes tooltip-appear{from{opacity:0;}to{opacity:1;}}/*!sc*/.dMjscx:hover::before,.dMjscx:active::before,.dMjscx:focus::before,.dMjscx:focus-within::before,.dMjscx:hover::after,.dMjscx:active::after,.dMjscx:focus::after,.dMjscx:focus-within::after{display:inline-block;-webkit-text-decoration:none;text-decoration:none;-webkit-animation-name:tooltip-appear;animation-name:tooltip-appear;-webkit-animation-duration:0.1s;animation-duration:0.1s;-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-timing-function:ease-in;animation-timing-function:ease-in;-webkit-animation-delay:0.4s;animation-delay:0.4s;}/*!sc*/.dMjscx.tooltipped-no-delay:hover::before,.dMjscx.tooltipped-no-delay:active::before,.dMjscx.tooltipped-no-delay:focus::before,.dMjscx.tooltipped-no-delay:focus-within::before,.dMjscx.tooltipped-no-delay:hover::after,.dMjscx.tooltipped-no-delay:active::after,.dMjscx.tooltipped-no-delay:focus::after,.dMjscx.tooltipped-no-delay:focus-within::after{-webkit-animation-delay:0s;animation-delay:0s;}/*!sc*/.dMjscx.tooltipped-multiline:hover::after,.dMjscx.tooltipped-multiline:active::after,.dMjscx.tooltipped-multiline:focus::after,.dMjscx.tooltipped-multiline:focus-within::after{display:table-cell;}/*!sc*/.dMjscx.tooltipped-s::after,.dMjscx.tooltipped-se::after,.dMjscx.tooltipped-sw::after{top:100%;right:50%;margin-top:6px;}/*!sc*/.dMjscx.tooltipped-s::before,.dMjscx.tooltipped-se::before,.dMjscx.tooltipped-sw::before{top:auto;right:50%;bottom:-7px;margin-right:-6px;border-bottom-color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));}/*!sc*/.dMjscx.tooltipped-se::after{right:auto;left:50%;margin-left:-16px;}/*!sc*/.dMjscx.tooltipped-sw::after{margin-right:-16px;}/*!sc*/.dMjscx.tooltipped-n::after,.dMjscx.tooltipped-ne::after,.dMjscx.tooltipped-nw::after{right:50%;bottom:100%;margin-bottom:6px;}/*!sc*/.dMjscx.tooltipped-n::before,.dMjscx.tooltipped-ne::before,.dMjscx.tooltipped-nw::before{top:-7px;right:50%;bottom:auto;margin-right:-6px;border-top-color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));}/*!sc*/.dMjscx.tooltipped-ne::after{right:auto;left:50%;margin-left:-16px;}/*!sc*/.dMjscx.tooltipped-nw::after{margin-right:-16px;}/*!sc*/.dMjscx.tooltipped-s::after,.dMjscx.tooltipped-n::after{-webkit-transform:translateX(50%);-ms-transform:translateX(50%);transform:translateX(50%);}/*!sc*/.dMjscx.tooltipped-w::after{right:100%;bottom:50%;margin-right:6px;-webkit-transform:translateY(50%);-ms-transform:translateY(50%);transform:translateY(50%);}/*!sc*/.dMjscx.tooltipped-w::before{top:50%;bottom:50%;left:-7px;margin-top:-6px;border-left-color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));}/*!sc*/.dMjscx.tooltipped-e::after{bottom:50%;left:100%;margin-left:6px;-webkit-transform:translateY(50%);-ms-transform:translateY(50%);transform:translateY(50%);}/*!sc*/.dMjscx.tooltipped-e::before{top:50%;right:-7px;bottom:50%;margin-top:-6px;border-right-color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));}/*!sc*/.dMjscx.tooltipped-multiline::after{width:-webkit-max-content;width:-moz-max-content;width:max-content;max-width:250px;word-wrap:break-word;white-space:pre-line;border-collapse:separate;}/*!sc*/.dMjscx.tooltipped-multiline.tooltipped-s::after,.dMjscx.tooltipped-multiline.tooltipped-n::after{right:auto;left:50%;-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%);transform:translateX(-50%);}/*!sc*/.dMjscx.tooltipped-multiline.tooltipped-w::after,.dMjscx.tooltipped-multiline.tooltipped-e::after{right:100%;}/*!sc*/.dMjscx.tooltipped-align-right-2::after{right:0;margin-right:0;}/*!sc*/.dMjscx.tooltipped-align-right-2::before{right:15px;}/*!sc*/.dMjscx.tooltipped-align-left-2::after{left:0;margin-left:0;}/*!sc*/.dMjscx.tooltipped-align-left-2::before{left:10px;}/*!sc*/data-styled.g18[id=\"Tooltip__TooltipBase-sc-17tf59c-0\"]{content:\"dMjscx,\"}/*!sc*/.bPgibo{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;list-style:none;white-space:nowrap;padding-top:0;padding-bottom:0;padding-left:0;padding-right:0;margin:0;margin-bottom:-1px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;gap:8px;position:relative;}/*!sc*/data-styled.g103[id=\"UnderlineNav__NavigationList-sc-1jfr31k-0\"]{content:\"bPgibo,\"}/*!sc*/</style> <!-- --> <!-- --> <div class=\"Box-sc-g0xbh4-0 izjvBm\"><div class=\"Box-sc-g0xbh4-0 rPQgy\"><div class=\"Box-sc-g0xbh4-0 eUMEDg\"></div></div><div class=\"Box-sc-g0xbh4-0 eLcVee\"><div class=\"Box-sc-g0xbh4-0 hsfLlq\"><div class=\"Box-sc-g0xbh4-0 gpKoUz\"><button type=\"button\" id=\"branch-picker-repos-header-ref-selector\" aria-haspopup=\"true\" tabindex=\"0\" aria-label=\"master branch\" data-testid=\"anchor-button\" class=\"types__StyledButton-sc-ws60qy-0 izDscS overview-ref-selector\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"text\"><div class=\"Box-sc-g0xbh4-0 bKgizp\"><div class=\"Box-sc-g0xbh4-0 iPGYsi\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-git-branch\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z\"></path></svg></div><div class=\"Box-sc-g0xbh4-0 dKmYfk ref-selector-button-text-container\"><span class=\"Text-sc-17v1xeu-0 bOMzPg\">\u00a0<!-- -->master</span></div></div></span><span data-component=\"trailingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-triangle-down\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z\"></path></svg></span></span></button><button hidden=\"\" data-hotkey-scope=\"read-only-cursor-text-area\"></button></div><div class=\"Box-sc-g0xbh4-0 laYubZ\"><a style=\"--button-color:fg.muted\" type=\"button\" href=\"/lucidrains/vector-quantize-pytorch/branches\" class=\"types__StyledButton-sc-ws60qy-0 cuOWTR\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-git-branch\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z\"></path></svg></span><span data-component=\"text\">Branches</span></span></a><a style=\"--button-color:fg.muted\" type=\"button\" href=\"/lucidrains/vector-quantize-pytorch/tags\" class=\"types__StyledButton-sc-ws60qy-0 cuOWTR\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-tag\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z\"></path></svg></span><span data-component=\"text\">Tags</span></span></a></div><div class=\"Box-sc-g0xbh4-0 swnaL\"><a style=\"--button-color:fg.muted\" type=\"button\" aria-label=\"Go to Branches page\" href=\"/lucidrains/vector-quantize-pytorch/branches\" data-no-visuals=\"true\" class=\"types__StyledButton-sc-ws60qy-0 tDSzd\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-git-branch\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z\"></path></svg></a><a style=\"--button-color:fg.muted\" type=\"button\" aria-label=\"Go to Tags page\" href=\"/lucidrains/vector-quantize-pytorch/tags\" data-no-visuals=\"true\" class=\"types__StyledButton-sc-ws60qy-0 tDSzd\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-tag\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z\"></path></svg></a></div></div><div class=\"Box-sc-g0xbh4-0 bWpuBf\"><div class=\"Box-sc-g0xbh4-0 grHjNb\"><div class=\"Box-sc-g0xbh4-0 dXTsqj\"><!--$!--><template></template><!--/$--></div><div class=\"Box-sc-g0xbh4-0 dCOrmu\"><button type=\"button\" data-no-visuals=\"true\" class=\"types__StyledButton-sc-ws60qy-0 ftZGca\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"text\">Go to file</span></span></button></div><div class=\"react-directory-add-file-icon\"></div><div class=\"react-directory-remove-file-icon\"></div></div><button type=\"button\" id=\":R2il5:\" aria-haspopup=\"true\" tabindex=\"0\" class=\"types__StyledButton-sc-ws60qy-0 gYvpXq\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><div class=\"Box-sc-g0xbh4-0 bVvbgP\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-code\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z\"></path></svg></div></span><span data-component=\"text\">Code</span></span><span data-component=\"trailingAction\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-triangle-down\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z\"></path></svg></span></button><div class=\"Box-sc-g0xbh4-0 bNDvfp\"><button data-component=\"IconButton\" type=\"button\" aria-label=\"Open more actions menu\" id=\":R3il5:\" aria-haspopup=\"true\" tabindex=\"0\" data-no-visuals=\"true\" class=\"types__StyledButton-sc-ws60qy-0 ftZGca\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-kebab-horizontal\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path></svg></button></div></div></div><div class=\"Box-sc-g0xbh4-0 yfPnm\"><div data-hpc=\"true\" class=\"Box-sc-g0xbh4-0\"><button hidden=\"\" data-testid=\"focus-next-element-button\" data-hotkey=\"j\" data-hotkey-scope=\"read-only-cursor-text-area\"></button><button hidden=\"\" data-hotkey=\"j\"></button><button hidden=\"\" data-testid=\"focus-previous-element-button\" data-hotkey=\"k\" data-hotkey-scope=\"read-only-cursor-text-area\"></button><button hidden=\"\" data-hotkey=\"k\"></button><h2 class=\"Heading__StyledHeading-sc-1c1dgg0-0 cgQnMS sr-only\" data-testid=\"screen-reader-heading\" id=\"folders-and-files\">Folders and files</h2><table aria-labelledby=\"folders-and-files\" class=\"Box-sc-g0xbh4-0 cAQuiW\"><thead class=\"Box-sc-g0xbh4-0 iiUlLN\"><tr class=\"Box-sc-g0xbh4-0 jmggSN\"><th colSpan=\"2\" class=\"Box-sc-g0xbh4-0 kvYunM\"><span class=\"Text-sc-17v1xeu-0 eUGNHp\">Name</span></th><th colSpan=\"1\" class=\"Box-sc-g0xbh4-0 hrLuxA\"><span class=\"Text-sc-17v1xeu-0 eUGNHp\">Name</span></th><th class=\"Box-sc-g0xbh4-0 ePjhhA\"><div title=\"Last commit message\" class=\"Truncate__StyledTruncate-sc-23o1d2-0 fUpWeN\"><span class=\"Text-sc-17v1xeu-0 eUGNHp\">Last commit message</span></div></th><th colSpan=\"1\" class=\"Box-sc-g0xbh4-0 cuEKae\"><div title=\"Last commit date\" class=\"Truncate__StyledTruncate-sc-23o1d2-0 fUpWeN\"><span class=\"Text-sc-17v1xeu-0 eUGNHp\">Last commit date</span></div></th></tr></thead><tbody><tr class=\"Box-sc-g0xbh4-0 jEbBOT\"><td colSpan=\"3\" class=\"Box-sc-g0xbh4-0 bTxCvM\"><div class=\"Box-sc-g0xbh4-0 eYedVD\"><h2 class=\"Heading__StyledHeading-sc-1c1dgg0-0 cgQnMS sr-only\" data-testid=\"screen-reader-heading\">Latest commit</h2><div style=\"width:120px\" class=\"Skeleton Skeleton--text\" data-testid=\"loading\">\u00a0</div><div class=\"Box-sc-g0xbh4-0 jGfYmh\"><div data-testid=\"latest-commit-details\" class=\"Box-sc-g0xbh4-0 lhFvfi\"></div><h2 class=\"Heading__StyledHeading-sc-1c1dgg0-0 cgQnMS sr-only\" data-testid=\"screen-reader-heading\">History</h2><a class=\"types__StyledButton-sc-ws60qy-0 fAkXQN react-last-commit-history-group\" href=\"/lucidrains/vector-quantize-pytorch/commits/master/\" data-size=\"small\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-history\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z\"></path></svg></span><span data-component=\"text\"><span class=\"Text-sc-17v1xeu-0 dALsKK\">268 Commits</span></span></span></a><div class=\"Box-sc-g0xbh4-0 bqgLjk\"></div><span role=\"tooltip\" aria-label=\"Commit history\" class=\"Tooltip__TooltipBase-sc-17tf59c-0 dMjscx tooltipped-n\"><a class=\"types__StyledButton-sc-ws60qy-0 fAkXQN react-last-commit-history-icon\" href=\"/lucidrains/vector-quantize-pytorch/commits/master/\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-history\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z\"></path></svg></span></span></a></span></div></div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-0\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"This path skips through empty directories\" aria-label=\".github/workflows, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vector-quantize-pytorch/tree/master/.github/workflows\"><span class=\"react-directory-default-color\" data-testid=\"path-name-segment\">.github/</span><span class=\"\" data-testid=\"path-name-segment\">workflows</span></a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"This path skips through empty directories\" aria-label=\".github/workflows, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vector-quantize-pytorch/tree/master/.github/workflows\"><span class=\"react-directory-default-color\" data-testid=\"path-name-segment\">.github/</span><span class=\"\" data-testid=\"path-name-segment\">workflows</span></a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-1\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"examples\" aria-label=\"examples, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vector-quantize-pytorch/tree/master/examples\">examples</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"examples\" aria-label=\"examples, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vector-quantize-pytorch/tree/master/examples\">examples</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-2\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"images\" aria-label=\"images, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vector-quantize-pytorch/tree/master/images\">images</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"images\" aria-label=\"images, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vector-quantize-pytorch/tree/master/images\">images</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-3\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"vector_quantize_pytorch\" aria-label=\"vector_quantize_pytorch, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vector-quantize-pytorch/tree/master/vector_quantize_pytorch\">vector_quantize_pytorch</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"vector_quantize_pytorch\" aria-label=\"vector_quantize_pytorch, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vector-quantize-pytorch/tree/master/vector_quantize_pytorch\">vector_quantize_pytorch</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-4\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\".gitignore\" aria-label=\".gitignore, (File)\" class=\"Link--primary\" href=\"/lucidrains/vector-quantize-pytorch/blob/master/.gitignore\">.gitignore</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\".gitignore\" aria-label=\".gitignore, (File)\" class=\"Link--primary\" href=\"/lucidrains/vector-quantize-pytorch/blob/master/.gitignore\">.gitignore</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-5\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"LICENSE\" aria-label=\"LICENSE, (File)\" class=\"Link--primary\" href=\"/lucidrains/vector-quantize-pytorch/blob/master/LICENSE\">LICENSE</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"LICENSE\" aria-label=\"LICENSE, (File)\" class=\"Link--primary\" href=\"/lucidrains/vector-quantize-pytorch/blob/master/LICENSE\">LICENSE</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-6\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"README.md\" aria-label=\"README.md, (File)\" class=\"Link--primary\" href=\"/lucidrains/vector-quantize-pytorch/blob/master/README.md\">README.md</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"README.md\" aria-label=\"README.md, (File)\" class=\"Link--primary\" href=\"/lucidrains/vector-quantize-pytorch/blob/master/README.md\">README.md</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-7\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"setup.py\" aria-label=\"setup.py, (File)\" class=\"Link--primary\" href=\"/lucidrains/vector-quantize-pytorch/blob/master/setup.py\">setup.py</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"setup.py\" aria-label=\"setup.py, (File)\" class=\"Link--primary\" href=\"/lucidrains/vector-quantize-pytorch/blob/master/setup.py\">setup.py</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"Box-sc-g0xbh4-0 epsqEd d-none\" data-testid=\"view-all-files-row\"><td colSpan=\"3\" class=\"Box-sc-g0xbh4-0 ldpruc\"><div><button class=\"Link__StyledLink-sc-14289xe-0 dheQRw\">View all files</button></div></td></tr></tbody></table></div><div class=\"Box-sc-g0xbh4-0 ehcSsh\"><div class=\"Box-sc-g0xbh4-0 iGmlUb\"><div class=\"Box-sc-g0xbh4-0 iRQGXA\"><h2 class=\"_VisuallyHidden__VisuallyHidden-sc-11jhm7a-0 rTZSs\">Repository files navigation</h2><nav aria-label=\"Repository files\" class=\"Box-sc-g0xbh4-0 dvTdPK\"><ul role=\"list\" class=\"UnderlineNav__NavigationList-sc-1jfr31k-0 bPgibo\"><li class=\"Box-sc-g0xbh4-0 gwuIGu\"><a href=\"#\" aria-current=\"page\" class=\"Link__StyledLink-sc-14289xe-0 vLMkZ\"><span data-component=\"icon\" class=\"Box-sc-g0xbh4-0 kOxwQs\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-book\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z\"></path></svg></span><span data-component=\"text\" data-content=\"README\" class=\"Box-sc-g0xbh4-0 kOgeFj\">README</span></a></li><li class=\"Box-sc-g0xbh4-0 gwuIGu\"><a href=\"#\" class=\"Link__StyledLink-sc-14289xe-0 bhqztV\"><span data-component=\"icon\" class=\"Box-sc-g0xbh4-0 kOxwQs\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-law\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M8.75.75V2h.985c.304 0 .603.08.867.231l1.29.736c.038.022.08.033.124.033h2.234a.75.75 0 0 1 0 1.5h-.427l2.111 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.006.005-.01.01-.045.04c-.21.176-.441.327-.686.45C14.556 10.78 13.88 11 13 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L12.178 4.5h-.162c-.305 0-.604-.079-.868-.231l-1.29-.736a.245.245 0 0 0-.124-.033H8.75V13h2.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1 0-1.5h2.5V3.5h-.984a.245.245 0 0 0-.124.033l-1.289.737c-.265.15-.564.23-.869.23h-.162l2.112 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.016.015-.045.04c-.21.176-.441.327-.686.45C4.556 10.78 3.88 11 3 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L2.178 4.5H1.75a.75.75 0 0 1 0-1.5h2.234a.249.249 0 0 0 .125-.033l1.288-.737c.265-.15.564-.23.869-.23h.984V.75a.75.75 0 0 1 1.5 0Zm2.945 8.477c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L13 6.327Zm-10 0c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L3 6.327Z\"></path></svg></span><span data-component=\"text\" data-content=\"MIT license\" class=\"Box-sc-g0xbh4-0\">MIT license</span></a></li></ul></nav><button style=\"--button-color:fg.subtle\" type=\"button\" aria-label=\"Outline\" id=\":Rdkl5:\" aria-haspopup=\"true\" tabindex=\"0\" class=\"types__StyledButton-sc-ws60qy-0 jPraEl\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-list-unordered\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z\"></path></svg></button></div><div class=\"Box-sc-g0xbh4-0 bJMeLZ js-snippet-clipboard-copy-unpositioned\" data-hpc=\"true\"><article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vector-quantize-pytorch/blob/master/images/vq.png\"><img src=\"/lucidrains/vector-quantize-pytorch/raw/master/images/vq.png\" width=\"500px\" style=\"max-width: 100%;\"></a></p><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Vector Quantization - Pytorch</h2><a id=\"user-content-vector-quantization---pytorch\" class=\"anchor-element\" aria-label=\"Permalink: Vector Quantization - Pytorch\" href=\"#vector-quantization---pytorch\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">A vector quantization library originally transcribed from Deepmind's tensorflow implementation, made conveniently into a package. It uses exponential moving averages to update the dictionary.</p><p dir=\"auto\">VQ has been successfully used by Deepmind and OpenAI for high quality generation of images (VQ-VAE-2) and music (Jukebox).</p><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Install</h2><a id=\"user-content-install\" class=\"anchor-element\" aria-label=\"Permalink: Install\" href=\"#install\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"$ pip install vector-quantize-pytorch\"><pre>$ pip install vector-quantize-pytorch</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Usage</h2><a id=\"user-content-usage\" class=\"anchor-element\" aria-label=\"Permalink: Usage\" href=\"#usage\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import VectorQuantizevq = VectorQuantize(    dim = 256,    codebook_size = 512,     # codebook size    decay = 0.8,             # the exponential moving average decay, lower means the dictionary will change faster    commitment_weight = 1.   # the weight on the commitment loss)x = torch.randn(1, 1024, 256)quantized, indices, commit_loss = vq(x) # (1, 1024, 256), (1, 1024), (1)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">VectorQuantize</span><span class=\"pl-s1\">vq</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">VectorQuantize</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">codebook_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,     <span class=\"pl-c\"># codebook size</span>    <span class=\"pl-s1\">decay</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.8</span>,             <span class=\"pl-c\"># the exponential moving average decay, lower means the dictionary will change faster</span>    <span class=\"pl-s1\">commitment_weight</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1.</span>   <span class=\"pl-c\"># the weight on the commitment loss</span>)<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">quantized</span>, <span class=\"pl-s1\">indices</span>, <span class=\"pl-s1\">commit_loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">vq</span>(<span class=\"pl-s1\">x</span>) <span class=\"pl-c\"># (1, 1024, 256), (1, 1024), (1)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Residual VQ</h2><a id=\"user-content-residual-vq\" class=\"anchor-element\" aria-label=\"Permalink: Residual VQ\" href=\"#residual-vq\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">This <a href=\"https://arxiv.org/abs/2107.03312\" rel=\"nofollow\">paper</a> proposes to use multiple vector quantizers to recursively quantize the residuals of the waveform. You can use this with the <code>ResidualVQ</code> class and one extra initialization parameter.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import ResidualVQresidual_vq = ResidualVQ(    dim = 256,    num_quantizers = 8,      # specify number of quantizers    codebook_size = 1024,    # codebook size)x = torch.randn(1, 1024, 256)quantized, indices, commit_loss = residual_vq(x)# (1, 1024, 256), (1, 1024, 8), (1, 8)# (batch, seq, dim), (batch, seq, quantizer), (batch, quantizer)# if you need all the codes across the quantization layers, just pass return_all_codes = Truequantized, indices, commit_loss, all_codes = residual_vq(x, return_all_codes = True)# *_, (8, 1, 1024, 256)# all_codes - (quantizer, batch, seq, dim)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ResidualVQ</span><span class=\"pl-s1\">residual_vq</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ResidualVQ</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">num_quantizers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,      <span class=\"pl-c\"># specify number of quantizers</span>    <span class=\"pl-s1\">codebook_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-c\"># codebook size</span>)<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">quantized</span>, <span class=\"pl-s1\">indices</span>, <span class=\"pl-s1\">commit_loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">residual_vq</span>(<span class=\"pl-s1\">x</span>)<span class=\"pl-c\"># (1, 1024, 256), (1, 1024, 8), (1, 8)</span><span class=\"pl-c\"># (batch, seq, dim), (batch, seq, quantizer), (batch, quantizer)</span><span class=\"pl-c\"># if you need all the codes across the quantization layers, just pass return_all_codes = True</span><span class=\"pl-s1\">quantized</span>, <span class=\"pl-s1\">indices</span>, <span class=\"pl-s1\">commit_loss</span>, <span class=\"pl-s1\">all_codes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">residual_vq</span>(<span class=\"pl-s1\">x</span>, <span class=\"pl-s1\">return_all_codes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>)<span class=\"pl-c\"># *_, (8, 1, 1024, 256)</span><span class=\"pl-c\"># all_codes - (quantizer, batch, seq, dim)</span></pre></div><p dir=\"auto\">Furthermore, <a href=\"https://arxiv.org/abs/2203.01941\" rel=\"nofollow\">this paper</a> uses Residual-VQ to construct the RQ-VAE, for generating high resolution images with more compressed codes.</p><p dir=\"auto\">They make two modifications. The first is to share the codebook across all quantizers. The second is to stochastically sample the codes rather than always taking the closest match. You can use both of these features with two extra keyword arguments.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import ResidualVQresidual_vq = ResidualVQ(    dim = 256,    num_quantizers = 8,    codebook_size = 1024,    stochastic_sample_codes = True,    sample_codebook_temp = 0.1,         # temperature for stochastically sampling codes, 0 would be equivalent to non-stochastic    shared_codebook = True              # whether to share the codebooks for all quantizers or not)x = torch.randn(1, 1024, 256)quantized, indices, commit_loss = residual_vq(x)# (1, 1024, 256), (8, 1, 1024), (8, 1)# (batch, seq, dim), (quantizer, batch, seq), (quantizer, batch)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ResidualVQ</span><span class=\"pl-s1\">residual_vq</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ResidualVQ</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">num_quantizers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">codebook_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">stochastic_sample_codes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>,    <span class=\"pl-s1\">sample_codebook_temp</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,         <span class=\"pl-c\"># temperature for stochastically sampling codes, 0 would be equivalent to non-stochastic</span>    <span class=\"pl-s1\">shared_codebook</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>              <span class=\"pl-c\"># whether to share the codebooks for all quantizers or not</span>)<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">quantized</span>, <span class=\"pl-s1\">indices</span>, <span class=\"pl-s1\">commit_loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">residual_vq</span>(<span class=\"pl-s1\">x</span>)<span class=\"pl-c\"># (1, 1024, 256), (8, 1, 1024), (8, 1)</span><span class=\"pl-c\"># (batch, seq, dim), (quantizer, batch, seq), (quantizer, batch)</span></pre></div><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2305.02765\" rel=\"nofollow\">A recent paper</a> further proposes to do residual VQ on groups of the feature dimension, showing equivalent results to Encodec while using far fewer codebooks. You can use it by importing <code>GroupedResidualVQ</code></p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import GroupedResidualVQresidual_vq = GroupedResidualVQ(    dim = 256,    num_quantizers = 8,      # specify number of quantizers    groups = 2,    codebook_size = 1024,    # codebook size)x = torch.randn(1, 1024, 256)quantized, indices, commit_loss = residual_vq(x)# (1, 1024, 256), (2, 1, 1024, 8), (2, 1, 8)# (batch, seq, dim), (groups, batch, seq, quantizer), (groups, batch, quantizer)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">GroupedResidualVQ</span><span class=\"pl-s1\">residual_vq</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">GroupedResidualVQ</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">num_quantizers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,      <span class=\"pl-c\"># specify number of quantizers</span>    <span class=\"pl-s1\">groups</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">codebook_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-c\"># codebook size</span>)<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">quantized</span>, <span class=\"pl-s1\">indices</span>, <span class=\"pl-s1\">commit_loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">residual_vq</span>(<span class=\"pl-s1\">x</span>)<span class=\"pl-c\"># (1, 1024, 256), (2, 1, 1024, 8), (2, 1, 8)</span><span class=\"pl-c\"># (batch, seq, dim), (groups, batch, seq, quantizer), (groups, batch, quantizer)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Initialization</h2><a id=\"user-content-initialization\" class=\"anchor-element\" aria-label=\"Permalink: Initialization\" href=\"#initialization\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">The SoundStream paper proposes that the codebook should be initialized by the kmeans centroids of the first batch. You can easily turn on this feature with one flag <code>kmeans_init = True</code>, for either <code>VectorQuantize</code> or <code>ResidualVQ</code> class</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import ResidualVQresidual_vq = ResidualVQ(    dim = 256,    codebook_size = 256,    num_quantizers = 4,    kmeans_init = True,   # set to True    kmeans_iters = 10     # number of kmeans iterations to calculate the centroids for the codebook on init)x = torch.randn(1, 1024, 256)quantized, indices, commit_loss = residual_vq(x)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ResidualVQ</span><span class=\"pl-s1\">residual_vq</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ResidualVQ</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">codebook_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">num_quantizers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,    <span class=\"pl-s1\">kmeans_init</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>,   <span class=\"pl-c\"># set to True</span>    <span class=\"pl-s1\">kmeans_iters</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">10</span>     <span class=\"pl-c\"># number of kmeans iterations to calculate the centroids for the codebook on init</span>)<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">quantized</span>, <span class=\"pl-s1\">indices</span>, <span class=\"pl-s1\">commit_loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">residual_vq</span>(<span class=\"pl-s1\">x</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Increasing codebook usage</h2><a id=\"user-content-increasing-codebook-usage\" class=\"anchor-element\" aria-label=\"Permalink: Increasing codebook usage\" href=\"#increasing-codebook-usage\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">This repository will contain a few techniques from various papers to combat \"dead\" codebook entries, which is a common problem when using vector quantizers.</p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Lower codebook dimension</h3><a id=\"user-content-lower-codebook-dimension\" class=\"anchor-element\" aria-label=\"Permalink: Lower codebook dimension\" href=\"#lower-codebook-dimension\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">The <a href=\"https://openreview.net/forum?id=pfNyExj7z2\" rel=\"nofollow\">Improved VQGAN paper</a> proposes to have the codebook kept in a lower dimension. The encoder values are projected down before being projected back to high dimensional after quantization. You can set this with the <code>codebook_dim</code> hyperparameter.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import VectorQuantizevq = VectorQuantize(    dim = 256,    codebook_size = 256,    codebook_dim = 16      # paper proposes setting this to 32 or as low as 8 to increase codebook usage)x = torch.randn(1, 1024, 256)quantized, indices, commit_loss = vq(x)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">VectorQuantize</span><span class=\"pl-s1\">vq</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">VectorQuantize</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">codebook_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">codebook_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>      <span class=\"pl-c\"># paper proposes setting this to 32 or as low as 8 to increase codebook usage</span>)<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">quantized</span>, <span class=\"pl-s1\">indices</span>, <span class=\"pl-s1\">commit_loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">vq</span>(<span class=\"pl-s1\">x</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Cosine similarity</h3><a id=\"user-content-cosine-similarity\" class=\"anchor-element\" aria-label=\"Permalink: Cosine similarity\" href=\"#cosine-similarity\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">The <a href=\"https://openreview.net/forum?id=pfNyExj7z2\" rel=\"nofollow\">Improved VQGAN paper</a> also proposes to l2 normalize the codes and the encoded vectors, which boils down to using cosine similarity for the distance. They claim enforcing the vectors on a sphere leads to improvements in code usage and downstream reconstruction. You can turn this on by setting <code>use_cosine_sim = True</code></p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import VectorQuantizevq = VectorQuantize(    dim = 256,    codebook_size = 256,    use_cosine_sim = True   # set this to True)x = torch.randn(1, 1024, 256)quantized, indices, commit_loss = vq(x)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">VectorQuantize</span><span class=\"pl-s1\">vq</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">VectorQuantize</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">codebook_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">use_cosine_sim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>   <span class=\"pl-c\"># set this to True</span>)<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">quantized</span>, <span class=\"pl-s1\">indices</span>, <span class=\"pl-s1\">commit_loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">vq</span>(<span class=\"pl-s1\">x</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Expiring stale codes</h3><a id=\"user-content-expiring-stale-codes\" class=\"anchor-element\" aria-label=\"Permalink: Expiring stale codes\" href=\"#expiring-stale-codes\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">Finally, the SoundStream paper has a scheme where they replace codes that have hits below a certain threshold with randomly selected vector from the current batch. You can set this threshold with <code>threshold_ema_dead_code</code> keyword.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import VectorQuantizevq = VectorQuantize(    dim = 256,    codebook_size = 512,    threshold_ema_dead_code = 2  # should actively replace any codes that have an exponential moving average cluster size less than 2)x = torch.randn(1, 1024, 256)quantized, indices, commit_loss = vq(x)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">VectorQuantize</span><span class=\"pl-s1\">vq</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">VectorQuantize</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">codebook_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,    <span class=\"pl-s1\">threshold_ema_dead_code</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>  <span class=\"pl-c\"># should actively replace any codes that have an exponential moving average cluster size less than 2</span>)<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">quantized</span>, <span class=\"pl-s1\">indices</span>, <span class=\"pl-s1\">commit_loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">vq</span>(<span class=\"pl-s1\">x</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Orthogonal regularization loss</h3><a id=\"user-content-orthogonal-regularization-loss\" class=\"anchor-element\" aria-label=\"Permalink: Orthogonal regularization loss\" href=\"#orthogonal-regularization-loss\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">VQ-VAE / VQ-GAN is quickly gaining popularity. A <a href=\"https://arxiv.org/abs/2112.00384\" rel=\"nofollow\">recent paper</a> proposes that when using vector quantization on images, enforcing the codebook to be orthogonal leads to translation equivariance of the discretized codes, leading to large improvements in downstream text to image generation tasks.</p><p dir=\"auto\">You can use this feature by simply setting the <code>orthogonal_reg_weight</code> to be greater than <code>0</code>, in which case the orthogonal regularization will be added to the auxiliary loss outputted by the module.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import VectorQuantizevq = VectorQuantize(    dim = 256,    codebook_size = 256,    accept_image_fmap = True,                   # set this true to be able to pass in an image feature map    orthogonal_reg_weight = 10,                 # in paper, they recommended a value of 10    orthogonal_reg_max_codes = 128,             # this would randomly sample from the codebook for the orthogonal regularization loss, for limiting memory usage    orthogonal_reg_active_codes_only = False    # set this to True if you have a very large codebook, and would only like to enforce the loss on the activated codes per batch)img_fmap = torch.randn(1, 256, 32, 32)quantized, indices, loss = vq(img_fmap) # (1, 256, 32, 32), (1, 32, 32), (1,)# loss now contains the orthogonal regularization loss with the weight as assigned\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">VectorQuantize</span><span class=\"pl-s1\">vq</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">VectorQuantize</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">codebook_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">accept_image_fmap</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>,                   <span class=\"pl-c\"># set this true to be able to pass in an image feature map</span>    <span class=\"pl-s1\">orthogonal_reg_weight</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">10</span>,                 <span class=\"pl-c\"># in paper, they recommended a value of 10</span>    <span class=\"pl-s1\">orthogonal_reg_max_codes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">128</span>,             <span class=\"pl-c\"># this would randomly sample from the codebook for the orthogonal regularization loss, for limiting memory usage</span>    <span class=\"pl-s1\">orthogonal_reg_active_codes_only</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">False</span>    <span class=\"pl-c\"># set this to True if you have a very large codebook, and would only like to enforce the loss on the activated codes per batch</span>)<span class=\"pl-s1\">img_fmap</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>)<span class=\"pl-s1\">quantized</span>, <span class=\"pl-s1\">indices</span>, <span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">vq</span>(<span class=\"pl-s1\">img_fmap</span>) <span class=\"pl-c\"># (1, 256, 32, 32), (1, 32, 32), (1,)</span><span class=\"pl-c\"># loss now contains the orthogonal regularization loss with the weight as assigned</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Multi-headed VQ</h3><a id=\"user-content-multi-headed-vq\" class=\"anchor-element\" aria-label=\"Permalink: Multi-headed VQ\" href=\"#multi-headed-vq\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">There has been a number of papers that proposes variants of discrete latent representations with a multi-headed approach (multiple codes per feature). I have decided to offer one variant where the same codebook is used to vector quantize across the input dimension <code>head</code> times.</p><p dir=\"auto\">You can also use a more proven approach (memcodes) from <a href=\"https://github.com/lucidrains/nwt-pytorch\">NWT paper</a></p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import VectorQuantizevq = VectorQuantize(    dim = 256,    codebook_dim = 32,                  # a number of papers have shown smaller codebook dimension to be acceptable    heads = 8,                          # number of heads to vector quantize, codebook shared across all heads    separate_codebook_per_head = True,  # whether to have a separate codebook per head. False would mean 1 shared codebook    codebook_size = 8196,    accept_image_fmap = True)img_fmap = torch.randn(1, 256, 32, 32)quantized, indices, loss = vq(img_fmap) # (1, 256, 32, 32), (1, 32, 32, 8), (1,)# indices shape - (batch, height, width, heads)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">VectorQuantize</span><span class=\"pl-s1\">vq</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">VectorQuantize</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">codebook_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,                  <span class=\"pl-c\"># a number of papers have shown smaller codebook dimension to be acceptable</span>    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,                          <span class=\"pl-c\"># number of heads to vector quantize, codebook shared across all heads</span>    <span class=\"pl-s1\">separate_codebook_per_head</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>,  <span class=\"pl-c\"># whether to have a separate codebook per head. False would mean 1 shared codebook</span>    <span class=\"pl-s1\">codebook_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8196</span>,    <span class=\"pl-s1\">accept_image_fmap</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>)<span class=\"pl-s1\">img_fmap</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>)<span class=\"pl-s1\">quantized</span>, <span class=\"pl-s1\">indices</span>, <span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">vq</span>(<span class=\"pl-s1\">img_fmap</span>) <span class=\"pl-c\"># (1, 256, 32, 32), (1, 32, 32, 8), (1,)</span><span class=\"pl-c\"># indices shape - (batch, height, width, heads)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Random Projection Quantizer</h3><a id=\"user-content-random-projection-quantizer\" class=\"anchor-element\" aria-label=\"Permalink: Random Projection Quantizer\" href=\"#random-projection-quantizer\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2202.01855\" rel=\"nofollow\">This paper</a> first proposed to use a random projection quantizer for masked speech modeling, where signals are projected with a randomly initialized matrix and then matched with a random initialized codebook. One therefore does not need to learn the quantizer. This technique was used by Google's <a href=\"https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html\" rel=\"nofollow\">Universal Speech Model</a> to achieve SOTA for speech-to-text modeling.</p><p dir=\"auto\">USM further proposes to use multiple codebook, and the masked speech modeling with a multi-softmax objective. You can do this easily by setting <code>num_codebooks</code> to be greater than 1</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import RandomProjectionQuantizerquantizer = RandomProjectionQuantizer(    dim = 512,               # input dimensions    num_codebooks = 16,      # in USM, they used up to 16 for 5% gain    codebook_dim = 256,      # codebook dimension    codebook_size = 1024     # codebook size)x = torch.randn(1, 1024, 512)indices = quantizer(x) # (1, 1024, 16) - (batch, seq, num_codebooks)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">RandomProjectionQuantizer</span><span class=\"pl-s1\">quantizer</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">RandomProjectionQuantizer</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,               <span class=\"pl-c\"># input dimensions</span>    <span class=\"pl-s1\">num_codebooks</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,      <span class=\"pl-c\"># in USM, they used up to 16 for 5% gain</span>    <span class=\"pl-s1\">codebook_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,      <span class=\"pl-c\"># codebook dimension</span>    <span class=\"pl-s1\">codebook_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>     <span class=\"pl-c\"># codebook size</span>)<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">512</span>)<span class=\"pl-s1\">indices</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">quantizer</span>(<span class=\"pl-s1\">x</span>) <span class=\"pl-c\"># (1, 1024, 16) - (batch, seq, num_codebooks)</span></pre></div><p dir=\"auto\">This repository should also automatically synchronizing the codebooks in a multi-process setting. If somehow it isn't, please open an issue. You can override whether to synchronize codebooks or not by setting <code>sync_codebook = True | False</code></p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Finite Scalar Quantization</h3><a id=\"user-content-finite-scalar-quantization\" class=\"anchor-element\" aria-label=\"Permalink: Finite Scalar Quantization\" href=\"#finite-scalar-quantization\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vector-quantize-pytorch/blob/master/images/fsq.png\"><img src=\"/lucidrains/vector-quantize-pytorch/raw/master/images/fsq.png\" width=\"500px\" style=\"max-width: 100%;\"></a></p><table><thead><tr><th></th><th>VQ</th><th>FSQ</th></tr></thead><tbody><tr><td>Quantization</td><td>argmin_c || z-c ||</td><td>round(f(z))</td></tr><tr><td>Gradients</td><td>Straight Through Estimation (STE)</td><td>STE</td></tr><tr><td>Auxiliary Losses</td><td>Commitment, codebook, entropy loss, ...</td><td>N/A</td></tr><tr><td>Tricks</td><td>EMA on codebook, codebook splitting, projections, ...</td><td>N/A</td></tr><tr><td>Parameters</td><td>Codebook</td><td>N/A</td></tr></tbody></table><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2309.15505\" rel=\"nofollow\">This</a> work out of Google Deepmind aims to vastly simplify the way vector quantization is done for generative modeling, removing the need for commitment losses, EMA updating of the codebook, as well as tackle the issues with codebook collapse or insufficient utilization. They simply round each scalar into discrete levels with straight through gradients; the codes become uniform points in a hypercube.</p><p dir=\"auto\">Thanks goes out to <a href=\"https://github.com/sekstini\">@sekstini</a> for porting over this implementation in record time!</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import FSQlevels = [8,5,5,5] # see 4.1 and A.4.1 in the paperquantizer = FSQ(levels)x = torch.randn(1, 1024, 4) # 4 since there are 4 levelsxhat, indices = quantizer(x)print(xhat.shape)    # (1, 1024, 4) - (batch, seq, dim)print(indices.shape) # (1, 1024)    - (batch, seq)assert xhat.shape == x.shapeassert torch.all(xhat == quantizer.indices_to_codes(indices))\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">FSQ</span><span class=\"pl-s1\">levels</span> <span class=\"pl-c1\">=</span> [<span class=\"pl-c1\">8</span>,<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">5</span>] <span class=\"pl-c\"># see 4.1 and A.4.1 in the paper</span><span class=\"pl-s1\">quantizer</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">FSQ</span>(<span class=\"pl-s1\">levels</span>)<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">4</span>) <span class=\"pl-c\"># 4 since there are 4 levels</span><span class=\"pl-s1\">xhat</span>, <span class=\"pl-s1\">indices</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">quantizer</span>(<span class=\"pl-s1\">x</span>)<span class=\"pl-en\">print</span>(<span class=\"pl-s1\">xhat</span>.<span class=\"pl-s1\">shape</span>)    <span class=\"pl-c\"># (1, 1024, 4) - (batch, seq, dim)</span><span class=\"pl-en\">print</span>(<span class=\"pl-s1\">indices</span>.<span class=\"pl-s1\">shape</span>) <span class=\"pl-c\"># (1, 1024)    - (batch, seq)</span><span class=\"pl-k\">assert</span> <span class=\"pl-s1\">xhat</span>.<span class=\"pl-s1\">shape</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s1\">x</span>.<span class=\"pl-s1\">shape</span><span class=\"pl-k\">assert</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">all</span>(<span class=\"pl-s1\">xhat</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s1\">quantizer</span>.<span class=\"pl-en\">indices_to_codes</span>(<span class=\"pl-s1\">indices</span>))</pre></div><p dir=\"auto\">An improvised Residual FSQ, for an attempt to improve audio encoding.</p><p dir=\"auto\">Credit goes to <a href=\"https://github.com/sekstini\">@sekstini</a> for originally incepting the idea <a href=\"https://github.com/lucidrains/vector-quantize-pytorch/pull/74#issuecomment-1742048597\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/lucidrains/vector-quantize-pytorch/pull/74/hovercard\">here</a></p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import ResidualFSQresidual_fsq = ResidualFSQ(    dim = 256,    levels = [8, 5, 5, 3],    num_quantizers = 8)x = torch.randn(1, 1024, 256)residual_fsq.eval()quantized, indices = residual_fsq(x)# (1, 1024, 256), (1, 1024, 8), (8)# (batch, seq, dim), (batch, seq, quantizers), (quantizers)quantized_out = residual_fsq.get_output_from_indices(indices)# (8, 1, 1024, 8)# (residual layers, batch, seq, quantizers)assert torch.all(quantized == quantized_out)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ResidualFSQ</span><span class=\"pl-s1\">residual_fsq</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ResidualFSQ</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">levels</span> <span class=\"pl-c1\">=</span> [<span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">3</span>],    <span class=\"pl-s1\">num_quantizers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>)<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">residual_fsq</span>.<span class=\"pl-en\">eval</span>()<span class=\"pl-s1\">quantized</span>, <span class=\"pl-s1\">indices</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">residual_fsq</span>(<span class=\"pl-s1\">x</span>)<span class=\"pl-c\"># (1, 1024, 256), (1, 1024, 8), (8)</span><span class=\"pl-c\"># (batch, seq, dim), (batch, seq, quantizers), (quantizers)</span><span class=\"pl-s1\">quantized_out</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">residual_fsq</span>.<span class=\"pl-en\">get_output_from_indices</span>(<span class=\"pl-s1\">indices</span>)<span class=\"pl-c\"># (8, 1, 1024, 8)</span><span class=\"pl-c\"># (residual layers, batch, seq, quantizers)</span><span class=\"pl-k\">assert</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">all</span>(<span class=\"pl-s1\">quantized</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s1\">quantized_out</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Lookup Free Quantization</h3><a id=\"user-content-lookup-free-quantization\" class=\"anchor-element\" aria-label=\"Permalink: Lookup Free Quantization\" href=\"#lookup-free-quantization\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vector-quantize-pytorch/blob/master/images/lfq.png\"><img src=\"/lucidrains/vector-quantize-pytorch/raw/master/images/lfq.png\" width=\"450px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">The research team behind <a href=\"https://arxiv.org/abs/2212.05199\" rel=\"nofollow\">MagViT</a> has released new SOTA results for generative video modeling. A core change between v1 and v2 include a new type of quantization, look-up free quantization (LFQ), which eliminates the codebook and embedding lookup entirely.</p><p dir=\"auto\">This paper presents a simple LFQ quantizer of using independent binary latents. Other implementations of LFQ exist. However, the team shows that MAGVIT-v2 with LFQ significantly improves on the ImageNet benchmark. The differences between LFQ and 2-level FSQ includes entropy regularizations as well as maintained commitment loss.</p><p dir=\"auto\">Developing a more advanced method of LFQ quantization without codebook-lookup could revolutionize generative modeling.</p><p dir=\"auto\">You can use it simply as follows. Will be dogfooded at <a href=\"https://github.com/lucidrains/magvit2-pytorch\">MagViT2 pytorch port</a></p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import LFQ# you can specify either dim or codebook_size# if both specified, will be validated against each otherquantizer = LFQ(    codebook_size = 65536,      # codebook size, must be a power of 2    dim = 16,                   # this is the input feature dimension, defaults to log2(codebook_size) if not defined    entropy_loss_weight = 0.1,  # how much weight to place on entropy loss    diversity_gamma = 1.        # within entropy loss, how much weight to give to diversity of codes, taken from https://arxiv.org/abs/1911.05894)image_feats = torch.randn(1, 16, 32, 32)quantized, indices, entropy_aux_loss = quantizer(image_feats, inv_temperature=100.)  # you may want to experiment with temperature# (1, 16, 32, 32), (1, 32, 32), (1,)assert image_feats.shape == quantized.shapeassert (quantized == quantizer.indices_to_codes(indices)).all()\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">LFQ</span><span class=\"pl-c\"># you can specify either dim or codebook_size</span><span class=\"pl-c\"># if both specified, will be validated against each other</span><span class=\"pl-s1\">quantizer</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">LFQ</span>(    <span class=\"pl-s1\">codebook_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">65536</span>,      <span class=\"pl-c\"># codebook size, must be a power of 2</span>    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,                   <span class=\"pl-c\"># this is the input feature dimension, defaults to log2(codebook_size) if not defined</span>    <span class=\"pl-s1\">entropy_loss_weight</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,  <span class=\"pl-c\"># how much weight to place on entropy loss</span>    <span class=\"pl-s1\">diversity_gamma</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1.</span>        <span class=\"pl-c\"># within entropy loss, how much weight to give to diversity of codes, taken from https://arxiv.org/abs/1911.05894</span>)<span class=\"pl-s1\">image_feats</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>)<span class=\"pl-s1\">quantized</span>, <span class=\"pl-s1\">indices</span>, <span class=\"pl-s1\">entropy_aux_loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">quantizer</span>(<span class=\"pl-s1\">image_feats</span>, <span class=\"pl-s1\">inv_temperature</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">100.</span>)  <span class=\"pl-c\"># you may want to experiment with temperature</span><span class=\"pl-c\"># (1, 16, 32, 32), (1, 32, 32), (1,)</span><span class=\"pl-k\">assert</span> <span class=\"pl-s1\">image_feats</span>.<span class=\"pl-s1\">shape</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s1\">quantized</span>.<span class=\"pl-s1\">shape</span><span class=\"pl-k\">assert</span> (<span class=\"pl-s1\">quantized</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s1\">quantizer</span>.<span class=\"pl-en\">indices_to_codes</span>(<span class=\"pl-s1\">indices</span>)).<span class=\"pl-en\">all</span>()</pre></div><p dir=\"auto\">You can also pass in video features as <code>(batch, feat, time, height, width)</code> or sequences as <code>(batch, seq, feat)</code></p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"seq = torch.randn(1, 32, 16)quantized, *_ = quantizer(seq)assert seq.shape == quantized.shapevideo_feats = torch.randn(1, 16, 10, 32, 32)quantized, *_ = quantizer(video_feats)assert video_feats.shape == quantized.shape\"><pre><span class=\"pl-s1\">seq</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">16</span>)<span class=\"pl-s1\">quantized</span>, <span class=\"pl-c1\">*</span><span class=\"pl-s1\">_</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">quantizer</span>(<span class=\"pl-s1\">seq</span>)<span class=\"pl-k\">assert</span> <span class=\"pl-s1\">seq</span>.<span class=\"pl-s1\">shape</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s1\">quantized</span>.<span class=\"pl-s1\">shape</span><span class=\"pl-s1\">video_feats</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>)<span class=\"pl-s1\">quantized</span>, <span class=\"pl-c1\">*</span><span class=\"pl-s1\">_</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">quantizer</span>(<span class=\"pl-s1\">video_feats</span>)<span class=\"pl-k\">assert</span> <span class=\"pl-s1\">video_feats</span>.<span class=\"pl-s1\">shape</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s1\">quantized</span>.<span class=\"pl-s1\">shape</span></pre></div><p dir=\"auto\">Or support multiple codebooks</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import LFQquantizer = LFQ(    codebook_size = 4096,    dim = 16,    num_codebooks = 4  # 4 codebooks, total codebook dimension is log2(4096) * 4)image_feats = torch.randn(1, 16, 32, 32)quantized, indices, entropy_aux_loss = quantizer(image_feats)# (1, 16, 32, 32), (1, 32, 32, 4), (1,)assert image_feats.shape == quantized.shapeassert (quantized == quantizer.indices_to_codes(indices)).all()\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">LFQ</span><span class=\"pl-s1\">quantizer</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">LFQ</span>(    <span class=\"pl-s1\">codebook_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4096</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">num_codebooks</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>  <span class=\"pl-c\"># 4 codebooks, total codebook dimension is log2(4096) * 4</span>)<span class=\"pl-s1\">image_feats</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>)<span class=\"pl-s1\">quantized</span>, <span class=\"pl-s1\">indices</span>, <span class=\"pl-s1\">entropy_aux_loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">quantizer</span>(<span class=\"pl-s1\">image_feats</span>)<span class=\"pl-c\"># (1, 16, 32, 32), (1, 32, 32, 4), (1,)</span><span class=\"pl-k\">assert</span> <span class=\"pl-s1\">image_feats</span>.<span class=\"pl-s1\">shape</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s1\">quantized</span>.<span class=\"pl-s1\">shape</span><span class=\"pl-k\">assert</span> (<span class=\"pl-s1\">quantized</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s1\">quantizer</span>.<span class=\"pl-en\">indices_to_codes</span>(<span class=\"pl-s1\">indices</span>)).<span class=\"pl-en\">all</span>()</pre></div><p dir=\"auto\">An improvised Residual LFQ, to see if it can lead to an improvement for audio compression.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import ResidualLFQresidual_lfq = ResidualLFQ(    dim = 256,    codebook_size = 256,    num_quantizers = 8)x = torch.randn(1, 1024, 256)residual_lfq.eval()quantized, indices, commit_loss = residual_lfq(x)# (1, 1024, 256), (1, 1024, 8), (8)# (batch, seq, dim), (batch, seq, quantizers), (quantizers)quantized_out = residual_lfq.get_output_from_indices(indices)# (8, 1, 1024, 8)# (residual layers, batch, seq, quantizers)assert torch.all(quantized == quantized_out)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ResidualLFQ</span><span class=\"pl-s1\">residual_lfq</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ResidualLFQ</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">codebook_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">num_quantizers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>)<span class=\"pl-s1\">x</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">residual_lfq</span>.<span class=\"pl-en\">eval</span>()<span class=\"pl-s1\">quantized</span>, <span class=\"pl-s1\">indices</span>, <span class=\"pl-s1\">commit_loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">residual_lfq</span>(<span class=\"pl-s1\">x</span>)<span class=\"pl-c\"># (1, 1024, 256), (1, 1024, 8), (8)</span><span class=\"pl-c\"># (batch, seq, dim), (batch, seq, quantizers), (quantizers)</span><span class=\"pl-s1\">quantized_out</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">residual_lfq</span>.<span class=\"pl-en\">get_output_from_indices</span>(<span class=\"pl-s1\">indices</span>)<span class=\"pl-c\"># (8, 1, 1024, 8)</span><span class=\"pl-c\"># (residual layers, batch, seq, quantizers)</span><span class=\"pl-k\">assert</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">all</span>(<span class=\"pl-s1\">quantized</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s1\">quantized_out</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Latent Quantization</h3><a id=\"user-content-latent-quantization\" class=\"anchor-element\" aria-label=\"Permalink: Latent Quantization\" href=\"#latent-quantization\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">Disentanglement is essential for representation learning as it promotes interpretability, generalization, improved learning, and robustness. It aligns with the goal of capturing meaningful and independent features of the data, facilitating more effective use of learned representations across various applications. For better disentanglement, the challenge is to disentangle underlying variations in a dataset without explicit ground truth information. This work introduces a key inductive bias aimed at encoding and decoding within an organized latent space. The strategy incorporated encompasses discretizing the latent space by assigning discrete code vectors through the utilization of an individual learnable scalar codebook for each dimension. This methodology enables their models to surpass robust prior methods effectively.</p><p dir=\"auto\">Be aware they had to use a very high weight decay for the results in this paper.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import LatentQuantize# you can specify either dim or codebook_size# if both specified, will be validated against each otherquantizer = LatentQuantize(    levels = [5, 5, 8],      # number of levels per codebook dimension    dim = 16,                   # input dim    commitment_loss_weight=0.1,      quantization_loss_weight=0.1,)image_feats = torch.randn(1, 16, 32, 32)quantized, indices, loss = quantizer(image_feats)# (1, 16, 32, 32), (1, 32, 32), (1,)assert image_feats.shape == quantized.shapeassert (quantized == quantizer.indices_to_codes(indices)).all()\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">LatentQuantize</span><span class=\"pl-c\"># you can specify either dim or codebook_size</span><span class=\"pl-c\"># if both specified, will be validated against each other</span><span class=\"pl-s1\">quantizer</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">LatentQuantize</span>(    <span class=\"pl-s1\">levels</span> <span class=\"pl-c1\">=</span> [<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">8</span>],      <span class=\"pl-c\"># number of levels per codebook dimension</span>    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,                   <span class=\"pl-c\"># input dim</span>    <span class=\"pl-s1\">commitment_loss_weight</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">0.1</span>,      <span class=\"pl-s1\">quantization_loss_weight</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">0.1</span>,)<span class=\"pl-s1\">image_feats</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>)<span class=\"pl-s1\">quantized</span>, <span class=\"pl-s1\">indices</span>, <span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">quantizer</span>(<span class=\"pl-s1\">image_feats</span>)<span class=\"pl-c\"># (1, 16, 32, 32), (1, 32, 32), (1,)</span><span class=\"pl-k\">assert</span> <span class=\"pl-s1\">image_feats</span>.<span class=\"pl-s1\">shape</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s1\">quantized</span>.<span class=\"pl-s1\">shape</span><span class=\"pl-k\">assert</span> (<span class=\"pl-s1\">quantized</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s1\">quantizer</span>.<span class=\"pl-en\">indices_to_codes</span>(<span class=\"pl-s1\">indices</span>)).<span class=\"pl-en\">all</span>()</pre></div><p dir=\"auto\">You can also pass in video features as <code>(batch, feat, time, height, width)</code> or sequences as <code>(batch, seq, feat)</code></p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"seq = torch.randn(1, 32, 16)quantized, *_ = quantizer(seq)assert seq.shape == quantized.shapevideo_feats = torch.randn(1, 16, 10, 32, 32)quantized, *_ = quantizer(video_feats)assert video_feats.shape == quantized.shape\"><pre><span class=\"pl-s1\">seq</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">16</span>)<span class=\"pl-s1\">quantized</span>, <span class=\"pl-c1\">*</span><span class=\"pl-s1\">_</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">quantizer</span>(<span class=\"pl-s1\">seq</span>)<span class=\"pl-k\">assert</span> <span class=\"pl-s1\">seq</span>.<span class=\"pl-s1\">shape</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s1\">quantized</span>.<span class=\"pl-s1\">shape</span><span class=\"pl-s1\">video_feats</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>)<span class=\"pl-s1\">quantized</span>, <span class=\"pl-c1\">*</span><span class=\"pl-s1\">_</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">quantizer</span>(<span class=\"pl-s1\">video_feats</span>)<span class=\"pl-k\">assert</span> <span class=\"pl-s1\">video_feats</span>.<span class=\"pl-s1\">shape</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s1\">quantized</span>.<span class=\"pl-s1\">shape</span></pre></div><p dir=\"auto\">Or support multiple codebooks</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vector_quantize_pytorch import LatentQuantizelevels = [4, 8, 16]dim = 9num_codebooks = 3model = LatentQuantize(levels, dim, num_codebooks=num_codebooks)input_tensor = torch.randn(2, 3, dim)output_tensor, indices, loss = model(input_tensor)assert output_tensor.shape == input_tensor.shapeassert indices.shape == (2, 3, num_codebooks)assert loss.item() &gt;= 0\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vector_quantize_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">LatentQuantize</span><span class=\"pl-s1\">levels</span> <span class=\"pl-c1\">=</span> [<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">16</span>]<span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">9</span><span class=\"pl-s1\">num_codebooks</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">LatentQuantize</span>(<span class=\"pl-s1\">levels</span>, <span class=\"pl-s1\">dim</span>, <span class=\"pl-s1\">num_codebooks</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">num_codebooks</span>)<span class=\"pl-s1\">input_tensor</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-s1\">dim</span>)<span class=\"pl-s1\">output_tensor</span>, <span class=\"pl-s1\">indices</span>, <span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">model</span>(<span class=\"pl-s1\">input_tensor</span>)<span class=\"pl-k\">assert</span> <span class=\"pl-s1\">output_tensor</span>.<span class=\"pl-s1\">shape</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s1\">input_tensor</span>.<span class=\"pl-s1\">shape</span><span class=\"pl-k\">assert</span> <span class=\"pl-s1\">indices</span>.<span class=\"pl-s1\">shape</span> <span class=\"pl-c1\">==</span> (<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-s1\">num_codebooks</span>)<span class=\"pl-k\">assert</span> <span class=\"pl-s1\">loss</span>.<span class=\"pl-en\">item</span>() <span class=\"pl-c1\">&gt;=</span> <span class=\"pl-c1\">0</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Citations</h2><a id=\"user-content-citations\" class=\"anchor-element\" aria-label=\"Permalink: Citations\" href=\"#citations\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{oord2018neural,    title   = {Neural Discrete Representation Learning},    author  = {Aaron van den Oord and Oriol Vinyals and Koray Kavukcuoglu},    year    = {2018},    eprint  = {1711.00937},    archivePrefix = {arXiv},    primaryClass = {cs.LG}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">oord2018neural</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Neural Discrete Representation Learning<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Aaron van den Oord and Oriol Vinyals and Koray Kavukcuoglu<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2018<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>1711.00937<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.LG<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{zeghidour2021soundstream,    title   = {SoundStream: An End-to-End Neural Audio Codec},    author  = {Neil Zeghidour and Alejandro Luebs and Ahmed Omran and Jan Skoglund and Marco Tagliasacchi},    year    = {2021},    eprint  = {2107.03312},    archivePrefix = {arXiv},    primaryClass = {cs.SD}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">zeghidour2021soundstream</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>SoundStream: An End-to-End Neural Audio Codec<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Neil Zeghidour and Alejandro Luebs and Ahmed Omran and Jan Skoglund and Marco Tagliasacchi<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2107.03312<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.SD<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{anonymous2022vectorquantized,    title   = {Vector-quantized Image Modeling with Improved {VQGAN}},    author  = {Anonymous},    booktitle = {Submitted to The Tenth International Conference on Learning Representations },    year    = {2022},    url     = {https://openreview.net/forum?id=pfNyExj7z2},    note    = {under review}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">anonymous2022vectorquantized</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Vector-quantized Image Modeling with Improved {VQGAN}<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Anonymous<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">booktitle</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Submitted to The Tenth International Conference on Learning Representations <span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://openreview.net/forum?id=pfNyExj7z2<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">note</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>under review<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@unknown{unknown,    author  = {Lee, Doyup and Kim, Chiheon and Kim, Saehoon and Cho, Minsu and Han, Wook-Shin},    year    = {2022},    month   = {03},    title   = {Autoregressive Image Generation using Residual Quantization}}\"><pre><span class=\"pl-k\">@unknown</span>{<span class=\"pl-en\">unknown</span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Lee, Doyup and Kim, Chiheon and Kim, Saehoon and Cho, Minsu and Han, Wook-Shin<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">month</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>03<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Autoregressive Image Generation using Residual Quantization<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{Defossez2022HighFN,    title   = {High Fidelity Neural Audio Compression},    author  = {Alexandre D'efossez and Jade Copet and Gabriel Synnaeve and Yossi Adi},    journal = {ArXiv},    year    = {2022},    volume  = {abs/2210.13438}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">Defossez2022HighFN</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>High Fidelity Neural Audio Compression<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Alexandre D'efossez and Jade Copet and Gabriel Synnaeve and Yossi Adi<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ArXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">volume</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>abs/2210.13438<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Chiu2022SelfsupervisedLW,    title   = {Self-supervised Learning with Random-projection Quantizer for Speech Recognition},    author  = {Chung-Cheng Chiu and James Qin and Yu Zhang and Jiahui Yu and Yonghui Wu},    booktitle = {International Conference on Machine Learning},    year    = {2022}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Chiu2022SelfsupervisedLW</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Self-supervised Learning with Random-projection Quantizer for Speech Recognition<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Chung-Cheng Chiu and James Qin and Yu Zhang and Jiahui Yu and Yonghui Wu<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">booktitle</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>International Conference on Machine Learning<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Zhang2023GoogleUS,    title   = {Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages},    author  = {Yu Zhang and Wei Han and James Qin and Yongqiang Wang and Ankur Bapna and Zhehuai Chen and Nanxin Chen and Bo Li and Vera Axelrod and Gary Wang and Zhong Meng and Ke Hu and Andrew Rosenberg and Rohit Prabhavalkar and Daniel S. Park and Parisa Haghani and Jason Riesa and Ginger Perng and Hagen Soltau and Trevor Strohman and Bhuvana Ramabhadran and Tara N. Sainath and Pedro J. Moreno and Chung-Cheng Chiu and Johan Schalkwyk and Franccoise Beaufays and Yonghui Wu},    year    = {2023}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Zhang2023GoogleUS</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Yu Zhang and Wei Han and James Qin and Yongqiang Wang and Ankur Bapna and Zhehuai Chen and Nanxin Chen and Bo Li and Vera Axelrod and Gary Wang and Zhong Meng and Ke Hu and Andrew Rosenberg and Rohit Prabhavalkar and Daniel S. Park and Parisa Haghani and Jason Riesa and Ginger Perng and Hagen Soltau and Trevor Strohman and Bhuvana Ramabhadran and Tara N. Sainath and Pedro J. Moreno and Chung-Cheng Chiu and Johan Schalkwyk and Franccoise Beaufays and Yonghui Wu<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Shen2023NaturalSpeech2L,    title   = {NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers},    author  = {Kai Shen and Zeqian Ju and Xu Tan and Yanqing Liu and Yichong Leng and Lei He and Tao Qin and Sheng Zhao and Jiang Bian},    year    = {2023}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Shen2023NaturalSpeech2L</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Kai Shen and Zeqian Ju and Xu Tan and Yanqing Liu and Yichong Leng and Lei He and Tao Qin and Sheng Zhao and Jiang Bian<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Yang2023HiFiCodecGV,    title   = {HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec},    author  = {Dongchao Yang and Songxiang Liu and Rongjie Huang and Jinchuan Tian and Chao Weng and Yuexian Zou},    year    = {2023}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Yang2023HiFiCodecGV</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Dongchao Yang and Songxiang Liu and Rongjie Huang and Jinchuan Tian and Chao Weng and Yuexian Zou<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{Liu2023BridgingDA,    title   = {Bridging Discrete and Backpropagation: Straight-Through and Beyond},    author  = {Liyuan Liu and Chengyu Dong and Xiaodong Liu and Bin Yu and Jianfeng Gao},    journal = {ArXiv},    year    = {2023},    volume  = {abs/2304.08612}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">Liu2023BridgingDA</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Bridging Discrete and Backpropagation: Straight-Through and Beyond<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Liyuan Liu and Chengyu Dong and Xiaodong Liu and Bin Yu and Jianfeng Gao<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ArXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">volume</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>abs/2304.08612<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{huh2023improvedvqste,    title   = {Straightening Out the Straight-Through Estimator: Overcoming Optimization Challenges in Vector Quantized Networks},    author  = {Huh, Minyoung and Cheung, Brian and Agrawal, Pulkit and Isola, Phillip},    booktitle = {International Conference on Machine Learning},    year    = {2023},    organization = {PMLR}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">huh2023improvedvqste</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Straightening Out the Straight-Through Estimator: Overcoming Optimization Challenges in Vector Quantized Networks<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Huh, Minyoung and Cheung, Brian and Agrawal, Pulkit and Isola, Phillip<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">booktitle</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>International Conference on Machine Learning<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">organization</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>PMLR<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{rogozhnikov2022einops,    title   = {Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation},    author  = {Alex Rogozhnikov},    booktitle = {International Conference on Learning Representations},    year    = {2022},    url     = {https://openreview.net/forum?id=oapKSVM2bcj}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">rogozhnikov2022einops</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Alex Rogozhnikov<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">booktitle</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>International Conference on Learning Representations<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://openreview.net/forum?id=oapKSVM2bcj<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{shin2021translationequivariant,    title   = {Translation-equivariant Image Quantizer for Bi-directional Image-Text Generation},    author  = {Woncheol Shin and Gyubok Lee and Jiyoung Lee and Joonseok Lee and Edward Choi},    year    = {2021},    eprint  = {2112.00384},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">shin2021translationequivariant</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Translation-equivariant Image Quantizer for Bi-directional Image-Text Generation<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Woncheol Shin and Gyubok Lee and Jiyoung Lee and Joonseok Lee and Edward Choi<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2112.00384<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{mentzer2023finite,    title   = {Finite Scalar Quantization: VQ-VAE Made Simple},    author  = {Fabian Mentzer and David Minnen and Eirikur Agustsson and Michael Tschannen},    year    = {2023},    eprint  = {2309.15505},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">mentzer2023finite</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Finite Scalar Quantization: VQ-VAE Made Simple<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Fabian Mentzer and David Minnen and Eirikur Agustsson and Michael Tschannen<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2309.15505<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{yu2023language,    title   = {Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation},    author  = {Lijun Yu and Jos\u00e9 Lezama and Nitesh B. Gundavarapu and Luca Versari and Kihyuk Sohn and David Minnen and Yong Cheng and Agrim Gupta and Xiuye Gu and Alexander G. Hauptmann and Boqing Gong and Ming-Hsuan Yang and Irfan Essa and David A. Ross and Lu Jiang},    year    = {2023},    eprint  = {2310.05737},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">yu2023language</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Lijun Yu and Jos\u00e9 Lezama and Nitesh B. Gundavarapu and Luca Versari and Kihyuk Sohn and David Minnen and Yong Cheng and Agrim Gupta and Xiuye Gu and Alexander G. Hauptmann and Boqing Gong and Ming-Hsuan Yang and Irfan Essa and David A. Ross and Lu Jiang<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2310.05737<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{hsu2023disentanglement,    title   = {Disentanglement via Latent Quantization},     author  = {Kyle Hsu and Will Dorrell and James C. R. Whittington and Jiajun Wu and Chelsea Finn},    year    = {2023},    eprint  = {2305.18378},    archivePrefix = {arXiv},    primaryClass = {cs.LG}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">hsu2023disentanglement</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Disentanglement via Latent Quantization<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Kyle Hsu and Will Dorrell and James C. R. Whittington and Jiajun Wu and Chelsea Finn<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2305.18378<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.LG<span class=\"pl-pds\">}</span></span>}</pre></div></article></div></div></div></div></div> <!-- --> <!-- --> <script type=\"application/json\" id=\"__PRIMER_DATA__\">{\"resolvedServerColorMode\":\"day\"}</script></div></react-partial>        <input type=\"hidden\" data-csrf=\"true\" value=\"IBcoAM9rw8SnJ0UEDWpGppBdxL+q8dPorrzeNA7jyXeqbD2fCWGmhKWU5wNK6Ec23wGYPWS9Klrf3fPtr3qsBQ==\" /></div>  <div data-view-component=\"true\" class=\"Layout-sidebar\">            <div class=\"BorderGrid about-margin\" data-pjax>        <div class=\"BorderGrid-row\">          <div class=\"BorderGrid-cell\">            <div class=\"hide-sm hide-md\">  <h2 class=\"mb-3 h4\">About</h2>      <p class=\"f4 my-3\">        Vector (and Scalar) Quantization, in Pytorch      </p>    <h3 class=\"sr-only\">Topics</h3>    <div class=\"my-3\">        <div class=\"f6\">      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:deep-learning\" href=\"/topics/deep-learning\" title=\"Topic: deep-learning\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  deep-learning</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:pytorch\" href=\"/topics/pytorch\" title=\"Topic: pytorch\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  pytorch</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:artificial-intelligence\" href=\"/topics/artificial-intelligence\" title=\"Topic: artificial-intelligence\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  artificial-intelligence</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:vector-quantization\" href=\"/topics/vector-quantization\" title=\"Topic: vector-quantization\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  vector-quantization</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:scalar-quantization\" href=\"/topics/scalar-quantization\" title=\"Topic: scalar-quantization\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  scalar-quantization</a>  </div>    </div>    <h3 class=\"sr-only\">Resources</h3>    <div class=\"mt-2\">      <a class=\"Link--muted\" data-analytics-event=\"{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:readme&quot;}\" href=\"#readme-ov-file\">        <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-book mr-2\">    <path d=\"M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z\"></path></svg>        Readme</a>    </div>      <h3 class=\"sr-only\">License</h3>  <div class=\"mt-2\">    <a href=\"#MIT-1-ov-file\"      class=\"Link--muted\"            data-analytics-event=\"{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:license&quot;}\"    >      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-law mr-2\">    <path d=\"M8.75.75V2h.985c.304 0 .603.08.867.231l1.29.736c.038.022.08.033.124.033h2.234a.75.75 0 0 1 0 1.5h-.427l2.111 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.006.005-.01.01-.045.04c-.21.176-.441.327-.686.45C14.556 10.78 13.88 11 13 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L12.178 4.5h-.162c-.305 0-.604-.079-.868-.231l-1.29-.736a.245.245 0 0 0-.124-.033H8.75V13h2.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1 0-1.5h2.5V3.5h-.984a.245.245 0 0 0-.124.033l-1.289.737c-.265.15-.564.23-.869.23h-.162l2.112 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.016.015-.045.04c-.21.176-.441.327-.686.45C4.556 10.78 3.88 11 3 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L2.178 4.5H1.75a.75.75 0 0 1 0-1.5h2.234a.249.249 0 0 0 .125-.033l1.288-.737c.265-.15.564-.23.869-.23h.984V.75a.75.75 0 0 1 1.5 0Zm2.945 8.477c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L13 6.327Zm-10 0c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L3 6.327Z\"></path></svg>     MIT license    </a>  </div>  <include-fragment  src=\"/lucidrains/vector-quantize-pytorch/hovercards/citation/sidebar_partial?tree_name=master\">  </include-fragment>  <div class=\"mt-2\">    <a href=\"/lucidrains/vector-quantize-pytorch/activity\" data-view-component=\"true\" class=\"Link Link--muted\">      <svg text=\"gray\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-pulse mr-2\">    <path d=\"M6 2c.306 0 .582.187.696.471L10 10.731l1.304-3.26A.751.751 0 0 1 12 7h3.25a.75.75 0 0 1 0 1.5h-2.742l-1.812 4.528a.751.751 0 0 1-1.392 0L6 4.77 4.696 8.03A.75.75 0 0 1 4 8.5H.75a.75.75 0 0 1 0-1.5h2.742l1.812-4.529A.751.751 0 0 1 6 2Z\"></path></svg>      <span class=\"color-fg-muted\">Activity</span></a>  </div>  <h3 class=\"sr-only\">Stars</h3>  <div class=\"mt-2\">    <a href=\"/lucidrains/vector-quantize-pytorch/stargazers\" data-view-component=\"true\" class=\"Link Link--muted\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-star mr-2\">    <path d=\"M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z\"></path></svg>      <strong>1.7k</strong>      stars</a>  </div>  <h3 class=\"sr-only\">Watchers</h3>  <div class=\"mt-2\">    <a href=\"/lucidrains/vector-quantize-pytorch/watchers\" data-view-component=\"true\" class=\"Link Link--muted\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-eye mr-2\">    <path d=\"M8 2c1.981 0 3.671.992 4.933 2.078 1.27 1.091 2.187 2.345 2.637 3.023a1.62 1.62 0 0 1 0 1.798c-.45.678-1.367 1.932-2.637 3.023C11.67 13.008 9.981 14 8 14c-1.981 0-3.671-.992-4.933-2.078C1.797 10.83.88 9.576.43 8.898a1.62 1.62 0 0 1 0-1.798c.45-.677 1.367-1.931 2.637-3.022C4.33 2.992 6.019 2 8 2ZM1.679 7.932a.12.12 0 0 0 0 .136c.411.622 1.241 1.75 2.366 2.717C5.176 11.758 6.527 12.5 8 12.5c1.473 0 2.825-.742 3.955-1.715 1.124-.967 1.954-2.096 2.366-2.717a.12.12 0 0 0 0-.136c-.412-.621-1.242-1.75-2.366-2.717C10.824 4.242 9.473 3.5 8 3.5c-1.473 0-2.825.742-3.955 1.715-1.124.967-1.954 2.096-2.366 2.717ZM8 10a2 2 0 1 1-.001-3.999A2 2 0 0 1 8 10Z\"></path></svg>      <strong>28</strong>      watching</a>  </div>  <h3 class=\"sr-only\">Forks</h3>  <div class=\"mt-2\">    <a href=\"/lucidrains/vector-quantize-pytorch/forks\" data-view-component=\"true\" class=\"Link Link--muted\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo-forked mr-2\">    <path d=\"M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z\"></path></svg>      <strong>151</strong>      forks</a>  </div>    <div class=\"mt-2\">      <a class=\"Link--muted\" href=\"/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Flucidrains%2Fvector-quantize-pytorch&amp;report=lucidrains+%28user%29\">          Report repository</a>    </div></div>          </div>        </div>                    <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\">  <a href=\"/lucidrains/vector-quantize-pytorch/releases\" data-view-component=\"true\" class=\"Link--primary no-underline Link\">    Releases      <span title=\"175\" data-view-component=\"true\" class=\"Counter\">175</span></a></h2>  <a class=\"Link--primary d-flex no-underline\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" href=\"/lucidrains/vector-quantize-pytorch/releases/tag/1.14.1\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-tag flex-shrink-0 mt-1 color-fg-success\">    <path d=\"M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z\"></path></svg>    <div class=\"ml-2 min-width-0\">      <div class=\"d-flex\">        <span class=\"css-truncate css-truncate-target text-bold mr-2\" style=\"max-width: none;\">1.14.1</span>        <span title=\"Label: Latest\" data-view-component=\"true\" class=\"Label Label--success flex-shrink-0\">          Latest</span>      </div>      <div class=\"text-small color-fg-muted\"><relative-time datetime=\"2024-02-18T13:48:18Z\" class=\"no-wrap\">Feb 18, 2024</relative-time></div>    </div></a>    <div data-view-component=\"true\" class=\"mt-3\">      <a text=\"small\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" href=\"/lucidrains/vector-quantize-pytorch/releases\" data-view-component=\"true\" class=\"Link\">        + 174 releases</a></div>              </div>            </div>                            <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\">  <a href=\"/users/lucidrains/packages?repo_name=vector-quantize-pytorch\" data-view-component=\"true\" class=\"Link--primary no-underline Link d-flex flex-items-center\">    Packages      <span title=\"0\" hidden=\"hidden\" data-view-component=\"true\" class=\"Counter ml-1\">0</span></a></h2>      <div class=\"text-small color-fg-muted\">        No packages published <br>      </div>              </div>            </div>                    <div class=\"BorderGrid-row\" hidden>              <div class=\"BorderGrid-cell\">                <include-fragment src=\"/lucidrains/vector-quantize-pytorch/used_by_list\" accept=\"text/fragment+html\"></include-fragment>              </div>            </div>                    <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\">  <a href=\"/lucidrains/vector-quantize-pytorch/graphs/contributors\" data-view-component=\"true\" class=\"Link--primary no-underline Link d-flex flex-items-center\">    Contributors      <span title=\"11\" data-view-component=\"true\" class=\"Counter ml-1\">11</span></a></h2>      <ul class=\"list-style-none d-flex flex-wrap mb-n2\">    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/lucidrains\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/lucidrains/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/108653?s=64&amp;v=4\" alt=\"@lucidrains\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/sekstini\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/sekstini/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/127142660?s=64&amp;v=4\" alt=\"@sekstini\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/theAdamColton\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/theAdamColton/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/72479734?s=64&amp;v=4\" alt=\"@theAdamColton\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/kashif\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/kashif/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/8100?s=64&amp;v=4\" alt=\"@kashif\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/falkaer\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/falkaer/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/878596?s=64&amp;v=4\" alt=\"@falkaer\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/kifarid\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/kifarid/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/23289286?s=64&amp;v=4\" alt=\"@kifarid\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/wesbz\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/wesbz/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/5843554?s=64&amp;v=4\" alt=\"@wesbz\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/amirhm\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/amirhm/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/2778581?s=64&amp;v=4\" alt=\"@amirhm\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/npuichigo\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/npuichigo/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/11533479?s=64&amp;v=4\" alt=\"@npuichigo\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/matwilso\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/matwilso/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/19582683?s=64&amp;v=4\" alt=\"@matwilso\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/Lijun-Yu\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/Lijun-Yu/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/36872895?s=64&amp;v=4\" alt=\"@Lijun-Yu\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li></ul>              </div>            </div>                            <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\">Languages</h2><div class=\"mb-2\">  <span data-view-component=\"true\" class=\"Progress\">    <span style=\"background-color:#3572A5 !important;;width: 100.0%;\" itemprop=\"keywords\" aria-label=\"Python 100.0\" data-view-component=\"true\" class=\"Progress-item color-bg-success-emphasis\"></span></span></div><ul class=\"list-style-none\">    <li class=\"d-inline\">        <a class=\"d-inline-flex flex-items-center flex-nowrap Link--secondary no-underline text-small mr-3\" href=\"/lucidrains/vector-quantize-pytorch/search?l=python\"  data-ga-click=\"Repository, language stats search click, location:repo overview\">          <svg style=\"color:#3572A5;\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-dot-fill mr-2\">    <path d=\"M8 4a4 4 0 1 1 0 8 4 4 0 0 1 0-8Z\"></path></svg>          <span class=\"color-fg-default text-bold mr-1\">Python</span>          <span>100.0%</span>        </a>    </li></ul>              </div>            </div>              </div></div>  </div></div>  </div>  </div></turbo-frame>    </main>  </div>  </div>          <footer class=\"footer pt-8 pb-6 f6 color-fg-muted p-responsive\" role=\"contentinfo\" >  <h2 class='sr-only'>Footer</h2>    <div class=\"d-flex flex-justify-center flex-items-center flex-column-reverse flex-lg-row flex-wrap flex-lg-nowrap\">    <div class=\"d-flex flex-items-center flex-shrink-0 mx-2\">      <a aria-label=\"Homepage\" title=\"GitHub\" class=\"footer-octicon mr-2\" href=\"https://github.com\">        <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-mark-github\">    <path d=\"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z\"></path></svg></a>      <span>        &copy; 2024 GitHub,&nbsp;Inc.      </span>    </div>    <nav aria-label=\"Footer\">      <h3 class=\"sr-only\" id=\"sr-footer-heading\">Footer navigation</h3>      <ul class=\"list-style-none d-flex flex-justify-center flex-wrap mb-2 mb-lg-0\" aria-labelledby=\"sr-footer-heading\">          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to Terms&quot;,&quot;label&quot;:&quot;text:terms&quot;}\" href=\"https://docs.github.com/site-policy/github-terms/github-terms-of-service\" data-view-component=\"true\" class=\"Link--secondary Link\">Terms</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to privacy&quot;,&quot;label&quot;:&quot;text:privacy&quot;}\" href=\"https://docs.github.com/site-policy/privacy-policies/github-privacy-statement\" data-view-component=\"true\" class=\"Link--secondary Link\">Privacy</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to security&quot;,&quot;label&quot;:&quot;text:security&quot;}\" href=\"/security\" data-view-component=\"true\" class=\"Link--secondary Link\">Security</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to status&quot;,&quot;label&quot;:&quot;text:status&quot;}\" href=\"https://www.githubstatus.com/\" data-view-component=\"true\" class=\"Link--secondary Link\">Status</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to docs&quot;,&quot;label&quot;:&quot;text:docs&quot;}\" href=\"https://docs.github.com\" data-view-component=\"true\" class=\"Link--secondary Link\">Docs</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to contact&quot;,&quot;label&quot;:&quot;text:contact&quot;}\" href=\"https://support.github.com?tags=dotcom-footer\" data-view-component=\"true\" class=\"Link--secondary Link\">Contact</a>          </li>          <li class=\"mx-2\" >  <cookie-consent-link>    <button type=\"button\" class=\"Link--secondary underline-on-hover border-0 p-0 color-bg-transparent\" data-action=\"click:cookie-consent-link#showConsentManagement\">      Manage cookies    </button>  </cookie-consent-link></li><li class=\"mx-2\">  <cookie-consent-link>    <button type=\"button\" class=\"Link--secondary underline-on-hover border-0 p-0 color-bg-transparent\" data-action=\"click:cookie-consent-link#showConsentManagement\">      Do not share my personal information    </button>  </cookie-consent-link></li>      </ul>    </nav>  </div></footer>    <cookie-consent id=\"cookie-consent-banner\" class=\"position-fixed bottom-0 left-0\" style=\"z-index: 999999\" data-initial-cookie-consent-allowed=\"\" data-cookie-consent-required=\"false\"></cookie-consent>  <div id=\"ajax-error-message\" class=\"ajax-error-message flash flash-error\" hidden>    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-alert\">    <path d=\"M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>    <button type=\"button\" class=\"flash-close js-ajax-error-dismiss\" aria-label=\"Dismiss error\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg>    </button>    You can\u2019t perform that action at this time.  </div>    <template id=\"site-details-dialog\">  <details class=\"details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm\" open>    <summary role=\"button\" aria-label=\"Close dialog\"></summary>    <details-dialog class=\"Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal\">      <button class=\"Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0\" type=\"button\" aria-label=\"Close dialog\" data-close-dialog>        <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg>      </button>      <div class=\"octocat-spinner my-6 js-details-dialog-spinner\"></div>    </details-dialog>  </details></template>    <div class=\"Popover js-hovercard-content position-absolute\" style=\"display: none; outline: none;\" tabindex=\"0\">  <div class=\"Popover-message Popover-message--bottom-left Popover-message--large Box color-shadow-large\" style=\"width:360px;\">  </div></div>    <template id=\"snippet-clipboard-copy-button\">  <div class=\"zeroclipboard-container position-absolute right-0 top-0\">    <clipboard-copy aria-label=\"Copy\" class=\"ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay\" data-copy-feedback=\"Copied!\" data-tooltip-direction=\"w\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copy js-clipboard-copy-icon m-2\">    <path d=\"M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z\"></path><path d=\"M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path></svg>      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2\">    <path d=\"M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z\"></path></svg>    </clipboard-copy>  </div></template><template id=\"snippet-clipboard-copy-button-unpositioned\">  <div class=\"zeroclipboard-container\">    <clipboard-copy aria-label=\"Copy\" class=\"ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center\" data-copy-feedback=\"Copied!\" data-tooltip-direction=\"w\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copy js-clipboard-copy-icon\">    <path d=\"M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z\"></path><path d=\"M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path></svg>      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-check js-clipboard-check-icon color-fg-success d-none\">    <path d=\"M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z\"></path></svg>    </clipboard-copy>  </div></template>    </div>    <div id=\"js-global-screen-reader-notice\" class=\"sr-only\" aria-live=\"polite\" aria-atomic=\"true\" ></div>    <div id=\"js-global-screen-reader-notice-assertive\" class=\"sr-only\" aria-live=\"assertive\" aria-atomic=\"true\"></div>  </body></html>",
  "embeddings": []
}