{
  "libtype": "pypi",
  "libname": "open-clip-torch",
  "url": "https://pypi.org/project/open-clip-torch/",
  "html": "<!DOCTYPE html><html lang=\"en\" dir=\"ltr\">  <head>    <meta charset=\"utf-8\">    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">    <meta name=\"defaultLanguage\" content=\"en\">    <meta name=\"availableLanguages\" content=\"en, es, fr, ja, pt_BR, uk, el, de, zh_Hans, zh_Hant, ru, he, eo\">    <title>open-clip-torch \u00b7 PyPI</title>    <meta name=\"description\" content=\"OpenCLIP\">    <link rel=\"stylesheet\" href=\"/static/css/warehouse-ltr.99b3104d.css\">    <link rel=\"stylesheet\" href=\"/static/css/fontawesome.b50b476c.css\">    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Source+Sans+3:400,400italic,600,600italic,700,700italic%7CSource+Code+Pro:500\">    <noscript>      <link rel=\"stylesheet\" href=\"/static/css/noscript.0673c9ea.css\">    </noscript>    <link rel=\"icon\" href=\"/static/images/favicon.35549fe8.ico\" type=\"image/x-icon\">    <link rel=\"alternate\" type=\"application/rss+xml\" title=\"RSS: 40 latest updates\" href=\"/rss/updates.xml\">    <link rel=\"alternate\" type=\"application/rss+xml\" title=\"RSS: 40 newest packages\" href=\"/rss/packages.xml\"><link rel=\"alternate\" type=\"application/rss+xml\" title=\"RSS: latest releases for open-clip-torch\" href=\"/rss/project/open-clip-torch/releases.xml\">    <link rel=\"canonical\" href=\"https://pypi.org/project/open-clip-torch/\">    <meta property=\"og:url\" content=\"https://pypi.org/project/open-clip-torch/\">    <meta property=\"og:site_name\" content=\"PyPI\">    <meta property=\"og:type\" content=\"website\">    <meta property=\"og:image\" content=\"https://pypi.org/static/images/twitter.abaf4b19.webp\">    <meta property=\"og:title\" content=\"open-clip-torch\">    <meta property=\"og:description\" content=\"OpenCLIP\">    <link rel=\"search\" type=\"application/opensearchdescription+xml\" title=\"PyPI\" href=\"/opensearch.xml\">    <script asyncdata-ga-id=\"UA-55961911-1\"data-ga4-id=\"G-RW7D75DF8V\"            src=\"/static/js/warehouse.dd4295c4.js\">    </script><script>MathJax = {  tex: {    inlineMath: [['$', '$'], ['\\\\(', '\\\\)']]  },};</script><script async  src=\"https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-svg.js\"  integrity=\"sha256-1CldwzdEg2k1wTmf7s5RWVd7NMXI/7nxxjJM2C4DqII=\"  crossorigin=\"anonymous\"></script><script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-55961911-1\"></script><script async src=\"https://www.googletagmanager.com/gtag/js?id=G-RW7D75DF8V\"></script><script defer src=\"https://www.fastly-insights.com/insights.js?k=6a52360a-f306-421e-8ed5-7417d0d4a4e9&dnt=true\"></script>    <script async        src=\"https://media.ethicalads.io/media/client/v1.4.0/ethicalads.min.js\"        integrity=\"sha256-U3hKDidudIaxBDEzwGJApJgPEf2mWk6cfMWghrAa6i0= sha384-UcmsCqcNRSLW/dV3Lo1oCi2/VaurXbib6p4HyUEOeIa/4OpsrnucrugAefzVZJfI sha512-q4t1L4xEjGV2R4hzqCa41P8jrgFUS8xTb8rdNv4FGvw7FpydVj/kkxBJHOiaoxHa8olCcx1Slk9K+3sNbsM4ug==\"        crossorigin=\"anonymous\"    ></script>  </head>  <body data-controller=\"viewport-toggle\">    <!-- Accessibility: this link should always be the first piece of content inside the body-->    <a href=\"#content\" class=\"skip-to-content\">Skip to main content</a>    <button type=\"button\" class=\"button button--primary button--switch-to-mobile hidden\" data-viewport-toggle-target=\"switchToMobile\" data-action=\"viewport-toggle#switchToMobile\">Switch to mobile version    </button>    <div id=\"sticky-notifications\" class=\"stick-to-top js-stick-to-top\">      <!-- Add browser warning. Will show for ie9 and below -->      <!--[if IE]>      <div class=\"notification-bar notification-bar--warning\" role=\"status\">        <span class=\"notification-bar__icon\">          <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"></i>          <span class=\"sr-only\">Warning</span>        </span>        <span class=\"notification-bar__message\">You are using an unsupported browser, upgrade to a newer version.</span>      </div>      <![endif]-->      <noscript>      <div class=\"notification-bar notification-bar--warning\" role=\"status\">        <span class=\"notification-bar__icon\">          <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"></i>          <span class=\"sr-only\">Warning</span>        </span>        <span class=\"notification-bar__message\">Some features may not work without JavaScript. Please try enabling it if you encounter problems.</span>      </div>      </noscript><div data-html-include=\"/_includes/notification-banners/\"></div>    </div><div data-html-include=\"/_includes/flash-messages/\"></div><div data-html-include=\"/_includes/session-notifications/\"></div>    <header class=\"site-header \">      <div class=\"site-container\">        <div class=\"split-layout\">          <div class=\"split-layout\">            <div>              <a class=\"site-header__logo\" href=\"/\">                <img alt=\"PyPI\" src=\"/static/images/logo-small.2a411bc6.svg\">              </a>            </div>            <form class=\"search-form search-form--primary\" action=\"/search/\" role=\"search\">              <label for=\"search\" class=\"sr-only\">Search PyPI</label>              <input id=\"search\" class=\"search-form__search\" type=\"text\" name=\"q\" placeholder=\"Search projects\" value=\"\" autocomplete=\"off\" autocapitalize=\"off\" spellcheck=\"false\" data-controller=\"search-focus\" data-action=\"keydown@window->search-focus#focusSearchField\" data-search-focus-target=\"searchField\">              <button type=\"submit\" class=\"search-form__button\">                <i class=\"fa fa-search\" aria-hidden=\"true\"></i>                <span class=\"sr-only\">Search</span>              </button>            </form>          </div><div data-html-include=\"/_includes/current-user-indicator/\">            <div id=\"user-indicator\" class=\"horizontal-menu horizontal-menu--light horizontal-menu--tall\">  <nav class=\"horizontal-menu horizontal-menu--light horizontal-menu--tall hide-on-tablet\" aria-label=\"Main navigation\">    <ul>      <li class=\"horizontal-menu__item\"><a href=\"/help/\" class=\"horizontal-menu__link\">Help</a></li>      <li class=\"horizontal-menu__item\"><a href=\"/sponsors/\" class=\"horizontal-menu__link\">Sponsors</a></li>      <li class=\"horizontal-menu__item\"><a href=\"/account/login/\" class=\"horizontal-menu__link\">Log in</a></li>      <li class=\"horizontal-menu__item\"><a href=\"/account/register/\" class=\"horizontal-menu__link\">Register</a></li>    </ul>  </nav>  <nav class=\"dropdown dropdown--on-menu hidden show-on-tablet\" aria-label=\"Main navigation\">    <button type=\"button\" class=\"horizontal-menu__link dropdown__trigger\" aria-haspopup=\"true\" aria-expanded=\"false\" aria-label=\"View menu\">Menu      <span class=\"dropdown__trigger-caret\">        <i class=\"fa fa-caret-down\" aria-hidden=\"true\"></i>      </span>    </button>    <ul class=\"dropdown__content\" aria-hidden=\"true\" aria-label=\"Main menu\">      <li><a class=\"dropdown__link\" href=\"/help/\">Help</a></li>      <li><a class=\"dropdown__link\" href=\"/sponsors/\">Sponsors</a></li>      <li><a class=\"dropdown__link\" href=\"/account/login/\">Log in</a></li>      <li><a class=\"dropdown__link\" href=\"/account/register/\">Register</a></li>    </ul>  </nav></div></div>        </div>      </div>    </header>    <div class=\"mobile-search\">      <form class=\"search-form search-form--fullwidth\" action=\"/search/\" role=\"search\">        <label for=\"mobile-search\" class=\"sr-only\">Search PyPI</label>        <input id=\"mobile-search\" class=\"search-form__search\" type=\"text\" name=\"q\" placeholder=\"Search projects\" value=\"\" autocomplete=\"off\" autocapitalize=\"off\" spellcheck=\"false\">                <button type=\"submit\" class=\"search-form__button\">          <i class=\"fa fa-search\" aria-hidden=\"true\"></i>          <span class=\"sr-only\">Search</span>        </button>      </form>    </div>    <main id=\"content\"><div class=\"hidden\"  data-controller=\"github-repo-stats\"  data-github-repo-stats-github-repo-info-outlet=\".github-repo-info\"  data-github-repo-stats-url-value=\"https://api.github.com/repos/mlfoundations/open_clip\"  data-github-repo-stats-issue-url-value=\"https://api.github.com/search/issues?q=repo:mlfoundations/open_clip+type:issue+state:open&amp;per_page=1\"></div><div class=\"banner\">  <div class=\"package-header\">    <div class=\"package-header__left\">      <h1 class=\"package-header__name\">        open-clip-torch 2.24.0      </h1>      <div data-controller=\"clipboard\">        <p class=\"package-header__pip-instructions\">          <span id=\"pip-command\" data-clipboard-target=\"source\">pip install open-clip-torch</span>          <button type=\"button\" class=\"copy-tooltip copy-tooltip-s\" data-action=\"clipboard#copy\" data-clipboard-target=\"tooltip\" data-clipboard-tooltip-value=\"Copy to clipboard\">            <i class=\"fa fa-copy\" aria-hidden=\"true\"></i>            <span class=\"sr-only\">Copy PIP instructions</span>          </button>        </p>      </div>    </div>    <div class=\"package-header__right\">      <a class=\"status-badge status-badge--good\" href=\"/project/open-clip-torch/\">        <span>Latest version</span>      </a>      <p class=\"package-header__date\">Released: <time datetime=\"2024-01-08T10:35:53+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Jan 8, 2024</time>      </p>    </div>  </div></div><div class=\"horizontal-section horizontal-section--grey horizontal-section--thin\">  <div class=\"site-container\"><div data-html-include=\"/_includes/administer-project-include/open-clip-torch\"></div>    <div class=\"split-layout split-layout--middle package-description\">      <p class=\"package-description__summary\">OpenCLIP</p><div data-html-include=\"/_includes/edit-project-button/open-clip-torch\"></div>    </div>  </div></div><div data-controller=\"project-tabs\">  <div class=\"tabs-container\">    <div class=\"vertical-tabs\">      <div class=\"vertical-tabs__tabs\">        <div class=\"sidebar-section\">          <h3 class=\"sidebar-section__title\">Navigation</h3>          <nav aria-label=\"Navigation for open-clip-torch\">            <ul class=\"vertical-tabs__list\" role=\"tablist\">              <li role=\"tab\">                <a id=\"description-tab\" href=\"#description\" data-project-tabs-target=\"tab\" data-action=\"project-tabs#onTabClick\" class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--is-active\" aria-selected=\"true\" aria-label=\"Project description. Focus will be moved to the description.\">                  <i class=\"fa fa-align-left\" aria-hidden=\"true\"></i>Project description                </a>              </li>              <li role=\"tab\">                <a id=\"history-tab\" href=\"#history\" data-project-tabs-target=\"tab\" data-action=\"project-tabs#onTabClick\" class=\"vertical-tabs__tab vertical-tabs__tab--with-icon\" aria-label=\"Release history. Focus will be moved to the history panel.\">                  <i class=\"fa fa-history\" aria-hidden=\"true\"></i>Release history                </a>              </li>              <li role=\"tab\">                <a id=\"files-tab\" href=\"#files\" data-project-tabs-target=\"tab\" data-action=\"project-tabs#onTabClick\" class=\"vertical-tabs__tab vertical-tabs__tab--with-icon\" aria-label=\"Download files. Focus will be moved to the project files.\">                  <i class=\"fa fa-download\" aria-hidden=\"true\"></i>Download files                </a>              </li>            </ul>          </nav>        </div><div class=\"sidebar-section\">  <h3 class=\"sidebar-section__title\">Project links</h3>  <ul class=\"vertical-tabs__list\">    <li>      <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\" href=\"https://github.com/mlfoundations/open_clip\" rel=\"nofollow\">        <i class=\"fas fa-home\" aria-hidden=\"true\"></i>Homepage      </a>    </li>  </ul></div><div class=\"sidebar-section\">  <h3 class=\"sidebar-section__title\">Statistics</h3>  <div class=\"hidden github-repo-info\" data-controller=\"github-repo-info\">GitHub statistics:    <ul class=\"vertical-tabs__list\">      <li>        <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\"           data-github-repo-info-target=\"stargazersUrl\" rel=\"noopener\">          <i class=\"fa fa-star\" aria-hidden=\"true\"></i>          <strong>Stars:</strong>          <span data-github-repo-info-target=\"stargazersCount\"></span>        </a>      </li>      <li>        <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\"           data-github-repo-info-target=\"forksUrl\" rel=\"noopener\">          <i class=\"fa fa-code-branch\" aria-hidden=\"true\"></i>          <strong>Forks:</strong>          <span data-github-repo-info-target=\"forksCount\"></span>        </a>      </li>      <li>        <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\"           data-github-repo-info-target=\"openIssuesUrl\" rel=\"noopener\">          <i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>          <strong>Open issues:</strong>          <span data-github-repo-info-target=\"openIssuesCount\"></span>        </a>      </li>      <li>        <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\"           data-github-repo-info-target=\"openPRsUrl\" rel=\"noopener\">          <i class=\"fa fa-code-pull-request\" aria-hidden=\"true\"></i>          <strong>Open PRs:</strong>          <span data-github-repo-info-target=\"openPRsCount\"></span>        </a>      </li>    </ul>  </div>  <p>View statistics for this project via <a href=\"https://libraries.io/pypi/open-clip-torch\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Libraries.io</a>, or by using <a href=\"https://packaging.python.org/guides/analyzing-pypi-package-downloads/\" target=\"_blank\" rel=\"noopener\">our public dataset on Google BigQuery</a>  </p></div><div class=\"sidebar-section\">  <h3 class=\"sidebar-section__title\">Meta</h3>  <p><strong>License:</strong> Apache Software License</p>  <p class=\"tags\">    <i class=\"fa fa-tags\" aria-hidden=\"true\"></i>    <span class=\"sr-only\">Tags</span>    <span class=\"package-keyword\">      CLIP,    </span>    <span class=\"package-keyword\">      pretrained    </span>  </p>  <p>    <strong>Requires:</strong> Python &gt;=3.7  </p></div><div class=\"sidebar-section\">    <h3 class=\"sidebar-section__title\">Maintainers</h3>      <span class=\"sidebar-section__maintainer\">        <a href=\"/user/ludwigschmidt/\" aria-label=\"ludwigschmidt\">          <span class=\"sidebar-section__user-gravatar\">            <img src=\"https://pypi-camo.freetls.fastly.net/4132ab97806f798fb703390d5acad9c2f3f686ec/68747470733a2f2f7365637572652e67726176617461722e636f6d2f6176617461722f63643466306232323065613336653133666637313135376366666539643964313f73697a653d3530\" height=\"50\" width=\"50\" alt=\"Avatar for ludwigschmidt from gravatar.com\" title=\"Avatar for ludwigschmidt from gravatar.com\">          </span>          <span class=\"sidebar-section__user-gravatar-text\">            ludwigschmidt          </span>        </a>      </span>      <span class=\"sidebar-section__maintainer\">        <a href=\"/user/rwightman/\" aria-label=\"rwightman\">          <span class=\"sidebar-section__user-gravatar\">            <img src=\"https://pypi-camo.freetls.fastly.net/ef32c24299ade8e74661c3911e6e0b65982ae14d/68747470733a2f2f7365637572652e67726176617461722e636f6d2f6176617461722f31393738633263343466636234383061613666306666383839623733656561363f73697a653d3530\" height=\"50\" width=\"50\" alt=\"Avatar for rwightman from gravatar.com\" title=\"Avatar for rwightman from gravatar.com\">          </span>          <span class=\"sidebar-section__user-gravatar-text\">            rwightman          </span>        </a>      </span></div><div class=\"sidebar-section\">  <h3 class=\"sidebar-section__title\">Classifiers</h3>  <ul class=\"sidebar-section__classifiers\">    <li>      <strong>Development Status</strong>      <ul>        <li>          <a href=\"/search/?c=Development+Status+%3A%3A+3+-+Alpha\">            3 - Alpha          </a>        </li>      </ul>    </li>    <li>      <strong>Intended Audience</strong>      <ul>        <li>          <a href=\"/search/?c=Intended+Audience+%3A%3A+Education\">            Education          </a>        </li>        <li>          <a href=\"/search/?c=Intended+Audience+%3A%3A+Science%2FResearch\">            Science/Research          </a>        </li>      </ul>    </li>    <li>      <strong>License</strong>      <ul>        <li>          <a href=\"/search/?c=License+%3A%3A+OSI+Approved+%3A%3A+Apache+Software+License\">            OSI Approved :: Apache Software License          </a>        </li>      </ul>    </li>    <li>      <strong>Programming Language</strong>      <ul>        <li>          <a href=\"/search/?c=Programming+Language+%3A%3A+Python+%3A%3A+3.7\">            Python :: 3.7          </a>        </li>        <li>          <a href=\"/search/?c=Programming+Language+%3A%3A+Python+%3A%3A+3.8\">            Python :: 3.8          </a>        </li>        <li>          <a href=\"/search/?c=Programming+Language+%3A%3A+Python+%3A%3A+3.9\">            Python :: 3.9          </a>        </li>        <li>          <a href=\"/search/?c=Programming+Language+%3A%3A+Python+%3A%3A+3.10\">            Python :: 3.10          </a>        </li>      </ul>    </li>    <li>      <strong>Topic</strong>      <ul>        <li>          <a href=\"/search/?c=Topic+%3A%3A+Scientific%2FEngineering\">            Scientific/Engineering          </a>        </li>        <li>          <a href=\"/search/?c=Topic+%3A%3A+Scientific%2FEngineering+%3A%3A+Artificial+Intelligence\">            Scientific/Engineering :: Artificial Intelligence          </a>        </li>        <li>          <a href=\"/search/?c=Topic+%3A%3A+Software+Development\">            Software Development          </a>        </li>        <li>          <a href=\"/search/?c=Topic+%3A%3A+Software+Development+%3A%3A+Libraries\">            Software Development :: Libraries          </a>        </li>        <li>          <a href=\"/search/?c=Topic+%3A%3A+Software+Development+%3A%3A+Libraries+%3A%3A+Python+Modules\">            Software Development :: Libraries :: Python Modules          </a>        </li>      </ul>    </li>  </ul></div><div class=\"sidebar-section\" data-ea-publisher=\"psf\" data-ea-type=\"psf\" data-ea-keywords=\"pypi-sidebar\"></div>      </div>      <div class=\"vertical-tabs__panel\">        <!-- mobile menu -->        <nav aria-label=\"Navigation for open-clip-torch\">          <ul class=\"vertical-tabs__list\" role=\"tablist\">            <li role=\"tab\">              <a id=\"mobile-description-tab\" href=\"#description\" data-project-tabs-target=\"mobileTab\" data-action=\"project-tabs#onTabClick\" class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--mobile vertical-tabs__tab--no-top-border vertical-tabs__tab--is-active\" aria-selected=\"true\" aria-label=\"Project description. Focus will be moved to the description.\">                <i class=\"fa fa-align-left\" aria-hidden=\"true\"></i>Project description              </a>            </li>            <li role=\"tab\">              <a id=\"mobile-data-tab\" href=\"#data\" data-project-tabs-target=\"mobileTab\" data-action=\"project-tabs#onTabClick\" class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--mobile\" aria-label=\"Project details. Focus will be moved to the project details.\">                <i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>Project details              </a>            </li>            <li role=\"tab\">              <a id=\"mobile-history-tab\" href=\"#history\" data-project-tabs-target=\"mobileTab\" data-action=\"project-tabs#onTabClick\" class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--mobile\" aria-label=\"Release history. Focus will be moved to the history panel.\">              <i class=\"fa fa-history\" aria-hidden=\"true\"></i>Release history            </a>            </li>            <li role=\"tab\">              <a id=\"mobile-files-tab\" href=\"#files\" data-project-tabs-target=\"mobileTab\" data-action=\"project-tabs#onTabClick\" class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--mobile\" aria-label=\"Download files. Focus will be moved to the project files.\">                <i class=\"fa fa-download\" aria-hidden=\"true\"></i>Download files              </a>            </li>          </ul>        </nav>        <div id=\"description\" data-project-tabs-target=\"content\" class=\"vertical-tabs__content\" role=\"tabpanel\" aria-labelledby=\"description-tab mobile-description-tab\" tabindex=\"-1\">          <h2 class=\"page-title\">Project description</h2>          <div class=\"project-description\">            <h1>OpenCLIP</h1><p><a href=\"https://arxiv.org/abs/2212.07143\" rel=nofollow>[Paper]</a> <a href=#citing rel=nofollow>[Citations]</a> <a href=\"https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb\" rel=nofollow>[Clip Colab]</a> <a href=\"https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb\" rel=nofollow>[Coca Colab]</a><a href=\"https://pypi.python.org/pypi/open_clip_torch\" rel=nofollow><img src=\"https://pypi-camo.freetls.fastly.net/889d9a25d0eaf1a0231f029321a80e0c7d16cf2c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6f70656e5f636c69705f746f7263682e737667\" alt=pypi></a></p><p>Welcome to an open source implementation of OpenAI's <a href=\"https://arxiv.org/abs/2103.00020\" rel=nofollow>CLIP</a> (Contrastive Language-Image Pre-training).</p><p>Using this codebase, we have trained several models on a variety of data sources and compute budgets, ranging from <a href=\"docs/LOW_ACC.md\" rel=nofollow>small-scale experiments</a> to larger runs including models trained on datasets such as <a href=\"https://arxiv.org/abs/2111.02114\" rel=nofollow>LAION-400M</a>, <a href=\"https://arxiv.org/abs/2210.08402\" rel=nofollow>LAION-2B</a> and <a href=\"https://arxiv.org/abs/2304.14108\" rel=nofollow>DataComp-1B</a>.Many of our models and their scaling properties are studied in detail in the paper <a href=\"https://arxiv.org/abs/2212.07143\" rel=nofollow>reproducible scaling laws for contrastive language-image learning</a>.Some of our best models and their zero-shot ImageNet-1k accuracy are shown below, along with the ViT-L model trained by OpenAI.We provide more details about our full collection of pretrained models <a href=\"docs/PRETRAINED.md\" rel=nofollow>here</a>, and zero-shot results for 38 datasets <a href=\"docs/openclip_results.csv\" rel=nofollow>here</a>.</p><table><thead><tr><th>Model</th><th>Training data</th><th>Resolution</th><th># of samples seen</th><th>ImageNet zero-shot acc.</th></tr></thead><tbody><tr><td>ConvNext-Base</td><td>LAION-2B</td><td>256px</td><td>13B</td><td>71.5%</td></tr><tr><td>ConvNext-Large</td><td>LAION-2B</td><td>320px</td><td>29B</td><td>76.9%</td></tr><tr><td>ConvNext-XXLarge</td><td>LAION-2B</td><td>256px</td><td>34B</td><td>79.5%</td></tr><tr><td>ViT-B/32</td><td>DataComp-1B</td><td>256px</td><td>34B</td><td>72.8%</td></tr><tr><td>ViT-B/16</td><td>DataComp-1B</td><td>224px</td><td>13B</td><td>73.5%</td></tr><tr><td>ViT-L/14</td><td>LAION-2B</td><td>224px</td><td>32B</td><td>75.3%</td></tr><tr><td>ViT-H/14</td><td>LAION-2B</td><td>224px</td><td>32B</td><td>78.0%</td></tr><tr><td>ViT-L/14</td><td>DataComp-1B</td><td>224px</td><td>13B</td><td>79.2%</td></tr><tr><td>ViT-G/14</td><td>LAION-2B</td><td>224px</td><td>34B</td><td>80.1%</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ViT-L/14</td><td>OpenAI's WIT</td><td>224px</td><td>13B</td><td>75.5%</td></tr></tbody></table><p>Model cards with additional model specific details can be found on the Hugging Face Hub under the OpenCLIP library tag: <a href=\"https://huggingface.co/models?library=open_clip\" rel=nofollow>https://huggingface.co/models?library=open_clip</a>.</p><p>If you found this repository useful, please consider <a href=#citing rel=nofollow>citing</a>.We welcome anyone to submit an issue or send an email if you have any other requests or suggestions.</p><p>Note that portions of <code>src/open_clip/</code> modelling and tokenizer code are adaptations of OpenAI's official <a href=\"https://github.com/openai/CLIP\" rel=nofollow>repository</a>.</p><h2>Approach</h2><table><thead><tr><th align=center><img src=\"https://pypi-camo.freetls.fastly.net/2b448e97644d92714796e417d7ee0d449927a84a/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6d6c666f756e646174696f6e732f6f70656e5f636c69702f6d61696e2f646f63732f434c49502e706e67\" alt=CLIP></th></tr></thead><tbody><tr><td align=center>Image Credit: <a href=\"https://github.com/openai/CLIP\" rel=nofollow>https://github.com/openai/CLIP</a></td></tr></tbody></table><h2>Usage</h2><pre><code>pip install open_clip_torch</code></pre><pre lang=python3><span class=kn>import</span> <span class=nn>torch</span><span class=kn>from</span> <span class=nn>PIL</span> <span class=kn>import</span> <span class=n>Image</span><span class=kn>import</span> <span class=nn>open_clip</span><span class=n>model</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>preprocess</span> <span class=o>=</span> <span class=n>open_clip</span><span class=o>.</span><span class=n>create_model_and_transforms</span><span class=p>(</span><span class=s1>'ViT-B-32'</span><span class=p>,</span> <span class=n>pretrained</span><span class=o>=</span><span class=s1>'laion2b_s34b_b79k'</span><span class=p>)</span><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>open_clip</span><span class=o>.</span><span class=n>get_tokenizer</span><span class=p>(</span><span class=s1>'ViT-B-32'</span><span class=p>)</span><span class=n>image</span> <span class=o>=</span> <span class=n>preprocess</span><span class=p>(</span><span class=n>Image</span><span class=o>.</span><span class=n>open</span><span class=p>(</span><span class=s2>\"CLIP.png\"</span><span class=p>))</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span><span class=n>text</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>([</span><span class=s2>\"a diagram\"</span><span class=p>,</span> <span class=s2>\"a dog\"</span><span class=p>,</span> <span class=s2>\"a cat\"</span><span class=p>])</span><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>(),</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>amp</span><span class=o>.</span><span class=n>autocast</span><span class=p>():</span>    <span class=n>image_features</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>encode_image</span><span class=p>(</span><span class=n>image</span><span class=p>)</span>    <span class=n>text_features</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>encode_text</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>    <span class=n>image_features</span> <span class=o>/=</span> <span class=n>image_features</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>    <span class=n>text_features</span> <span class=o>/=</span> <span class=n>text_features</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>    <span class=n>text_probs</span> <span class=o>=</span> <span class=p>(</span><span class=mf>100.0</span> <span class=o>*</span> <span class=n>image_features</span> <span class=o>@</span> <span class=n>text_features</span><span class=o>.</span><span class=n>T</span><span class=p>)</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span><span class=nb>print</span><span class=p>(</span><span class=s2>\"Label probs:\"</span><span class=p>,</span> <span class=n>text_probs</span><span class=p>)</span>  <span class=c1># prints: [[1., 0., 0.]]</span></pre><p>See also this <a href=\"https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb\" rel=nofollow>[Clip Colab]</a>.</p><p>To compute billions of embeddings efficiently, you can use <a href=\"https://github.com/rom1504/clip-retrieval\" rel=nofollow>clip-retrieval</a> which has openclip support.</p><h3>Pretrained models</h3><p>We offer a simple model interface to instantiate both pre-trained and untrained models.To see which pretrained models are available, use the following code snippet.More details about our pretrained models are available <a href=\"docs/PRETRAINED.md\" rel=nofollow>here</a>.</p><pre lang=python3><span class=o>&gt;&gt;&gt;</span> <span class=kn>import</span> <span class=nn>open_clip</span><span class=o>&gt;&gt;&gt;</span> <span class=n>open_clip</span><span class=o>.</span><span class=n>list_pretrained</span><span class=p>()</span></pre><p>You can find more about the models we support (e.g. number of parameters, FLOPs) in <a href=\"docs/model_profile.csv\" rel=nofollow>this table</a>.</p><p>NOTE: Many existing checkpoints use the QuickGELU activation from the original OpenAI models. This activation is actually less efficient than native torch.nn.GELU in recent versions of PyTorch. The model defaults are now nn.GELU, so one should use model definitions with <code>-quickgelu</code> postfix for the OpenCLIP pretrained weights. All OpenAI pretrained weights will always default to QuickGELU. One can also use the non <code>-quickgelu</code> model definitions with pretrained weights using QuickGELU but there will be an accuracy drop, for fine-tune that will likely vanish for longer runs.Future trained models will use nn.GELU.</p><h3>Loading models</h3><p>Models can be loaded with <code>open_clip.create_model_and_transforms</code>, as shown in the example below. The model name and corresponding <code>pretrained</code> keys are compatible with the outputs of <code>open_clip.list_pretrained()</code>.</p><p>The <code>pretrained</code> argument also accepts local paths, for example <code>/path/to/my/b32.pt</code>.You can also load checkpoints from huggingface this way. To do so, download the <code>open_clip_pytorch_model.bin</code> file (for example, <a href=\"https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/blob/main/open_clip_pytorch_model.bin\" rel=nofollow>https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/tree/main</a>), and use <code>pretrained=/path/to/open_clip_pytorch_model.bin</code>.</p><pre lang=python3><span class=c1># pretrained also accepts local paths</span><span class=n>model</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>preprocess</span> <span class=o>=</span> <span class=n>open_clip</span><span class=o>.</span><span class=n>create_model_and_transforms</span><span class=p>(</span><span class=s1>'ViT-B-32'</span><span class=p>,</span> <span class=n>pretrained</span><span class=o>=</span><span class=s1>'laion2b_s34b_b79k'</span><span class=p>)</span> </pre><h2>Fine-tuning on classification tasks</h2><p>This repository is focused on training CLIP models. To fine-tune a <em>trained</em> zero-shot model on a downstream classification task such as ImageNet, please see <a href=\"https://github.com/mlfoundations/wise-ft\" rel=nofollow>our other repository: WiSE-FT</a>. The <a href=\"https://github.com/mlfoundations/wise-ft\" rel=nofollow>WiSE-FT repository</a> contains code for our paper on <a href=\"https://arxiv.org/abs/2109.01903\" rel=nofollow>Robust Fine-tuning of Zero-shot Models</a>, in which we introduce a technique for fine-tuning zero-shot models while preserving robustness under distribution shift.</p><h2>Data</h2><p>To download datasets as webdataset, we recommend <a href=\"https://github.com/rom1504/img2dataset\" rel=nofollow>img2dataset</a>.</p><h3>Conceptual Captions</h3><p>See <a href=\"https://github.com/rom1504/img2dataset/blob/main/dataset_examples/cc3m.md\" rel=nofollow>cc3m img2dataset example</a>.</p><h3>YFCC and other datasets</h3><p>In addition to specifying the training data via CSV files as mentioned above, our codebase also supports <a href=\"https://github.com/webdataset/webdataset\" rel=nofollow>webdataset</a>, which is recommended for larger scale datasets. The expected format is a series of <code>.tar</code> files. Each of these <code>.tar</code> files should contain two files for each training example, one for the image and one for the corresponding text. Both files should have the same name but different extensions. For instance, <code>shard_001.tar</code> could contain files such as <code>abc.jpg</code> and <code>abc.txt</code>. You can learn more about <code>webdataset</code> at <a href=\"https://github.com/webdataset/webdataset\" rel=nofollow>https://github.com/webdataset/webdataset</a>. We use <code>.tar</code> files with 1,000 data points each, which we create using <a href=\"https://github.com/webdataset/tarp\" rel=nofollow>tarp</a>.</p><p>You can download the YFCC dataset from <a href=\"http://mmcommons.org/\" rel=nofollow>Multimedia Commons</a>.Similar to OpenAI, we used a subset of YFCC to reach the aforementioned accuracy numbers.The indices of images in this subset are in <a href=\"https://github.com/openai/CLIP/blob/main/data/yfcc100m.md\" rel=nofollow>OpenAI's CLIP repository</a>.</p><h2>Training CLIP</h2><h3>Install</h3><p>We advise you first create a virtual environment with:</p><pre><code>python3 -m venv .envsource .env/bin/activatepip install -U pip</code></pre><p>You can then install openclip for training with <code>pip install 'open_clip_torch[training]'</code>.</p><h4>Development</h4><p>If you want to make changes to contribute code, you can clone openclip then run <code>make install</code> in openclip folder (after creating a virtualenv)</p><p>Install pip PyTorch as per <a href=\"https://pytorch.org/get-started/locally/\" rel=nofollow>https://pytorch.org/get-started/locally/</a></p><p>You may run <code>make install-training</code> to install training deps</p><h4>Testing</h4><p>Test can be run with <code>make install-test</code> then <code>make test</code></p><p><code>python -m pytest -x -s -v tests -k \"training\"</code> to run a specific test</p><p>Running regression tests against a specific git revision or tag:</p><ol><li><p>Generate testing data</p><pre lang=sh>python<span class=w> </span>tests/util_test.py<span class=w> </span>--model<span class=w> </span>RN50<span class=w> </span>RN101<span class=w> </span>--save_model_list<span class=w> </span>models.txt<span class=w> </span>--git_revision<span class=w> </span>9d31b2ec4df6d8228f370ff20c8267ec6ba39383</pre><p><strong><em>WARNING</em>: This will invoke git and modify your working tree, but will reset it to the current state after data has been generated! <br>Don't modify your working tree while test data is being generated this way.</strong></p></li><li><p>Run regression tests</p><pre lang=sh><span class=nv>OPEN_CLIP_TEST_REG_MODELS</span><span class=o>=</span>models.txt<span class=w> </span>python<span class=w> </span>-m<span class=w> </span>pytest<span class=w> </span>-x<span class=w> </span>-s<span class=w> </span>-v<span class=w> </span>-m<span class=w> </span>regression_test</pre></li></ol><h3>Sample single-process running code:</h3><pre lang=bash>python<span class=w> </span>-m<span class=w> </span>training.main<span class=w> </span><span class=se>\\</span><span class=w>    </span>--save-frequency<span class=w> </span><span class=m>1</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--zeroshot-frequency<span class=w> </span><span class=m>1</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--report-to<span class=w> </span>tensorboard<span class=w> </span><span class=se>\\</span><span class=w>    </span>--train-data<span class=o>=</span><span class=s2>\"/path/to/train_data.csv\"</span><span class=w>  </span><span class=se>\\</span><span class=w>    </span>--val-data<span class=o>=</span><span class=s2>\"/path/to/validation_data.csv\"</span><span class=w>  </span><span class=se>\\</span><span class=w>    </span>--csv-img-key<span class=w> </span>filepath<span class=w> </span><span class=se>\\</span><span class=w>    </span>--csv-caption-key<span class=w> </span>title<span class=w> </span><span class=se>\\</span><span class=w>    </span>--imagenet-val<span class=o>=</span>/path/to/imagenet/root/val/<span class=w> </span><span class=se>\\</span><span class=w>    </span>--warmup<span class=w> </span><span class=m>10000</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--batch-size<span class=o>=</span><span class=m>128</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--lr<span class=o>=</span>1e-3<span class=w> </span><span class=se>\\</span><span class=w>    </span>--wd<span class=o>=</span><span class=m>0</span>.1<span class=w> </span><span class=se>\\</span><span class=w>    </span>--epochs<span class=o>=</span><span class=m>30</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--workers<span class=o>=</span><span class=m>8</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--model<span class=w> </span>RN50</pre><p>Note: <code>imagenet-val</code> is the path to the <em>validation</em> set of ImageNet for zero-shot evaluation, not the training set!You can remove this argument if you do not want to perform zero-shot evaluation on ImageNet throughout training. Note that the <code>val</code> folder should contain subfolders. If it does not, please use <a href=\"https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh\" rel=nofollow>this script</a>.</p><h3>Multi-GPU and Beyond</h3><p>This code has been battle tested up to 1024 A100s and offers a variety of solutionsfor distributed training. We include native support for SLURM clusters.</p><p>As the number of devices used to train increases, so does the space complexity ofthe the logit matrix. Using a na\u00efve all-gather scheme, space complexity will be<code>O(n^2)</code>. Instead, complexity may become effectively linear if the flags<code>--gather-with-grad</code> and <code>--local-loss</code> are used. This alteration results in one-to-onenumerical results as the na\u00efve method.</p><h4>Epochs</h4><p>For larger datasets (eg Laion2B), we recommend setting <code>--train-num-samples</code> to a lower value than the full epoch, for example <code>--train-num-samples 135646078</code> to 1/16 of an epoch in conjunction with <code>--dataset-resampled</code> to do sampling with replacement. This allows having frequent checkpoints to evaluate more often.</p><h4>Patch Dropout</h4><p><a href=\"https://arxiv.org/abs/2212.00794\" rel=nofollow>Recent research</a> has shown that one can dropout half to three-quarters of the visual tokens, leading to up to 2-3x training speeds without loss of accuracy.</p><p>You can set this on your visual transformer config with the key <code>patch_dropout</code>.</p><p>In the paper, they also finetuned without the patch dropout at the end. You can do this with the command-line argument <code>--force-patch-dropout 0.</code></p><h4>Multiple data sources</h4><p>OpenCLIP supports using multiple data sources, by separating different data paths with <code>::</code>.For instance, to train on CC12M and on LAION, one might use <code>--train-data \"/data/cc12m/cc12m-train-{0000..2175}.tar::/data/LAION-400M/{00000..41455}.tar\"</code>.Using <code>--dataset-resampled</code> is recommended for these cases.</p><p>By default, on expectation the amount of times the model will see a sample from each source is proportional to the size of the source.For instance, when training on one data source with size 400M and one with size 10M, samples from the first source are 40x more likely to be seen in expectation.</p><p>We also support different weighting of the data sources, by using the <code>--train-data-upsampling-factors</code> flag.For instance, using <code>--train-data-upsampling-factors=1::1</code> in the above scenario is equivalent to not using the flag, and <code>--train-data-upsampling-factors=1::2</code> is equivalent to upsampling the second data source twice.If you want to sample from data sources with the same frequency, the upsampling factors should be inversely proportional to the sizes of the data sources.For instance, if dataset <code>A</code> has 1000 samples and dataset <code>B</code> has 100 samples, you can use <code>--train-data-upsampling-factors=0.001::0.01</code> (or analogously, <code>--train-data-upsampling-factors=1::10</code>).</p><h4>Single-Node</h4><p>We make use of <code>torchrun</code> to launch distributed jobs. The following launches aa job on a node of 4 GPUs:</p><pre lang=bash><span class=nb>cd</span><span class=w> </span>open_clip/srctorchrun<span class=w> </span>--nproc_per_node<span class=w> </span><span class=m>4</span><span class=w> </span>-m<span class=w> </span>training.main<span class=w> </span><span class=se>\\</span><span class=w>    </span>--train-data<span class=w> </span><span class=s1>'/data/cc12m/cc12m-train-{0000..2175}.tar'</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--train-num-samples<span class=w> </span><span class=m>10968539</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--dataset-type<span class=w> </span>webdataset<span class=w> </span><span class=se>\\</span><span class=w>    </span>--batch-size<span class=w> </span><span class=m>320</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--precision<span class=w> </span>amp<span class=w> </span><span class=se>\\</span><span class=w>    </span>--workers<span class=w> </span><span class=m>4</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--imagenet-val<span class=w> </span>/data/imagenet/validation/</pre><h4>Multi-Node</h4><p>The same script above works, so long as users include information about the numberof nodes and host node.</p><pre lang=bash><span class=nb>cd</span><span class=w> </span>open_clip/srctorchrun<span class=w> </span>--nproc_per_node<span class=o>=</span><span class=m>4</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--rdzv_endpoint<span class=o>=</span><span class=nv>$HOSTE_NODE_ADDR</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>-m<span class=w> </span>training.main<span class=w> </span><span class=se>\\</span><span class=w>    </span>--train-data<span class=w> </span><span class=s1>'/data/cc12m/cc12m-train-{0000..2175}.tar'</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--train-num-samples<span class=w> </span><span class=m>10968539</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--dataset-type<span class=w> </span>webdataset<span class=w> </span><span class=se>\\</span><span class=w>    </span>--batch-size<span class=w> </span><span class=m>320</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--precision<span class=w> </span>amp<span class=w> </span><span class=se>\\</span><span class=w>    </span>--workers<span class=w> </span><span class=m>4</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--imagenet-val<span class=w> </span>/data/imagenet/validation/</pre><h4>SLURM</h4><p>This is likely the easiest solution to utilize. The following script was used totrain our largest models:</p><pre lang=bash><span class=ch>#!/bin/bash -x</span><span class=c1>#SBATCH --nodes=32</span><span class=c1>#SBATCH --gres=gpu:4</span><span class=c1>#SBATCH --ntasks-per-node=4</span><span class=c1>#SBATCH --cpus-per-task=6</span><span class=c1>#SBATCH --wait-all-nodes=1</span><span class=c1>#SBATCH --job-name=open_clip</span><span class=c1>#SBATCH --account=ACCOUNT_NAME</span><span class=c1>#SBATCH --partition PARTITION_NAME</span><span class=nb>eval</span><span class=w> </span><span class=s2>\"</span><span class=k>$(</span>/path/to/conda/bin/conda<span class=w> </span>shell.bash<span class=w> </span>hook<span class=k>)</span><span class=s2>\"</span><span class=w> </span><span class=c1># init conda</span>conda<span class=w> </span>activate<span class=w> </span>open_clip<span class=nb>export</span><span class=w> </span><span class=nv>CUDA_VISIBLE_DEVICES</span><span class=o>=</span><span class=m>0</span>,1,2,3<span class=nb>export</span><span class=w> </span><span class=nv>MASTER_PORT</span><span class=o>=</span><span class=m>12802</span><span class=nv>master_addr</span><span class=o>=</span><span class=k>$(</span>scontrol<span class=w> </span>show<span class=w> </span>hostnames<span class=w> </span><span class=s2>\"</span><span class=nv>$SLURM_JOB_NODELIST</span><span class=s2>\"</span><span class=w> </span><span class=p>|</span><span class=w> </span>head<span class=w> </span>-n<span class=w> </span><span class=m>1</span><span class=k>)</span><span class=nb>export</span><span class=w> </span><span class=nv>MASTER_ADDR</span><span class=o>=</span><span class=nv>$master_addr</span><span class=nb>cd</span><span class=w> </span>/shared/open_clip<span class=nb>export</span><span class=w> </span><span class=nv>PYTHONPATH</span><span class=o>=</span><span class=s2>\"</span><span class=nv>$PYTHONPATH</span><span class=s2>:</span><span class=nv>$PWD</span><span class=s2>/src\"</span>srun<span class=w> </span>--cpu_bind<span class=o>=</span>v<span class=w> </span>--accel-bind<span class=o>=</span>gn<span class=w> </span>python<span class=w> </span>-u<span class=w> </span>src/training/main.py<span class=w> </span><span class=se>\\</span><span class=w>    </span>--save-frequency<span class=w> </span><span class=m>1</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--report-to<span class=w> </span>tensorboard<span class=w> </span><span class=se>\\</span><span class=w>    </span>--train-data<span class=o>=</span><span class=s2>\"/data/LAION-400M/{00000..41455}.tar\"</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--warmup<span class=w> </span><span class=m>2000</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--batch-size<span class=o>=</span><span class=m>256</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--epochs<span class=o>=</span><span class=m>32</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--workers<span class=o>=</span><span class=m>8</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--model<span class=w> </span>ViT-B-32<span class=w> </span><span class=se>\\</span><span class=w>    </span>--name<span class=w> </span><span class=s2>\"ViT-B-32-Vanilla\"</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--seed<span class=w> </span><span class=m>0</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--local-loss<span class=w> </span><span class=se>\\</span><span class=w>    </span>--gather-with-grad</pre><h3>Resuming from a checkpoint:</h3><pre lang=bash>python<span class=w> </span>-m<span class=w> </span>training.main<span class=w> </span><span class=se>\\</span><span class=w>    </span>--train-data<span class=o>=</span><span class=s2>\"/path/to/train_data.csv\"</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--val-data<span class=o>=</span><span class=s2>\"/path/to/validation_data.csv\"</span><span class=w>  </span><span class=se>\\</span><span class=w>    </span>--resume<span class=w> </span>/path/to/checkpoints/epoch_K.pt</pre><h3>Training CoCa:</h3><p>Training <a href=\"https://arxiv.org/abs/2205.01917\" rel=nofollow>CoCa</a> models is enabled through specifying a CoCa config using the <code>--model</code> parameter of the training script. Currently available configs are \"coca_base\", \"coca_ViT-B-32\", and \"coca_roberta-ViT-B-32\" (which uses RoBERTa as the text encoder). CoCa configs are different from CLIP configs because they have an additional \"multimodal_cfg\" component which specifies parameters for the multimodal text decoder. Here's an example from the coca_ViT-B-32 config:</p><pre lang=json><span class=nt>\"multimodal_cfg\"</span><span class=p>:</span><span class=w> </span><span class=p>{</span><span class=w></span><span class=nt>\"context_length\"</span><span class=p>:</span><span class=w> </span><span class=mi>76</span><span class=p>,</span><span class=w></span><span class=nt>\"vocab_size\"</span><span class=p>:</span><span class=w> </span><span class=mi>49408</span><span class=p>,</span><span class=w></span><span class=nt>\"width\"</span><span class=p>:</span><span class=w> </span><span class=mi>512</span><span class=p>,</span><span class=w></span><span class=nt>\"heads\"</span><span class=p>:</span><span class=w> </span><span class=mi>8</span><span class=p>,</span><span class=w></span><span class=nt>\"layers\"</span><span class=p>:</span><span class=w> </span><span class=mi>12</span><span class=p>,</span><span class=w></span><span class=nt>\"latent_dim\"</span><span class=p>:</span><span class=w> </span><span class=mi>512</span><span class=p>,</span><span class=w></span><span class=nt>\"attn_pooler_heads\"</span><span class=p>:</span><span class=w> </span><span class=mi>8</span><span class=p>}</span></pre><p>Credit to <a href=\"https://github.com/lucidrains\" rel=nofollow>lucidrains</a> for <a href=\"https://github.com/lucidrains/CoCa-pytorch\" rel=nofollow>initial code</a>, <a href=\"https://github.com/gpucce\" rel=nofollow>gpucce</a> for adapting the code to open_clip, and <a href=\"https://github.com/iejMac\" rel=nofollow>iejMac</a> for training the models.</p><h3>Generating text with CoCa</h3><pre lang=python3><span class=kn>import</span> <span class=nn>open_clip</span><span class=kn>import</span> <span class=nn>torch</span><span class=kn>from</span> <span class=nn>PIL</span> <span class=kn>import</span> <span class=n>Image</span><span class=n>model</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>transform</span> <span class=o>=</span> <span class=n>open_clip</span><span class=o>.</span><span class=n>create_model_and_transforms</span><span class=p>(</span>  <span class=n>model_name</span><span class=o>=</span><span class=s2>\"coca_ViT-L-14\"</span><span class=p>,</span>  <span class=n>pretrained</span><span class=o>=</span><span class=s2>\"mscoco_finetuned_laion2B-s13B-b90k\"</span><span class=p>)</span><span class=n>im</span> <span class=o>=</span> <span class=n>Image</span><span class=o>.</span><span class=n>open</span><span class=p>(</span><span class=s2>\"cat.jpg\"</span><span class=p>)</span><span class=o>.</span><span class=n>convert</span><span class=p>(</span><span class=s2>\"RGB\"</span><span class=p>)</span><span class=n>im</span> <span class=o>=</span> <span class=n>transform</span><span class=p>(</span><span class=n>im</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>(),</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>amp</span><span class=o>.</span><span class=n>autocast</span><span class=p>():</span>  <span class=n>generated</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>im</span><span class=p>)</span><span class=nb>print</span><span class=p>(</span><span class=n>open_clip</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>generated</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s2>\"&lt;end_of_text&gt;\"</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s2>\"&lt;start_of_text&gt;\"</span><span class=p>,</span> <span class=s2>\"\"</span><span class=p>))</span></pre><p>See also this <a href=\"https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb\" rel=nofollow>[Coca Colab]</a></p><h3>Fine Tuning CoCa</h3><p>To fine-tune coca on mscoco, first create the dataset, one way is using a csvdataset and perhaps the simplest way to do it is using <a href=\"https://github.com/LAION-AI/CLIP_benchmark\" rel=nofollow>CLIP_benchmark</a> which in turn uses <a href=\"https://github.com/cocodataset/cocoapi\" rel=nofollow>pycocotools</a> (that can be used also by itself).</p><pre lang=python3><span class=kn>from</span> <span class=nn>clip_benchmark.datasets.builder</span> <span class=kn>import</span> <span class=n>build_dataset</span><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span><span class=kn>import</span> <span class=nn>os</span><span class=n>root_path</span> <span class=o>=</span> <span class=s2>\"path/to/data/dir\"</span> <span class=c1># set this to smth meaningful</span><span class=n>ds</span> <span class=o>=</span> <span class=n>build_dataset</span><span class=p>(</span><span class=s2>\"mscoco_captions\"</span><span class=p>,</span> <span class=n>root</span><span class=o>=</span><span class=n>root_path</span><span class=p>,</span> <span class=n>split</span><span class=o>=</span><span class=s2>\"train\"</span><span class=p>)</span> <span class=c1># this downloads the dataset if it is not there already</span><span class=n>coco</span> <span class=o>=</span> <span class=n>ds</span><span class=o>.</span><span class=n>coco</span><span class=n>imgs</span> <span class=o>=</span> <span class=n>coco</span><span class=o>.</span><span class=n>loadImgs</span><span class=p>(</span><span class=n>coco</span><span class=o>.</span><span class=n>getImgIds</span><span class=p>())</span><span class=n>future_df</span> <span class=o>=</span> <span class=p>{</span><span class=s2>\"filepath\"</span><span class=p>:[],</span> <span class=s2>\"title\"</span><span class=p>:[]}</span><span class=k>for</span> <span class=n>img</span> <span class=ow>in</span> <span class=n>imgs</span><span class=p>:</span>    <span class=n>caps</span> <span class=o>=</span> <span class=n>coco</span><span class=o>.</span><span class=n>imgToAnns</span><span class=p>[</span><span class=n>img</span><span class=p>[</span><span class=s2>\"id\"</span><span class=p>]]</span>    <span class=k>for</span> <span class=n>cap</span> <span class=ow>in</span> <span class=n>caps</span><span class=p>:</span>        <span class=n>future_df</span><span class=p>[</span><span class=s2>\"filepath\"</span><span class=p>]</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>img</span><span class=p>[</span><span class=s2>\"file_name\"</span><span class=p>])</span>        <span class=n>future_df</span><span class=p>[</span><span class=s2>\"title\"</span><span class=p>]</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>cap</span><span class=p>[</span><span class=s2>\"caption\"</span><span class=p>])</span><span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=o>.</span><span class=n>from_dict</span><span class=p>(</span><span class=n>future_df</span><span class=p>)</span><span class=o>.</span><span class=n>to_csv</span><span class=p>(</span>  <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>root_path</span><span class=p>,</span> <span class=s2>\"train2014.csv\"</span><span class=p>),</span> <span class=n>index</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>sep</span><span class=o>=</span><span class=s2>\"</span><span class=se>\\t</span><span class=s2>\"</span><span class=p>)</span></pre><p>This should create a csv dataset that one can use to fine-tune coca with open_clip</p><pre lang=bash>python<span class=w> </span>-m<span class=w> </span>training.main<span class=w> </span><span class=se>\\</span><span class=w>    </span>--dataset-type<span class=w> </span><span class=s2>\"csv\"</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--train-data<span class=w> </span><span class=s2>\"path/to/data/dir/train2014.csv\"</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--warmup<span class=w> </span><span class=m>1000</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--batch-size<span class=w> </span><span class=m>128</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--lr<span class=w> </span>1e-5<span class=w> </span><span class=se>\\</span><span class=w>    </span>--wd<span class=w> </span><span class=m>0</span>.1<span class=w> </span><span class=se>\\</span><span class=w>    </span>--epochs<span class=w> </span><span class=m>1</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--workers<span class=w> </span><span class=m>3</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--model<span class=w> </span><span class=s2>\"coca_ViT-L-14\"</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--report-to<span class=w> </span><span class=s2>\"wandb\"</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--coca-contrastive-loss-weight<span class=w> </span><span class=m>0</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--coca-caption-loss-weight<span class=w> </span><span class=m>1</span><span class=w> </span><span class=se>\\</span><span class=w>    </span>--log-every-n-steps<span class=w> </span><span class=m>100</span></pre><p>This is a general setting, open_clip has very parameters that can be set, <code>python -m training.main --help</code> should show them. The only relevant change compared to pre-training are the two arguments</p><pre lang=bash>--coca-contrastive-loss-weight<span class=w> </span><span class=m>0</span>--coca-caption-loss-weight<span class=w> </span><span class=m>1</span></pre><p>which make the model only train the generative side.</p><h3>Training with pre-trained language models as text encoder:</h3><p>If you wish to use different language models as the text encoder for CLIP you can do so by using one of the Hugging Face model configs in <code>src/open_clip/model_configs</code> and passing in it's tokenizer as the <code>--model</code> and <code>--hf-tokenizer-name</code> parameters respectively. Currently we only support RoBERTa (\"test-roberta\" config), however adding new models should be trivial. You can also determine how many layers, from the end, to leave unfrozen with the <code>--lock-text-unlocked-layers</code> parameter. Here's an example command to train CLIP with the RoBERTa LM that has it's last 10 layers unfrozen:</p><pre lang=bash>python<span class=w> </span>-m<span class=w> </span>training.main<span class=w> </span><span class=se>\\</span><span class=w>         </span>--train-data<span class=o>=</span><span class=s2>\"pipe:aws s3 cp s3://s-mas/cc3m/{00000..00329}.tar -\"</span><span class=w> </span><span class=se>\\</span><span class=w>         </span>--train-num-samples<span class=w> </span><span class=m>3000000</span><span class=w> </span><span class=se>\\</span><span class=w>         </span>--val-data<span class=o>=</span><span class=s2>\"pipe:aws s3 cp s3://s-mas/cc3m/{00330..00331}.tar -\"</span><span class=w> </span><span class=se>\\</span><span class=w>         </span>--val-num-samples<span class=w> </span><span class=m>10000</span><span class=w> </span><span class=se>\\</span><span class=w>         </span>--dataset-type<span class=w> </span>webdataset<span class=w> </span><span class=se>\\</span><span class=w>         </span>--batch-size<span class=w> </span><span class=m>256</span><span class=w> </span><span class=se>\\</span><span class=w>         </span>--warmup<span class=w> </span><span class=m>2000</span><span class=w> </span><span class=se>\\</span><span class=w>         </span>--epochs<span class=w> </span><span class=m>10</span><span class=w> </span><span class=se>\\</span><span class=w>         </span>--lr<span class=w> </span>5e-4<span class=w> </span><span class=se>\\</span><span class=w>         </span>--precision<span class=w> </span>amp<span class=w> </span><span class=se>\\</span><span class=w>         </span>--workers<span class=w> </span><span class=m>6</span><span class=w> </span><span class=se>\\</span><span class=w>         </span>--model<span class=w> </span><span class=s2>\"roberta-ViT-B-32\"</span><span class=w> </span><span class=se>\\</span><span class=w>         </span>--lock-text<span class=w> </span><span class=se>\\</span><span class=w>         </span>--lock-text-unlocked-layers<span class=w> </span><span class=m>10</span><span class=w> </span><span class=se>\\</span><span class=w>         </span>--name<span class=w> </span><span class=s2>\"10_unfrozen\"</span><span class=w> </span><span class=se>\\</span><span class=w>         </span>--report-to<span class=w> </span><span class=s2>\"tensorboard\"</span><span class=w> </span><span class=se>\\</span></pre><h3>Loss Curves</h3><p>When run on a machine with 8 GPUs the command should produce the following training curve for Conceptual Captions:</p><p><img src=\"https://pypi-camo.freetls.fastly.net/c5ff190fdee5793d0e35cf8290f299964b3da275/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6d6c666f756e646174696f6e732f6f70656e5f636c69702f6d61696e2f646f63732f636c69705f7a65726f73686f742e706e67\" alt=\"CLIP zero shot training curve\"></p><p>More detailed curves for Conceptual Captions are given at <a href=\"/docs/clip_conceptual_captions.md\" rel=nofollow>/docs/clip_conceptual_captions.md</a>.</p><p>When training a RN50 on YFCC the same hyperparameters as above are used, with the exception of <code>lr=5e-4</code> and <code>epochs=32</code>.</p><p>Note that to use another model, like <code>ViT-B/32</code> or <code>RN50x4</code> or <code>RN50x16</code> or <code>ViT-B/16</code>, specify with <code>--model RN50x4</code>.</p><h3>Logging</h3><p>For tensorboard logging, run:</p><pre lang=bash>tensorboard<span class=w> </span>--logdir<span class=o>=</span>logs/tensorboard/<span class=w> </span>--port<span class=o>=</span><span class=m>7777</span></pre><p>For wandb logging, we recommend looking at the <code>step</code> variable instead of <code>Step</code>, since the later was not properly set in earlier versions of this codebase.For older runs with models trained before <a href=\"https://github.com/mlfoundations/open_clip/pull/613\" rel=nofollow>https://github.com/mlfoundations/open_clip/pull/613</a>, the <code>Step</code> variable should be ignored.For newer runs, after that PR, the two variables are the same.</p><h2>Evaluation / Zero-Shot</h2><p>We recommend <a href=\"https://github.com/LAION-AI/CLIP_benchmark#how-to-use\" rel=nofollow>https://github.com/LAION-AI/CLIP_benchmark#how-to-use</a> for systematic evaluation on 40 datasets.</p><h3>Evaluating local checkpoint:</h3><pre lang=bash>python<span class=w> </span>-m<span class=w> </span>training.main<span class=w> </span><span class=se>\\</span><span class=w>    </span>--val-data<span class=o>=</span><span class=s2>\"/path/to/validation_data.csv\"</span><span class=w>  </span><span class=se>\\</span><span class=w>    </span>--model<span class=w> </span>RN101<span class=w> </span><span class=se>\\</span><span class=w>    </span>--pretrained<span class=w> </span>/path/to/checkpoints/epoch_K.pt</pre><h3>Evaluating hosted pretrained checkpoint on ImageNet zero-shot prediction:</h3><pre lang=bash>python<span class=w> </span>-m<span class=w> </span>training.main<span class=w> </span><span class=se>\\</span><span class=w>    </span>--imagenet-val<span class=w> </span>/path/to/imagenet/validation<span class=w> </span><span class=se>\\</span><span class=w>    </span>--model<span class=w> </span>ViT-B-32-quickgelu<span class=w> </span><span class=se>\\</span><span class=w>    </span>--pretrained<span class=w> </span>laion400m_e32</pre><h3>Model distillation</h3><p>You can distill from a pre-trained by using <code>--distill-model</code> and <code>--distill-pretrained</code> to specify the model you'd like to distill from.For instance, to distill from OpenAI ViT-L/14 use <code>--distill-model ViT-L-14 --distill-pretrained openai</code>.</p><h3>Gradient accumulation</h3><p>To simulate larger batches use <code>--accum-freq k</code>. If per gpu batch size, <code>--batch-size</code>, is <code>m</code>, then the effective batch size will be <code>k * m * num_gpus</code>.</p><p>When increasing <code>--accum-freq</code> from its default of 1, samples/s will remain approximately constant (batch size will double, as will time-per-batch). It is recommended to use other features to reduce batch size such as <code>--grad-checkpointing --local-loss --gather-with-grad</code> before increasing <code>--accum-freq</code>. <code>--accum-freq</code> can be used in addition to these features.</p><p>Instead of 1 forward pass per example, there are now 2 forward passes per-example. However, the first is done with <code>torch.no_grad</code>.</p><p>There is some additional GPU memory required --- the features and data from all <code>m</code> batches are stored in memory.</p><p>There are also <code>m</code> loss computations instead of the usual 1.</p><p>For more information see Cui et al. (<a href=\"https://arxiv.org/abs/2112.09331\" rel=nofollow>https://arxiv.org/abs/2112.09331</a>) or Pham et al. (<a href=\"https://arxiv.org/abs/2111.10050\" rel=nofollow>https://arxiv.org/abs/2111.10050</a>).</p><h3>Int8 Support</h3><p>We have beta support for int8 training and inference.You can enable int8 training with <code>--use-bnb-linear SwitchBackLinearGlobal</code> or <code>--use-bnb-linear SwitchBackLinearGlobalMemEfficient</code>.Please see the bitsandbytes library for definitions for these layers.For CLIP VIT-Huge this should currently correspond to a 10% training speedup with no accuracy loss.More speedups comin when the attention layer is refactored so that linear layers man be replaced there, too.</p><p>See the tutorial <a href=\"https://github.com/mlfoundations/open_clip/blob/main/tutorials/int8_tutorial.ipynb\" rel=nofollow>https://github.com/mlfoundations/open_clip/blob/main/tutorials/int8_tutorial.ipynb</a> or <a href=\"https://arxiv.org/abs/2304.13013\" rel=nofollow>paper</a>.</p><h3>Support for remote loading/training</h3><p>It is always possible to resume directly from a remote file, e.g., a file in an s3 bucket. Just set <code>--resume s3://&lt;path-to-checkpoint&gt; </code>.This will work with any filesystem supported by <code>fsspec</code>.</p><p>It is also possible to train <code>open_clip</code> models while continuously backing up to s3. This can help to avoid slow local file systems.</p><p>Say that your node has a local ssd <code>/scratch</code>, an s3 bucket <code>s3://&lt;path-to-bucket&gt;</code>.</p><p>In that case, set <code>--logs /scratch</code> and <code>--remote-sync s3://&lt;path-to-bucket&gt;</code>. Then, a background process will sync <code>/scratch/&lt;run-name&gt;</code> to <code>s3://&lt;path-to-bucket&gt;/&lt;run-name&gt;</code>. After syncing, the background process will sleep for <code>--remote-sync-frequency</code> seconds, which defaults to 5 minutes.</p><p>There is also experimental support for syncing to other remote file systems, not just s3. To do so, specify <code>--remote-sync-protocol fsspec</code>. However, this is currently very slow and not recommended.</p><p>Also, to optionally avoid saving too many checkpoints locally when using these features, you can use <code>--delete-previous-checkpoint</code> which deletes the previous checkpoint after saving a new one.</p><p>Note: if you are using this feature with <code>--resume latest</code>, there are a few warnings. First, use with <code>--save-most-recent</code> is not supported. Second, only <code>s3</code> is supported. Finally, since the sync happens in the background, it is possible that the most recent checkpoint may not be finished syncing to the remote.</p><h3>Pushing Models to Hugging Face Hub</h3><p>The module <code>open_clip.push_to_hf_hub</code> includes helpers for pushing models /w weights and config to the HF Hub.</p><p>The tool can be run from command line, ex:<code>python -m open_clip.push_to_hf_hub --model convnext_large_d_320 --pretrained /train/checkpoints/epoch_12.pt --repo-id laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft</code></p><h2>Acknowledgments</h2><p>We gratefully acknowledge the Gauss Centre for Supercomputing e.V. (<a href=\"http://www.gauss-centre.eu\" rel=nofollow>www.gauss-centre.eu</a>) for funding this part of work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at J\u00fclich Supercomputing Centre (JSC).</p><h2>The Team</h2><p>Current development of this repository is led by <a href=\"https://rwightman.com/\" rel=nofollow>Ross Wightman</a>, <a href=\"https://github.com/rom1504\" rel=nofollow>Romain Beaumont</a>, <a href=\"http://cadegordon.io/\" rel=nofollow>Cade Gordon</a>, and <a href=\"http://vaishaal.com/\" rel=nofollow>Vaishaal Shankar</a>.</p><p>The original version of this repository is from a group of researchers at UW, Google, Stanford, Amazon, Columbia, and Berkeley.</p><p><a href=\"http://gabrielilharco.com/\" rel=nofollow>Gabriel Ilharco*</a>, <a href=\"https://mitchellnw.github.io/\" rel=nofollow>Mitchell Wortsman*</a>, <a href=\"https://nicholas.carlini.com/\" rel=nofollow>Nicholas Carlini</a>, <a href=\"https://www.rohantaori.com/\" rel=nofollow>Rohan Taori</a>, <a href=\"http://www.achaldave.com/\" rel=nofollow>Achal Dave</a>, <a href=\"http://vaishaal.com/\" rel=nofollow>Vaishaal Shankar</a>, <a href=\"https://people.eecs.berkeley.edu/~miller_john/\" rel=nofollow>John Miller</a>, <a href=\"https://hsnamkoong.github.io/\" rel=nofollow>Hongseok Namkoong</a>, <a href=\"https://homes.cs.washington.edu/~hannaneh/\" rel=nofollow>Hannaneh Hajishirzi</a>, <a href=\"https://homes.cs.washington.edu/~ali/\" rel=nofollow>Ali Farhadi</a>, <a href=\"https://people.csail.mit.edu/ludwigs/\" rel=nofollow>Ludwig Schmidt</a></p><p>Special thanks to <a href=\"https://jongwook.kim/\" rel=nofollow>Jong Wook Kim</a> and <a href=\"https://github.com/Newmu\" rel=nofollow>Alec Radford</a> for help with reproducing CLIP!</p><h2>Citing</h2><p>If you found this repository useful, please consider citing:</p><pre lang=bibtex><span class=nc>@software</span><span class=p>{</span><span class=nl>ilharco_gabriel_2021_5143773</span><span class=p>,</span><span class=w>  </span><span class=na>author</span><span class=w>       </span><span class=p>=</span><span class=w> </span><span class=s>{Ilharco, Gabriel and</span><span class=s>                  Wortsman, Mitchell and</span><span class=s>                  Wightman, Ross and</span><span class=s>                  Gordon, Cade and</span><span class=s>                  Carlini, Nicholas and</span><span class=s>                  Taori, Rohan and</span><span class=s>                  Dave, Achal and</span><span class=s>                  Shankar, Vaishaal and</span><span class=s>                  Namkoong, Hongseok and</span><span class=s>                  Miller, John and</span><span class=s>                  Hajishirzi, Hannaneh and</span><span class=s>                  Farhadi, Ali and</span><span class=s>                  Schmidt, Ludwig}</span><span class=p>,</span><span class=w>  </span><span class=na>title</span><span class=w>        </span><span class=p>=</span><span class=w> </span><span class=s>{OpenCLIP}</span><span class=p>,</span><span class=w>  </span><span class=na>month</span><span class=w>        </span><span class=p>=</span><span class=w> </span><span class=nv>jul</span><span class=p>,</span><span class=w>  </span><span class=na>year</span><span class=w>         </span><span class=p>=</span><span class=w> </span><span class=m>2021</span><span class=p>,</span><span class=w>  </span><span class=na>note</span><span class=w>         </span><span class=p>=</span><span class=w> </span><span class=s>{If you use this software, please cite it as below.}</span><span class=p>,</span><span class=w>  </span><span class=na>publisher</span><span class=w>    </span><span class=p>=</span><span class=w> </span><span class=s>{Zenodo}</span><span class=p>,</span><span class=w>  </span><span class=na>version</span><span class=w>      </span><span class=p>=</span><span class=w> </span><span class=s>{0.1}</span><span class=p>,</span><span class=w>  </span><span class=na>doi</span><span class=w>          </span><span class=p>=</span><span class=w> </span><span class=s>{10.5281/zenodo.5143773}</span><span class=p>,</span><span class=w>  </span><span class=na>url</span><span class=w>          </span><span class=p>=</span><span class=w> </span><span class=s>{https://doi.org/10.5281/zenodo.5143773}</span><span class=p>}</span></pre><pre lang=bibtex><span class=nc>@inproceedings</span><span class=p>{</span><span class=nl>cherti2023reproducible</span><span class=p>,</span><span class=w>  </span><span class=na>title</span><span class=p>=</span><span class=s>{Reproducible scaling laws for contrastive language-image learning}</span><span class=p>,</span><span class=w>  </span><span class=na>author</span><span class=p>=</span><span class=s>{Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia}</span><span class=p>,</span><span class=w>  </span><span class=na>booktitle</span><span class=p>=</span><span class=s>{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}</span><span class=p>,</span><span class=w>  </span><span class=na>pages</span><span class=p>=</span><span class=s>{2818--2829}</span><span class=p>,</span><span class=w>  </span><span class=na>year</span><span class=p>=</span><span class=s>{2023}</span><span class=p>}</span></pre><pre lang=bibtex><span class=nc>@inproceedings</span><span class=p>{</span><span class=nl>Radford2021LearningTV</span><span class=p>,</span><span class=w>  </span><span class=na>title</span><span class=p>=</span><span class=s>{Learning Transferable Visual Models From Natural Language Supervision}</span><span class=p>,</span><span class=w>  </span><span class=na>author</span><span class=p>=</span><span class=s>{Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever}</span><span class=p>,</span><span class=w>  </span><span class=na>booktitle</span><span class=p>=</span><span class=s>{ICML}</span><span class=p>,</span><span class=w>  </span><span class=na>year</span><span class=p>=</span><span class=s>{2021}</span><span class=p>}</span></pre><pre lang=bibtex><span class=nc>@inproceedings</span><span class=p>{</span><span class=nl>schuhmann2022laionb</span><span class=p>,</span><span class=w>  </span><span class=na>title</span><span class=p>=</span><span class=s>{{LAION}-5B: An open large-scale dataset for training next generation image-text models}</span><span class=p>,</span><span class=w>  </span><span class=na>author</span><span class=p>=</span><span class=s>{Christoph Schuhmann and</span><span class=s>          Romain Beaumont and</span><span class=s>          Richard Vencu and</span><span class=s>          Cade W Gordon and</span><span class=s>          Ross Wightman and</span><span class=s>          Mehdi Cherti and</span><span class=s>          Theo Coombes and</span><span class=s>          Aarush Katta and</span><span class=s>          Clayton Mullis and</span><span class=s>          Mitchell Wortsman and</span><span class=s>          Patrick Schramowski and</span><span class=s>          Srivatsa R Kundurthy and</span><span class=s>          Katherine Crowson and</span><span class=s>          Ludwig Schmidt and</span><span class=s>          Robert Kaczmarczyk and</span><span class=s>          Jenia Jitsev}</span><span class=p>,</span><span class=w>  </span><span class=na>booktitle</span><span class=p>=</span><span class=s>{Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track}</span><span class=p>,</span><span class=w>  </span><span class=na>year</span><span class=p>=</span><span class=s>{2022}</span><span class=p>,</span><span class=w>  </span><span class=na>url</span><span class=p>=</span><span class=s>{https://openreview.net/forum?id=M3Y74vmsMcY}</span><span class=p>}</span></pre><p><a href=\"https://zenodo.org/badge/latestdoi/390536799\" rel=nofollow><img src=\"https://pypi-camo.freetls.fastly.net/48a06f5e7c6433a727afed707d2f0674cb616fd1/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3339303533363739392e737667\" alt=DOI></a></p>          </div>        </div>        <div id=\"data\" data-project-tabs-target=\"content\" class=\"vertical-tabs__content\" role=\"tabpanel\" aria-labelledby=\"mobile-data-tab\" tabindex=\"-1\">          <h2 class=\"page-title\">Project details</h2><div class=\"sidebar-section\">  <h3 class=\"sidebar-section__title\">Project links</h3>  <ul class=\"vertical-tabs__list\">    <li>      <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\" href=\"https://github.com/mlfoundations/open_clip\" rel=\"nofollow\">        <i class=\"fas fa-home\" aria-hidden=\"true\"></i>Homepage      </a>    </li>  </ul></div><div class=\"sidebar-section\">  <h3 class=\"sidebar-section__title\">Statistics</h3>  <div class=\"hidden github-repo-info\" data-controller=\"github-repo-info\">GitHub statistics:    <ul class=\"vertical-tabs__list\">      <li>        <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\"           data-github-repo-info-target=\"stargazersUrl\" rel=\"noopener\">          <i class=\"fa fa-star\" aria-hidden=\"true\"></i>          <strong>Stars:</strong>          <span data-github-repo-info-target=\"stargazersCount\"></span>        </a>      </li>      <li>        <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\"           data-github-repo-info-target=\"forksUrl\" rel=\"noopener\">          <i class=\"fa fa-code-branch\" aria-hidden=\"true\"></i>          <strong>Forks:</strong>          <span data-github-repo-info-target=\"forksCount\"></span>        </a>      </li>      <li>        <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\"           data-github-repo-info-target=\"openIssuesUrl\" rel=\"noopener\">          <i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>          <strong>Open issues:</strong>          <span data-github-repo-info-target=\"openIssuesCount\"></span>        </a>      </li>      <li>        <a class=\"vertical-tabs__tab vertical-tabs__tab--with-icon vertical-tabs__tab--condensed\"           data-github-repo-info-target=\"openPRsUrl\" rel=\"noopener\">          <i class=\"fa fa-code-pull-request\" aria-hidden=\"true\"></i>          <strong>Open PRs:</strong>          <span data-github-repo-info-target=\"openPRsCount\"></span>        </a>      </li>    </ul>  </div>  <p>View statistics for this project via <a href=\"https://libraries.io/pypi/open-clip-torch\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Libraries.io</a>, or by using <a href=\"https://packaging.python.org/guides/analyzing-pypi-package-downloads/\" target=\"_blank\" rel=\"noopener\">our public dataset on Google BigQuery</a>  </p></div><div class=\"sidebar-section\">  <h3 class=\"sidebar-section__title\">Meta</h3>  <p><strong>License:</strong> Apache Software License</p>  <p class=\"tags\">    <i class=\"fa fa-tags\" aria-hidden=\"true\"></i>    <span class=\"sr-only\">Tags</span>    <span class=\"package-keyword\">      CLIP,    </span>    <span class=\"package-keyword\">      pretrained    </span>  </p>  <p>    <strong>Requires:</strong> Python &gt;=3.7  </p></div><div class=\"sidebar-section\">    <h3 class=\"sidebar-section__title\">Maintainers</h3>      <span class=\"sidebar-section__maintainer\">        <a href=\"/user/ludwigschmidt/\" aria-label=\"ludwigschmidt\">          <span class=\"sidebar-section__user-gravatar\">            <img src=\"https://pypi-camo.freetls.fastly.net/4132ab97806f798fb703390d5acad9c2f3f686ec/68747470733a2f2f7365637572652e67726176617461722e636f6d2f6176617461722f63643466306232323065613336653133666637313135376366666539643964313f73697a653d3530\" height=\"50\" width=\"50\" alt=\"Avatar for ludwigschmidt from gravatar.com\" title=\"Avatar for ludwigschmidt from gravatar.com\">          </span>          <span class=\"sidebar-section__user-gravatar-text\">            ludwigschmidt          </span>        </a>      </span>      <span class=\"sidebar-section__maintainer\">        <a href=\"/user/rwightman/\" aria-label=\"rwightman\">          <span class=\"sidebar-section__user-gravatar\">            <img src=\"https://pypi-camo.freetls.fastly.net/ef32c24299ade8e74661c3911e6e0b65982ae14d/68747470733a2f2f7365637572652e67726176617461722e636f6d2f6176617461722f31393738633263343466636234383061613666306666383839623733656561363f73697a653d3530\" height=\"50\" width=\"50\" alt=\"Avatar for rwightman from gravatar.com\" title=\"Avatar for rwightman from gravatar.com\">          </span>          <span class=\"sidebar-section__user-gravatar-text\">            rwightman          </span>        </a>      </span></div><div class=\"sidebar-section\">  <h3 class=\"sidebar-section__title\">Classifiers</h3>  <ul class=\"sidebar-section__classifiers\">    <li>      <strong>Development Status</strong>      <ul>        <li>          <a href=\"/search/?c=Development+Status+%3A%3A+3+-+Alpha\">            3 - Alpha          </a>        </li>      </ul>    </li>    <li>      <strong>Intended Audience</strong>      <ul>        <li>          <a href=\"/search/?c=Intended+Audience+%3A%3A+Education\">            Education          </a>        </li>        <li>          <a href=\"/search/?c=Intended+Audience+%3A%3A+Science%2FResearch\">            Science/Research          </a>        </li>      </ul>    </li>    <li>      <strong>License</strong>      <ul>        <li>          <a href=\"/search/?c=License+%3A%3A+OSI+Approved+%3A%3A+Apache+Software+License\">            OSI Approved :: Apache Software License          </a>        </li>      </ul>    </li>    <li>      <strong>Programming Language</strong>      <ul>        <li>          <a href=\"/search/?c=Programming+Language+%3A%3A+Python+%3A%3A+3.7\">            Python :: 3.7          </a>        </li>        <li>          <a href=\"/search/?c=Programming+Language+%3A%3A+Python+%3A%3A+3.8\">            Python :: 3.8          </a>        </li>        <li>          <a href=\"/search/?c=Programming+Language+%3A%3A+Python+%3A%3A+3.9\">            Python :: 3.9          </a>        </li>        <li>          <a href=\"/search/?c=Programming+Language+%3A%3A+Python+%3A%3A+3.10\">            Python :: 3.10          </a>        </li>      </ul>    </li>    <li>      <strong>Topic</strong>      <ul>        <li>          <a href=\"/search/?c=Topic+%3A%3A+Scientific%2FEngineering\">            Scientific/Engineering          </a>        </li>        <li>          <a href=\"/search/?c=Topic+%3A%3A+Scientific%2FEngineering+%3A%3A+Artificial+Intelligence\">            Scientific/Engineering :: Artificial Intelligence          </a>        </li>        <li>          <a href=\"/search/?c=Topic+%3A%3A+Software+Development\">            Software Development          </a>        </li>        <li>          <a href=\"/search/?c=Topic+%3A%3A+Software+Development+%3A%3A+Libraries\">            Software Development :: Libraries          </a>        </li>        <li>          <a href=\"/search/?c=Topic+%3A%3A+Software+Development+%3A%3A+Libraries+%3A%3A+Python+Modules\">            Software Development :: Libraries :: Python Modules          </a>        </li>      </ul>    </li>  </ul></div>          <br>        </div>        <div id=\"history\" data-project-tabs-target=\"content\" class=\"vertical-tabs__content\" role=\"tabpanel\" aria-labelledby=\"history-tab mobile-history-tab\" tabindex=\"-1\">          <h2 class=\"page-title split-layout\">            <span>Release history</span>            <span class=\"reset-text margin-top\">              <a href=\"/help/#project-release-notifications\">Release notifications</a> |              <a href=\"/rss/project/open-clip-torch/releases.xml\">RSS feed <i class=\"fa fa-rss\" aria-hidden=\"true\"></i></a>            </span>          </h2>          <div class=\"release-timeline\">            <div class=\"release release--latest release--current\">              <div class=\"release__meta\">                <span class=\"badge\">This version</span>              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/blue-cube.572a5bfb.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.24.0/\">                <p class=\"release__version\">                  2.24.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2024-01-08T10:35:53+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Jan 8, 2024</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.23.0/\">                <p class=\"release__version\">                  2.23.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-10-24T16:03:13+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Oct 24, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.22.0/\">                <p class=\"release__version\">                  2.22.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-10-06T17:50:48+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Oct 6, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.20.0/\">                <p class=\"release__version\">                  2.20.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-05-15T15:41:37+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  May 15, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.19.0/\">                <p class=\"release__version\">                  2.19.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-05-04T05:51:04+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  May 4, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.18.0/\">                <p class=\"release__version\">                  2.18.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-04-26T20:53:18+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Apr 26, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.17.2/\">                <p class=\"release__version\">                  2.17.2                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-04-26T00:17:34+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Apr 26, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.17.1/\">                <p class=\"release__version\">                  2.17.1                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-04-18T06:26:07+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Apr 18, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.16.2/\">                <p class=\"release__version\">                  2.16.2                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-04-16T16:42:21+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Apr 16, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.16.1/\">                <p class=\"release__version\">                  2.16.1                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-04-16T16:20:19+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Apr 16, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.16.0/\">                <p class=\"release__version\">                  2.16.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-03-06T01:13:45+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Mar 6, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.15.0/\">                <p class=\"release__version\">                  2.15.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-02-27T01:23:18+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Feb 27, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.14.0/\">                <p class=\"release__version\">                  2.14.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-02-16T17:29:15+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Feb 16, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.13.0/\">                <p class=\"release__version\">                  2.13.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-02-12T22:47:10+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Feb 12, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.12.0/\">                <p class=\"release__version\">                  2.12.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-02-11T10:15:19+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Feb 11, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.11.1/\">                <p class=\"release__version\">                  2.11.1                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-02-04T00:09:08+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Feb 4, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.11.0/\">                <p class=\"release__version\">                  2.11.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-02-03T19:15:42+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Feb 3, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.10.1/\">                <p class=\"release__version\">                  2.10.1                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-01-24T01:18:18+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Jan 24, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.10.0/\">                <p class=\"release__version\">                  2.10.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-01-23T23:57:08+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Jan 23, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.9.3/\">                <p class=\"release__version\">                  2.9.3                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-01-09T22:01:40+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Jan 9, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.9.2/\">                <p class=\"release__version\">                  2.9.2                </p>                <p class=\"release__version-date\">                  <time datetime=\"2023-01-05T14:35:27+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Jan 5, 2023</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.9.1/\">                <p class=\"release__version\">                  2.9.1                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-12-29T22:21:47+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Dec 29, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.8.2/\">                <p class=\"release__version\">                  2.8.2                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-12-17T00:00:21+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Dec 17, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.8.1/\">                <p class=\"release__version\">                  2.8.1                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-12-15T17:50:29+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Dec 15, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.8.0/\">                <p class=\"release__version\">                  2.8.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-12-14T21:09:52+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Dec 14, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.7.0/\">                <p class=\"release__version\">                  2.7.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-11-18T21:25:54+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Nov 18, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.6.1/\">                <p class=\"release__version\">                  2.6.1                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-11-17T20:58:03+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Nov 17, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.5.0/\">                <p class=\"release__version\">                  2.5.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-11-14T16:21:04+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Nov 14, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.4.1/\">                <p class=\"release__version\">                  2.4.1                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-11-10T11:23:04+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Nov 10, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.4.0/\">                <p class=\"release__version\">                  2.4.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-11-10T10:25:46+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Nov 10, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.3.1/\">                <p class=\"release__version\">                  2.3.1                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-11-07T20:52:39+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Nov 7, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.3.0/\">                <p class=\"release__version\">                  2.3.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-11-07T19:48:59+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Nov 7, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.2.0/\">                <p class=\"release__version\">                  2.2.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-11-07T18:08:11+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Nov 7, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.0.2/\">                <p class=\"release__version\">                  2.0.2                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-09-16T15:08:09+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Sep 16, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.0.1/\">                <p class=\"release__version\">                  2.0.1                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-09-15T23:53:30+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Sep 15, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/2.0.0/\">                <p class=\"release__version\">                  2.0.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-09-15T19:04:12+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Sep 15, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/1.3.0/\">                <p class=\"release__version\">                  1.3.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-06-03T23:08:58+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Jun 3, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/1.2.1/\">                <p class=\"release__version\">                  1.2.1                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-05-21T21:35:47+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  May 21, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/1.2.0/\">                <p class=\"release__version\">                  1.2.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-05-20T22:27:05+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  May 20, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/1.1.1/\">                <p class=\"release__version\">                  1.1.1                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-05-15T21:12:14+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  May 15, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/1.0.1/\">                <p class=\"release__version\">                  1.0.1                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-04-26T23:29:11+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Apr 26, 2022</time>                </p>              </a>            </div>            <div class=\"release\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/0.2.1/\">                <p class=\"release__version\">                  0.2.1                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-04-08T16:39:42+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Apr 8, 2022</time>                </p>              </a>            </div>            <div class=\"release release--oldest\">              <div class=\"release__meta\">              </div>              <div class=\"release__graphic\">                <div class=\"release__line\"></div>                <img class=\"release__node\" alt=\"\" src=\"https://pypi.org/static/images/white-cube.2351a86c.svg\">              </div>              <a class=\"card release__card\" href=\"/project/open-clip-torch/0.2.0/\">                <p class=\"release__version\">                  0.2.0                </p>                <p class=\"release__version-date\">                  <time datetime=\"2022-04-04T21:25:48+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Apr 4, 2022</time>                </p>              </a>            </div>          </div>        </div>          <div id=\"files\" data-project-tabs-target=\"content\" class=\"vertical-tabs__content\" role=\"tabpanel\" aria-labelledby=\"files-tab mobile-files-tab\" tabindex=\"-1\">            <h2 class=\"page-title\">Download files</h2>            <p>Download the file for your platform. If you're not sure which to choose, learn more about <a href=\"https://packaging.python.org/tutorials/installing-packages/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">installing packages</a>.</p>            <h3>Source Distribution            </h3>                  <div class=\"file\">      <div class=\"file__graphic\">        <i class=\"far fa-file\" aria-hidden=\"true\"></i>      </div>      <div class=\"card file__card\">        <a href=\"https://files.pythonhosted.org/packages/2a/a3/4dc94bfbfc45cbc49f0b7cac1aee1f2b15420013b26e18a94fdcdd906a2a/open_clip_torch-2.24.0.tar.gz\">          open_clip_torch-2.24.0.tar.gz        </a>        (1.5 MB        <a href=\"#copy-hash-modal-f5769f0f-237e-46b2-a08d-9671ca41f5ed\">view hashes</a>)        <p class=\"file__meta\">          Uploaded <time datetime=\"2024-01-08T10:35:55+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Jan 8, 2024</time>          <code>source</code>        </p>      </div>    </div>            <h3>Built Distribution            </h3>                <div class=\"file\">      <div class=\"file__graphic\">        <i class=\"far fa-file\" aria-hidden=\"true\"></i>      </div>      <div class=\"card file__card\">        <a href=\"https://files.pythonhosted.org/packages/d9/d2/6ae2ee32d0d2ea9982774920e0ef96d439ee332f459f6d8a941149b1b4ad/open_clip_torch-2.24.0-py3-none-any.whl\">          open_clip_torch-2.24.0-py3-none-any.whl        </a>        (1.5 MB        <a href=\"#copy-hash-modal-d7363ed1-1de1-4b75-af74-bafae4f549d2\">view hashes</a>)        <p class=\"file__meta\">          Uploaded <time datetime=\"2024-01-08T10:35:53+0000\" data-controller=\"localized-time\" data-localized-time-relative=\"true\" data-localized-time-show-time=\"false\">  Jan 8, 2024</time>          <code>py3</code>        </p>      </div>    </div>          </div><div id=\"copy-hash-modal-f5769f0f-237e-46b2-a08d-9671ca41f5ed\" class=\"modal modal--wide\">  <div class=\"modal__content\" role=\"dialog\">    <a href=\"#modal-close\" title=\"Close\" class=\"modal__close\">      <i class=\"fa fa-times\" aria-hidden=\"true\"></i>      <span class=\"sr-only\">Close</span>    </a>    <div class=\"modal__body\">      <h3 class=\"modal__title\"><a href=\"https://pip.pypa.io/en/stable/topics/secure-installs/#hash-checking-mode\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Hashes</a> for open_clip_torch-2.24.0.tar.gz      </h3>      <table class=\"table table--hashes\">        <caption class=\"sr-only\">Hashes for open_clip_torch-2.24.0.tar.gz</caption>        <thead>          <tr>            <th scope=\"col\">Algorithm</th>            <th scope=\"col\">Hash digest</th>            <th></th>          </tr>        </thead>        <tbody>          <tr data-controller=\"clipboard\">            <th scope=\"row\">SHA256</th>            <td><code data-clipboard-target=\"source\">1ae2482aee313827c399eb8a4e735f0b0cd31e4c62085ce2dbfa3a13190219ff</code></td>            <td class=\"table__align-right\">              <button type=\"button\" class=\"button button--small copy-tooltip copy-tooltip-w\" data-action=\"clipboard#copy\" data-clipboard-target=\"tooltip\" data-clipboard-tooltip-value=\"Copy to clipboard\">Copy              </button>            </td>          </tr>          <tr data-controller=\"clipboard\">            <th scope=\"row\">MD5</th>            <td><code data-clipboard-target=\"source\">21f4fdf3e649d8c8370c00902374c787</code></td>            <td class=\"table__align-right\">              <button type=\"button\" class=\"button button--small copy-tooltip copy-tooltip-w\" data-action=\"clipboard#copy\" data-clipboard-target=\"tooltip\" data-clipboard-tooltip-value=\"Copy to clipboard\">Copy              </button>            </td>          </tr>          <tr data-controller=\"clipboard\">            <th scope=\"row\">BLAKE2b-256</th>            <td><code data-clipboard-target=\"source\">2aa34dc94bfbfc45cbc49f0b7cac1aee1f2b15420013b26e18a94fdcdd906a2a</code></td>            <td class=\"table__align-right\">              <button type=\"button\" class=\"button button--small copy-tooltip copy-tooltip-w\" data-action=\"clipboard#copy\" data-clipboard-target=\"tooltip\" data-clipboard-tooltip-value=\"Copy to clipboard\">Copy              </button>            </td>          </tr>        </tbody>      </table>    </div>    <div class=\"modal__footer\">      <a href=\"#modal-close\" class=\"button button--primary modal__action\">Close</a>    </div>  </div></div><div id=\"copy-hash-modal-d7363ed1-1de1-4b75-af74-bafae4f549d2\" class=\"modal modal--wide\">  <div class=\"modal__content\" role=\"dialog\">    <a href=\"#modal-close\" title=\"Close\" class=\"modal__close\">      <i class=\"fa fa-times\" aria-hidden=\"true\"></i>      <span class=\"sr-only\">Close</span>    </a>    <div class=\"modal__body\">      <h3 class=\"modal__title\"><a href=\"https://pip.pypa.io/en/stable/topics/secure-installs/#hash-checking-mode\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Hashes</a> for open_clip_torch-2.24.0-py3-none-any.whl      </h3>      <table class=\"table table--hashes\">        <caption class=\"sr-only\">Hashes for open_clip_torch-2.24.0-py3-none-any.whl</caption>        <thead>          <tr>            <th scope=\"col\">Algorithm</th>            <th scope=\"col\">Hash digest</th>            <th></th>          </tr>        </thead>        <tbody>          <tr data-controller=\"clipboard\">            <th scope=\"row\">SHA256</th>            <td><code data-clipboard-target=\"source\">2537dbe76c8008caa46652bc97cb32bceeae56baff6289e7b4eb22539a80c801</code></td>            <td class=\"table__align-right\">              <button type=\"button\" class=\"button button--small copy-tooltip copy-tooltip-w\" data-action=\"clipboard#copy\" data-clipboard-target=\"tooltip\" data-clipboard-tooltip-value=\"Copy to clipboard\">Copy              </button>            </td>          </tr>          <tr data-controller=\"clipboard\">            <th scope=\"row\">MD5</th>            <td><code data-clipboard-target=\"source\">3eb63c42f73f57229640125f7584ceae</code></td>            <td class=\"table__align-right\">              <button type=\"button\" class=\"button button--small copy-tooltip copy-tooltip-w\" data-action=\"clipboard#copy\" data-clipboard-target=\"tooltip\" data-clipboard-tooltip-value=\"Copy to clipboard\">Copy              </button>            </td>          </tr>          <tr data-controller=\"clipboard\">            <th scope=\"row\">BLAKE2b-256</th>            <td><code data-clipboard-target=\"source\">d9d26ae2ee32d0d2ea9982774920e0ef96d439ee332f459f6d8a941149b1b4ad</code></td>            <td class=\"table__align-right\">              <button type=\"button\" class=\"button button--small copy-tooltip copy-tooltip-w\" data-action=\"clipboard#copy\" data-clipboard-target=\"tooltip\" data-clipboard-tooltip-value=\"Copy to clipboard\">Copy              </button>            </td>          </tr>        </tbody>      </table>    </div>    <div class=\"modal__footer\">      <a href=\"#modal-close\" class=\"button button--primary modal__action\">Close</a>    </div>  </div></div>      </div>    </div>  </div></div>    </main>    <footer class=\"footer\">      <div class=\"footer__logo\">        <img src=\"/static/images/white-cube.2351a86c.svg\" alt=\"\" class=\"-js-white-cube\">      </div>      <div class=\"footer__menus\">        <div class=\"footer__menu\">          <h2>Help</h2>          <nav aria-label=\"Help navigation\">            <ul>              <li><a href=\"https://packaging.python.org/tutorials/installing-packages/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Installing packages</a></li>              <li><a href=\"https://packaging.python.org/tutorials/packaging-projects/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Uploading packages</a></li>              <li><a href=\"https://packaging.python.org/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">User guide</a></li>              <li><a href=\"https://www.python.org/dev/peps/pep-0541/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Project name retention</a></li>              <li><a href=\"/help/\">FAQs</a></li>            </ul>          </nav>        </div>        <div class=\"footer__menu\">          <h2>About PyPI</h2>          <nav aria-label=\"About PyPI navigation\">            <ul>              <li><a href=\"https://twitter.com/PyPI\" title=\"External link\" target=\"_blank\" rel=\"noopener\">PyPI on Twitter</a></li>              <li><a href=\"https://dtdg.co/pypi\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Infrastructure dashboard</a></li>              <li><a href=\"/stats/\">Statistics</a></li>              <li><a href=\"/trademarks/\">Logos & trademarks</a></li>              <li><a href=\"/sponsors/\">Our sponsors</a></li>            </ul>          </nav>        </div>        <div class=\"footer__menu\">          <h2>Contributing to PyPI</h2>          <nav aria-label=\"How to contribute navigation\">            <ul>              <li><a href=\"/help/#feedback\">Bugs and feedback</a></li>              <li><a href=\"https://github.com/pypi/warehouse\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Contribute on GitHub</a></li>              <li><a href=\"https://hosted.weblate.org/projects/pypa/warehouse/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Translate PyPI</a></li>              <li><a href=\"/sponsors/\">Sponsor PyPI</a></li>              <li><a href=\"https://github.com/pypi/warehouse/graphs/contributors\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Development credits</a></li>            </ul>          </nav>        </div>        <div class=\"footer__menu\">          <h2>Using PyPI</h2>          <nav aria-label=\"Using PyPI navigation\">            <ul>              <li><a href=\"https://github.com/pypa/.github/blob/main/CODE_OF_CONDUCT.md\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Code of conduct</a></li>              <li><a href=\"/security/\">Report security issue</a></li>              <li><a href=\"https://www.python.org/privacy/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Privacy policy</a></li>              <li><a href=\"/policy/terms-of-use/\">Terms of use</a></li>              <li><a href=\"/policy/acceptable-use-policy/\">Acceptable Use Policy</a></li>            </ul>          </nav>        </div>      </div>      <hr class=\"footer__divider\">      <div class=\"footer__text\">        <p>Status:<a href=\"https://status.python.org/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">          <span data-statuspage-domain=\"https://2p66nmmycsj3.statuspage.io\">all systems operational</span></a>        </p>        <p>Developed and maintained by the Python community, for the Python community.          <br>          <a href=\"https://donate.pypi.org\">Donate today!</a>        </p>        <p>          \"PyPI\", \"Python Package Index\", and the blocks logos are registered <a href=\"/trademarks/\">trademarks</a> of the <a href=\"https://python.org/psf-landing\" target=\"_blank\" rel=\"noopener\">Python Software Foundation</a>.<br>        </p>        <p>          \u00a9 2024 <a href=\"https://www.python.org/psf-landing/\" title=\"External link\" target=\"_blank\" rel=\"noopener\">Python Software Foundation</a><br>          <a href=\"/sitemap/\">Site map</a>        </p>      </div>      <div class=\"centered hide-on-desktop\">        <button type=\"button\" class=\"button button--switch-to-desktop hidden\" data-viewport-toggle-target=\"switchToDesktop\" data-action=\"viewport-toggle#switchToDesktop\">Switch to desktop version        </button>      </div>    </footer>    <div class=\"language-switcher\">      <form action=\"/locale/\">        <ul>          <li>            <button              class=\"language-switcher__selected\"              name=\"locale_id\" value=\"en\" type=\"submit\"            >              English            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"es\" type=\"submit\"            >              espa\u00f1ol            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"fr\" type=\"submit\"            >              fran\u00e7ais            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"ja\" type=\"submit\"            >              \u65e5\u672c\u8a9e            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"pt_BR\" type=\"submit\"            >              portugu\u00eas (Brasil)            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"uk\" type=\"submit\"            >              \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"el\" type=\"submit\"            >              \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"de\" type=\"submit\"            >              Deutsch            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"zh_Hans\" type=\"submit\"            >              \u4e2d\u6587 (\u7b80\u4f53)            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"zh_Hant\" type=\"submit\"            >              \u4e2d\u6587 (\u7e41\u9ad4)            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"ru\" type=\"submit\"            >              \u0440\u0443\u0441\u0441\u043a\u0438\u0439            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"he\" type=\"submit\"            >              \u05e2\u05d1\u05e8\u05d9\u05ea            </button>          </li>          <li>            <button              name=\"locale_id\" value=\"eo\" type=\"submit\"            >              esperanto            </button>          </li>        </ul>      </form>    </div><div class=\"sponsors\">  <p class=\"sponsors__title\">Supported by</p>  <div class=\"sponsors__divider\"></div>        <a class=\"sponsors__sponsor\" target=\"_blank\" rel=\"noopener\" href=\"https://aws.amazon.com/\">          <img class=sponsors__image src=\"https://pypi-camo.freetls.fastly.net/ed7074cadad1a06f56bc520ad9bd3e00d0704c5b/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f6177732d77686974652d6c6f676f2d7443615473387a432e706e67\" alt=AWS loading=lazy>          <span class=\"sponsors__name\">AWS</span>          <span class=\"sponsors__service\">            Cloud computing and Security Sponsor          </span>        </a>        <a class=\"sponsors__sponsor\" target=\"_blank\" rel=\"noopener\" href=\"https://www.datadoghq.com/\">          <img class=sponsors__image src=\"https://pypi-camo.freetls.fastly.net/8855f7c063a3bdb5b0ce8d91bfc50cf851cc5c51/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f64617461646f672d77686974652d6c6f676f2d6668644c4e666c6f2e706e67\" alt=Datadog loading=lazy>          <span class=\"sponsors__name\">Datadog</span>          <span class=\"sponsors__service\">            Monitoring          </span>        </a>        <a class=\"sponsors__sponsor\" target=\"_blank\" rel=\"noopener\" href=\"https://www.fastly.com/\">          <img class=sponsors__image src=\"https://pypi-camo.freetls.fastly.net/df6fe8829cbff2d7f668d98571df1fd011f36192/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f666173746c792d77686974652d6c6f676f2d65684d3077735f6f2e706e67\" alt=Fastly loading=lazy>          <span class=\"sponsors__name\">Fastly</span>          <span class=\"sponsors__service\">            CDN          </span>        </a>        <a class=\"sponsors__sponsor\" target=\"_blank\" rel=\"noopener\" href=\"https://careers.google.com/\">          <img class=sponsors__image src=\"https://pypi-camo.freetls.fastly.net/420cc8cf360bac879e24c923b2f50ba7d1314fb0/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f676f6f676c652d77686974652d6c6f676f2d616734424e3774332e706e67\" alt=Google loading=lazy>          <span class=\"sponsors__name\">Google</span>          <span class=\"sponsors__service\">            Download Analytics          </span>        </a>        <a class=\"sponsors__sponsor\" target=\"_blank\" rel=\"noopener\" href=\"https://www.python.org/psf/sponsors/#microsoft\">          <img class=sponsors__image src=\"https://pypi-camo.freetls.fastly.net/524d1ce72f7772294ca4c1fe05d21dec8fa3f8ea/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f6d6963726f736f66742d77686974652d6c6f676f2d5a443172685444462e706e67\" alt=Microsoft loading=lazy>          <span class=\"sponsors__name\">Microsoft</span>          <span class=\"sponsors__service\">            PSF Sponsor          </span>        </a>        <a class=\"sponsors__sponsor\" target=\"_blank\" rel=\"noopener\" href=\"https://www.pingdom.com/\">          <img class=sponsors__image src=\"https://pypi-camo.freetls.fastly.net/d01053c02f3a626b73ffcb06b96367fdbbf9e230/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f70696e67646f6d2d77686974652d6c6f676f2d67355831547546362e706e67\" alt=Pingdom loading=lazy>          <span class=\"sponsors__name\">Pingdom</span>          <span class=\"sponsors__service\">            Monitoring          </span>        </a>        <a class=\"sponsors__sponsor\" target=\"_blank\" rel=\"noopener\" href=\"https://getsentry.com/for/python\">          <img class=sponsors__image src=\"https://pypi-camo.freetls.fastly.net/67af7117035e2345bacb5a82e9aa8b5b3e70701d/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f73656e7472792d77686974652d6c6f676f2d4a2d6b64742d706e2e706e67\" alt=Sentry loading=lazy>          <span class=\"sponsors__name\">Sentry</span>          <span class=\"sponsors__service\">            Error logging          </span>        </a>        <a class=\"sponsors__sponsor\" target=\"_blank\" rel=\"noopener\" href=\"https://statuspage.io\">          <img class=sponsors__image src=\"https://pypi-camo.freetls.fastly.net/b611884ff90435a0575dbab7d9b0d3e60f136466/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f707970692d6173736574732f73706f6e736f726c6f676f732f737461747573706167652d77686974652d6c6f676f2d5467476c6a4a2d502e706e67\" alt=StatusPage loading=lazy>          <span class=\"sponsors__name\">StatusPage</span>          <span class=\"sponsors__service\">            Status page          </span>        </a></div>  </body></html>",
  "embeddings": []
}