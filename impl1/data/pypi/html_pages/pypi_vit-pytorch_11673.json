{
  "libtype": "pypi",
  "libname": "vit-pytorch",
  "url": "https://github.com/lucidrains/vit-pytorch",
  "html": "<!DOCTYPE html><html  lang=\"en\"    data-color-mode=\"auto\" data-light-theme=\"light\" data-dark-theme=\"dark\"  data-a11y-animated-images=\"system\" data-a11y-link-underlines=\"true\"  >  <head>    <meta charset=\"utf-8\">  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">  <link rel=\"dns-prefetch\" href=\"https://avatars.githubusercontent.com\">  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">  <link rel=\"dns-prefetch\" href=\"https://user-images.githubusercontent.com/\">  <link rel=\"preconnect\" href=\"https://github.githubassets.com\" crossorigin>  <link rel=\"preconnect\" href=\"https://avatars.githubusercontent.com\">    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/light-0eace2597ca3.css\" /><link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/dark-a167e256da9c.css\" /><link data-color-theme=\"dark_dimmed\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/dark_dimmed-d11f2cf8009b.css\" /><link data-color-theme=\"dark_high_contrast\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/dark_high_contrast-ea7373db06c8.css\" /><link data-color-theme=\"dark_colorblind\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/dark_colorblind-afa99dcf40f7.css\" /><link data-color-theme=\"light_colorblind\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/light_colorblind-af6c685139ba.css\" /><link data-color-theme=\"light_high_contrast\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/light_high_contrast-578cdbc8a5a9.css\" /><link data-color-theme=\"light_tritanopia\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/light_tritanopia-5cb699a7e247.css\" /><link data-color-theme=\"dark_tritanopia\" crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" data-href=\"https://github.githubassets.com/assets/dark_tritanopia-9b32204967c6.css\" />    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/primer-primitives-2ef2a46b27ee.css\" />    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/primer-711f412bb361.css\" />    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/global-4803cd254267.css\" />    <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/github-f4d857cbc96a.css\" />  <link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/repository-6247ca238fd4.css\" /><link crossorigin=\"anonymous\" media=\"all\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/code-6d7b4ef0ea51.css\" />    <script type=\"application/json\" id=\"client-env\">{\"locale\":\"en\",\"featureFlags\":[\"code_vulnerability_scanning\",\"copilot_conversational_ux_history_refs\",\"copilot_chat_attach_knowledge\",\"copilot_chat_knowledge_base_copy\",\"copilot_smell_icebreaker_ux\",\"copilot_implicit_context\",\"docset_management_ui\",\"copilot_chat_settings\",\"failbot_handle_non_errors\",\"geojson_azure_maps\",\"image_metric_tracking\",\"marketing_forms_api_integration_contact_request\",\"marketing_pages_search_explore_provider\",\"turbo_experiment_risky\",\"sample_network_conn_type\",\"no_character_key_shortcuts_in_inputs\",\"custom_inp\",\"remove_child_patch\"]}</script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/wp-runtime-47578fb192fd.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_dompurify_dist_purify_js-6890e890956f.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_stacktrace-parser_dist_stack-trace-parser_esm_js-node_modules_github_bro-a4c183-79f9611c275b.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_hydro-analytics-client_dist_analytics-client_js-node_modules_gith-6a10dd-e66ebda625fb.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_failbot_failbot_ts-479802999bcc.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/environment-fe7570f3bc38.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_selector-observer_dist_index_esm_js-9f960d9b217c.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_behaviors_dist_esm_focus-zone_js-086f7a27bac0.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_relative-time-element_dist_index_js-c76945c5961a.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_delegated-events_dist_index_js-node_modules_github_details-dialog-elemen-29dc30-a2a71f11a507.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_auto-complete-element_dist_index_js-12366198e7a5.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_text-expander-element_dist_index_js-8a621df59e80.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_filter-input-element_dist_index_js-node_modules_github_remote-inp-b7d8f4-654130b7cde5.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_file-attachment-element_dist_index_js-node_modules_primer_view-co-5dccdf-e5e2b9fa3c0c.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/github-elements-e4eda4896b4e.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/element-registry-b99c9d8fad1d.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_catalyst_lib_index_js-node_modules_github_hydro-analytics-client_-978abc0-add939c751ce.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_lit-html_lit-html_js-5b376145beff.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_mini-throttle_dist_index_js-node_modules_github_alive-client_dist-bf5aa2-1b562c29ab8e.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_morphdom_dist_morphdom-esm_js-5bff297a06de.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_turbo_dist_turbo_es2017-esm_js-c91f4ad18b62.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_color-convert_index_js-72c9fbde5ad4.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_scroll-anchoring_dist_scro-231ccf-aa129238d13b.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_behaviors_dist_esm_dimensions_js-node_modules_github_jtml_lib_index_js-95b84ee6bc34.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_session-resume_dist_index_js-node_modules_primer_behaviors_dist_e-da6ec6-3f39339c9d98.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_paste-markdown_dist_index_esm_js-node_modules_github_quote-select-67e0dc-1aa35af077a4.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_updatable-content_ts-ee3fc84d7fb0.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_behaviors_task-list_ts-app_assets_modules_github_onfocus_ts-app_ass-421cec-9de4213015af.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_sticky-scroll-into-view_ts-94209c43e6af.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_behaviors_ajax-error_ts-app_assets_modules_github_behaviors_include-467754-f9bd433e9591.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_behaviors_commenting_edit_ts-app_assets_modules_github_behaviors_ht-83c235-9285faa0e011.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_blob-anchor_ts-app_assets_modules_github_filter-sort_ts-app_assets_-c96432-da3733f430b8.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/behaviors-1fb9e5061509.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_delegated-events_dist_index_js-node_modules_github_catalyst_lib_index_js-d0256ebff5cd.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/notifications-global-352d84c6cc82.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_virtualized-list_es_index_js-node_modules_github_template-parts_lib_index_js-878844713bc9.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_delegated-events_dist_inde-c537341-c7f6a41a084c.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_github_ref-selector_ts-b593b93f23f5.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/codespaces-1a8626dd714a.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_filter-input-element_dist_index_js-node_modules_github_mini-throt-08ab15-3e0517baca99.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_file-attachment-element_dist_index_js-node_modules_github_mini-th-55cf52-e14cb4b719b4.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/repositories-69068e0899f9.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/code-menu-614feb194539.js\"></script>    <title>GitHub - lucidrains/vit-pytorch: Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch</title>  <meta name=\"route-pattern\" content=\"/:user_id/:repository\" data-turbo-transient>  <meta name=\"route-controller\" content=\"files\" data-turbo-transient>  <meta name=\"route-action\" content=\"disambiguate\" data-turbo-transient>      <meta name=\"current-catalog-service-hash\" content=\"82c569b93da5c18ed649ebd4c2c79437db4611a6a1373e805a3cb001c64130b7\">  <meta name=\"request-id\" content=\"CBCA:7F8B:C091EB:11B539A:65E793E5\" data-pjax-transient=\"true\"/><meta name=\"html-safe-nonce\" content=\"c010b4761cc9d9bd2804fbd68a81569871edcab7d9c3c493fefc9846a76a47c3\" data-pjax-transient=\"true\"/><meta name=\"visitor-payload\" content=\"eyJyZWZlcnJlciI6IiIsInJlcXVlc3RfaWQiOiJDQkNBOjdGOEI6QzA5MUVCOjExQjUzOUE6NjVFNzkzRTUiLCJ2aXNpdG9yX2lkIjoiMTQ1NjgxMjM4NTQ5NjQzNzczMyIsInJlZ2lvbl9lZGdlIjoiaWFkIiwicmVnaW9uX3JlbmRlciI6ImlhZCJ9\" data-pjax-transient=\"true\"/><meta name=\"visitor-hmac\" content=\"745fb923c7764fd29a3c8930a5bf9f0559a27f5a9abbb87373f49e0fbed60549\" data-pjax-transient=\"true\"/>    <meta name=\"hovercard-subject-tag\" content=\"repository:300996055\" data-turbo-transient>  <meta name=\"github-keyboard-shortcuts\" content=\"repository,copilot\" data-turbo-transient=\"true\" />    <meta name=\"selected-link\" value=\"repo_source\" data-turbo-transient>  <link rel=\"assets\" href=\"https://github.githubassets.com/\">    <meta name=\"google-site-verification\" content=\"c1kuD-K2HIVF635lypcsWPoD4kilo5-jA_wBFyT4uMY\">  <meta name=\"google-site-verification\" content=\"KT5gs8h0wvaagLKAVWq8bbeNwnZZK1r1XQysX3xurLU\">  <meta name=\"google-site-verification\" content=\"ZzhVyEFwb7w3e0-uOTltm8Jsck2F5StVihD0exw2fsA\">  <meta name=\"google-site-verification\" content=\"GXs5KoUUkNCoaAZn7wPN-t01Pywp9M3sEjnt_3_ZWPc\">  <meta name=\"google-site-verification\" content=\"Apib7-x98H0j5cPqHWwSMm6dNU4GmODRoqxLiDzdx9I\"><meta name=\"octolytics-url\" content=\"https://collector.github.com/github/collect\" />  <meta name=\"analytics-location\" content=\"/&lt;user-name&gt;/&lt;repo-name&gt;\" data-turbo-transient=\"true\" />        <meta name=\"user-login\" content=\"\">      <meta name=\"viewport\" content=\"width=device-width\">          <meta name=\"description\" content=\"Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch - lucidrains/vit-pytorch\">      <link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"/opensearch.xml\" title=\"GitHub\">    <link rel=\"fluid-icon\" href=\"https://github.com/fluidicon.png\" title=\"GitHub\">    <meta property=\"fb:app_id\" content=\"1401488693436528\">    <meta name=\"apple-itunes-app\" content=\"app-id=1477376905, app-argument=https://github.com/lucidrains/vit-pytorch\" />      <meta name=\"twitter:image:src\" content=\"https://opengraph.githubassets.com/5b1048e831f10e3db012f0dfb19ff833562a3788b8b07f041516dd06d980a5c5/lucidrains/vit-pytorch\" /><meta name=\"twitter:site\" content=\"@github\" /><meta name=\"twitter:card\" content=\"summary_large_image\" /><meta name=\"twitter:title\" content=\"GitHub - lucidrains/vit-pytorch: Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch\" /><meta name=\"twitter:description\" content=\"Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch - lucidrains/vit-pytorch\" />      <meta property=\"og:image\" content=\"https://opengraph.githubassets.com/5b1048e831f10e3db012f0dfb19ff833562a3788b8b07f041516dd06d980a5c5/lucidrains/vit-pytorch\" /><meta property=\"og:image:alt\" content=\"Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch - lucidrains/vit-pytorch\" /><meta property=\"og:image:width\" content=\"1200\" /><meta property=\"og:image:height\" content=\"600\" /><meta property=\"og:site_name\" content=\"GitHub\" /><meta property=\"og:type\" content=\"object\" /><meta property=\"og:title\" content=\"GitHub - lucidrains/vit-pytorch: Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch\" /><meta property=\"og:url\" content=\"https://github.com/lucidrains/vit-pytorch\" /><meta property=\"og:description\" content=\"Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch - lucidrains/vit-pytorch\" />              <meta name=\"hostname\" content=\"github.com\">        <meta name=\"expected-hostname\" content=\"github.com\">  <meta http-equiv=\"x-pjax-version\" content=\"b9fa4cafade57d606c6dcfafff1d08bd597980af7b9837ed473fdf0cdea8a3bc\" data-turbo-track=\"reload\">  <meta http-equiv=\"x-pjax-csp-version\" content=\"5dcfbec3488c5fd5a334e287ce6a17058b7d4beb91db2d4d184e4d55bbf1d7d7\" data-turbo-track=\"reload\">  <meta http-equiv=\"x-pjax-css-version\" content=\"d33c7c2fcff40783f3002896023f41e2c17ec62b12ddbe7434e2001d743fb853\" data-turbo-track=\"reload\">  <meta http-equiv=\"x-pjax-js-version\" content=\"4ba4a7cc07194c8d5f24291dea4fbc790ffd83ba40beacaf8d0117187b571b4d\" data-turbo-track=\"reload\">  <meta name=\"turbo-cache-control\" content=\"no-preview\" data-turbo-transient=\"\">      <meta data-hydrostats=\"publish\">  <meta name=\"go-import\" content=\"github.com/lucidrains/vit-pytorch git https://github.com/lucidrains/vit-pytorch.git\">  <meta name=\"octolytics-dimension-user_id\" content=\"108653\" /><meta name=\"octolytics-dimension-user_login\" content=\"lucidrains\" /><meta name=\"octolytics-dimension-repository_id\" content=\"300996055\" /><meta name=\"octolytics-dimension-repository_nwo\" content=\"lucidrains/vit-pytorch\" /><meta name=\"octolytics-dimension-repository_public\" content=\"true\" /><meta name=\"octolytics-dimension-repository_is_fork\" content=\"false\" /><meta name=\"octolytics-dimension-repository_network_root_id\" content=\"300996055\" /><meta name=\"octolytics-dimension-repository_network_root_nwo\" content=\"lucidrains/vit-pytorch\" />    <link rel=\"canonical\" href=\"https://github.com/lucidrains/vit-pytorch\" data-turbo-transient>  <meta name=\"turbo-body-classes\" content=\"logged-out env-production page-responsive\">  <meta name=\"browser-stats-url\" content=\"https://api.github.com/_private/browser/stats\">  <meta name=\"browser-errors-url\" content=\"https://api.github.com/_private/browser/errors\">  <link rel=\"mask-icon\" href=\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\" color=\"#000000\">  <link rel=\"alternate icon\" class=\"js-site-favicon\" type=\"image/png\" href=\"https://github.githubassets.com/favicons/favicon.png\">  <link rel=\"icon\" class=\"js-site-favicon\" type=\"image/svg+xml\" href=\"https://github.githubassets.com/favicons/favicon.svg\"><meta name=\"theme-color\" content=\"#1e2327\"><meta name=\"color-scheme\" content=\"light dark\" />  <link rel=\"manifest\" href=\"/manifest.json\" crossOrigin=\"use-credentials\">  </head>  <body class=\"logged-out env-production page-responsive\" style=\"word-wrap: break-word;\">    <div data-turbo-body class=\"logged-out env-production page-responsive\" style=\"word-wrap: break-word;\">          <div class=\"position-relative js-header-wrapper \">      <a href=\"#start-of-content\" class=\"px-2 py-4 color-bg-accent-emphasis color-fg-on-emphasis show-on-focus js-skip-to-content\">Skip to content</a>      <span data-view-component=\"true\" class=\"progress-pjax-loader Progress position-fixed width-full\">    <span style=\"width: 0%;\" data-view-component=\"true\" class=\"Progress-item progress-pjax-loader-bar left-0 top-0 color-bg-accent-emphasis\"></span></span>              <script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Button_IconButton_js-node_modules_primer_react_lib--23bcad-a89698f38643.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/keyboard-shortcuts-dialog-a23eda2bcf8d.js\"></script><react-partial  partial-name=\"keyboard-shortcuts-dialog\"  data-ssr=\"false\">    <script type=\"application/json\" data-target=\"react-partial.embeddedData\">{\"props\":{}}</script>  <div data-target=\"react-partial.reactRoot\"></div></react-partial>                          <script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_delegated-events_dist_inde-94fd67-99519581d0f8.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/sessions-585a7232e50a.js\"></script><header class=\"Header-old header-logged-out js-details-container Details position-relative f4 py-3\" role=\"banner\" data-color-mode=light data-light-theme=light data-dark-theme=dark>  <button type=\"button\" class=\"Header-backdrop d-lg-none border-0 position-fixed top-0 left-0 width-full height-full js-details-target\" aria-label=\"Toggle navigation\">    <span class=\"d-none\">Toggle navigation</span>  </button>  <div class=\" d-flex flex-column flex-lg-row flex-items-center p-responsive height-full position-relative z-1\">    <div class=\"d-flex flex-justify-between flex-items-center width-full width-lg-auto\">      <a class=\"mr-lg-3 color-fg-inherit flex-order-2\" href=\"https://github.com/\" aria-label=\"Homepage\" data-ga-click=\"(Logged out) Header, go to homepage, icon:logo-wordmark\">        <svg height=\"32\" aria-hidden=\"true\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"32\" data-view-component=\"true\" class=\"octicon octicon-mark-github\">    <path d=\"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z\"></path></svg>      </a>      <div class=\"flex-1\">        <a href=\"/login?return_to=https%3A%2F%2Fgithub.com%2Flucidrains%2Fvit-pytorch\"          class=\"d-inline-block d-lg-none flex-order-1 f5 no-underline border color-border-default rounded-2 px-2 py-1 color-fg-inherit\"          data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/vit-pytorch&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"49c15a216452a5a641aa1cbf08fdf1633dcdd1e0144dbf8cc191b15b1ba5e4e0\"          data-ga-click=\"(Logged out) Header, clicked Sign in, text:sign-in\">          Sign in        </a>      </div>      <div class=\"flex-1 flex-order-2 text-right\">        <button aria-label=\"Toggle navigation\" aria-expanded=\"false\" type=\"button\" data-view-component=\"true\" class=\"js-details-target Button--link Button--medium Button d-lg-none color-fg-inherit p-1\">  <span class=\"Button-content\">    <span class=\"Button-label\"><div class=\"HeaderMenu-toggle-bar rounded my-1\"></div>            <div class=\"HeaderMenu-toggle-bar rounded my-1\"></div>            <div class=\"HeaderMenu-toggle-bar rounded my-1\"></div></span>  </span></button>      </div>    </div>    <div class=\"HeaderMenu--logged-out p-responsive height-fit position-lg-relative d-lg-flex flex-column flex-auto pt-7 pb-4 top-0\">      <div class=\"header-menu-wrapper d-flex flex-column flex-self-end flex-lg-row flex-justify-between flex-auto p-3 p-lg-0 rounded rounded-lg-0 mt-3 mt-lg-0\">          <nav class=\"mt-0 px-3 px-lg-0 mb-3 mb-lg-0\" aria-label=\"Global\">            <ul class=\"d-lg-flex list-style-none\">                <li class=\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\">      <button type=\"button\" class=\"HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target\" aria-expanded=\"false\">        Product        <svg opacity=\"0.5\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-chevron-down HeaderMenu-icon ml-1\">    <path d=\"M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z\"></path></svg>      </button>      <div class=\"HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 d-lg-flex dropdown-menu-wide\">          <div class=\"px-lg-4 border-lg-right mb-4 mb-lg-0 pr-lg-7\">            <ul class=\"list-style-none f5\" >                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}\" href=\"/features/actions\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-workflow color-fg-subtle mr-3\">    <path d=\"M1 3a2 2 0 0 1 2-2h6.5a2 2 0 0 1 2 2v6.5a2 2 0 0 1-2 2H7v4.063C7 16.355 7.644 17 8.438 17H12.5v-2.5a2 2 0 0 1 2-2H21a2 2 0 0 1 2 2V21a2 2 0 0 1-2 2h-6.5a2 2 0 0 1-2-2v-2.5H8.437A2.939 2.939 0 0 1 5.5 15.562V11.5H3a2 2 0 0 1-2-2Zm2-.5a.5.5 0 0 0-.5.5v6.5a.5.5 0 0 0 .5.5h6.5a.5.5 0 0 0 .5-.5V3a.5.5 0 0 0-.5-.5ZM14.5 14a.5.5 0 0 0-.5.5V21a.5.5 0 0 0 .5.5H21a.5.5 0 0 0 .5-.5v-6.5a.5.5 0 0 0-.5-.5Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Actions</div>        Automate any workflow      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}\" href=\"/features/packages\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-package color-fg-subtle mr-3\">    <path d=\"M12.876.64V.639l8.25 4.763c.541.313.875.89.875 1.515v9.525a1.75 1.75 0 0 1-.875 1.516l-8.25 4.762a1.748 1.748 0 0 1-1.75 0l-8.25-4.763a1.75 1.75 0 0 1-.875-1.515V6.917c0-.625.334-1.202.875-1.515L11.126.64a1.748 1.748 0 0 1 1.75 0Zm-1 1.298L4.251 6.34l7.75 4.474 7.75-4.474-7.625-4.402a.248.248 0 0 0-.25 0Zm.875 19.123 7.625-4.402a.25.25 0 0 0 .125-.216V7.639l-7.75 4.474ZM3.501 7.64v8.803c0 .09.048.172.125.216l7.625 4.402v-8.947Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Packages</div>        Host and manage packages      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}\" href=\"/features/security\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-shield-check color-fg-subtle mr-3\">    <path d=\"M16.53 9.78a.75.75 0 0 0-1.06-1.06L11 13.19l-1.97-1.97a.75.75 0 0 0-1.06 1.06l2.5 2.5a.75.75 0 0 0 1.06 0l5-5Z\"></path><path d=\"m12.54.637 8.25 2.675A1.75 1.75 0 0 1 22 4.976V10c0 6.19-3.771 10.704-9.401 12.83a1.704 1.704 0 0 1-1.198 0C5.77 20.705 2 16.19 2 10V4.976c0-.758.489-1.43 1.21-1.664L11.46.637a1.748 1.748 0 0 1 1.08 0Zm-.617 1.426-8.25 2.676a.249.249 0 0 0-.173.237V10c0 5.46 3.28 9.483 8.43 11.426a.199.199 0 0 0 .14 0C17.22 19.483 20.5 15.461 20.5 10V4.976a.25.25 0 0 0-.173-.237l-8.25-2.676a.253.253 0 0 0-.154 0Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Security</div>        Find and fix vulnerabilities      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}\" href=\"/features/codespaces\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-codespaces color-fg-subtle mr-3\">    <path d=\"M3.5 3.75C3.5 2.784 4.284 2 5.25 2h13.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 18.75 13H5.25a1.75 1.75 0 0 1-1.75-1.75Zm-2 12c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v4a1.75 1.75 0 0 1-1.75 1.75H3.25a1.75 1.75 0 0 1-1.75-1.75ZM5.25 3.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h13.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Zm-2 12a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h17.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25Z\"></path><path d=\"M10 17.75a.75.75 0 0 1 .75-.75h6.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Codespaces</div>        Instant dev environments      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}\" href=\"/features/copilot\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-copilot color-fg-subtle mr-3\">    <path d=\"M23.922 16.992c-.861 1.495-5.859 5.023-11.922 5.023-6.063 0-11.061-3.528-11.922-5.023A.641.641 0 0 1 0 16.736v-2.869a.841.841 0 0 1 .053-.22c.372-.935 1.347-2.292 2.605-2.656.167-.429.414-1.055.644-1.517a10.195 10.195 0 0 1-.052-1.086c0-1.331.282-2.499 1.132-3.368.397-.406.89-.717 1.474-.952 1.399-1.136 3.392-2.093 6.122-2.093 2.731 0 4.767.957 6.166 2.093.584.235 1.077.546 1.474.952.85.869 1.132 2.037 1.132 3.368 0 .368-.014.733-.052 1.086.23.462.477 1.088.644 1.517 1.258.364 2.233 1.721 2.605 2.656a.832.832 0 0 1 .053.22v2.869a.641.641 0 0 1-.078.256ZM12.172 11h-.344a4.323 4.323 0 0 1-.355.508C10.703 12.455 9.555 13 7.965 13c-1.725 0-2.989-.359-3.782-1.259a2.005 2.005 0 0 1-.085-.104L4 11.741v6.585c1.435.779 4.514 2.179 8 2.179 3.486 0 6.565-1.4 8-2.179v-6.585l-.098-.104s-.033.045-.085.104c-.793.9-2.057 1.259-3.782 1.259-1.59 0-2.738-.545-3.508-1.492a4.323 4.323 0 0 1-.355-.508h-.016.016Zm.641-2.935c.136 1.057.403 1.913.878 2.497.442.544 1.134.938 2.344.938 1.573 0 2.292-.337 2.657-.751.384-.435.558-1.15.558-2.361 0-1.14-.243-1.847-.705-2.319-.477-.488-1.319-.862-2.824-1.025-1.487-.161-2.192.138-2.533.529-.269.307-.437.808-.438 1.578v.021c0 .265.021.562.063.893Zm-1.626 0c.042-.331.063-.628.063-.894v-.02c-.001-.77-.169-1.271-.438-1.578-.341-.391-1.046-.69-2.533-.529-1.505.163-2.347.537-2.824 1.025-.462.472-.705 1.179-.705 2.319 0 1.211.175 1.926.558 2.361.365.414 1.084.751 2.657.751 1.21 0 1.902-.394 2.344-.938.475-.584.742-1.44.878-2.497Z\"></path><path d=\"M14.5 14.25a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Zm-5 0a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Copilot</div>        Write better code with AI      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}\" href=\"/features/code-review\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-code-review color-fg-subtle mr-3\">    <path d=\"M10.3 6.74a.75.75 0 0 1-.04 1.06l-2.908 2.7 2.908 2.7a.75.75 0 1 1-1.02 1.1l-3.5-3.25a.75.75 0 0 1 0-1.1l3.5-3.25a.75.75 0 0 1 1.06.04Zm3.44 1.06a.75.75 0 1 1 1.02-1.1l3.5 3.25a.75.75 0 0 1 0 1.1l-3.5 3.25a.75.75 0 1 1-1.02-1.1l2.908-2.7-2.908-2.7Z\"></path><path d=\"M1.5 4.25c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v12.5a1.75 1.75 0 0 1-1.75 1.75h-9.69l-3.573 3.573A1.458 1.458 0 0 1 5 21.043V18.5H3.25a1.75 1.75 0 0 1-1.75-1.75ZM3.25 4a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h2.5a.75.75 0 0 1 .75.75v3.19l3.72-3.72a.749.749 0 0 1 .53-.22h10a.25.25 0 0 0 .25-.25V4.25a.25.25 0 0 0-.25-.25Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Code review</div>        Manage code changes      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center pb-lg-3\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}\" href=\"/features/issues\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-issue-opened color-fg-subtle mr-3\">    <path d=\"M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1ZM2.5 12a9.5 9.5 0 0 0 9.5 9.5 9.5 9.5 0 0 0 9.5-9.5A9.5 9.5 0 0 0 12 2.5 9.5 9.5 0 0 0 2.5 12Zm9.5 2a2 2 0 1 1-.001-3.999A2 2 0 0 1 12 14Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Issues</div>        Plan and track work      </div>    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}\" href=\"/features/discussions\">      <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-comment-discussion color-fg-subtle mr-3\">    <path d=\"M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 14.25 14H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 15.543V14H1.75A1.75 1.75 0 0 1 0 12.25v-9.5C0 1.784.784 1 1.75 1ZM1.5 2.75v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Z\"></path><path d=\"M22.5 8.75a.25.25 0 0 0-.25-.25h-3.5a.75.75 0 0 1 0-1.5h3.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 22.25 20H21v1.543a1.457 1.457 0 0 1-2.487 1.03L15.939 20H10.75A1.75 1.75 0 0 1 9 18.25v-1.465a.75.75 0 0 1 1.5 0v1.465c0 .138.112.25.25.25h5.5a.75.75 0 0 1 .53.22l2.72 2.72v-2.19a.75.75 0 0 1 .75-.75h2a.25.25 0 0 0 .25-.25v-9.5Z\"></path></svg>      <div>        <div class=\"color-fg-default h4\">Discussions</div>        Collaborate outside of code      </div>    </a></li>            </ul>          </div>          <div class=\"px-lg-4\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"product-explore-heading\">Explore</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"product-explore-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to All features&quot;,&quot;label&quot;:&quot;ref_cta:All features;&quot;}\" href=\"/features\">      All features    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Documentation&quot;,&quot;label&quot;:&quot;ref_cta:Documentation;&quot;}\" href=\"https://docs.github.com\">      Documentation    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to GitHub Skills&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Skills;&quot;}\" href=\"https://skills.github.com/\">      GitHub Skills    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Blog&quot;,&quot;label&quot;:&quot;ref_cta:Blog;&quot;}\" href=\"https://github.blog\">      Blog    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>            </ul>          </div>      </div></li>                <li class=\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\">      <button type=\"button\" class=\"HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target\" aria-expanded=\"false\">        Solutions        <svg opacity=\"0.5\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-chevron-down HeaderMenu-icon ml-1\">    <path d=\"M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z\"></path></svg>      </button>      <div class=\"HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 px-lg-4\">          <div class=\"border-bottom pb-3 mb-3\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"solutions-for-heading\">For</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"solutions-for-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Enterprise&quot;,&quot;label&quot;:&quot;ref_cta:Enterprise;&quot;}\" href=\"/enterprise\">      Enterprise    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Teams&quot;,&quot;label&quot;:&quot;ref_cta:Teams;&quot;}\" href=\"/team\">      Teams    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Startups&quot;,&quot;label&quot;:&quot;ref_cta:Startups;&quot;}\" href=\"/enterprise/startups\">      Startups    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Education&quot;,&quot;label&quot;:&quot;ref_cta:Education;&quot;}\" href=\"https://education.github.com\">      Education    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>            </ul>          </div>          <div class=\"border-bottom pb-3 mb-3\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"solutions-by-solution-heading\">By Solution</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"solutions-by-solution-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to CI/CD &amp;amp; Automation&quot;,&quot;label&quot;:&quot;ref_cta:CI/CD &amp;amp; Automation;&quot;}\" href=\"/solutions/ci-cd/\">      CI/CD &amp; Automation    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to DevOps&quot;,&quot;label&quot;:&quot;ref_cta:DevOps;&quot;}\" href=\"/solutions/devops/\">      DevOps    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to DevSecOps&quot;,&quot;label&quot;:&quot;ref_cta:DevSecOps;&quot;}\" href=\"https://resources.github.com/devops/fundamentals/devsecops/\">      DevSecOps    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>            </ul>          </div>          <div class=\"\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"solutions-resources-heading\">Resources</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"solutions-resources-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Learning Pathways&quot;,&quot;label&quot;:&quot;ref_cta:Learning Pathways;&quot;}\" href=\"https://resources.github.com/learn/pathways/\">      Learning Pathways    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to White papers, Ebooks, Webinars&quot;,&quot;label&quot;:&quot;ref_cta:White papers, Ebooks, Webinars;&quot;}\" href=\"https://resources.github.com/\">      White papers, Ebooks, Webinars    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Customer Stories&quot;,&quot;label&quot;:&quot;ref_cta:Customer Stories;&quot;}\" href=\"/customer-stories\">      Customer Stories    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" target=\"_blank\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Partners&quot;,&quot;label&quot;:&quot;ref_cta:Partners;&quot;}\" href=\"https://partner.github.com/\">      Partners    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\">    <path d=\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\"></path></svg></a></li>            </ul>          </div>      </div></li>                <li class=\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\">      <button type=\"button\" class=\"HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-3 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target\" aria-expanded=\"false\">        Open Source        <svg opacity=\"0.5\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-chevron-down HeaderMenu-icon ml-1\">    <path d=\"M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z\"></path></svg>      </button>      <div class=\"HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 py-2 py-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 px-lg-4\">          <div class=\"border-bottom pb-3 mb-3\">            <ul class=\"list-style-none f5\" >                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}\" href=\"/sponsors\">            <div>        <div class=\"color-fg-default h4\">GitHub Sponsors</div>        Fund open source developers      </div>    </a></li>            </ul>          </div>          <div class=\"border-bottom pb-3 mb-3\">            <ul class=\"list-style-none f5\" >                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}\" href=\"/readme\">            <div>        <div class=\"color-fg-default h4\">The ReadME Project</div>        GitHub community articles      </div>    </a></li>            </ul>          </div>          <div class=\"\">              <span class=\"d-block h4 color-fg-default my-1\" id=\"open-source-repositories-heading\">Repositories</span>            <ul class=\"list-style-none f5\" aria-labelledby=\"open-source-repositories-heading\">                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Topics&quot;,&quot;label&quot;:&quot;ref_cta:Topics;&quot;}\" href=\"/topics\">      Topics    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Trending&quot;,&quot;label&quot;:&quot;ref_cta:Trending;&quot;}\" href=\"/trending\">      Trending    </a></li>                <li>  <a class=\"HeaderMenu-dropdown-link lh-condensed d-block no-underline position-relative py-2 Link--secondary\" data-analytics-event=\"{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Collections&quot;,&quot;label&quot;:&quot;ref_cta:Collections;&quot;}\" href=\"/collections\">      Collections    </a></li>            </ul>          </div>      </div></li>                <li class=\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\">    <a class=\"HeaderMenu-link no-underline px-0 px-lg-2 py-3 py-lg-2 d-block d-lg-inline-block\" data-analytics-event=\"{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}\" href=\"/pricing\">Pricing</a></li>            </ul>          </nav>        <div class=\"d-lg-flex flex-items-center mb-3 mb-lg-0 text-center text-lg-left ml-3\" style=\"\">                <qbsearch-input class=\"search-input\" data-scope=\"repo:lucidrains/vit-pytorch\" data-custom-scopes-path=\"/search/custom_scopes\" data-delete-custom-scopes-csrf=\"Q4MtD84pJUn1y5gIvNmHosSnY8CsXcIBbjBhwxIn4UIA1nbdt2XNoVKjGukaJD8XPB1dONib9BWy8JZ7c301_g\" data-max-custom-scopes=\"10\" data-header-redesign-enabled=\"false\" data-initial-value=\"\" data-blackbird-suggestions-path=\"/search/suggestions\" data-jump-to-suggestions-path=\"/_graphql/GetSuggestedNavigationDestinations\" data-current-repository=\"lucidrains/vit-pytorch\" data-current-org=\"\" data-current-owner=\"lucidrains\" data-logged-in=\"false\" data-copilot-chat-enabled=\"false\" data-blackbird-indexed-repo-csrf=\"<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=gGDITc1pJNbBHhoniVAVn3Cfp0UVYPZE6RG52UbU5OGLFBWGf1ylijDRcg6RoWcZjb4hkKK9BiEwFsCYZvAxCeMyQxT9I0qzpwPMXpB6u9a7RZdJxeimEWfdb65ruU0l%2B8n9mh2lB8fNFdlHF4NBflJ5gXHiGNSxX20hUMr4rilmYkqn4N%2F0gHh%2FjWZqs5uauGYdscxKQKAJcsq9jyUv7VBtVSpHKjHdUZ42DKjAdoazQVJ%2F838VG2VSyq7GGaUjB%2FBS1%2FrE%2FofWg5lMdvPyTH1GZgR%2Bd%2B5pkoip7wjBELdOF11gm9EkXcqq4Hi84svAccz78YQKoLtMIqF%2FYO6sTbWhLxsua8f9VtbXz7tKb6u%2FiJBcXRrFgbk8ALzZNhfypOgrw0ONcX3SPggAUi9PIhcJp52hoARfdX3EaIIRhovhDjejUJhNT%2FZTQGrI5y56K%2BfCArrsy296BHszMFoGzlM32IJzUEFzUyigntf2Dw9xzQuFh2PTd2bO%2BzpkeaDPFwBgRX5aqe5q2echwX%2B37dDJqWHaDw%3D%3D--p4gnCrKZPgOSC6hN--2gNsGydQHje9E%2Bdd4yUGlQ%3D%3D&quot; />\">  <div    class=\"search-input-container search-with-dialog position-relative d-flex flex-row flex-items-center mr-4 rounded\"    data-action=\"click:qbsearch-input#searchInputContainerClicked\"  >      <button        type=\"button\"        class=\"header-search-button placeholder  input-button form-control d-flex flex-1 flex-self-stretch flex-items-center no-wrap width-full py-0 pl-2 pr-0 text-left border-0 box-shadow-none\"        data-target=\"qbsearch-input.inputButton\"        placeholder=\"Search or jump to...\"        data-hotkey=s,/        autocapitalize=\"off\"        data-action=\"click:qbsearch-input#handleExpand\"      >        <div class=\"mr-2 color-fg-muted\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-search\">    <path d=\"M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z\"></path></svg>        </div>        <span class=\"flex-1\" data-target=\"qbsearch-input.inputButtonText\">Search or jump to...</span>          <div class=\"d-flex\" data-target=\"qbsearch-input.hotkeyIndicator\">            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"22\" height=\"20\" aria-hidden=\"true\" class=\"mr-1\"><path fill=\"none\" stroke=\"#979A9C\" opacity=\".4\" d=\"M3.5.5h12c1.7 0 3 1.3 3 3v13c0 1.7-1.3 3-3 3h-12c-1.7 0-3-1.3-3-3v-13c0-1.7 1.3-3 3-3z\"></path><path fill=\"#979A9C\" d=\"M11.8 6L8 15.1h-.9L10.8 6h1z\"></path></svg>          </div>      </button>    <input type=\"hidden\" name=\"type\" class=\"js-site-search-type-field\">    <div class=\"Overlay--hidden \" data-modal-dialog-overlay>  <modal-dialog data-action=\"close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose\" data-target=\"qbsearch-input.searchSuggestionsDialog\" role=\"dialog\" id=\"search-suggestions-dialog\" aria-modal=\"true\" aria-labelledby=\"search-suggestions-dialog-header\" data-view-component=\"true\" class=\"Overlay Overlay--width-large Overlay--height-auto\">      <h1 id=\"search-suggestions-dialog-header\" class=\"sr-only\">Search code, repositories, users, issues, pull requests...</h1>    <div class=\"Overlay-body Overlay-body--paddingNone\">                <div data-view-component=\"true\">        <div class=\"search-suggestions position-fixed width-full color-shadow-large border color-fg-default color-bg-default overflow-hidden d-flex flex-column query-builder-container\"          style=\"border-radius: 12px;\"          data-target=\"qbsearch-input.queryBuilderContainer\"          hidden        >          <!-- '\"` --><!-- </textarea></xmp> --></option></form><form id=\"query-builder-test-form\" action=\"\" accept-charset=\"UTF-8\" method=\"get\">  <query-builder data-target=\"qbsearch-input.queryBuilder\" id=\"query-builder-query-builder-test\" data-filter-key=\":\" data-view-component=\"true\" class=\"QueryBuilder search-query-builder\">    <div class=\"FormControl FormControl--fullWidth\">      <label id=\"query-builder-test-label\" for=\"query-builder-test\" class=\"FormControl-label sr-only\">        Search      </label>      <div        class=\"QueryBuilder-StyledInput width-fit \"        data-target=\"query-builder.styledInput\"      >          <span id=\"query-builder-test-leadingvisual-wrap\" class=\"FormControl-input-leadingVisualWrap QueryBuilder-leadingVisualWrap\">            <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-search FormControl-input-leadingVisual\">    <path d=\"M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z\"></path></svg>          </span>        <div data-target=\"query-builder.styledInputContainer\" class=\"QueryBuilder-StyledInputContainer\">          <div            aria-hidden=\"true\"            class=\"QueryBuilder-StyledInputContent\"            data-target=\"query-builder.styledInputContent\"          ></div>          <div class=\"QueryBuilder-InputWrapper\">            <div aria-hidden=\"true\" class=\"QueryBuilder-Sizer\" data-target=\"query-builder.sizer\"></div>            <input id=\"query-builder-test\" name=\"query-builder-test\" value=\"\" autocomplete=\"off\" type=\"text\" role=\"combobox\" spellcheck=\"false\" aria-expanded=\"false\" aria-describedby=\"validation-64e2a9b6-4d89-40f1-a366-f7edb4b78542\" data-target=\"query-builder.input\" data-action=\"          input:query-builder#inputChange          blur:query-builder#inputBlur          keydown:query-builder#inputKeydown          focus:query-builder#inputFocus        \" data-view-component=\"true\" class=\"FormControl-input QueryBuilder-Input FormControl-medium\" />          </div>        </div>          <span class=\"sr-only\" id=\"query-builder-test-clear\">Clear</span>          <button role=\"button\" id=\"query-builder-test-clear-button\" aria-labelledby=\"query-builder-test-clear query-builder-test-label\" data-target=\"query-builder.clearButton\" data-action=\"                click:query-builder#clear                focus:query-builder#clearButtonFocus                blur:query-builder#clearButtonBlur              \" variant=\"small\" hidden=\"hidden\" type=\"button\" data-view-component=\"true\" class=\"Button Button--iconOnly Button--invisible Button--medium mr-1 px-2 py-0 d-flex flex-items-center rounded-1 color-fg-muted\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x-circle-fill Button-visual\">    <path d=\"M2.343 13.657A8 8 0 1 1 13.658 2.343 8 8 0 0 1 2.343 13.657ZM6.03 4.97a.751.751 0 0 0-1.042.018.751.751 0 0 0-.018 1.042L6.94 8 4.97 9.97a.749.749 0 0 0 .326 1.275.749.749 0 0 0 .734-.215L8 9.06l1.97 1.97a.749.749 0 0 0 1.275-.326.749.749 0 0 0-.215-.734L9.06 8l1.97-1.97a.749.749 0 0 0-.326-1.275.749.749 0 0 0-.734.215L8 6.94Z\"></path></svg></button>      </div>      <template id=\"search-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-search\">    <path d=\"M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z\"></path></svg></template><template id=\"code-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code\">    <path d=\"m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z\"></path></svg></template><template id=\"file-code-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-file-code\">    <path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0 1 14.25 15h-9a.75.75 0 0 1 0-1.5h9a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 10 4.25V1.5H5.75a.25.25 0 0 0-.25.25v2.5a.75.75 0 0 1-1.5 0Zm1.72 4.97a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734l1.47-1.47-1.47-1.47a.75.75 0 0 1 0-1.06ZM3.28 7.78 1.81 9.25l1.47 1.47a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018l-2-2a.75.75 0 0 1 0-1.06l2-2a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Zm8.22-6.218V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg></template><template id=\"history-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-history\">    <path d=\"m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z\"></path></svg></template><template id=\"repo-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo\">    <path d=\"M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z\"></path></svg></template><template id=\"bookmark-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-bookmark\">    <path d=\"M3 2.75C3 1.784 3.784 1 4.75 1h6.5c.966 0 1.75.784 1.75 1.75v11.5a.75.75 0 0 1-1.227.579L8 11.722l-3.773 3.107A.751.751 0 0 1 3 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.91l3.023-2.489a.75.75 0 0 1 .954 0l3.023 2.49V2.75a.25.25 0 0 0-.25-.25Z\"></path></svg></template><template id=\"plus-circle-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-plus-circle\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm7.25-3.25v2.5h2.5a.75.75 0 0 1 0 1.5h-2.5v2.5a.75.75 0 0 1-1.5 0v-2.5h-2.5a.75.75 0 0 1 0-1.5h2.5v-2.5a.75.75 0 0 1 1.5 0Z\"></path></svg></template><template id=\"circle-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-dot-fill\">    <path d=\"M8 4a4 4 0 1 1 0 8 4 4 0 0 1 0-8Z\"></path></svg></template><template id=\"trash-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-trash\">    <path d=\"M11 1.75V3h2.25a.75.75 0 0 1 0 1.5H2.75a.75.75 0 0 1 0-1.5H5V1.75C5 .784 5.784 0 6.75 0h2.5C10.216 0 11 .784 11 1.75ZM4.496 6.675l.66 6.6a.25.25 0 0 0 .249.225h5.19a.25.25 0 0 0 .249-.225l.66-6.6a.75.75 0 0 1 1.492.149l-.66 6.6A1.748 1.748 0 0 1 10.595 15h-5.19a1.75 1.75 0 0 1-1.741-1.575l-.66-6.6a.75.75 0 1 1 1.492-.15ZM6.5 1.75V3h3V1.75a.25.25 0 0 0-.25-.25h-2.5a.25.25 0 0 0-.25.25Z\"></path></svg></template><template id=\"team-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-people\">    <path d=\"M2 5.5a3.5 3.5 0 1 1 5.898 2.549 5.508 5.508 0 0 1 3.034 4.084.75.75 0 1 1-1.482.235 4 4 0 0 0-7.9 0 .75.75 0 0 1-1.482-.236A5.507 5.507 0 0 1 3.102 8.05 3.493 3.493 0 0 1 2 5.5ZM11 4a3.001 3.001 0 0 1 2.22 5.018 5.01 5.01 0 0 1 2.56 3.012.749.749 0 0 1-.885.954.752.752 0 0 1-.549-.514 3.507 3.507 0 0 0-2.522-2.372.75.75 0 0 1-.574-.73v-.352a.75.75 0 0 1 .416-.672A1.5 1.5 0 0 0 11 5.5.75.75 0 0 1 11 4Zm-5.5-.5a2 2 0 1 0-.001 3.999A2 2 0 0 0 5.5 3.5Z\"></path></svg></template><template id=\"project-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-project\">    <path d=\"M1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0ZM1.5 1.75v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25ZM11.75 3a.75.75 0 0 1 .75.75v7.5a.75.75 0 0 1-1.5 0v-7.5a.75.75 0 0 1 .75-.75Zm-8.25.75a.75.75 0 0 1 1.5 0v5.5a.75.75 0 0 1-1.5 0ZM8 3a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 3Z\"></path></svg></template><template id=\"pencil-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-pencil\">    <path d=\"M11.013 1.427a1.75 1.75 0 0 1 2.474 0l1.086 1.086a1.75 1.75 0 0 1 0 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 0 1-.927-.928l.929-3.25c.081-.286.235-.547.445-.758l8.61-8.61Zm.176 4.823L9.75 4.81l-6.286 6.287a.253.253 0 0 0-.064.108l-.558 1.953 1.953-.558a.253.253 0 0 0 .108-.064Zm1.238-3.763a.25.25 0 0 0-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 0 0 0-.354Z\"></path></svg></template><template id=\"copilot-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copilot\">    <path d=\"M7.998 15.035c-4.562 0-7.873-2.914-7.998-3.749V9.338c.085-.628.677-1.686 1.588-2.065.013-.07.024-.143.036-.218.029-.183.06-.384.126-.612-.201-.508-.254-1.084-.254-1.656 0-.87.128-1.769.693-2.484.579-.733 1.494-1.124 2.724-1.261 1.206-.134 2.262.034 2.944.765.05.053.096.108.139.165.044-.057.094-.112.143-.165.682-.731 1.738-.899 2.944-.765 1.23.137 2.145.528 2.724 1.261.566.715.693 1.614.693 2.484 0 .572-.053 1.148-.254 1.656.066.228.098.429.126.612.012.076.024.148.037.218.924.385 1.522 1.471 1.591 2.095v1.872c0 .766-3.351 3.795-8.002 3.795Zm0-1.485c2.28 0 4.584-1.11 5.002-1.433V7.862l-.023-.116c-.49.21-1.075.291-1.727.291-1.146 0-2.059-.327-2.71-.991A3.222 3.222 0 0 1 8 6.303a3.24 3.24 0 0 1-.544.743c-.65.664-1.563.991-2.71.991-.652 0-1.236-.081-1.727-.291l-.023.116v4.255c.419.323 2.722 1.433 5.002 1.433ZM6.762 2.83c-.193-.206-.637-.413-1.682-.297-1.019.113-1.479.404-1.713.7-.247.312-.369.789-.369 1.554 0 .793.129 1.171.308 1.371.162.181.519.379 1.442.379.853 0 1.339-.235 1.638-.54.315-.322.527-.827.617-1.553.117-.935-.037-1.395-.241-1.614Zm4.155-.297c-1.044-.116-1.488.091-1.681.297-.204.219-.359.679-.242 1.614.091.726.303 1.231.618 1.553.299.305.784.54 1.638.54.922 0 1.28-.198 1.442-.379.179-.2.308-.578.308-1.371 0-.765-.123-1.242-.37-1.554-.233-.296-.693-.587-1.713-.7Z\"></path><path d=\"M6.25 9.037a.75.75 0 0 1 .75.75v1.501a.75.75 0 0 1-1.5 0V9.787a.75.75 0 0 1 .75-.75Zm4.25.75v1.501a.75.75 0 0 1-1.5 0V9.787a.75.75 0 0 1 1.5 0Z\"></path></svg></template><template id=\"workflow-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-workflow\">    <path d=\"M0 1.75C0 .784.784 0 1.75 0h3.5C6.216 0 7 .784 7 1.75v3.5A1.75 1.75 0 0 1 5.25 7H4v4a1 1 0 0 0 1 1h4v-1.25C9 9.784 9.784 9 10.75 9h3.5c.966 0 1.75.784 1.75 1.75v3.5A1.75 1.75 0 0 1 14.25 16h-3.5A1.75 1.75 0 0 1 9 14.25v-.75H5A2.5 2.5 0 0 1 2.5 11V7h-.75A1.75 1.75 0 0 1 0 5.25Zm1.75-.25a.25.25 0 0 0-.25.25v3.5c0 .138.112.25.25.25h3.5a.25.25 0 0 0 .25-.25v-3.5a.25.25 0 0 0-.25-.25Zm9 9a.25.25 0 0 0-.25.25v3.5c0 .138.112.25.25.25h3.5a.25.25 0 0 0 .25-.25v-3.5a.25.25 0 0 0-.25-.25Z\"></path></svg></template><template id=\"book-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-book\">    <path d=\"M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z\"></path></svg></template><template id=\"code-review-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code-review\">    <path d=\"M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 13H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25v-8.5C0 1.784.784 1 1.75 1ZM1.5 2.75v8.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-8.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Zm5.28 1.72a.75.75 0 0 1 0 1.06L5.31 7l1.47 1.47a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018l-2-2a.75.75 0 0 1 0-1.06l2-2a.75.75 0 0 1 1.06 0Zm2.44 0a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L10.69 7 9.22 5.53a.75.75 0 0 1 0-1.06Z\"></path></svg></template><template id=\"codespaces-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-codespaces\">    <path d=\"M0 11.25c0-.966.784-1.75 1.75-1.75h12.5c.966 0 1.75.784 1.75 1.75v3A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25Zm2-9.5C2 .784 2.784 0 3.75 0h8.5C13.216 0 14 .784 14 1.75v5a1.75 1.75 0 0 1-1.75 1.75h-8.5A1.75 1.75 0 0 1 2 6.75Zm1.75-.25a.25.25 0 0 0-.25.25v5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-5a.25.25 0 0 0-.25-.25Zm-2 9.5a.25.25 0 0 0-.25.25v3c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-3a.25.25 0 0 0-.25-.25Z\"></path><path d=\"M7 12.75a.75.75 0 0 1 .75-.75h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z\"></path></svg></template><template id=\"comment-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-comment\">    <path d=\"M1 2.75C1 1.784 1.784 1 2.75 1h10.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 13.25 12H9.06l-2.573 2.573A1.458 1.458 0 0 1 4 13.543V12H2.75A1.75 1.75 0 0 1 1 10.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h4.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path></svg></template><template id=\"comment-discussion-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-comment-discussion\">    <path d=\"M1.75 1h8.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 10.25 10H7.061l-2.574 2.573A1.458 1.458 0 0 1 2 11.543V10h-.25A1.75 1.75 0 0 1 0 8.25v-5.5C0 1.784.784 1 1.75 1ZM1.5 2.75v5.5c0 .138.112.25.25.25h1a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h3.5a.25.25 0 0 0 .25-.25v-5.5a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25Zm13 2a.25.25 0 0 0-.25-.25h-.5a.75.75 0 0 1 0-1.5h.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 14.25 12H14v1.543a1.458 1.458 0 0 1-2.487 1.03L9.22 12.28a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l2.22 2.22v-2.19a.75.75 0 0 1 .75-.75h1a.25.25 0 0 0 .25-.25Z\"></path></svg></template><template id=\"organization-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-organization\">    <path d=\"M1.75 16A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0h8.5C11.216 0 12 .784 12 1.75v12.5c0 .085-.006.168-.018.25h2.268a.25.25 0 0 0 .25-.25V8.285a.25.25 0 0 0-.111-.208l-1.055-.703a.749.749 0 1 1 .832-1.248l1.055.703c.487.325.779.871.779 1.456v5.965A1.75 1.75 0 0 1 14.25 16h-3.5a.766.766 0 0 1-.197-.026c-.099.017-.2.026-.303.026h-3a.75.75 0 0 1-.75-.75V14h-1v1.25a.75.75 0 0 1-.75.75Zm-.25-1.75c0 .138.112.25.25.25H4v-1.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 .75.75v1.25h2.25a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25ZM3.75 6h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5ZM3 3.75A.75.75 0 0 1 3.75 3h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 3.75Zm4 3A.75.75 0 0 1 7.75 6h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 7 6.75ZM7.75 3h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5ZM3 9.75A.75.75 0 0 1 3.75 9h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 9.75ZM7.75 9h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5Z\"></path></svg></template><template id=\"rocket-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-rocket\">    <path d=\"M14.064 0h.186C15.216 0 16 .784 16 1.75v.186a8.752 8.752 0 0 1-2.564 6.186l-.458.459c-.314.314-.641.616-.979.904v3.207c0 .608-.315 1.172-.833 1.49l-2.774 1.707a.749.749 0 0 1-1.11-.418l-.954-3.102a1.214 1.214 0 0 1-.145-.125L3.754 9.816a1.218 1.218 0 0 1-.124-.145L.528 8.717a.749.749 0 0 1-.418-1.11l1.71-2.774A1.748 1.748 0 0 1 3.31 4h3.204c.288-.338.59-.665.904-.979l.459-.458A8.749 8.749 0 0 1 14.064 0ZM8.938 3.623h-.002l-.458.458c-.76.76-1.437 1.598-2.02 2.5l-1.5 2.317 2.143 2.143 2.317-1.5c.902-.583 1.74-1.26 2.499-2.02l.459-.458a7.25 7.25 0 0 0 2.123-5.127V1.75a.25.25 0 0 0-.25-.25h-.186a7.249 7.249 0 0 0-5.125 2.123ZM3.56 14.56c-.732.732-2.334 1.045-3.005 1.148a.234.234 0 0 1-.201-.064.234.234 0 0 1-.064-.201c.103-.671.416-2.273 1.15-3.003a1.502 1.502 0 1 1 2.12 2.12Zm6.94-3.935c-.088.06-.177.118-.266.175l-2.35 1.521.548 1.783 1.949-1.2a.25.25 0 0 0 .119-.213ZM3.678 8.116 5.2 5.766c.058-.09.117-.178.176-.266H3.309a.25.25 0 0 0-.213.119l-1.2 1.95ZM12 5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg></template><template id=\"shield-check-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-shield-check\">    <path d=\"m8.533.133 5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667l5.25-1.68a1.748 1.748 0 0 1 1.066 0Zm-.61 1.429.001.001-5.25 1.68a.251.251 0 0 0-.174.237V7c0 1.36.275 2.666 1.057 3.859.784 1.194 2.121 2.342 4.366 3.298a.196.196 0 0 0 .154 0c2.245-.957 3.582-2.103 4.366-3.297C13.225 9.666 13.5 8.358 13.5 7V3.48a.25.25 0 0 0-.174-.238l-5.25-1.68a.25.25 0 0 0-.153 0ZM11.28 6.28l-3.5 3.5a.75.75 0 0 1-1.06 0l-1.5-1.5a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l.97.97 2.97-2.97a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z\"></path></svg></template><template id=\"heart-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-heart\">    <path d=\"m8 14.25.345.666a.75.75 0 0 1-.69 0l-.008-.004-.018-.01a7.152 7.152 0 0 1-.31-.17 22.055 22.055 0 0 1-3.434-2.414C2.045 10.731 0 8.35 0 5.5 0 2.836 2.086 1 4.25 1 5.797 1 7.153 1.802 8 3.02 8.847 1.802 10.203 1 11.75 1 13.914 1 16 2.836 16 5.5c0 2.85-2.045 5.231-3.885 6.818a22.066 22.066 0 0 1-3.744 2.584l-.018.01-.006.003h-.002ZM4.25 2.5c-1.336 0-2.75 1.164-2.75 3 0 2.15 1.58 4.144 3.365 5.682A20.58 20.58 0 0 0 8 13.393a20.58 20.58 0 0 0 3.135-2.211C12.92 9.644 14.5 7.65 14.5 5.5c0-1.836-1.414-3-2.75-3-1.373 0-2.609.986-3.029 2.456a.749.749 0 0 1-1.442 0C6.859 3.486 5.623 2.5 4.25 2.5Z\"></path></svg></template><template id=\"server-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-server\">    <path d=\"M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v4c0 .372-.116.717-.314 1 .198.283.314.628.314 1v4a1.75 1.75 0 0 1-1.75 1.75H1.75A1.75 1.75 0 0 1 0 12.75v-4c0-.358.109-.707.314-1a1.739 1.739 0 0 1-.314-1v-4C0 1.784.784 1 1.75 1ZM1.5 2.75v4c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Zm.25 5.75a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25ZM7 4.75A.75.75 0 0 1 7.75 4h4.5a.75.75 0 0 1 0 1.5h-4.5A.75.75 0 0 1 7 4.75ZM7.75 10h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM3 4.75A.75.75 0 0 1 3.75 4h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 4.75ZM3.75 10h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5Z\"></path></svg></template><template id=\"globe-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-globe\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM5.78 8.75a9.64 9.64 0 0 0 1.363 4.177c.255.426.542.832.857 1.215.245-.296.551-.705.857-1.215A9.64 9.64 0 0 0 10.22 8.75Zm4.44-1.5a9.64 9.64 0 0 0-1.363-4.177c-.307-.51-.612-.919-.857-1.215a9.927 9.927 0 0 0-.857 1.215A9.64 9.64 0 0 0 5.78 7.25Zm-5.944 1.5H1.543a6.507 6.507 0 0 0 4.666 5.5c-.123-.181-.24-.365-.352-.552-.715-1.192-1.437-2.874-1.581-4.948Zm-2.733-1.5h2.733c.144-2.074.866-3.756 1.58-4.948.12-.197.237-.381.353-.552a6.507 6.507 0 0 0-4.666 5.5Zm10.181 1.5c-.144 2.074-.866 3.756-1.58 4.948-.12.197-.237.381-.353.552a6.507 6.507 0 0 0 4.666-5.5Zm2.733-1.5a6.507 6.507 0 0 0-4.666-5.5c.123.181.24.365.353.552.714 1.192 1.436 2.874 1.58 4.948Z\"></path></svg></template><template id=\"issue-opened-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-issue-opened\">    <path d=\"M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path><path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z\"></path></svg></template><template id=\"device-mobile-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-device-mobile\">    <path d=\"M3.75 0h8.5C13.216 0 14 .784 14 1.75v12.5A1.75 1.75 0 0 1 12.25 16h-8.5A1.75 1.75 0 0 1 2 14.25V1.75C2 .784 2.784 0 3.75 0ZM3.5 1.75v12.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25ZM8 13a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z\"></path></svg></template><template id=\"package-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-package\">    <path d=\"m8.878.392 5.25 3.045c.54.314.872.89.872 1.514v6.098a1.75 1.75 0 0 1-.872 1.514l-5.25 3.045a1.75 1.75 0 0 1-1.756 0l-5.25-3.045A1.75 1.75 0 0 1 1 11.049V4.951c0-.624.332-1.201.872-1.514L7.122.392a1.75 1.75 0 0 1 1.756 0ZM7.875 1.69l-4.63 2.685L8 7.133l4.755-2.758-4.63-2.685a.248.248 0 0 0-.25 0ZM2.5 5.677v5.372c0 .09.047.171.125.216l4.625 2.683V8.432Zm6.25 8.271 4.625-2.683a.25.25 0 0 0 .125-.216V5.677L8.75 8.432Z\"></path></svg></template><template id=\"credit-card-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-credit-card\">    <path d=\"M10.75 9a.75.75 0 0 0 0 1.5h1.5a.75.75 0 0 0 0-1.5h-1.5Z\"></path><path d=\"M0 3.75C0 2.784.784 2 1.75 2h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 14H1.75A1.75 1.75 0 0 1 0 12.25ZM14.5 6.5h-13v5.75c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25Zm0-2.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25V5h13Z\"></path></svg></template><template id=\"play-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-play\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z\"></path></svg></template><template id=\"gift-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-gift\">    <path d=\"M2 2.75A2.75 2.75 0 0 1 4.75 0c.983 0 1.873.42 2.57 1.232.268.318.497.668.68 1.042.183-.375.411-.725.68-1.044C9.376.42 10.266 0 11.25 0a2.75 2.75 0 0 1 2.45 4h.55c.966 0 1.75.784 1.75 1.75v2c0 .698-.409 1.301-1 1.582v4.918A1.75 1.75 0 0 1 13.25 16H2.75A1.75 1.75 0 0 1 1 14.25V9.332C.409 9.05 0 8.448 0 7.75v-2C0 4.784.784 4 1.75 4h.55c-.192-.375-.3-.8-.3-1.25ZM7.25 9.5H2.5v4.75c0 .138.112.25.25.25h4.5Zm1.5 0v5h4.5a.25.25 0 0 0 .25-.25V9.5Zm0-4V8h5.5a.25.25 0 0 0 .25-.25v-2a.25.25 0 0 0-.25-.25Zm-7 0a.25.25 0 0 0-.25.25v2c0 .138.112.25.25.25h5.5V5.5h-5.5Zm3-4a1.25 1.25 0 0 0 0 2.5h2.309c-.233-.818-.542-1.401-.878-1.793-.43-.502-.915-.707-1.431-.707ZM8.941 4h2.309a1.25 1.25 0 0 0 0-2.5c-.516 0-1 .205-1.43.707-.337.392-.646.975-.879 1.793Z\"></path></svg></template><template id=\"code-square-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code-square\">    <path d=\"M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25Zm7.47 3.97a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L10.69 8 9.22 6.53a.75.75 0 0 1 0-1.06ZM6.78 6.53 5.31 8l1.47 1.47a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215l-2-2a.75.75 0 0 1 0-1.06l2-2a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z\"></path></svg></template><template id=\"device-desktop-icon\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-device-desktop\">    <path d=\"M14.25 1c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 14.25 12h-3.727c.099 1.041.52 1.872 1.292 2.757A.752.752 0 0 1 11.25 16h-6.5a.75.75 0 0 1-.565-1.243c.772-.885 1.192-1.716 1.292-2.757H1.75A1.75 1.75 0 0 1 0 10.25v-7.5C0 1.784.784 1 1.75 1ZM1.75 2.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25ZM9.018 12H6.982a5.72 5.72 0 0 1-.765 2.5h3.566a5.72 5.72 0 0 1-.765-2.5Z\"></path></svg></template>        <div class=\"position-relative\">                <ul                  role=\"listbox\"                  class=\"ActionListWrap QueryBuilder-ListWrap\"                  aria-label=\"Suggestions\"                  data-action=\"                    combobox-commit:query-builder#comboboxCommit                    mousedown:query-builder#resultsMousedown                  \"                  data-target=\"query-builder.resultsList\"                  data-persist-list=false                  id=\"query-builder-test-results\"                ></ul>        </div>      <div class=\"FormControl-inlineValidation\" id=\"validation-64e2a9b6-4d89-40f1-a366-f7edb4b78542\" hidden=\"hidden\">        <span class=\"FormControl-inlineValidation--visual\">          <svg aria-hidden=\"true\" height=\"12\" viewBox=\"0 0 12 12\" version=\"1.1\" width=\"12\" data-view-component=\"true\" class=\"octicon octicon-alert-fill\">    <path d=\"M4.855.708c.5-.896 1.79-.896 2.29 0l4.675 8.351a1.312 1.312 0 0 1-1.146 1.954H1.33A1.313 1.313 0 0 1 .183 9.058ZM7 7V3H5v4Zm-1 3a1 1 0 1 0 0-2 1 1 0 0 0 0 2Z\"></path></svg>        </span>        <span></span></div>    </div>    <div data-target=\"query-builder.screenReaderFeedback\" aria-live=\"polite\" aria-atomic=\"true\" class=\"sr-only\"></div></query-builder></form>          <div class=\"d-flex flex-row color-fg-muted px-3 text-small color-bg-default search-feedback-prompt\">            <a target=\"_blank\" href=\"https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax\" data-view-component=\"true\" class=\"Link color-fg-accent text-normal ml-2\">              Search syntax tips</a>            <div class=\"d-flex flex-1\"></div>          </div>        </div></div>    </div></modal-dialog></div>  </div>  <div data-action=\"click:qbsearch-input#retract\" class=\"dark-backdrop position-fixed\" hidden data-target=\"qbsearch-input.darkBackdrop\"></div>  <div class=\"color-fg-default\">    <dialog-helper>  <dialog data-target=\"qbsearch-input.feedbackDialog\" data-action=\"close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose\" id=\"feedback-dialog\" aria-modal=\"true\" aria-labelledby=\"feedback-dialog-title\" aria-describedby=\"feedback-dialog-description\" data-view-component=\"true\" class=\"Overlay Overlay-whenNarrow Overlay--size-medium Overlay--motion-scaleFade\">    <div data-view-component=\"true\" class=\"Overlay-header\">  <div class=\"Overlay-headerContentWrap\">    <div class=\"Overlay-titleWrap\">      <h1 class=\"Overlay-title \" id=\"feedback-dialog-title\">        Provide feedback      </h1>    </div>    <div class=\"Overlay-actionWrap\">      <button data-close-dialog-id=\"feedback-dialog\" aria-label=\"Close\" type=\"button\" data-view-component=\"true\" class=\"close-button Overlay-closeButton\"><svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg></button>    </div>  </div></div>      <scrollable-region data-labelled-by=\"feedback-dialog-title\">        <div data-view-component=\"true\" class=\"Overlay-body\">        <!-- '\"` --><!-- </textarea></xmp> --></option></form><form id=\"code-search-feedback-form\" data-turbo=\"false\" action=\"/search/feedback\" accept-charset=\"UTF-8\" method=\"post\"><input type=\"hidden\" data-csrf=\"true\" name=\"authenticity_token\" value=\"2XNbF0fwJIArdqtCs8ieFt04YJ3rnAqfYkttivSvCH+HnwZg6d7gCANfggqDR7uP4MF7qGdMsMyUGdXk3Jzj6g==\" />          <p>We read every piece of feedback, and take your input very seriously.</p>          <textarea name=\"feedback\" class=\"form-control width-full mb-2\" style=\"height: 120px\" id=\"feedback\"></textarea>          <input name=\"include_email\" id=\"include_email\" aria-label=\"Include my email address so I can be contacted\" class=\"form-control mr-2\" type=\"checkbox\">          <label for=\"include_email\" style=\"font-weight: normal\">Include my email address so I can be contacted</label></form></div>      </scrollable-region>      <div data-view-component=\"true\" class=\"Overlay-footer Overlay-footer--alignEnd\">          <button data-close-dialog-id=\"feedback-dialog\" type=\"button\" data-view-component=\"true\" class=\"btn\">    Cancel</button>          <button form=\"code-search-feedback-form\" data-action=\"click:qbsearch-input#submitFeedback\" type=\"submit\" data-view-component=\"true\" class=\"btn-primary btn\">    Submit feedback</button></div></dialog></dialog-helper>    <custom-scopes data-target=\"qbsearch-input.customScopesManager\">    <dialog-helper>  <dialog data-target=\"custom-scopes.customScopesModalDialog\" data-action=\"close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose\" id=\"custom-scopes-dialog\" aria-modal=\"true\" aria-labelledby=\"custom-scopes-dialog-title\" aria-describedby=\"custom-scopes-dialog-description\" data-view-component=\"true\" class=\"Overlay Overlay-whenNarrow Overlay--size-medium Overlay--motion-scaleFade\">    <div data-view-component=\"true\" class=\"Overlay-header Overlay-header--divided\">  <div class=\"Overlay-headerContentWrap\">    <div class=\"Overlay-titleWrap\">      <h1 class=\"Overlay-title \" id=\"custom-scopes-dialog-title\">        Saved searches      </h1>        <h2 id=\"custom-scopes-dialog-description\" class=\"Overlay-description\">Use saved searches to filter your results more quickly</h2>    </div>    <div class=\"Overlay-actionWrap\">      <button data-close-dialog-id=\"custom-scopes-dialog\" aria-label=\"Close\" type=\"button\" data-view-component=\"true\" class=\"close-button Overlay-closeButton\"><svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg></button>    </div>  </div></div>      <scrollable-region data-labelled-by=\"custom-scopes-dialog-title\">        <div data-view-component=\"true\" class=\"Overlay-body\">        <div data-target=\"custom-scopes.customScopesModalDialogFlash\"></div>        <div hidden class=\"create-custom-scope-form\" data-target=\"custom-scopes.createCustomScopeForm\">        <!-- '\"` --><!-- </textarea></xmp> --></option></form><form id=\"custom-scopes-dialog-form\" data-turbo=\"false\" action=\"/search/custom_scopes\" accept-charset=\"UTF-8\" method=\"post\"><input type=\"hidden\" data-csrf=\"true\" name=\"authenticity_token\" value=\"dHi71tThJAtHaotASgLZsb5U6/7gE3+C1FsCYBnK2JcKQb9HcJ/SM0AzEgM8/z7xTZbx/bdoWWew0RzwCnMqxg==\" />          <div data-target=\"custom-scopes.customScopesModalDialogFlash\"></div>          <input type=\"hidden\" id=\"custom_scope_id\" name=\"custom_scope_id\" data-target=\"custom-scopes.customScopesIdField\">          <div class=\"form-group\">            <label for=\"custom_scope_name\">Name</label>            <auto-check src=\"/search/custom_scopes/check_name\" required>              <input                type=\"text\"                name=\"custom_scope_name\"                id=\"custom_scope_name\"                data-target=\"custom-scopes.customScopesNameField\"                class=\"form-control\"                autocomplete=\"off\"                placeholder=\"github-ruby\"                required                maxlength=\"50\">              <input type=\"hidden\" data-csrf=\"true\" value=\"7TgcDoE4w2qDupUI310uSce8mr+S00vuKITl9JzQRMiE5k1h7zaodWJ+p0JpNMZOmP+/eD1V7O+Nm+IB7SyYZQ==\" />            </auto-check>          </div>          <div class=\"form-group\">            <label for=\"custom_scope_query\">Query</label>            <input              type=\"text\"              name=\"custom_scope_query\"              id=\"custom_scope_query\"              data-target=\"custom-scopes.customScopesQueryField\"              class=\"form-control\"              autocomplete=\"off\"              placeholder=\"(repo:mona/a OR repo:mona/b) AND lang:python\"              required              maxlength=\"500\">          </div>          <p class=\"text-small color-fg-muted\">            To see all available qualifiers, see our <a class=\"Link--inTextBlock\" href=\"https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax\">documentation</a>.          </p></form>        </div>        <div data-target=\"custom-scopes.manageCustomScopesForm\">          <div data-target=\"custom-scopes.list\"></div>        </div></div>      </scrollable-region>      <div data-view-component=\"true\" class=\"Overlay-footer Overlay-footer--alignEnd Overlay-footer--divided\">          <button data-action=\"click:custom-scopes#customScopesCancel\" type=\"button\" data-view-component=\"true\" class=\"btn\">    Cancel</button>          <button form=\"custom-scopes-dialog-form\" data-action=\"click:custom-scopes#customScopesSubmit\" data-target=\"custom-scopes.customScopesSubmitButton\" type=\"submit\" data-view-component=\"true\" class=\"btn-primary btn\">    Create saved search</button></div></dialog></dialog-helper>    </custom-scopes>  </div></qbsearch-input><input type=\"hidden\" data-csrf=\"true\" class=\"js-data-jump-to-suggestions-path-csrf\" value=\"CectXLCDuUFIuUP5+rwBpeqT/FJ+6dR/yYyywA/TeTWjnWtsWWu+ycLN/1Kbb/0ndENgWPOGytuSZvFAdq+4bg==\" />          <div class=\"position-relative mr-lg-3 d-lg-inline-block\">            <a href=\"/login?return_to=https%3A%2F%2Fgithub.com%2Flucidrains%2Fvit-pytorch\"              class=\"HeaderMenu-link HeaderMenu-link--sign-in flex-shrink-0 no-underline d-block d-lg-inline-block border border-lg-0 rounded rounded-lg-0 p-2 p-lg-0\"              data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/vit-pytorch&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"49c15a216452a5a641aa1cbf08fdf1633dcdd1e0144dbf8cc191b15b1ba5e4e0\"              data-ga-click=\"(Logged out) Header, clicked Sign in, text:sign-in\">              Sign in            </a>          </div>            <a href=\"/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=lucidrains%2Fvit-pytorch\"              class=\"HeaderMenu-link HeaderMenu-link--sign-up flex-shrink-0 d-none d-lg-inline-block no-underline border color-border-default rounded px-2 py-1\"              data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/vit-pytorch&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"49c15a216452a5a641aa1cbf08fdf1633dcdd1e0144dbf8cc191b15b1ba5e4e0\"              data-analytics-event=\"{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/&lt;user-name&gt;/&lt;repo-name&gt;;ref_cta:Sign up;ref_loc:header logged out&quot;}\"            >              Sign up            </a>        </div>      </div>    </div>  </div></header>      <div hidden=\"hidden\" data-view-component=\"true\" class=\"js-stale-session-flash stale-session-flash flash flash-warn flash-full mb-3\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-alert\">    <path d=\"M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>        <span class=\"js-stale-session-flash-signed-in\" hidden>You signed in with another tab or window. <a class=\"Link--inTextBlock\" href=\"\">Reload</a> to refresh your session.</span>        <span class=\"js-stale-session-flash-signed-out\" hidden>You signed out in another tab or window. <a class=\"Link--inTextBlock\" href=\"\">Reload</a> to refresh your session.</span>        <span class=\"js-stale-session-flash-switched\" hidden>You switched accounts on another tab or window. <a class=\"Link--inTextBlock\" href=\"\">Reload</a> to refresh your session.</span>    <button id=\"icon-button-413e83ac-9a56-4526-a05c-f9f9b33be3a9\" aria-labelledby=\"tooltip-e1fe8a0e-f44a-4385-aaef-5419f2344cef\" type=\"button\" data-view-component=\"true\" class=\"Button Button--iconOnly Button--invisible Button--medium flash-close js-flash-close\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x Button-visual\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg></button><tool-tip id=\"tooltip-e1fe8a0e-f44a-4385-aaef-5419f2344cef\" for=\"icon-button-413e83ac-9a56-4526-a05c-f9f9b33be3a9\" popover=\"manual\" data-direction=\"s\" data-type=\"label\" data-view-component=\"true\" class=\"sr-only position-absolute\">Dismiss alert</tool-tip>  </div>    </div>  <div id=\"start-of-content\" class=\"show-on-focus\"></div>    <div id=\"js-flash-container\" data-turbo-replace>  <template class=\"js-flash-template\">    <div class=\"flash flash-full   {{ className }}\">  <div class=\"px-2\" >    <button autofocus class=\"flash-close js-flash-close\" type=\"button\" aria-label=\"Dismiss this message\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg>    </button>    <div aria-atomic=\"true\" role=\"alert\" class=\"js-flash-alert\">            <div>{{ message }}</div>    </div>  </div></div>  </template></div>        <include-fragment class=\"js-notification-shelf-include-fragment\" data-base-src=\"https://github.com/notifications/beta/shelf\"></include-fragment>  <div    class=\"application-main \"    data-commit-hovercards-enabled    data-discussion-hovercards-enabled    data-issue-and-pr-hovercards-enabled  >        <div itemscope itemtype=\"http://schema.org/SoftwareSourceCode\" class=\"\">    <main id=\"js-repo-pjax-container\" >                <div id=\"repository-container-header\"  class=\"pt-3 hide-full-screen\" style=\"background-color: var(--page-header-bgColor, var(--color-page-header-bg));\" data-turbo-replace>      <div class=\"d-flex flex-wrap flex-justify-end mb-3  px-3 px-md-4 px-lg-5\" style=\"gap: 1rem;\">        <div class=\"flex-auto min-width-0 width-fit mr-3\">              <div class=\" d-flex flex-wrap flex-items-center wb-break-word f3 text-normal\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo color-fg-muted mr-2\">    <path d=\"M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z\"></path></svg>        <span class=\"author flex-self-stretch\" itemprop=\"author\">      <a class=\"url fn\" rel=\"author\" data-hovercard-type=\"user\" data-hovercard-url=\"/users/lucidrains/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"/lucidrains\">        lucidrains</a>    </span>    <span class=\"mx-1 flex-self-stretch color-fg-muted\">/</span>    <strong itemprop=\"name\" class=\"mr-2 flex-self-stretch\">      <a data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" href=\"/lucidrains/vit-pytorch\">vit-pytorch</a>    </strong>    <span></span><span class=\"Label Label--secondary v-align-middle mr-1\">Public</span>  </div>        </div>        <div id=\"repository-details-container\" data-turbo-replace>            <ul class=\"pagehead-actions flex-shrink-0 d-none d-md-inline\" style=\"padding: 2px 0;\">            <li>          <include-fragment src=\"/lucidrains/vit-pytorch/sponsor_button\"></include-fragment>        </li>        <li>            <a href=\"/login?return_to=%2Flucidrains%2Fvit-pytorch\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/vit-pytorch&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"696fa4d2c9e175258c21b1b661ccf3c98fc23f391f6cc5552990952619087488\" aria-label=\"You must be signed in to change notification settings\" data-view-component=\"true\" class=\"tooltipped tooltipped-s btn-sm btn\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-bell mr-2\">    <path d=\"M8 16a2 2 0 0 0 1.985-1.75c.017-.137-.097-.25-.235-.25h-3.5c-.138 0-.252.113-.235.25A2 2 0 0 0 8 16ZM3 5a5 5 0 0 1 10 0v2.947c0 .05.015.098.042.139l1.703 2.555A1.519 1.519 0 0 1 13.482 13H2.518a1.516 1.516 0 0 1-1.263-2.36l1.703-2.554A.255.255 0 0 0 3 7.947Zm5-3.5A3.5 3.5 0 0 0 4.5 5v2.947c0 .346-.102.683-.294.97l-1.703 2.556a.017.017 0 0 0-.003.01l.001.006c0 .002.002.004.004.006l.006.004.007.001h10.964l.007-.001.006-.004.004-.006.001-.007a.017.017 0 0 0-.003-.01l-1.703-2.554a1.745 1.745 0 0 1-.294-.97V5A3.5 3.5 0 0 0 8 1.5Z\"></path></svg>Notifications</a>  </li>  <li>          <a icon=\"repo-forked\" id=\"fork-button\" href=\"/login?return_to=%2Flucidrains%2Fvit-pytorch\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;repo details fork button&quot;,&quot;repository_id&quot;:300996055,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/vit-pytorch&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"9f1c4515c670912b77c2682053bccf5f787631e3e406e48cbb2ab13255a9ebf5\" data-view-component=\"true\" class=\"btn-sm btn\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo-forked mr-2\">    <path d=\"M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z\"></path></svg>Fork    <span id=\"repo-network-counter\" data-pjax-replace=\"true\" data-turbo-replace=\"true\" title=\"2,696\" data-view-component=\"true\" class=\"Counter\">2.7k</span></a>  </li>  <li>        <div data-view-component=\"true\" class=\"BtnGroup d-flex\">        <a href=\"/login?return_to=%2Flucidrains%2Fvit-pytorch\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:300996055,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/vit-pytorch&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"a94f9645d7e61e55f7e65024bf1dbeb53ceee43998959d69864ba188a1311003\" aria-label=\"You must be signed in to star a repository\" data-view-component=\"true\" class=\"tooltipped tooltipped-s btn-sm btn BtnGroup-item\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-star v-align-text-bottom d-inline-block mr-2\">    <path d=\"M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z\"></path></svg><span data-view-component=\"true\" class=\"d-inline\">          Star</span>          <span id=\"repo-stars-counter-star\" aria-label=\"17259 users starred this repository\" data-singular-suffix=\"user starred this repository\" data-plural-suffix=\"users starred this repository\" data-turbo-replace=\"true\" title=\"17,259\" data-view-component=\"true\" class=\"Counter js-social-count\">17.3k</span></a>        <button aria-label=\"You must be signed in to add this repository to a list\" type=\"button\" disabled=\"disabled\" data-view-component=\"true\" class=\"btn-sm btn BtnGroup-item px-2\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-triangle-down\">    <path d=\"m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z\"></path></svg></button></div>  </li>    <li>            </li></ul>        </div>      </div>        <div id=\"responsive-meta-container\" data-turbo-replace>      <div class=\"d-block d-md-none mb-2 px-3 px-md-4 px-lg-5\">      <p class=\"f4 mb-3 \">        Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch      </p>          <h3 class=\"sr-only\">License</h3>  <div class=\"mb-2\">    <a href=\"/lucidrains/vit-pytorch/blob/main/LICENSE\"      class=\"Link--muted\"            data-analytics-event=\"{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:license&quot;}\"    >      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-law mr-2\">    <path d=\"M8.75.75V2h.985c.304 0 .603.08.867.231l1.29.736c.038.022.08.033.124.033h2.234a.75.75 0 0 1 0 1.5h-.427l2.111 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.006.005-.01.01-.045.04c-.21.176-.441.327-.686.45C14.556 10.78 13.88 11 13 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L12.178 4.5h-.162c-.305 0-.604-.079-.868-.231l-1.29-.736a.245.245 0 0 0-.124-.033H8.75V13h2.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1 0-1.5h2.5V3.5h-.984a.245.245 0 0 0-.124.033l-1.289.737c-.265.15-.564.23-.869.23h-.162l2.112 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.016.015-.045.04c-.21.176-.441.327-.686.45C4.556 10.78 3.88 11 3 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L2.178 4.5H1.75a.75.75 0 0 1 0-1.5h2.234a.249.249 0 0 0 .125-.033l1.288-.737c.265-.15.564-.23.869-.23h.984V.75a.75.75 0 0 1 1.5 0Zm2.945 8.477c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L13 6.327Zm-10 0c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L3 6.327Z\"></path></svg>     MIT license    </a>  </div>    <div class=\"mb-3\">        <a class=\"Link--secondary no-underline mr-3\" href=\"/lucidrains/vit-pytorch/stargazers\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-star mr-1\">    <path d=\"M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z\"></path></svg>          <span class=\"text-bold\">17.3k</span>          stars</a>        <a class=\"Link--secondary no-underline mr-3\" href=\"/lucidrains/vit-pytorch/forks\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo-forked mr-1\">    <path d=\"M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z\"></path></svg>          <span class=\"text-bold\">2.7k</span>          forks</a>          <a class=\"Link--secondary no-underline mr-3 d-inline-block\" href=\"/lucidrains/vit-pytorch/branches\">            <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-git-branch mr-1\">    <path d=\"M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z\"></path></svg>            <span>Branches</span></a>          <a class=\"Link--secondary no-underline d-inline-block\" href=\"/lucidrains/vit-pytorch/tags\">            <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-tag mr-1\">    <path d=\"M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z\"></path></svg>            <span>Tags</span></a>        <a class=\"Link--secondary no-underline d-inline-block\" href=\"/lucidrains/vit-pytorch/activity\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-pulse mr-1\">    <path d=\"M6 2c.306 0 .582.187.696.471L10 10.731l1.304-3.26A.751.751 0 0 1 12 7h3.25a.75.75 0 0 1 0 1.5h-2.742l-1.812 4.528a.751.751 0 0 1-1.392 0L6 4.77 4.696 8.03A.75.75 0 0 1 4 8.5H.75a.75.75 0 0 1 0-1.5h2.742l1.812-4.529A.751.751 0 0 1 6 2Z\"></path></svg>          <span>Activity</span></a>    </div>      <div class=\"d-flex flex-wrap gap-2\">        <div class=\"flex-1\">            <div data-view-component=\"true\" class=\"BtnGroup d-flex\">        <a href=\"/login?return_to=%2Flucidrains%2Fvit-pytorch\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:300996055,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/vit-pytorch&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"a94f9645d7e61e55f7e65024bf1dbeb53ceee43998959d69864ba188a1311003\" aria-label=\"You must be signed in to star a repository\" data-view-component=\"true\" class=\"tooltipped tooltipped-s btn-sm btn btn-block BtnGroup-item\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-star v-align-text-bottom d-inline-block mr-2\">    <path d=\"M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z\"></path></svg><span data-view-component=\"true\" class=\"d-inline\">          Star</span></a>        <button aria-label=\"You must be signed in to add this repository to a list\" type=\"button\" disabled=\"disabled\" data-view-component=\"true\" class=\"btn-sm btn BtnGroup-item px-2\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-triangle-down\">    <path d=\"m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z\"></path></svg></button></div>        </div>        <div class=\"flex-1\">                <a href=\"/login?return_to=%2Flucidrains%2Fvit-pytorch\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/lucidrains/vit-pytorch&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"696fa4d2c9e175258c21b1b661ccf3c98fc23f391f6cc5552990952619087488\" aria-label=\"You must be signed in to change notification settings\" data-view-component=\"true\" class=\"tooltipped tooltipped-s btn-sm btn btn-block\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-bell mr-2\">    <path d=\"M8 16a2 2 0 0 0 1.985-1.75c.017-.137-.097-.25-.235-.25h-3.5c-.138 0-.252.113-.235.25A2 2 0 0 0 8 16ZM3 5a5 5 0 0 1 10 0v2.947c0 .05.015.098.042.139l1.703 2.555A1.519 1.519 0 0 1 13.482 13H2.518a1.516 1.516 0 0 1-1.263-2.36l1.703-2.554A.255.255 0 0 0 3 7.947Zm5-3.5A3.5 3.5 0 0 0 4.5 5v2.947c0 .346-.102.683-.294.97l-1.703 2.556a.017.017 0 0 0-.003.01l.001.006c0 .002.002.004.004.006l.006.004.007.001h10.964l.007-.001.006-.004.004-.006.001-.007a.017.017 0 0 0-.003-.01l-1.703-2.554a1.745 1.745 0 0 1-.294-.97V5A3.5 3.5 0 0 0 8 1.5Z\"></path></svg>Notifications</a>        </div>          <span>                      </span>      </div>  </div></div>          <nav data-pjax=\"#js-repo-pjax-container\" aria-label=\"Repository\" data-view-component=\"true\" class=\"js-repo-nav js-sidenav-container-pjax js-responsive-underlinenav overflow-hidden UnderlineNav px-3 px-md-4 px-lg-5\">  <ul data-view-component=\"true\" class=\"UnderlineNav-body list-style-none\">      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"code-tab\" href=\"/lucidrains/vit-pytorch\" data-tab-item=\"i0code-tab\" data-selected-links=\"repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches repo_packages repo_deployments repo_attestations /lucidrains/vit-pytorch\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g c\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Code&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" aria-current=\"page\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item selected\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z\"></path></svg>        <span data-content=\"Code\">Code</span>          <span id=\"code-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"Not available\" data-view-component=\"true\" class=\"Counter\"></span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"issues-tab\" href=\"/lucidrains/vit-pytorch/issues\" data-tab-item=\"i1issues-tab\" data-selected-links=\"repo_issues repo_labels repo_milestones /lucidrains/vit-pytorch/issues\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g i\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Issues&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-issue-opened UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path><path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z\"></path></svg>        <span data-content=\"Issues\">Issues</span>          <span id=\"issues-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"113\" data-view-component=\"true\" class=\"Counter\">113</span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"pull-requests-tab\" href=\"/lucidrains/vit-pytorch/pulls\" data-tab-item=\"i2pull-requests-tab\" data-selected-links=\"repo_pulls checks /lucidrains/vit-pytorch/pulls\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g p\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Pull requests&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-git-pull-request UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M1.5 3.25a2.25 2.25 0 1 1 3 2.122v5.256a2.251 2.251 0 1 1-1.5 0V5.372A2.25 2.25 0 0 1 1.5 3.25Zm5.677-.177L9.573.677A.25.25 0 0 1 10 .854V2.5h1A2.5 2.5 0 0 1 13.5 5v5.628a2.251 2.251 0 1 1-1.5 0V5a1 1 0 0 0-1-1h-1v1.646a.25.25 0 0 1-.427.177L7.177 3.427a.25.25 0 0 1 0-.354ZM3.75 2.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm0 9.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm8.25.75a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Z\"></path></svg>        <span data-content=\"Pull requests\">Pull requests</span>          <span id=\"pull-requests-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"4\" data-view-component=\"true\" class=\"Counter\">4</span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"discussions-tab\" href=\"/lucidrains/vit-pytorch/discussions\" data-tab-item=\"i3discussions-tab\" data-selected-links=\"repo_discussions /lucidrains/vit-pytorch/discussions\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g g\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Discussions&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-comment-discussion UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M1.75 1h8.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 10.25 10H7.061l-2.574 2.573A1.458 1.458 0 0 1 2 11.543V10h-.25A1.75 1.75 0 0 1 0 8.25v-5.5C0 1.784.784 1 1.75 1ZM1.5 2.75v5.5c0 .138.112.25.25.25h1a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h3.5a.25.25 0 0 0 .25-.25v-5.5a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25Zm13 2a.25.25 0 0 0-.25-.25h-.5a.75.75 0 0 1 0-1.5h.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 14.25 12H14v1.543a1.458 1.458 0 0 1-2.487 1.03L9.22 12.28a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l2.22 2.22v-2.19a.75.75 0 0 1 .75-.75h1a.25.25 0 0 0 .25-.25Z\"></path></svg>        <span data-content=\"Discussions\">Discussions</span>          <span id=\"discussions-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"Not available\" data-view-component=\"true\" class=\"Counter\"></span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"actions-tab\" href=\"/lucidrains/vit-pytorch/actions\" data-tab-item=\"i4actions-tab\" data-selected-links=\"repo_actions /lucidrains/vit-pytorch/actions\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g a\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Actions&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-play UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z\"></path></svg>        <span data-content=\"Actions\">Actions</span>          <span id=\"actions-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"Not available\" data-view-component=\"true\" class=\"Counter\"></span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"projects-tab\" href=\"/lucidrains/vit-pytorch/projects\" data-tab-item=\"i5projects-tab\" data-selected-links=\"repo_projects new_repo_project repo_project /lucidrains/vit-pytorch/projects\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g b\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Projects&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-table UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25ZM6.5 6.5v8h7.75a.25.25 0 0 0 .25-.25V6.5Zm8-1.5V1.75a.25.25 0 0 0-.25-.25H6.5V5Zm-13 1.5v7.75c0 .138.112.25.25.25H5v-8ZM5 5V1.5H1.75a.25.25 0 0 0-.25.25V5Z\"></path></svg>        <span data-content=\"Projects\">Projects</span>          <span id=\"projects-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"0\" hidden=\"hidden\" data-view-component=\"true\" class=\"Counter\">0</span>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"security-tab\" href=\"/lucidrains/vit-pytorch/security\" data-tab-item=\"i6security-tab\" data-selected-links=\"security overview alerts policy token_scanning code_scanning /lucidrains/vit-pytorch/security\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-hotkey=\"g s\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Security&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-shield UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M7.467.133a1.748 1.748 0 0 1 1.066 0l5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667Zm.61 1.429a.25.25 0 0 0-.153 0l-5.25 1.68a.25.25 0 0 0-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.196.196 0 0 0 .154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.251.251 0 0 0-.174-.237l-5.25-1.68ZM8.75 4.75v3a.75.75 0 0 1-1.5 0v-3a.75.75 0 0 1 1.5 0ZM9 10.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>        <span data-content=\"Security\">Security</span>          <include-fragment src=\"/lucidrains/vit-pytorch/security/overall-count\" accept=\"text/fragment+html\"></include-fragment>    </a></li>      <li data-view-component=\"true\" class=\"d-inline-flex\">  <a id=\"insights-tab\" href=\"/lucidrains/vit-pytorch/pulse\" data-tab-item=\"i7insights-tab\" data-selected-links=\"repo_graphs repo_contributors dependency_graph dependabot_updates pulse people community /lucidrains/vit-pytorch/pulse\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" data-analytics-event=\"{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Insights&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}\" data-view-component=\"true\" class=\"UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item\">                  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-graph UnderlineNav-octicon d-none d-sm-inline\">    <path d=\"M1.5 1.75V13.5h13.75a.75.75 0 0 1 0 1.5H.75a.75.75 0 0 1-.75-.75V1.75a.75.75 0 0 1 1.5 0Zm14.28 2.53-5.25 5.25a.75.75 0 0 1-1.06 0L7 7.06 4.28 9.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.25-3.25a.75.75 0 0 1 1.06 0L10 7.94l4.72-4.72a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z\"></path></svg>        <span data-content=\"Insights\">Insights</span>          <span id=\"insights-repo-tab-count\" data-pjax-replace=\"\" data-turbo-replace=\"\" title=\"Not available\" data-view-component=\"true\" class=\"Counter\"></span>    </a></li></ul>    <div style=\"visibility:hidden;\" data-view-component=\"true\" class=\"UnderlineNav-actions js-responsive-underlinenav-overflow position-absolute pr-3 pr-md-4 pr-lg-5 right-0\">      <action-menu data-select-variant=\"none\" data-view-component=\"true\">  <focus-group direction=\"vertical\" mnemonics retain>    <button id=\"action-menu-79044795-3ce6-49b9-82c8-7039df41080a-button\" popovertarget=\"action-menu-79044795-3ce6-49b9-82c8-7039df41080a-overlay\" aria-controls=\"action-menu-79044795-3ce6-49b9-82c8-7039df41080a-list\" aria-haspopup=\"true\" aria-labelledby=\"tooltip-74e7709c-1709-4e0d-bf9f-08d02697bd80\" type=\"button\" data-view-component=\"true\" class=\"Button Button--iconOnly Button--secondary Button--medium UnderlineNav-item\">  <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-kebab-horizontal Button-visual\">    <path d=\"M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path></svg></button><tool-tip id=\"tooltip-74e7709c-1709-4e0d-bf9f-08d02697bd80\" for=\"action-menu-79044795-3ce6-49b9-82c8-7039df41080a-button\" popover=\"manual\" data-direction=\"s\" data-type=\"label\" data-view-component=\"true\" class=\"sr-only position-absolute\">Additional navigation options</tool-tip><anchored-position id=\"action-menu-79044795-3ce6-49b9-82c8-7039df41080a-overlay\" anchor=\"action-menu-79044795-3ce6-49b9-82c8-7039df41080a-button\" align=\"start\" side=\"outside-bottom\" anchor-offset=\"normal\" popover=\"auto\" data-view-component=\"true\">  <div data-view-component=\"true\" class=\"Overlay Overlay--size-auto\">          <div data-view-component=\"true\" class=\"Overlay-body Overlay-body--paddingNone\">          <div data-view-component=\"true\">  <ul aria-labelledby=\"action-menu-79044795-3ce6-49b9-82c8-7039df41080a-button\" id=\"action-menu-79044795-3ce6-49b9-82c8-7039df41080a-list\" role=\"menu\" data-view-component=\"true\" class=\"ActionListWrap--inset ActionListWrap\">      <li hidden=\"hidden\" data-menu-item=\"i0code-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-be56ce00-2039-40bb-850a-10077a29888f\" href=\"/lucidrains/vit-pytorch\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-code\">    <path d=\"m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Code</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i1issues-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-817361ba-94be-4ab0-bc4d-3416dc0ee8a8\" href=\"/lucidrains/vit-pytorch/issues\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-issue-opened\">    <path d=\"M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path><path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Issues</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i2pull-requests-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-e8210060-adc3-459c-b45c-db7248c84b15\" href=\"/lucidrains/vit-pytorch/pulls\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-git-pull-request\">    <path d=\"M1.5 3.25a2.25 2.25 0 1 1 3 2.122v5.256a2.251 2.251 0 1 1-1.5 0V5.372A2.25 2.25 0 0 1 1.5 3.25Zm5.677-.177L9.573.677A.25.25 0 0 1 10 .854V2.5h1A2.5 2.5 0 0 1 13.5 5v5.628a2.251 2.251 0 1 1-1.5 0V5a1 1 0 0 0-1-1h-1v1.646a.25.25 0 0 1-.427.177L7.177 3.427a.25.25 0 0 1 0-.354ZM3.75 2.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm0 9.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm8.25.75a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Pull requests</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i3discussions-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-a0966bf4-6600-43f7-bdf8-4fc15b21129e\" href=\"/lucidrains/vit-pytorch/discussions\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-comment-discussion\">    <path d=\"M1.75 1h8.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 10.25 10H7.061l-2.574 2.573A1.458 1.458 0 0 1 2 11.543V10h-.25A1.75 1.75 0 0 1 0 8.25v-5.5C0 1.784.784 1 1.75 1ZM1.5 2.75v5.5c0 .138.112.25.25.25h1a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h3.5a.25.25 0 0 0 .25-.25v-5.5a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25Zm13 2a.25.25 0 0 0-.25-.25h-.5a.75.75 0 0 1 0-1.5h.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 14.25 12H14v1.543a1.458 1.458 0 0 1-2.487 1.03L9.22 12.28a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l2.22 2.22v-2.19a.75.75 0 0 1 .75-.75h1a.25.25 0 0 0 .25-.25Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Discussions</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i4actions-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-7e0db6c6-89f0-468c-84f7-779d726a66a3\" href=\"/lucidrains/vit-pytorch/actions\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-play\">    <path d=\"M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Actions</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i5projects-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-9e6ef450-fab6-41eb-95b1-ca5cc112ddec\" href=\"/lucidrains/vit-pytorch/projects\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-table\">    <path d=\"M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25ZM6.5 6.5v8h7.75a.25.25 0 0 0 .25-.25V6.5Zm8-1.5V1.75a.25.25 0 0 0-.25-.25H6.5V5Zm-13 1.5v7.75c0 .138.112.25.25.25H5v-8ZM5 5V1.5H1.75a.25.25 0 0 0-.25.25V5Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Projects</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i6security-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-4164335f-2858-41c5-9ceb-6a89e59c3510\" href=\"/lucidrains/vit-pytorch/security\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-shield\">    <path d=\"M7.467.133a1.748 1.748 0 0 1 1.066 0l5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667Zm.61 1.429a.25.25 0 0 0-.153 0l-5.25 1.68a.25.25 0 0 0-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.196.196 0 0 0 .154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.251.251 0 0 0-.174-.237l-5.25-1.68ZM8.75 4.75v3a.75.75 0 0 1-1.5 0v-3a.75.75 0 0 1 1.5 0ZM9 10.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Security</span></a>    </li>      <li hidden=\"hidden\" data-menu-item=\"i7insights-tab\" data-targets=\"action-list.items action-list.items\" role=\"none\" data-view-component=\"true\" class=\"ActionListItem\">        <a tabindex=\"-1\" id=\"item-d0d74080-0e84-48cc-baec-c678e04e290c\" href=\"/lucidrains/vit-pytorch/pulse\" role=\"menuitem\" data-view-component=\"true\" class=\"ActionListContent ActionListContent--visual16\">        <span class=\"ActionListItem-visual ActionListItem-visual--leading\">          <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-graph\">    <path d=\"M1.5 1.75V13.5h13.75a.75.75 0 0 1 0 1.5H.75a.75.75 0 0 1-.75-.75V1.75a.75.75 0 0 1 1.5 0Zm14.28 2.53-5.25 5.25a.75.75 0 0 1-1.06 0L7 7.06 4.28 9.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.25-3.25a.75.75 0 0 1 1.06 0L10 7.94l4.72-4.72a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z\"></path></svg>        </span>              <span data-view-component=\"true\" class=\"ActionListItem-label\">          Insights</span></a>    </li></ul>  </div></div>      </div></anchored-position>  </focus-group></action-menu></div></nav>  </div>  <turbo-frame id=\"repo-content-turbo-frame\" target=\"_top\" data-turbo-action=\"advance\" class=\"\">    <div id=\"repo-content-pjax-container\" class=\"repository-content \" >                <h1 class='sr-only'>lucidrains/vit-pytorch</h1>  <div class=\"clearfix container-xl px-md-4 px-lg-5 px-3\">    <div>  <div id=\"spoof-warning\" class=\"mt-0 pb-3\" hidden aria-hidden>  <div data-view-component=\"true\" class=\"flash flash-warn mt-0 clearfix\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-alert float-left mt-1\">    <path d=\"M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>      <div class=\"overflow-hidden\">This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.</div>  </div></div>  <include-fragment src=\"/lucidrains/vit-pytorch/spoofed_commit_check/5578ac472faf3903d4739ba783f3875b77177e57\" data-test-selector=\"spoofed-commit-check\"></include-fragment>  <div style=\"max-width: 100%\" data-view-component=\"true\" class=\"Layout Layout--flowRow-until-md react-repos-overview-margin Layout--sidebarPosition-end Layout--sidebarPosition-flowRow-end\">  <div data-view-component=\"true\" class=\"Layout-main\">        <script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/react-lib-1fbfc5be2c18.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_octicons-react_dist_index_esm_js-node_modules_primer_react_lib-es-2e8e7c-adc8451a70cf.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Box_Box_js-8f8c5e2a2cbf.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Button_Button_js-67fe00b5266a.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_ActionList_index_js-2dd4d13d3ae6.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Overlay_Overlay_js-node_modules_primer_react_lib-es-fa1130-829932cf63db.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Text_Text_js-node_modules_primer_react_lib-esm_Text-85a14b-236dc9716ad0.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_ActionMenu_ActionMenu_js-eaf74522e470.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_react-router-dom_dist_index_js-3b41341d50fe.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_Dialog_js-node_modules_primer_react_lib-esm_Label_L-857e1c-77794958a54a.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_UnderlineNav_index_js-89fa5806aa3c.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/vendors-node_modules_primer_react_lib-esm_AvatarStack_AvatarStack_js-node_modules_primer_reac-e445e7-175b51e43dcc.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_react-core_create-browser-history_ts-ui_packages_react-core_AppContextProvider_ts-809ab9-bf008735d0bb.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_paths_index_ts-7137b25aa38b.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_ref-selector_RefSelector_tsx-dbbdef4348e2.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/ui_packages_commit-attribution_index_ts-ui_packages_commit-checks-status_index_ts-ui_packages-ffbe33-4c4ddf7d268d.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/app_assets_modules_react-code-view_components_directory_DirectoryContent_index_ts-app_assets_-1fd1f5-c96303590595.js\"></script><script crossorigin=\"anonymous\" defer=\"defer\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/repos-overview-523b8f59ec33.js\"></script><react-partial  partial-name=\"repos-overview\"  data-ssr=\"true\">    <script type=\"application/json\" data-target=\"react-partial.embeddedData\">{\"props\":{\"initialPayload\":{\"allShortcutsEnabled\":false,\"path\":\"/\",\"repo\":{\"id\":300996055,\"defaultBranch\":\"main\",\"name\":\"vit-pytorch\",\"ownerLogin\":\"lucidrains\",\"currentUserCanPush\":false,\"isFork\":false,\"isEmpty\":false,\"createdAt\":\"2020-10-03T22:47:24.000Z\",\"ownerAvatar\":\"https://avatars.githubusercontent.com/u/108653?v=4\",\"public\":true,\"private\":false,\"isOrgOwned\":false},\"currentUser\":null,\"refInfo\":{\"name\":\"main\",\"listCacheKey\":\"v0:1703347907.0\",\"canEdit\":false,\"refType\":\"branch\",\"currentOid\":\"5578ac472faf3903d4739ba783f3875b77177e57\"},\"tree\":{\"items\":[{\"name\":\".github\",\"path\":\".github\",\"contentType\":\"directory\"},{\"name\":\"examples\",\"path\":\"examples\",\"contentType\":\"directory\"},{\"name\":\"images\",\"path\":\"images\",\"contentType\":\"directory\"},{\"name\":\"tests\",\"path\":\"tests\",\"contentType\":\"directory\"},{\"name\":\"vit_pytorch\",\"path\":\"vit_pytorch\",\"contentType\":\"directory\"},{\"name\":\".gitignore\",\"path\":\".gitignore\",\"contentType\":\"file\"},{\"name\":\"LICENSE\",\"path\":\"LICENSE\",\"contentType\":\"file\"},{\"name\":\"MANIFEST.in\",\"path\":\"MANIFEST.in\",\"contentType\":\"file\"},{\"name\":\"README.md\",\"path\":\"README.md\",\"contentType\":\"file\"},{\"name\":\"setup.py\",\"path\":\"setup.py\",\"contentType\":\"file\"}],\"templateDirectorySuggestionUrl\":null,\"readme\":null,\"totalCount\":10,\"showBranchInfobar\":false},\"fileTree\":null,\"fileTreeProcessingTime\":null,\"foldersToFetch\":[],\"treeExpanded\":false,\"symbolsExpanded\":false,\"isOverview\":true,\"overview\":{\"banners\":{\"shouldRecommendReadme\":false,\"isPersonalRepo\":false,\"showUseActionBanner\":false,\"actionSlug\":null,\"actionId\":null,\"showProtectBranchBanner\":false,\"recentlyTouchedDataChannel\":null,\"publishBannersInfo\":{\"dismissActionNoticePath\":\"/settings/dismiss-notice/publish_action_from_repo\",\"releasePath\":\"/lucidrains/vit-pytorch/releases/new?marketplace=true\",\"showPublishActionBanner\":false},\"interactionLimitBanner\":null,\"showInvitationBanner\":false,\"inviterName\":null},\"codeButton\":{\"contactPath\":\"/contact\",\"isEnterprise\":false,\"local\":{\"protocolInfo\":{\"httpAvailable\":true,\"sshAvailable\":null,\"httpUrl\":\"https://github.com/lucidrains/vit-pytorch.git\",\"showCloneWarning\":null,\"sshUrl\":null,\"sshCertificatesRequired\":null,\"sshCertificatesAvailable\":null,\"ghCliUrl\":\"gh repo clone lucidrains/vit-pytorch\",\"defaultProtocol\":\"http\",\"newSshKeyUrl\":\"/settings/ssh/new\",\"setProtocolPath\":\"/users/set_protocol\"},\"platformInfo\":{\"cloneUrl\":\"https://desktop.github.com\",\"showVisualStudioCloneButton\":false,\"visualStudioCloneUrl\":\"https://windows.github.com\",\"showXcodeCloneButton\":false,\"xcodeCloneUrl\":\"https://developer.apple.com\",\"zipballUrl\":\"/lucidrains/vit-pytorch/archive/refs/heads/main.zip\"}},\"newCodespacePath\":\"/codespaces/new?hide_repo_select=true\\u0026repo=300996055\"},\"popovers\":{\"rename\":null,\"renamedParentRepo\":null},\"commitCount\":\"302\",\"overviewFiles\":[{\"displayName\":\"README.md\",\"repoName\":\"vit-pytorch\",\"refName\":\"main\",\"path\":\"README.md\",\"preferredFileType\":\"readme\",\"tabName\":\"README\",\"richText\":\"\\u003carticle class=\\\"markdown-body entry-content container-lg\\\" itemprop=\\\"text\\\"\\u003e\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/vit.gif\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/vit.gif\\\" width=\\\"500px\\\" data-animated-image=\\\"\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eTable of Contents\\u003c/h2\\u003e\\u003ca id=\\\"user-content-table-of-contents\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Table of Contents\\\" href=\\\"#table-of-contents\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cul dir=\\\"auto\\\"\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#vision-transformer---pytorch\\\"\\u003eVision Transformer - Pytorch\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#install\\\"\\u003eInstall\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#usage\\\"\\u003eUsage\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#parameters\\\"\\u003eParameters\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#simple-vit\\\"\\u003eSimple ViT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#navit\\\"\\u003eNaViT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#distillation\\\"\\u003eDistillation\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#deep-vit\\\"\\u003eDeep ViT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#cait\\\"\\u003eCaiT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#token-to-token-vit\\\"\\u003eToken-to-Token ViT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#cct\\\"\\u003eCCT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#cross-vit\\\"\\u003eCross ViT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#pit\\\"\\u003ePiT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#levit\\\"\\u003eLeViT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#cvt\\\"\\u003eCvT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#twins-svt\\\"\\u003eTwins SVT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#crossformer\\\"\\u003eCrossFormer\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#regionvit\\\"\\u003eRegionViT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#scalablevit\\\"\\u003eScalableViT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#sepvit\\\"\\u003eSepViT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#maxvit\\\"\\u003eMaxViT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#nest\\\"\\u003eNesT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#mobilevit\\\"\\u003eMobileViT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#xcit\\\"\\u003eXCiT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#masked-autoencoder\\\"\\u003eMasked Autoencoder\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#simple-masked-image-modeling\\\"\\u003eSimple Masked Image Modeling\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#masked-patch-prediction\\\"\\u003eMasked Patch Prediction\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#masked-position-prediction\\\"\\u003eMasked Position Prediction\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#adaptive-token-sampling\\\"\\u003eAdaptive Token Sampling\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#patch-merger\\\"\\u003ePatch Merger\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#vision-transformer-for-small-datasets\\\"\\u003eVision Transformer for Small Datasets\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#3d-vit\\\"\\u003e3D Vit\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#vivit\\\"\\u003eViVit\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#parallel-vit\\\"\\u003eParallel ViT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#learnable-memory-vit\\\"\\u003eLearnable Memory ViT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#dino\\\"\\u003eDino\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#esvit\\\"\\u003eEsViT\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#accessing-attention\\\"\\u003eAccessing Attention\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#research-ideas\\\"\\u003eResearch Ideas\\u003c/a\\u003e\\n\\u003cul dir=\\\"auto\\\"\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#efficient-attention\\\"\\u003eEfficient Attention\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#combining-with-other-transformer-improvements\\\"\\u003eCombining with other Transformer improvements\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003c/ul\\u003e\\n\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#faq\\\"\\u003eFAQ\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#resources\\\"\\u003eResources\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ca href=\\\"#citations\\\"\\u003eCitations\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003c/ul\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eVision Transformer - Pytorch\\u003c/h2\\u003e\\u003ca id=\\\"user-content-vision-transformer---pytorch\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Vision Transformer - Pytorch\\\" href=\\\"#vision-transformer---pytorch\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eImplementation of \\u003ca href=\\\"https://openreview.net/pdf?id=YicbFdNTTy\\\" rel=\\\"nofollow\\\"\\u003eVision Transformer\\u003c/a\\u003e, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch. Significance is further explained in \\u003ca href=\\\"https://www.youtube.com/watch?v=TrdevFK_am4\\\" rel=\\\"nofollow\\\"\\u003eYannic Kilcher's\\u003c/a\\u003e video. There's really not much to code here, but may as well lay it out for everyone so we expedite the attention revolution.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eFor a Pytorch implementation with pretrained models, please see Ross Wightman's repository \\u003ca href=\\\"https://github.com/rwightman/pytorch-image-models\\\"\\u003ehere\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe official Jax repository is \\u003ca href=\\\"https://github.com/google-research/vision_transformer\\\"\\u003ehere\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eA tensorflow2 translation also exists \\u003ca href=\\\"https://github.com/taki0112/vit-tensorflow\\\"\\u003ehere\\u003c/a\\u003e, created by research scientist \\u003ca href=\\\"https://github.com/taki0112\\\"\\u003eJunho Kim\\u003c/a\\u003e! \ud83d\ude4f\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://github.com/conceptofmind/vit-flax\\\"\\u003eFlax translation\\u003c/a\\u003e by \\u003ca href=\\\"https://github.com/conceptofmind\\\"\\u003eEnrico Shippole\\u003c/a\\u003e!\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eInstall\\u003c/h2\\u003e\\u003ca id=\\\"user-content-install\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Install\\\" href=\\\"#install\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"$ pip install vit-pytorch\\\"\\u003e\\u003cpre\\u003e$ pip install vit-pytorch\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eUsage\\u003c/h2\\u003e\\u003ca id=\\\"user-content-usage\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Usage\\\" href=\\\"#usage\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch import ViT\\n\\nv = ViT(\\n    image_size = 256,\\n    patch_size = 32,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 6,\\n    heads = 16,\\n    mlp_dim = 2048,\\n    dropout = 0.1,\\n    emb_dropout = 0.1\\n)\\n\\nimg = torch.randn(1, 3, 256, 256)\\n\\npreds = v(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eParameters\\u003c/h2\\u003e\\u003ca id=\\\"user-content-parameters\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Parameters\\\" href=\\\"#parameters\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cul dir=\\\"auto\\\"\\u003e\\n\\u003cli\\u003e\\u003ccode\\u003eimage_size\\u003c/code\\u003e: int.\\u003cbr\\u003e\\nImage size. If you have rectangular images, make sure your image size is the maximum of the width and height\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ccode\\u003epatch_size\\u003c/code\\u003e: int.\\u003cbr\\u003e\\nSize of patches. \\u003ccode\\u003eimage_size\\u003c/code\\u003e must be divisible by \\u003ccode\\u003epatch_size\\u003c/code\\u003e.\\u003cbr\\u003e\\nThe number of patches is: \\u003ccode\\u003e n = (image_size // patch_size) ** 2\\u003c/code\\u003e and \\u003ccode\\u003en\\u003c/code\\u003e \\u003cstrong\\u003emust be greater than 16\\u003c/strong\\u003e.\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ccode\\u003enum_classes\\u003c/code\\u003e: int.\\u003cbr\\u003e\\nNumber of classes to classify.\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ccode\\u003edim\\u003c/code\\u003e: int.\\u003cbr\\u003e\\nLast dimension of output tensor after linear transformation \\u003ccode\\u003enn.Linear(..., dim)\\u003c/code\\u003e.\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ccode\\u003edepth\\u003c/code\\u003e: int.\\u003cbr\\u003e\\nNumber of Transformer blocks.\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ccode\\u003eheads\\u003c/code\\u003e: int.\\u003cbr\\u003e\\nNumber of heads in Multi-head Attention layer.\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ccode\\u003emlp_dim\\u003c/code\\u003e: int.\\u003cbr\\u003e\\nDimension of the MLP (FeedForward) layer.\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ccode\\u003echannels\\u003c/code\\u003e: int, default \\u003ccode\\u003e3\\u003c/code\\u003e.\\u003cbr\\u003e\\nNumber of image's channels.\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ccode\\u003edropout\\u003c/code\\u003e: float between \\u003ccode\\u003e[0, 1]\\u003c/code\\u003e, default \\u003ccode\\u003e0.\\u003c/code\\u003e.\\u003cbr\\u003e\\nDropout rate.\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ccode\\u003eemb_dropout\\u003c/code\\u003e: float between \\u003ccode\\u003e[0, 1]\\u003c/code\\u003e, default \\u003ccode\\u003e0\\u003c/code\\u003e.\\u003cbr\\u003e\\nEmbedding dropout rate.\\u003c/li\\u003e\\n\\u003cli\\u003e\\u003ccode\\u003epool\\u003c/code\\u003e: string, either \\u003ccode\\u003ecls\\u003c/code\\u003e token pooling or \\u003ccode\\u003emean\\u003c/code\\u003e pooling\\u003c/li\\u003e\\n\\u003c/ul\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eSimple ViT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-simple-vit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Simple ViT\\\" href=\\\"#simple-vit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2205.01580\\\" rel=\\\"nofollow\\\"\\u003eAn update\\u003c/a\\u003e from some of the same authors of the original paper proposes simplifications to \\u003ccode\\u003eViT\\u003c/code\\u003e that allows it to train faster and better.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eAmong these simplifications include 2d sinusoidal positional embedding, global average pooling (no CLS token), no dropout, batch sizes of 1024 rather than 4096, and use of RandAugment and MixUp augmentations. They also show that a simple linear at the end is not significantly worse than the original MLP head\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use it by importing the \\u003ccode\\u003eSimpleViT\\u003c/code\\u003e as shown below\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch import SimpleViT\\n\\nv = SimpleViT(\\n    image_size = 256,\\n    patch_size = 32,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 6,\\n    heads = 16,\\n    mlp_dim = 2048\\n)\\n\\nimg = torch.randn(1, 3, 256, 256)\\n\\npreds = v(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eSimpleViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eSimpleViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eNaViT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-navit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: NaViT\\\" href=\\\"#navit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/navit.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/navit.png\\\" width=\\\"450px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2307.06304\\\" rel=\\\"nofollow\\\"\\u003eThis paper\\u003c/a\\u003e proposes to leverage the flexibility of attention and masking for variable lengthed sequences to train images of multiple resolution, packed into a single batch. They demonstrate much faster training and improved accuracies, with the only cost being extra complexity in the architecture and dataloading. They use factorized 2d positional encodings, token dropping, as well as query-key normalization.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use it as follows\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.na_vit import NaViT\\n\\nv = NaViT(\\n    image_size = 256,\\n    patch_size = 32,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 6,\\n    heads = 16,\\n    mlp_dim = 2048,\\n    dropout = 0.1,\\n    emb_dropout = 0.1,\\n    token_dropout_prob = 0.1  # token dropout of 10% (keep 90% of tokens)\\n)\\n\\n# 5 images of different resolutions - List[List[Tensor]]\\n\\n# for now, you'll have to correctly place images in same batch element as to not exceed maximum allowed sequence length for self-attention w/ masking\\n\\nimages = [\\n    [torch.randn(3, 256, 256), torch.randn(3, 128, 128)],\\n    [torch.randn(3, 128, 256), torch.randn(3, 256, 128)],\\n    [torch.randn(3, 64, 256)]\\n]\\n\\npreds = v(images) # (5, 1000) - 5, because 5 images of different resolution above\\n\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003ena_vit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eNaViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eNaViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003etoken_dropout_prob\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c\\\"\\u003e# token dropout of 10% (keep 90% of tokens)\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# 5 images of different resolutions - List[List[Tensor]]\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# for now, you'll have to correctly place images in same batch element as to not exceed maximum allowed sequence length for self-attention w/ masking\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimages\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e [\\n    [\\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e), \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e)],\\n    [\\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e), \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e)],\\n    [\\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e64\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)]\\n]\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimages\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (5, 1000) - 5, because 5 images of different resolution above\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eOr if you would rather that the framework auto group the images into variable lengthed sequences that do not exceed a certain max length\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"images = [\\n    torch.randn(3, 256, 256),\\n    torch.randn(3, 128, 128),\\n    torch.randn(3, 128, 256),\\n    torch.randn(3, 256, 128),\\n    torch.randn(3, 64, 256)\\n]\\n\\npreds = v(\\n    images,\\n    group_images = True,\\n    group_max_seq_len = 64\\n) # (5, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-s1\\\"\\u003eimages\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e [\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e),\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e),\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e),\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e),\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e64\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n]\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimages\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003egroup_images\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003egroup_max_seq_len\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e64\\u003c/span\\u003e\\n) \\u003cspan class=\\\"pl-c\\\"\\u003e# (5, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eDistillation\\u003c/h2\\u003e\\u003ca id=\\\"user-content-distillation\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Distillation\\\" href=\\\"#distillation\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/distill.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/distill.png\\\" width=\\\"300px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eA recent \\u003ca href=\\\"https://arxiv.org/abs/2012.12877\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e has shown that use of a distillation token for distilling knowledge from convolutional nets to vision transformer can yield small and efficient vision transformers. This repository offers the means to do distillation easily.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eex. distilling from Resnet50 (or any teacher) to a vision transformer\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom torchvision.models import resnet50\\n\\nfrom vit_pytorch.distill import DistillableViT, DistillWrapper\\n\\nteacher = resnet50(pretrained = True)\\n\\nv = DistillableViT(\\n    image_size = 256,\\n    patch_size = 32,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 6,\\n    heads = 8,\\n    mlp_dim = 2048,\\n    dropout = 0.1,\\n    emb_dropout = 0.1\\n)\\n\\ndistiller = DistillWrapper(\\n    student = v,\\n    teacher = teacher,\\n    temperature = 3,           # temperature of distillation\\n    alpha = 0.5,               # trade between main loss and distillation loss\\n    hard = False               # whether to use soft or hard distillation\\n)\\n\\nimg = torch.randn(2, 3, 256, 256)\\nlabels = torch.randint(0, 1000, (2,))\\n\\nloss = distiller(img, labels)\\nloss.backward()\\n\\n# after lots of training above ...\\n\\npred = v(img) # (2, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorchvision\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003emodels\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eresnet50\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003edistill\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDistillableViT\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDistillWrapper\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eteacher\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003eresnet50\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003epretrained\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDistillableViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003edistiller\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDistillWrapper\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003estudent\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eteacher\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eteacher\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003etemperature\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,           \\u003cspan class=\\\"pl-c\\\"\\u003e# temperature of distillation\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ealpha\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.5\\u003c/span\\u003e,               \\u003cspan class=\\\"pl-c\\\"\\u003e# trade between main loss and distillation loss\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ehard\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eFalse\\u003c/span\\u003e               \\u003cspan class=\\\"pl-c\\\"\\u003e# whether to use soft or hard distillation\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003elabels\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandint\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e, (\\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,))\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003edistiller\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003elabels\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ebackward\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# after lots of training above ...\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epred\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (2, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThe \\u003ccode\\u003eDistillableViT\\u003c/code\\u003e class is identical to \\u003ccode\\u003eViT\\u003c/code\\u003e except for how the forward pass is handled, so you should be able to load the parameters back to \\u003ccode\\u003eViT\\u003c/code\\u003e after you have completed distillation training.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can also use the handy \\u003ccode\\u003e.to_vit\\u003c/code\\u003e method on the \\u003ccode\\u003eDistillableViT\\u003c/code\\u003e instance to get back a \\u003ccode\\u003eViT\\u003c/code\\u003e instance.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"v = v.to_vit()\\ntype(v) # \\u0026lt;class 'vit_pytorch.vit_pytorch.ViT'\\u0026gt;\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eto_vit\\u003c/span\\u003e()\\n\\u003cspan class=\\\"pl-en\\\"\\u003etype\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# \\u0026lt;class 'vit_pytorch.vit_pytorch.ViT'\\u0026gt;\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eDeep ViT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-deep-vit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Deep ViT\\\" href=\\\"#deep-vit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis \\u003ca href=\\\"https://arxiv.org/abs/2103.11886\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e notes that ViT struggles to attend at greater depths (past 12 layers), and suggests mixing the attention of each head post-softmax as a solution, dubbed Re-attention. The results line up with the \\u003ca href=\\\"https://github.com/lucidrains/x-transformers#talking-heads-attention\\\"\\u003eTalking Heads\\u003c/a\\u003e paper from NLP.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use it as follows\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.deepvit import DeepViT\\n\\nv = DeepViT(\\n    image_size = 256,\\n    patch_size = 32,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 6,\\n    heads = 16,\\n    mlp_dim = 2048,\\n    dropout = 0.1,\\n    emb_dropout = 0.1\\n)\\n\\nimg = torch.randn(1, 3, 256, 256)\\n\\npreds = v(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003edeepvit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDeepViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDeepViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eCaiT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-cait\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: CaiT\\\" href=\\\"#cait\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2103.17239\\\" rel=\\\"nofollow\\\"\\u003eThis paper\\u003c/a\\u003e also notes difficulty in training vision transformers at greater depths and proposes two solutions. First it proposes to do per-channel multiplication of the output of the residual block. Second, it proposes to have the patches attend to one another, and only allow the CLS token to attend to the patches in the last few layers.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThey also add \\u003ca href=\\\"https://github.com/lucidrains/x-transformers#talking-heads-attention\\\"\\u003eTalking Heads\\u003c/a\\u003e, noting improvements\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use this scheme as follows\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.cait import CaiT\\n\\nv = CaiT(\\n    image_size = 256,\\n    patch_size = 32,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 12,             # depth of transformer for patch to patch attention only\\n    cls_depth = 2,          # depth of cross attention of CLS tokens to patch\\n    heads = 16,\\n    mlp_dim = 2048,\\n    dropout = 0.1,\\n    emb_dropout = 0.1,\\n    layer_dropout = 0.05    # randomly dropout 5% of the layers\\n)\\n\\nimg = torch.randn(1, 3, 256, 256)\\n\\npreds = v(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003ecait\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eCaiT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eCaiT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e12\\u003c/span\\u003e,             \\u003cspan class=\\\"pl-c\\\"\\u003e# depth of transformer for patch to patch attention only\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecls_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,          \\u003cspan class=\\\"pl-c\\\"\\u003e# depth of cross attention of CLS tokens to patch\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elayer_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.05\\u003c/span\\u003e    \\u003cspan class=\\\"pl-c\\\"\\u003e# randomly dropout 5% of the layers\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eToken-to-Token ViT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-token-to-token-vit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Token-to-Token ViT\\\" href=\\\"#token-to-token-vit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/t2t.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/t2t.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2101.11986\\\" rel=\\\"nofollow\\\"\\u003eThis paper\\u003c/a\\u003e proposes that the first couple layers should downsample the image sequence by unfolding, leading to overlapping image data in each token as shown in the figure above. You can use this variant of the \\u003ccode\\u003eViT\\u003c/code\\u003e as follows.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.t2t import T2TViT\\n\\nv = T2TViT(\\n    dim = 512,\\n    image_size = 224,\\n    depth = 5,\\n    heads = 8,\\n    mlp_dim = 512,\\n    num_classes = 1000,\\n    t2t_layers = ((7, 4), (3, 2), (3, 2)) # tuples of the kernel size and stride of each consecutive layers of the initial token to token module\\n)\\n\\nimg = torch.randn(1, 3, 224, 224)\\n\\npreds = v(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003et2t\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eT2TViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eT2TViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e5\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003et2t_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e ((\\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e), (\\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e), (\\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e)) \\u003cspan class=\\\"pl-c\\\"\\u003e# tuples of the kernel size and stride of each consecutive layers of the initial token to token module\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eCCT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-cct\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: CCT\\\" href=\\\"#cct\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer nofollow\\\" href=\\\"https://raw.githubusercontent.com/SHI-Labs/Compact-Transformers/main/images/model_sym.png\\\"\\u003e\\u003cimg src=\\\"https://raw.githubusercontent.com/SHI-Labs/Compact-Transformers/main/images/model_sym.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2104.05704\\\" rel=\\\"nofollow\\\"\\u003eCCT\\u003c/a\\u003e proposes compact transformers\\nby using convolutions instead of patching and performing sequence pooling. This\\nallows for CCT to have high accuracy and a low number of parameters.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use this with two methods\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.cct import CCT\\n\\ncct = CCT(\\n    img_size = (224, 448),\\n    embedding_dim = 384,\\n    n_conv_layers = 2,\\n    kernel_size = 7,\\n    stride = 2,\\n    padding = 3,\\n    pooling_kernel_size = 3,\\n    pooling_stride = 2,\\n    pooling_padding = 1,\\n    num_layers = 14,\\n    num_heads = 6,\\n    mlp_ratio = 3.,\\n    num_classes = 1000,\\n    positional_embedding = 'learnable', # ['sine', 'learnable', 'none']\\n)\\n\\nimg = torch.randn(1, 3, 224, 448)\\npred = cct(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003ecct\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eCCT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ecct\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eCCT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimg_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e448\\u003c/span\\u003e),\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eembedding_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e384\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003en_conv_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ekernel_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003estride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epadding\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epooling_kernel_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epooling_stride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epooling_padding\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e14\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_ratio\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3.\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epositional_embedding\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s\\\"\\u003e'learnable'\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c\\\"\\u003e# ['sine', 'learnable', 'none']\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e448\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epred\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ecct\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eAlternatively you can use one of several pre-defined models \\u003ccode\\u003e[2,4,6,7,8,14,16]\\u003c/code\\u003e\\nwhich pre-define the number of layers, number of attention heads, the mlp ratio,\\nand the embedding dimension.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.cct import cct_14\\n\\ncct = cct_14(\\n    img_size = 224,\\n    n_conv_layers = 1,\\n    kernel_size = 7,\\n    stride = 2,\\n    padding = 3,\\n    pooling_kernel_size = 3,\\n    pooling_stride = 2,\\n    pooling_padding = 1,\\n    num_classes = 1000,\\n    positional_embedding = 'learnable', # ['sine', 'learnable', 'none']\\n)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003ecct\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ecct_14\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ecct\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ecct_14\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimg_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003en_conv_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ekernel_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003estride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epadding\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epooling_kernel_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epooling_stride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epooling_padding\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epositional_embedding\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s\\\"\\u003e'learnable'\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c\\\"\\u003e# ['sine', 'learnable', 'none']\\u003c/span\\u003e\\n)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://github.com/SHI-Labs/Compact-Transformers\\\"\\u003eOfficial\\nRepository\\u003c/a\\u003e includes links to pretrained model checkpoints.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eCross ViT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-cross-vit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Cross ViT\\\" href=\\\"#cross-vit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/cross_vit.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/cross_vit.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2103.14899\\\" rel=\\\"nofollow\\\"\\u003eThis paper\\u003c/a\\u003e proposes to have two vision transformers processing the image at different scales, cross attending to one every so often. They show improvements on top of the base vision transformer.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.cross_vit import CrossViT\\n\\nv = CrossViT(\\n    image_size = 256,\\n    num_classes = 1000,\\n    depth = 4,               # number of multi-scale encoding blocks\\n    sm_dim = 192,            # high res dimension\\n    sm_patch_size = 16,      # high res patch size (should be smaller than lg_patch_size)\\n    sm_enc_depth = 2,        # high res depth\\n    sm_enc_heads = 8,        # high res heads\\n    sm_enc_mlp_dim = 2048,   # high res feedforward dimension\\n    lg_dim = 384,            # low res dimension\\n    lg_patch_size = 64,      # low res patch size\\n    lg_enc_depth = 3,        # low res depth\\n    lg_enc_heads = 8,        # low res heads\\n    lg_enc_mlp_dim = 2048,   # low res feedforward dimensions\\n    cross_attn_depth = 2,    # cross attention rounds\\n    cross_attn_heads = 8,    # cross attention heads\\n    dropout = 0.1,\\n    emb_dropout = 0.1\\n)\\n\\nimg = torch.randn(1, 3, 256, 256)\\n\\npred = v(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003ecross_vit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eCrossViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eCrossViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,               \\u003cspan class=\\\"pl-c\\\"\\u003e# number of multi-scale encoding blocks\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003esm_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e192\\u003c/span\\u003e,            \\u003cspan class=\\\"pl-c\\\"\\u003e# high res dimension\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003esm_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# high res patch size (should be smaller than lg_patch_size)\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003esm_enc_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,        \\u003cspan class=\\\"pl-c\\\"\\u003e# high res depth\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003esm_enc_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,        \\u003cspan class=\\\"pl-c\\\"\\u003e# high res heads\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003esm_enc_mlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,   \\u003cspan class=\\\"pl-c\\\"\\u003e# high res feedforward dimension\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elg_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e384\\u003c/span\\u003e,            \\u003cspan class=\\\"pl-c\\\"\\u003e# low res dimension\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elg_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e64\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# low res patch size\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elg_enc_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,        \\u003cspan class=\\\"pl-c\\\"\\u003e# low res depth\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elg_enc_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,        \\u003cspan class=\\\"pl-c\\\"\\u003e# low res heads\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elg_enc_mlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,   \\u003cspan class=\\\"pl-c\\\"\\u003e# low res feedforward dimensions\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecross_attn_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,    \\u003cspan class=\\\"pl-c\\\"\\u003e# cross attention rounds\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecross_attn_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,    \\u003cspan class=\\\"pl-c\\\"\\u003e# cross attention heads\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epred\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003ePiT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-pit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: PiT\\\" href=\\\"#pit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/pit.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/pit.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2103.16302\\\" rel=\\\"nofollow\\\"\\u003eThis paper\\u003c/a\\u003e proposes to downsample the tokens through a pooling procedure using depth-wise convolutions.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.pit import PiT\\n\\nv = PiT(\\n    image_size = 224,\\n    patch_size = 14,\\n    dim = 256,\\n    num_classes = 1000,\\n    depth = (3, 3, 3),     # list of depths, indicating the number of rounds of each stage before a downsample\\n    heads = 16,\\n    mlp_dim = 2048,\\n    dropout = 0.1,\\n    emb_dropout = 0.1\\n)\\n\\n# forward pass now returns predictions and the attention maps\\n\\nimg = torch.randn(1, 3, 224, 224)\\n\\npreds = v(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003epit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003ePiT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003ePiT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e14\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e),     \\u003cspan class=\\\"pl-c\\\"\\u003e# list of depths, indicating the number of rounds of each stage before a downsample\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# forward pass now returns predictions and the attention maps\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eLeViT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-levit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: LeViT\\\" href=\\\"#levit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/levit.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/levit.png\\\" width=\\\"300px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2104.01136\\\" rel=\\\"nofollow\\\"\\u003eThis paper\\u003c/a\\u003e proposes a number of changes, including (1) convolutional embedding instead of patch-wise projection (2) downsampling in stages (3) extra non-linearity in attention (4) 2d relative positional biases instead of initial absolute positional bias (5) batchnorm in place of layernorm.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://github.com/facebookresearch/LeViT\\\"\\u003eOfficial repository\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.levit import LeViT\\n\\nlevit = LeViT(\\n    image_size = 224,\\n    num_classes = 1000,\\n    stages = 3,             # number of stages\\n    dim = (256, 384, 512),  # dimensions at each stage\\n    depth = 4,              # transformer of depth 4 at each stage\\n    heads = (4, 6, 8),      # heads at each stage\\n    mlp_mult = 2,\\n    dropout = 0.1\\n)\\n\\nimg = torch.randn(1, 3, 224, 224)\\n\\nlevit(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003elevit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eLeViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003elevit\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eLeViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003estages\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,             \\u003cspan class=\\\"pl-c\\\"\\u003e# number of stages\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e384\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e),  \\u003cspan class=\\\"pl-c\\\"\\u003e# dimensions at each stage\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,              \\u003cspan class=\\\"pl-c\\\"\\u003e# transformer of depth 4 at each stage\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e),      \\u003cspan class=\\\"pl-c\\\"\\u003e# heads at each stage\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_mult\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-en\\\"\\u003elevit\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eCvT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-cvt\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: CvT\\\" href=\\\"#cvt\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/cvt.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/cvt.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2103.15808\\\" rel=\\\"nofollow\\\"\\u003eThis paper\\u003c/a\\u003e proposes mixing convolutions and attention. Specifically, convolutions are used to embed and downsample the image / feature map in three stages. Depthwise-convoltion is also used to project the queries, keys, and values for attention.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.cvt import CvT\\n\\nv = CvT(\\n    num_classes = 1000,\\n    s1_emb_dim = 64,        # stage 1 - dimension\\n    s1_emb_kernel = 7,      # stage 1 - conv kernel\\n    s1_emb_stride = 4,      # stage 1 - conv stride\\n    s1_proj_kernel = 3,     # stage 1 - attention ds-conv kernel size\\n    s1_kv_proj_stride = 2,  # stage 1 - attention key / value projection stride\\n    s1_heads = 1,           # stage 1 - heads\\n    s1_depth = 1,           # stage 1 - depth\\n    s1_mlp_mult = 4,        # stage 1 - feedforward expansion factor\\n    s2_emb_dim = 192,       # stage 2 - (same as above)\\n    s2_emb_kernel = 3,\\n    s2_emb_stride = 2,\\n    s2_proj_kernel = 3,\\n    s2_kv_proj_stride = 2,\\n    s2_heads = 3,\\n    s2_depth = 2,\\n    s2_mlp_mult = 4,\\n    s3_emb_dim = 384,       # stage 3 - (same as above)\\n    s3_emb_kernel = 3,\\n    s3_emb_stride = 2,\\n    s3_proj_kernel = 3,\\n    s3_kv_proj_stride = 2,\\n    s3_heads = 4,\\n    s3_depth = 10,\\n    s3_mlp_mult = 4,\\n    dropout = 0.\\n)\\n\\nimg = torch.randn(1, 3, 224, 224)\\n\\npred = v(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003ecvt\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eCvT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eCvT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_emb_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e64\\u003c/span\\u003e,        \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 1 - dimension\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_emb_kernel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 1 - conv kernel\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_emb_stride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 1 - conv stride\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_proj_kernel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,     \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 1 - attention ds-conv kernel size\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_kv_proj_stride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,  \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 1 - attention key / value projection stride\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e,           \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 1 - heads\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e,           \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 1 - depth\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_mlp_mult\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,        \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 1 - feedforward expansion factor\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_emb_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e192\\u003c/span\\u003e,       \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 2 - (same as above)\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_emb_kernel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_emb_stride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_proj_kernel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_kv_proj_stride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_mlp_mult\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_emb_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e384\\u003c/span\\u003e,       \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 3 - (same as above)\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_emb_kernel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_emb_stride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_proj_kernel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_kv_proj_stride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e10\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_mlp_mult\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epred\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eTwins SVT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-twins-svt\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Twins SVT\\\" href=\\\"#twins-svt\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/twins_svt.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/twins_svt.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis \\u003ca href=\\\"https://arxiv.org/abs/2104.13840\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e proposes mixing local and global attention, along with position encoding generator (proposed in \\u003ca href=\\\"https://arxiv.org/abs/2102.10882\\\" rel=\\\"nofollow\\\"\\u003eCPVT\\u003c/a\\u003e) and global average pooling, to achieve the same results as \\u003ca href=\\\"https://arxiv.org/abs/2103.14030\\\" rel=\\\"nofollow\\\"\\u003eSwin\\u003c/a\\u003e, without the extra complexity of shifted windows, CLS tokens, nor positional embeddings.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.twins_svt import TwinsSVT\\n\\nmodel = TwinsSVT(\\n    num_classes = 1000,       # number of output classes\\n    s1_emb_dim = 64,          # stage 1 - patch embedding projected dimension\\n    s1_patch_size = 4,        # stage 1 - patch size for patch embedding\\n    s1_local_patch_size = 7,  # stage 1 - patch size for local attention\\n    s1_global_k = 7,          # stage 1 - global attention key / value reduction factor, defaults to 7 as specified in paper\\n    s1_depth = 1,             # stage 1 - number of transformer blocks (local attn -\\u0026gt; ff -\\u0026gt; global attn -\\u0026gt; ff)\\n    s2_emb_dim = 128,         # stage 2 (same as above)\\n    s2_patch_size = 2,\\n    s2_local_patch_size = 7,\\n    s2_global_k = 7,\\n    s2_depth = 1,\\n    s3_emb_dim = 256,         # stage 3 (same as above)\\n    s3_patch_size = 2,\\n    s3_local_patch_size = 7,\\n    s3_global_k = 7,\\n    s3_depth = 5,\\n    s4_emb_dim = 512,         # stage 4 (same as above)\\n    s4_patch_size = 2,\\n    s4_local_patch_size = 7,\\n    s4_global_k = 7,\\n    s4_depth = 4,\\n    peg_kernel_size = 3,      # positional encoding generator kernel size\\n    dropout = 0.              # dropout\\n)\\n\\nimg = torch.randn(1, 3, 224, 224)\\n\\npred = model(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003etwins_svt\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTwinsSVT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eTwinsSVT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,       \\u003cspan class=\\\"pl-c\\\"\\u003e# number of output classes\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_emb_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e64\\u003c/span\\u003e,          \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 1 - patch embedding projected dimension\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,        \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 1 - patch size for patch embedding\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_local_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e,  \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 1 - patch size for local attention\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_global_k\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e,          \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 1 - global attention key / value reduction factor, defaults to 7 as specified in paper\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e,             \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 1 - number of transformer blocks (local attn -\\u0026gt; ff -\\u0026gt; global attn -\\u0026gt; ff)\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_emb_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e,         \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 2 (same as above)\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_local_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_global_k\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_emb_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,         \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 3 (same as above)\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_local_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_global_k\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e5\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es4_emb_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,         \\u003cspan class=\\\"pl-c\\\"\\u003e# stage 4 (same as above)\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es4_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es4_local_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es4_global_k\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es4_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epeg_kernel_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# positional encoding generator kernel size\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.\\u003c/span\\u003e              \\u003cspan class=\\\"pl-c\\\"\\u003e# dropout\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epred\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eRegionViT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-regionvit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: RegionViT\\\" href=\\\"#regionvit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/regionvit.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/regionvit.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/regionvit2.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/regionvit2.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2106.02689\\\" rel=\\\"nofollow\\\"\\u003eThis paper\\u003c/a\\u003e proposes to divide up the feature map into local regions, whereby the local tokens attend to each other. Each local region has its own regional token which then attends to all its local tokens, as well as other regional tokens.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use it as follows\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.regionvit import RegionViT\\n\\nmodel = RegionViT(\\n    dim = (64, 128, 256, 512),      # tuple of size 4, indicating dimension at each stage\\n    depth = (2, 2, 8, 2),           # depth of the region to local transformer at each stage\\n    window_size = 7,                # window size, which should be either 7 or 14\\n    num_classes = 1000,             # number of output classes\\n    tokenize_local_3_conv = False,  # whether to use a 3 layer convolution to encode the local tokens from the image. the paper uses this for the smaller models, but uses only 1 conv (set to False) for the larger models\\n    use_peg = False,                # whether to use positional generating module. they used this for object detection for a boost in performance\\n)\\n\\nimg = torch.randn(1, 3, 224, 224)\\n\\npred = model(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eregionvit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eRegionViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eRegionViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e64\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e),      \\u003cspan class=\\\"pl-c\\\"\\u003e# tuple of size 4, indicating dimension at each stage\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e),           \\u003cspan class=\\\"pl-c\\\"\\u003e# depth of the region to local transformer at each stage\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ewindow_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e,                \\u003cspan class=\\\"pl-c\\\"\\u003e# window size, which should be either 7 or 14\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,             \\u003cspan class=\\\"pl-c\\\"\\u003e# number of output classes\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003etokenize_local_3_conv\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eFalse\\u003c/span\\u003e,  \\u003cspan class=\\\"pl-c\\\"\\u003e# whether to use a 3 layer convolution to encode the local tokens from the image. the paper uses this for the smaller models, but uses only 1 conv (set to False) for the larger models\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003euse_peg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eFalse\\u003c/span\\u003e,                \\u003cspan class=\\\"pl-c\\\"\\u003e# whether to use positional generating module. they used this for object detection for a boost in performance\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epred\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eCrossFormer\\u003c/h2\\u003e\\u003ca id=\\\"user-content-crossformer\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: CrossFormer\\\" href=\\\"#crossformer\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/crossformer.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/crossformer.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/crossformer2.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/crossformer2.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis \\u003ca href=\\\"https://arxiv.org/abs/2108.00154\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e beats PVT and Swin using alternating local and global attention. The global attention is done across the windowing dimension for reduced complexity, much like the scheme used for axial attention.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThey also have cross-scale embedding layer, which they shown to be a generic layer that can improve all vision transformers. Dynamic relative positional bias was also formulated to allow the net to generalize to images of greater resolution.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.crossformer import CrossFormer\\n\\nmodel = CrossFormer(\\n    num_classes = 1000,                # number of output classes\\n    dim = (64, 128, 256, 512),         # dimension at each stage\\n    depth = (2, 2, 8, 2),              # depth of transformer at each stage\\n    global_window_size = (8, 4, 2, 1), # global window sizes at each stage\\n    local_window_size = 7,             # local window size (can be customized for each stage, but in paper, held constant at 7 for all stages)\\n)\\n\\nimg = torch.randn(1, 3, 224, 224)\\n\\npred = model(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003ecrossformer\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eCrossFormer\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eCrossFormer\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,                \\u003cspan class=\\\"pl-c\\\"\\u003e# number of output classes\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e64\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e),         \\u003cspan class=\\\"pl-c\\\"\\u003e# dimension at each stage\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e),              \\u003cspan class=\\\"pl-c\\\"\\u003e# depth of transformer at each stage\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eglobal_window_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e), \\u003cspan class=\\\"pl-c\\\"\\u003e# global window sizes at each stage\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elocal_window_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e,             \\u003cspan class=\\\"pl-c\\\"\\u003e# local window size (can be customized for each stage, but in paper, held constant at 7 for all stages)\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epred\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eScalableViT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-scalablevit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: ScalableViT\\\" href=\\\"#scalablevit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/scalable-vit-1.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/scalable-vit-1.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/scalable-vit-2.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/scalable-vit-2.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis Bytedance AI \\u003ca href=\\\"https://arxiv.org/abs/2203.10790\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e proposes the Scalable Self Attention (SSA) and the Interactive Windowed Self Attention (IWSA) modules. The SSA alleviates the computation needed at earlier stages by reducing the key / value feature map by some factor (\\u003ccode\\u003ereduction_factor\\u003c/code\\u003e), while modulating the dimension of the queries and keys (\\u003ccode\\u003essa_dim_key\\u003c/code\\u003e). The IWSA performs self attention within local windows, similar to other vision transformer papers. However, they add a residual of the values, passed through a convolution of kernel size 3, which they named Local Interactive Module (LIM).\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThey make the claim in this paper that this scheme outperforms Swin Transformer, and also demonstrate competitive performance against Crossformer.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use it as follows (ex. ScalableViT-S)\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.scalable_vit import ScalableViT\\n\\nmodel = ScalableViT(\\n    num_classes = 1000,\\n    dim = 64,                               # starting model dimension. at every stage, dimension is doubled\\n    heads = (2, 4, 8, 16),                  # number of attention heads at each stage\\n    depth = (2, 2, 20, 2),                  # number of transformer blocks at each stage\\n    ssa_dim_key = (40, 40, 40, 32),         # the dimension of the attention keys (and queries) for SSA. in the paper, they represented this as a scale factor on the base dimension per key (ssa_dim_key / dim_key)\\n    reduction_factor = (8, 4, 2, 1),        # downsampling of the key / values in SSA. in the paper, this was represented as (reduction_factor ** -2)\\n    window_size = (64, 32, None, None),     # window size of the IWSA at each stage. None means no windowing needed\\n    dropout = 0.1,                          # attention and feedforward dropout\\n)\\n\\nimg = torch.randn(1, 3, 256, 256)\\n\\npreds = model(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003escalable_vit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eScalableViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eScalableViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e64\\u003c/span\\u003e,                               \\u003cspan class=\\\"pl-c\\\"\\u003e# starting model dimension. at every stage, dimension is doubled\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e),                  \\u003cspan class=\\\"pl-c\\\"\\u003e# number of attention heads at each stage\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e20\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e),                  \\u003cspan class=\\\"pl-c\\\"\\u003e# number of transformer blocks at each stage\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003essa_dim_key\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e40\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e40\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e40\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e),         \\u003cspan class=\\\"pl-c\\\"\\u003e# the dimension of the attention keys (and queries) for SSA. in the paper, they represented this as a scale factor on the base dimension per key (ssa_dim_key / dim_key)\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ereduction_factor\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e),        \\u003cspan class=\\\"pl-c\\\"\\u003e# downsampling of the key / values in SSA. in the paper, this was represented as (reduction_factor ** -2)\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ewindow_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e64\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003eNone\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003eNone\\u003c/span\\u003e),     \\u003cspan class=\\\"pl-c\\\"\\u003e# window size of the IWSA at each stage. None means no windowing needed\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,                          \\u003cspan class=\\\"pl-c\\\"\\u003e# attention and feedforward dropout\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emodel\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eSepViT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-sepvit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: SepViT\\\" href=\\\"#sepvit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/sep-vit.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/sep-vit.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eAnother \\u003ca href=\\\"https://arxiv.org/abs/2203.15380\\\" rel=\\\"nofollow\\\"\\u003eBytedance AI paper\\u003c/a\\u003e, it proposes a depthwise-pointwise self-attention layer that seems largely inspired by mobilenet's depthwise-separable convolution. The most interesting aspect is the reuse of the feature map from the depthwise self-attention stage as the values for the pointwise self-attention, as shown in the diagram above.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eI have decided to include only the version of \\u003ccode\\u003eSepViT\\u003c/code\\u003e with this specific self-attention layer, as the grouped attention layers are not remarkable nor novel, and the authors were not clear on how they treated the window tokens for the group self-attention layer. Besides, it seems like with \\u003ccode\\u003eDSSA\\u003c/code\\u003e layer alone, they were able to beat Swin.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eex. SepViT-Lite\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.sep_vit import SepViT\\n\\nv = SepViT(\\n    num_classes = 1000,\\n    dim = 32,               # dimensions of first stage, which doubles every stage (32, 64, 128, 256) for SepViT-Lite\\n    dim_head = 32,          # attention head dimension\\n    heads = (1, 2, 4, 8),   # number of heads per stage\\n    depth = (1, 2, 6, 2),   # number of transformer blocks per stage\\n    window_size = 7,        # window size of DSS Attention block\\n    dropout = 0.1           # dropout\\n)\\n\\nimg = torch.randn(1, 3, 224, 224)\\n\\npreds = v(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003esep_vit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eSepViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eSepViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,               \\u003cspan class=\\\"pl-c\\\"\\u003e# dimensions of first stage, which doubles every stage (32, 64, 128, 256) for SepViT-Lite\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim_head\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,          \\u003cspan class=\\\"pl-c\\\"\\u003e# attention head dimension\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e),   \\u003cspan class=\\\"pl-c\\\"\\u003e# number of heads per stage\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e),   \\u003cspan class=\\\"pl-c\\\"\\u003e# number of transformer blocks per stage\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ewindow_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e,        \\u003cspan class=\\\"pl-c\\\"\\u003e# window size of DSS Attention block\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e           \\u003cspan class=\\\"pl-c\\\"\\u003e# dropout\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eMaxViT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-maxvit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: MaxViT\\\" href=\\\"#maxvit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/max-vit.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/max-vit.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2204.01697\\\" rel=\\\"nofollow\\\"\\u003eThis paper\\u003c/a\\u003e proposes a hybrid convolutional / attention network, using MBConv from the convolution side, and then block / grid axial sparse attention.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThey also claim this specific vision transformer is good for generative models (GANs).\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eex. MaxViT-S\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.max_vit import MaxViT\\n\\nv = MaxViT(\\n    num_classes = 1000,\\n    dim_conv_stem = 64,               # dimension of the convolutional stem, would default to dimension of first layer if not specified\\n    dim = 96,                         # dimension of first layer, doubles every layer\\n    dim_head = 32,                    # dimension of attention heads, kept at 32 in paper\\n    depth = (2, 2, 5, 2),             # number of MaxViT blocks per stage, which consists of MBConv, block-like attention, grid-like attention\\n    window_size = 7,                  # window size for block and grids\\n    mbconv_expansion_rate = 4,        # expansion rate of MBConv\\n    mbconv_shrinkage_rate = 0.25,     # shrinkage rate of squeeze-excitation in MBConv\\n    dropout = 0.1                     # dropout\\n)\\n\\nimg = torch.randn(2, 3, 224, 224)\\n\\npreds = v(img) # (2, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003emax_vit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eMaxViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eMaxViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim_conv_stem\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e64\\u003c/span\\u003e,               \\u003cspan class=\\\"pl-c\\\"\\u003e# dimension of the convolutional stem, would default to dimension of first layer if not specified\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e96\\u003c/span\\u003e,                         \\u003cspan class=\\\"pl-c\\\"\\u003e# dimension of first layer, doubles every layer\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim_head\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,                    \\u003cspan class=\\\"pl-c\\\"\\u003e# dimension of attention heads, kept at 32 in paper\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e5\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e),             \\u003cspan class=\\\"pl-c\\\"\\u003e# number of MaxViT blocks per stage, which consists of MBConv, block-like attention, grid-like attention\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ewindow_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e,                  \\u003cspan class=\\\"pl-c\\\"\\u003e# window size for block and grids\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003embconv_expansion_rate\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,        \\u003cspan class=\\\"pl-c\\\"\\u003e# expansion rate of MBConv\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003embconv_shrinkage_rate\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.25\\u003c/span\\u003e,     \\u003cspan class=\\\"pl-c\\\"\\u003e# shrinkage rate of squeeze-excitation in MBConv\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e                     \\u003cspan class=\\\"pl-c\\\"\\u003e# dropout\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (2, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eNesT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-nest\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: NesT\\\" href=\\\"#nest\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/nest.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/nest.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis \\u003ca href=\\\"https://arxiv.org/abs/2105.12723\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e decided to process the image in hierarchical stages, with attention only within tokens of local blocks, which aggregate as it moves up the hierarchy. The aggregation is done in the image plane, and contains a convolution and subsequent maxpool to allow it to pass information across the boundary.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use it with the following code (ex. NesT-T)\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.nest import NesT\\n\\nnest = NesT(\\n    image_size = 224,\\n    patch_size = 4,\\n    dim = 96,\\n    heads = 3,\\n    num_hierarchies = 3,        # number of hierarchies\\n    block_repeats = (2, 2, 8),  # the number of transformer blocks at each hierarchy, starting from the bottom\\n    num_classes = 1000\\n)\\n\\nimg = torch.randn(1, 3, 224, 224)\\n\\npred = nest(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003enest\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eNesT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003enest\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eNesT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e96\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_hierarchies\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,        \\u003cspan class=\\\"pl-c\\\"\\u003e# number of hierarchies\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eblock_repeats\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e),  \\u003cspan class=\\\"pl-c\\\"\\u003e# the number of transformer blocks at each hierarchy, starting from the bottom\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epred\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003enest\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eMobileViT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-mobilevit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: MobileViT\\\" href=\\\"#mobilevit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/mbvit.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/mbvit.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis \\u003ca href=\\\"https://arxiv.org/abs/2110.02178\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e introduce MobileViT, a light-weight and general purpose vision transformer for mobile devices. MobileViT presents a different\\nperspective for the global processing of information with transformers.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use it with the following code (ex. mobilevit_xs)\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.mobile_vit import MobileViT\\n\\nmbvit_xs = MobileViT(\\n    image_size = (256, 256),\\n    dims = [96, 120, 144],\\n    channels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],\\n    num_classes = 1000\\n)\\n\\nimg = torch.randn(1, 3, 256, 256)\\n\\npred = mbvit_xs(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003emobile_vit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eMobileViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003embvit_xs\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eMobileViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e),\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edims\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e [\\u003cspan class=\\\"pl-c1\\\"\\u003e96\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e120\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e144\\u003c/span\\u003e],\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003echannels\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e [\\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e48\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e48\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e64\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e64\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e80\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e80\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e96\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e96\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e384\\u003c/span\\u003e],\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epred\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003embvit_xs\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eXCiT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-xcit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: XCiT\\\" href=\\\"#xcit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/xcit.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/xcit.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis \\u003ca href=\\\"https://arxiv.org/abs/2106.09681\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e introduces the cross covariance attention (abbreviated XCA). One can think of it as doing attention across the features dimension rather than the spatial one (another perspective would be a dynamic 1x1 convolution, the kernel being attention map defined by spatial correlations).\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eTechnically, this amounts to simply transposing the query, key, values before executing cosine similarity attention with learned temperature.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.xcit import XCiT\\n\\nv = XCiT(\\n    image_size = 256,\\n    patch_size = 32,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 12,                     # depth of xcit transformer\\n    cls_depth = 2,                  # depth of cross attention of CLS tokens to patch, attention pool at end\\n    heads = 16,\\n    mlp_dim = 2048,\\n    dropout = 0.1,\\n    emb_dropout = 0.1,\\n    layer_dropout = 0.05,           # randomly dropout 5% of the layers\\n    local_patch_kernel_size = 3     # kernel size of the local patch interaction module (depthwise convs)\\n)\\n\\nimg = torch.randn(1, 3, 256, 256)\\n\\npreds = v(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003excit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eXCiT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eXCiT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e12\\u003c/span\\u003e,                     \\u003cspan class=\\\"pl-c\\\"\\u003e# depth of xcit transformer\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecls_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,                  \\u003cspan class=\\\"pl-c\\\"\\u003e# depth of cross attention of CLS tokens to patch, attention pool at end\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elayer_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.05\\u003c/span\\u003e,           \\u003cspan class=\\\"pl-c\\\"\\u003e# randomly dropout 5% of the layers\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elocal_patch_kernel_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e     \\u003cspan class=\\\"pl-c\\\"\\u003e# kernel size of the local patch interaction module (depthwise convs)\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eSimple Masked Image Modeling\\u003c/h2\\u003e\\u003ca id=\\\"user-content-simple-masked-image-modeling\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Simple Masked Image Modeling\\\" href=\\\"#simple-masked-image-modeling\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/simmim.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/simmim.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis \\u003ca href=\\\"https://arxiv.org/abs/2111.09886\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e proposes a simple masked image modeling (SimMIM) scheme, using only a linear projection off the masked tokens into pixel space followed by an L1 loss with the pixel values of the masked patches. Results are competitive with other more complicated approaches.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use this as follows\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch import ViT\\nfrom vit_pytorch.simmim import SimMIM\\n\\nv = ViT(\\n    image_size = 256,\\n    patch_size = 32,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 6,\\n    heads = 8,\\n    mlp_dim = 2048\\n)\\n\\nmim = SimMIM(\\n    encoder = v,\\n    masking_ratio = 0.5  # they found 50% to yield the best results\\n)\\n\\nimages = torch.randn(8, 3, 256, 256)\\n\\nloss = mim(images)\\nloss.backward()\\n\\n# that's all!\\n# do the above in a for loop many times with a lot of images and your vision transformer will learn\\n\\ntorch.save(v.state_dict(), './trained-vit.pt')\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003esimmim\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eSimMIM\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eSimMIM\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eencoder\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emasking_ratio\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.5\\u003c/span\\u003e  \\u003cspan class=\\\"pl-c\\\"\\u003e# they found 50% to yield the best results\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimages\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emim\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimages\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ebackward\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# that's all!\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# do the above in a for loop many times with a lot of images and your vision transformer will learn\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003esave\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003estate_dict\\u003c/span\\u003e(), \\u003cspan class=\\\"pl-s\\\"\\u003e'./trained-vit.pt'\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eMasked Autoencoder\\u003c/h2\\u003e\\u003ca id=\\\"user-content-masked-autoencoder\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Masked Autoencoder\\\" href=\\\"#masked-autoencoder\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/mae.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/mae.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eA new \\u003ca href=\\\"https://arxiv.org/abs/2111.06377\\\" rel=\\\"nofollow\\\"\\u003eKaiming He paper\\u003c/a\\u003e proposes a simple autoencoder scheme where the vision transformer attends to a set of unmasked patches, and a smaller decoder tries to reconstruct the masked pixel values.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://www.youtube.com/watch?v=LKixq2S2Pz8\\\" rel=\\\"nofollow\\\"\\u003eDeepReader quick paper review\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://www.youtube.com/watch?v=Dp6iICL2dVI\\\" rel=\\\"nofollow\\\"\\u003eAI Coffeebreak with Letitia\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use it with the following code\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch import ViT, MAE\\n\\nv = ViT(\\n    image_size = 256,\\n    patch_size = 32,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 6,\\n    heads = 8,\\n    mlp_dim = 2048\\n)\\n\\nmae = MAE(\\n    encoder = v,\\n    masking_ratio = 0.75,   # the paper recommended 75% masked patches\\n    decoder_dim = 512,      # paper showed good results with just 512\\n    decoder_depth = 6       # anywhere from 1 to 8\\n)\\n\\nimages = torch.randn(8, 3, 256, 256)\\n\\nloss = mae(images)\\nloss.backward()\\n\\n# that's all!\\n# do the above in a for loop many times with a lot of images and your vision transformer will learn\\n\\n# save your improved vision transformer\\ntorch.save(v.state_dict(), './trained-vit.pt')\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eMAE\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emae\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eMAE\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eencoder\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emasking_ratio\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.75\\u003c/span\\u003e,   \\u003cspan class=\\\"pl-c\\\"\\u003e# the paper recommended 75% masked patches\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edecoder_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# paper showed good results with just 512\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edecoder_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e       \\u003cspan class=\\\"pl-c\\\"\\u003e# anywhere from 1 to 8\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimages\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emae\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimages\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ebackward\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# that's all!\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# do the above in a for loop many times with a lot of images and your vision transformer will learn\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# save your improved vision transformer\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003esave\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003estate_dict\\u003c/span\\u003e(), \\u003cspan class=\\\"pl-s\\\"\\u003e'./trained-vit.pt'\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eMasked Patch Prediction\\u003c/h2\\u003e\\u003ca id=\\\"user-content-masked-patch-prediction\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Masked Patch Prediction\\\" href=\\\"#masked-patch-prediction\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThanks to \\u003ca href=\\\"https://github.com/zankner\\\"\\u003eZach\\u003c/a\\u003e, you can train using the original masked patch prediction task presented in the paper, with the following code.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch import ViT\\nfrom vit_pytorch.mpp import MPP\\n\\nmodel = ViT(\\n    image_size=256,\\n    patch_size=32,\\n    num_classes=1000,\\n    dim=1024,\\n    depth=6,\\n    heads=8,\\n    mlp_dim=2048,\\n    dropout=0.1,\\n    emb_dropout=0.1\\n)\\n\\nmpp_trainer = MPP(\\n    transformer=model,\\n    patch_size=32,\\n    dim=1024,\\n    mask_prob=0.15,          # probability of using token in masked prediction task\\n    random_patch_prob=0.30,  # probability of randomly replacing a token being used for mpp\\n    replace_prob=0.50,       # probability of replacing a token being used for mpp with the mask token\\n)\\n\\nopt = torch.optim.Adam(mpp_trainer.parameters(), lr=3e-4)\\n\\ndef sample_unlabelled_images():\\n    return torch.FloatTensor(20, 3, 256, 256).uniform_(0., 1.)\\n\\nfor _ in range(100):\\n    images = sample_unlabelled_images()\\n    loss = mpp_trainer(images)\\n    opt.zero_grad()\\n    loss.backward()\\n    opt.step()\\n\\n# save your improved network\\ntorch.save(model.state_dict(), './pretrained-net.pt')\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003empp\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eMPP\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003empp_trainer\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eMPP\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003etransformer\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emask_prob\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e0.15\\u003c/span\\u003e,          \\u003cspan class=\\\"pl-c\\\"\\u003e# probability of using token in masked prediction task\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003erandom_patch_prob\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e0.30\\u003c/span\\u003e,  \\u003cspan class=\\\"pl-c\\\"\\u003e# probability of randomly replacing a token being used for mpp\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ereplace_prob\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e0.50\\u003c/span\\u003e,       \\u003cspan class=\\\"pl-c\\\"\\u003e# probability of replacing a token being used for mpp with the mask token\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eopt\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eoptim\\u003c/span\\u003e.\\u003cspan class=\\\"pl-v\\\"\\u003eAdam\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003empp_trainer\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eparameters\\u003c/span\\u003e(), \\u003cspan class=\\\"pl-s1\\\"\\u003elr\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e\\u003cspan class=\\\"pl-c1\\\"\\u003e3e-4\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003edef\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003esample_unlabelled_images\\u003c/span\\u003e():\\n    \\u003cspan class=\\\"pl-k\\\"\\u003ereturn\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-v\\\"\\u003eFloatTensor\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e20\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e).\\u003cspan class=\\\"pl-en\\\"\\u003euniform_\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e0.\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1.\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003efor\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003e_\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003ein\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003erange\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e100\\u003c/span\\u003e):\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimages\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003esample_unlabelled_images\\u003c/span\\u003e()\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003empp_trainer\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimages\\u003c/span\\u003e)\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eopt\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ezero_grad\\u003c/span\\u003e()\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ebackward\\u003c/span\\u003e()\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eopt\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003estep\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# save your improved network\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003esave\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003estate_dict\\u003c/span\\u003e(), \\u003cspan class=\\\"pl-s\\\"\\u003e'./pretrained-net.pt'\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eMasked Position Prediction\\u003c/h2\\u003e\\u003ca id=\\\"user-content-masked-position-prediction\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Masked Position Prediction\\\" href=\\\"#masked-position-prediction\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/mp3.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/mp3.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eNew \\u003ca href=\\\"https://arxiv.org/abs/2207.07611\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e that introduces masked position prediction pre-training criteria. This strategy is more efficient than the Masked Autoencoder strategy and has comparable performance.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.mp3 import ViT, MP3\\n\\nv = ViT(\\n    num_classes = 1000,\\n    image_size = 256,\\n    patch_size = 8,\\n    dim = 1024,\\n    depth = 6,\\n    heads = 8,\\n    mlp_dim = 2048,\\n    dropout = 0.1,\\n)\\n\\nmp3 = MP3(\\n    vit = v,\\n    masking_ratio = 0.75\\n)\\n\\nimages = torch.randn(8, 3, 256, 256)\\n\\nloss = mp3(images)\\nloss.backward()\\n\\n# that's all!\\n# do the above in a for loop many times with a lot of images and your vision transformer will learn\\n\\n# save your improved vision transformer\\ntorch.save(v.state_dict(), './trained-vit.pt')\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003emp3\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eMP3\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emp3\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eMP3\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003evit\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emasking_ratio\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.75\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimages\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emp3\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimages\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ebackward\\u003c/span\\u003e()\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# that's all!\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# do the above in a for loop many times with a lot of images and your vision transformer will learn\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# save your improved vision transformer\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003esave\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003estate_dict\\u003c/span\\u003e(), \\u003cspan class=\\\"pl-s\\\"\\u003e'./trained-vit.pt'\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eAdaptive Token Sampling\\u003c/h2\\u003e\\u003ca id=\\\"user-content-adaptive-token-sampling\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Adaptive Token Sampling\\\" href=\\\"#adaptive-token-sampling\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/ats.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/ats.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis \\u003ca href=\\\"https://arxiv.org/abs/2111.15667\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e proposes to use the CLS attention scores, re-weighed by the norms of the value heads, as means to discard unimportant tokens at different layers.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.ats_vit import ViT\\n\\nv = ViT(\\n    image_size = 256,\\n    patch_size = 16,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 6,\\n    max_tokens_per_depth = (256, 128, 64, 32, 16, 8), # a tuple that denotes the maximum number of tokens that any given layer should have. if the layer has greater than this amount, it will undergo adaptive token sampling\\n    heads = 16,\\n    mlp_dim = 2048,\\n    dropout = 0.1,\\n    emb_dropout = 0.1\\n)\\n\\nimg = torch.randn(4, 3, 256, 256)\\n\\npreds = v(img) # (4, 1000)\\n\\n# you can also get a list of the final sampled patch ids\\n# a value of -1 denotes padding\\n\\npreds, token_ids = v(img, return_sampled_token_ids = True) # (4, 1000), (4, \\u0026lt;=8)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eats_vit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emax_tokens_per_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e64\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e), \\u003cspan class=\\\"pl-c\\\"\\u003e# a tuple that denotes the maximum number of tokens that any given layer should have. if the layer has greater than this amount, it will undergo adaptive token sampling\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (4, 1000)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# you can also get a list of the final sampled patch ids\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# a value of -1 denotes padding\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003etoken_ids\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003ereturn_sampled_token_ids\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (4, 1000), (4, \\u0026lt;=8)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003ePatch Merger\\u003c/h2\\u003e\\u003ca id=\\\"user-content-patch-merger\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Patch Merger\\\" href=\\\"#patch-merger\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/patch_merger.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/patch_merger.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis \\u003ca href=\\\"https://arxiv.org/abs/2202.12015\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e proposes a simple module (Patch Merger) for reducing the number of tokens at any layer of a vision transformer without sacrificing performance.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.vit_with_patch_merger import ViT\\n\\nv = ViT(\\n    image_size = 256,\\n    patch_size = 16,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 12,\\n    heads = 8,\\n    patch_merge_layer = 6,        # at which transformer layer to do patch merging\\n    patch_merge_num_tokens = 8,   # the output number of tokens from the patch merge\\n    mlp_dim = 2048,\\n    dropout = 0.1,\\n    emb_dropout = 0.1\\n)\\n\\nimg = torch.randn(4, 3, 256, 256)\\n\\npreds = v(img) # (4, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003evit_with_patch_merger\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e12\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_merge_layer\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,        \\u003cspan class=\\\"pl-c\\\"\\u003e# at which transformer layer to do patch merging\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_merge_num_tokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,   \\u003cspan class=\\\"pl-c\\\"\\u003e# the output number of tokens from the patch merge\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (4, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eOne can also use the \\u003ccode\\u003ePatchMerger\\u003c/code\\u003e module by itself\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.vit_with_patch_merger import PatchMerger\\n\\nmerger = PatchMerger(\\n    dim = 1024,\\n    num_tokens_out = 8   # output number of tokens\\n)\\n\\nfeatures = torch.randn(4, 256, 1024) # (batch, num tokens, dimension)\\n\\nout = merger(features) # (4, 8, 1024)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003evit_with_patch_merger\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003ePatchMerger\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emerger\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003ePatchMerger\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_tokens_out\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e   \\u003cspan class=\\\"pl-c\\\"\\u003e# output number of tokens\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003efeatures\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (batch, num tokens, dimension)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003emerger\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003efeatures\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (4, 8, 1024)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eVision Transformer for Small Datasets\\u003c/h2\\u003e\\u003ca id=\\\"user-content-vision-transformer-for-small-datasets\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Vision Transformer for Small Datasets\\\" href=\\\"#vision-transformer-for-small-datasets\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/vit_for_small_datasets.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/vit_for_small_datasets.png\\\" width=\\\"400px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis \\u003ca href=\\\"https://arxiv.org/abs/2112.13492\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e proposes a new image to patch function that incorporates shifts of the image, before normalizing and dividing the image into patches. I have found shifting to be extremely helpful in some other transformers work, so decided to include this for further explorations. It also includes the \\u003ccode\\u003eLSA\\u003c/code\\u003e with the learned temperature and masking out of a token's attention to itself.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use as follows:\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.vit_for_small_dataset import ViT\\n\\nv = ViT(\\n    image_size = 256,\\n    patch_size = 16,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 6,\\n    heads = 16,\\n    mlp_dim = 2048,\\n    dropout = 0.1,\\n    emb_dropout = 0.1\\n)\\n\\nimg = torch.randn(4, 3, 256, 256)\\n\\npreds = v(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003evit_for_small_dataset\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can also use the \\u003ccode\\u003eSPT\\u003c/code\\u003e from this paper as a standalone module\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.vit_for_small_dataset import SPT\\n\\nspt = SPT(\\n    dim = 1024,\\n    patch_size = 16,\\n    channels = 3\\n)\\n\\nimg = torch.randn(4, 3, 256, 256)\\n\\ntokens = spt(img) # (4, 256, 1024)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003evit_for_small_dataset\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eSPT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003espt\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eSPT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003echannels\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003etokens\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003espt\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (4, 256, 1024)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003e3D ViT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-3d-vit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: 3D ViT\\\" href=\\\"#3d-vit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eBy popular request, I will start extending a few of the architectures in this repository to 3D ViTs, for use with video, medical imaging, etc.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou will need to pass in two additional hyperparameters: (1) the number of frames \\u003ccode\\u003eframes\\u003c/code\\u003e and (2) patch size along the frame dimension \\u003ccode\\u003eframe_patch_size\\u003c/code\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eFor starters, 3D ViT\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.vit_3d import ViT\\n\\nv = ViT(\\n    image_size = 128,          # image size\\n    frames = 16,               # number of frames\\n    image_patch_size = 16,     # image patch size\\n    frame_patch_size = 2,      # frame patch size\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 6,\\n    heads = 8,\\n    mlp_dim = 2048,\\n    dropout = 0.1,\\n    emb_dropout = 0.1\\n)\\n\\nvideo = torch.randn(4, 3, 16, 128, 128) # (batch, channels, frames, height, width)\\n\\npreds = v(video) # (4, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003evit_3d\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e,          \\u003cspan class=\\\"pl-c\\\"\\u003e# image size\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eframes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,               \\u003cspan class=\\\"pl-c\\\"\\u003e# number of frames\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,     \\u003cspan class=\\\"pl-c\\\"\\u003e# image patch size\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eframe_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# frame patch size\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003evideo\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (batch, channels, frames, height, width)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003evideo\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (4, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e3D Simple ViT\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.simple_vit_3d import SimpleViT\\n\\nv = SimpleViT(\\n    image_size = 128,          # image size\\n    frames = 16,               # number of frames\\n    image_patch_size = 16,     # image patch size\\n    frame_patch_size = 2,      # frame patch size\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 6,\\n    heads = 8,\\n    mlp_dim = 2048\\n)\\n\\nvideo = torch.randn(4, 3, 16, 128, 128) # (batch, channels, frames, height, width)\\n\\npreds = v(video) # (4, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003esimple_vit_3d\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eSimpleViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eSimpleViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e,          \\u003cspan class=\\\"pl-c\\\"\\u003e# image size\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eframes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,               \\u003cspan class=\\\"pl-c\\\"\\u003e# number of frames\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,     \\u003cspan class=\\\"pl-c\\\"\\u003e# image patch size\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eframe_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# frame patch size\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003evideo\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (batch, channels, frames, height, width)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003evideo\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (4, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e3D version of \\u003ca href=\\\"https://github.com/lucidrains/vit-pytorch#cct\\\"\\u003eCCT\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.cct_3d import CCT\\n\\ncct = CCT(\\n    img_size = 224,\\n    num_frames = 8,\\n    embedding_dim = 384,\\n    n_conv_layers = 2,\\n    frame_kernel_size = 3,\\n    kernel_size = 7,\\n    stride = 2,\\n    padding = 3,\\n    pooling_kernel_size = 3,\\n    pooling_stride = 2,\\n    pooling_padding = 1,\\n    num_layers = 14,\\n    num_heads = 6,\\n    mlp_ratio = 3.,\\n    num_classes = 1000,\\n    positional_embedding = 'learnable'\\n)\\n\\nvideo = torch.randn(1, 3, 8, 224, 224) # (batch, channels, frames, height, width)\\npred = cct(video)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003ecct_3d\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eCCT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ecct\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eCCT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimg_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_frames\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eembedding_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e384\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003en_conv_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eframe_kernel_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ekernel_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003estride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epadding\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epooling_kernel_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epooling_stride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epooling_padding\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e14\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_ratio\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3.\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epositional_embedding\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s\\\"\\u003e'learnable'\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003evideo\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (batch, channels, frames, height, width)\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epred\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ecct\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003evideo\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eViViT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-vivit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: ViViT\\\" href=\\\"#vivit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/vivit.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/vivit.png\\\" width=\\\"350px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis \\u003ca href=\\\"https://arxiv.org/abs/2103.15691\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e offers 3 different types of architectures for efficient attention of videos, with the main theme being factorizing the attention across space and time. This repository will offer the first variant, which is a spatial transformer followed by a temporal one.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.vivit import ViT\\n\\nv = ViT(\\n    image_size = 128,          # image size\\n    frames = 16,               # number of frames\\n    image_patch_size = 16,     # image patch size\\n    frame_patch_size = 2,      # frame patch size\\n    num_classes = 1000,\\n    dim = 1024,\\n    spatial_depth = 6,         # depth of the spatial transformer\\n    temporal_depth = 6,        # depth of the temporal transformer\\n    heads = 8,\\n    mlp_dim = 2048\\n)\\n\\nvideo = torch.randn(4, 3, 16, 128, 128) # (batch, channels, frames, height, width)\\n\\npreds = v(video) # (4, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003evivit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e,          \\u003cspan class=\\\"pl-c\\\"\\u003e# image size\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eframes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,               \\u003cspan class=\\\"pl-c\\\"\\u003e# number of frames\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,     \\u003cspan class=\\\"pl-c\\\"\\u003e# image patch size\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eframe_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# frame patch size\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003espatial_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,         \\u003cspan class=\\\"pl-c\\\"\\u003e# depth of the spatial transformer\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003etemporal_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,        \\u003cspan class=\\\"pl-c\\\"\\u003e# depth of the temporal transformer\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003evideo\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (batch, channels, frames, height, width)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003evideo\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (4, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eParallel ViT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-parallel-vit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Parallel ViT\\\" href=\\\"#parallel-vit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/parallel-vit.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/parallel-vit.png\\\" width=\\\"350px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis \\u003ca href=\\\"https://arxiv.org/abs/2203.09795\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e propose parallelizing multiple attention and feedforward blocks per layer (2 blocks), claiming that it is easier to train without loss of performance.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can try this variant as follows\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.parallel_vit import ViT\\n\\nv = ViT(\\n    image_size = 256,\\n    patch_size = 16,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 6,\\n    heads = 8,\\n    mlp_dim = 2048,\\n    num_parallel_branches = 2,  # in paper, they claimed 2 was optimal\\n    dropout = 0.1,\\n    emb_dropout = 0.1\\n)\\n\\nimg = torch.randn(4, 3, 256, 256)\\n\\npreds = v(img) # (4, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eparallel_vit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_parallel_branches\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,  \\u003cspan class=\\\"pl-c\\\"\\u003e# in paper, they claimed 2 was optimal\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (4, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eLearnable Memory ViT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-learnable-memory-vit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Learnable Memory ViT\\\" href=\\\"#learnable-memory-vit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/learnable-memory-vit.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/learnable-memory-vit.png\\\" width=\\\"350px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis \\u003ca href=\\\"https://arxiv.org/abs/2203.15243\\\" rel=\\\"nofollow\\\"\\u003epaper\\u003c/a\\u003e shows that adding learnable memory tokens at each layer of a vision transformer can greatly enhance fine-tuning results (in addition to learnable task specific CLS token and adapter head).\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can use this with a specially modified \\u003ccode\\u003eViT\\u003c/code\\u003e as follows\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.learnable_memory_vit import ViT, Adapter\\n\\n# normal base ViT\\n\\nv = ViT(\\n    image_size = 256,\\n    patch_size = 16,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 6,\\n    heads = 8,\\n    mlp_dim = 2048,\\n    dropout = 0.1,\\n    emb_dropout = 0.1\\n)\\n\\nimg = torch.randn(4, 3, 256, 256)\\nlogits = v(img) # (4, 1000)\\n\\n# do your usual training with ViT\\n# ...\\n\\n\\n# then, to finetune, just pass the ViT into the Adapter class\\n# you can do this for multiple Adapters, as shown below\\n\\nadapter1 = Adapter(\\n    vit = v,\\n    num_classes = 2,               # number of output classes for this specific task\\n    num_memories_per_layer = 5     # number of learnable memories per layer, 10 was sufficient in paper\\n)\\n\\nlogits1 = adapter1(img) # (4, 2) - predict 2 classes off frozen ViT backbone with learnable memories and task specific head\\n\\n# yet another task to finetune on, this time with 4 classes\\n\\nadapter2 = Adapter(\\n    vit = v,\\n    num_classes = 4,\\n    num_memories_per_layer = 10\\n)\\n\\nlogits2 = adapter2(img) # (4, 4) - predict 4 classes off frozen ViT backbone with learnable memories and task specific head\\n\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003elearnable_memory_vit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eAdapter\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# normal base ViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003elogits\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (4, 1000)\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# do your usual training with ViT\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# ...\\u003c/span\\u003e\\n\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# then, to finetune, just pass the ViT into the Adapter class\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# you can do this for multiple Adapters, as shown below\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eadapter1\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eAdapter\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003evit\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,               \\u003cspan class=\\\"pl-c\\\"\\u003e# number of output classes for this specific task\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_memories_per_layer\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e5\\u003c/span\\u003e     \\u003cspan class=\\\"pl-c\\\"\\u003e# number of learnable memories per layer, 10 was sufficient in paper\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003elogits1\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003eadapter1\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (4, 2) - predict 2 classes off frozen ViT backbone with learnable memories and task specific head\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# yet another task to finetune on, this time with 4 classes\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eadapter2\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eAdapter\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003evit\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_memories_per_layer\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e10\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003elogits2\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003eadapter2\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (4, 4) - predict 4 classes off frozen ViT backbone with learnable memories and task specific head\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eDino\\u003c/h2\\u003e\\u003ca id=\\\"user-content-dino\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Dino\\\" href=\\\"#dino\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/dino.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/dino.png\\\" width=\\\"350px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can train \\u003ccode\\u003eViT\\u003c/code\\u003e with the recent SOTA self-supervised learning technique, \\u003ca href=\\\"https://arxiv.org/abs/2104.14294\\\" rel=\\\"nofollow\\\"\\u003eDino\\u003c/a\\u003e, with the following code.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://www.youtube.com/watch?v=h3ij3F3cPIk\\\" rel=\\\"nofollow\\\"\\u003eYannic Kilcher\\u003c/a\\u003e video\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch import ViT, Dino\\n\\nmodel = ViT(\\n    image_size = 256,\\n    patch_size = 32,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 6,\\n    heads = 8,\\n    mlp_dim = 2048\\n)\\n\\nlearner = Dino(\\n    model,\\n    image_size = 256,\\n    hidden_layer = 'to_latent',        # hidden layer name or index, from which to extract the embedding\\n    projection_hidden_size = 256,      # projector network hidden dimension\\n    projection_layers = 4,             # number of layers in projection network\\n    num_classes_K = 65336,             # output logits dimensions (referenced as K in paper)\\n    student_temp = 0.9,                # student temperature\\n    teacher_temp = 0.04,               # teacher temperature, needs to be annealed from 0.04 to 0.07 over 30 epochs\\n    local_upper_crop_scale = 0.4,      # upper bound for local crop - 0.4 was recommended in the paper \\n    global_lower_crop_scale = 0.5,     # lower bound for global crop - 0.5 was recommended in the paper\\n    moving_average_decay = 0.9,        # moving average of encoder - paper showed anywhere from 0.9 to 0.999 was ok\\n    center_moving_average_decay = 0.9, # moving average of teacher centers - paper showed anywhere from 0.9 to 0.999 was ok\\n)\\n\\nopt = torch.optim.Adam(learner.parameters(), lr = 3e-4)\\n\\ndef sample_unlabelled_images():\\n    return torch.randn(20, 3, 256, 256)\\n\\nfor _ in range(100):\\n    images = sample_unlabelled_images()\\n    loss = learner(images)\\n    opt.zero_grad()\\n    loss.backward()\\n    opt.step()\\n    learner.update_moving_average() # update moving average of teacher encoder and teacher centers\\n\\n# save your improved network\\ntorch.save(model.state_dict(), './pretrained-net.pt')\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e, \\u003cspan class=\\\"pl-v\\\"\\u003eDino\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003elearner\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eDino\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ehidden_layer\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s\\\"\\u003e'to_latent'\\u003c/span\\u003e,        \\u003cspan class=\\\"pl-c\\\"\\u003e# hidden layer name or index, from which to extract the embedding\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eprojection_hidden_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# projector network hidden dimension\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eprojection_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,             \\u003cspan class=\\\"pl-c\\\"\\u003e# number of layers in projection network\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes_K\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e65336\\u003c/span\\u003e,             \\u003cspan class=\\\"pl-c\\\"\\u003e# output logits dimensions (referenced as K in paper)\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003estudent_temp\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.9\\u003c/span\\u003e,                \\u003cspan class=\\\"pl-c\\\"\\u003e# student temperature\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eteacher_temp\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.04\\u003c/span\\u003e,               \\u003cspan class=\\\"pl-c\\\"\\u003e# teacher temperature, needs to be annealed from 0.04 to 0.07 over 30 epochs\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elocal_upper_crop_scale\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.4\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# upper bound for local crop - 0.4 was recommended in the paper \\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eglobal_lower_crop_scale\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.5\\u003c/span\\u003e,     \\u003cspan class=\\\"pl-c\\\"\\u003e# lower bound for global crop - 0.5 was recommended in the paper\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emoving_average_decay\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.9\\u003c/span\\u003e,        \\u003cspan class=\\\"pl-c\\\"\\u003e# moving average of encoder - paper showed anywhere from 0.9 to 0.999 was ok\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecenter_moving_average_decay\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.9\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c\\\"\\u003e# moving average of teacher centers - paper showed anywhere from 0.9 to 0.999 was ok\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eopt\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eoptim\\u003c/span\\u003e.\\u003cspan class=\\\"pl-v\\\"\\u003eAdam\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003elearner\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eparameters\\u003c/span\\u003e(), \\u003cspan class=\\\"pl-s1\\\"\\u003elr\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3e-4\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003edef\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003esample_unlabelled_images\\u003c/span\\u003e():\\n    \\u003cspan class=\\\"pl-k\\\"\\u003ereturn\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e20\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003efor\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003e_\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003ein\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003erange\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e100\\u003c/span\\u003e):\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimages\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003esample_unlabelled_images\\u003c/span\\u003e()\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003elearner\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimages\\u003c/span\\u003e)\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eopt\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ezero_grad\\u003c/span\\u003e()\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ebackward\\u003c/span\\u003e()\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eopt\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003estep\\u003c/span\\u003e()\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elearner\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eupdate_moving_average\\u003c/span\\u003e() \\u003cspan class=\\\"pl-c\\\"\\u003e# update moving average of teacher encoder and teacher centers\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# save your improved network\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003esave\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003emodel\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003estate_dict\\u003c/span\\u003e(), \\u003cspan class=\\\"pl-s\\\"\\u003e'./pretrained-net.pt'\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eEsViT\\u003c/h2\\u003e\\u003ca id=\\\"user-content-esvit\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: EsViT\\\" href=\\\"#esvit\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/lucidrains/vit-pytorch/blob/main/images/esvit.png\\\"\\u003e\\u003cimg src=\\\"/lucidrains/vit-pytorch/raw/main/images/esvit.png\\\" width=\\\"350px\\\" style=\\\"max-width: 100%;\\\"\\u003e\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://arxiv.org/abs/2106.09785\\\" rel=\\\"nofollow\\\"\\u003e\\u003ccode\\u003eEsViT\\u003c/code\\u003e\\u003c/a\\u003e is a variant of Dino (from above) re-engineered to support efficient \\u003ccode\\u003eViT\\u003c/code\\u003es with patch merging / downsampling by taking into an account an extra regional loss between the augmented views. To quote the abstract, it \\u003ccode\\u003eoutperforms its supervised counterpart on 17 out of 18 datasets\\u003c/code\\u003e at 3 times higher throughput.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eEven though it is named as though it were a new \\u003ccode\\u003eViT\\u003c/code\\u003e variant, it actually is just a strategy for training any multistage \\u003ccode\\u003eViT\\u003c/code\\u003e (in the paper, they focused on Swin). The example below will show how to use it with \\u003ccode\\u003eCvT\\u003c/code\\u003e. You'll need to set the \\u003ccode\\u003ehidden_layer\\u003c/code\\u003e to the name of the layer within your efficient ViT that outputs the non-average pooled visual representations, just before the global pooling and projection to logits.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.cvt import CvT\\nfrom vit_pytorch.es_vit import EsViTTrainer\\n\\ncvt = CvT(\\n    num_classes = 1000,\\n    s1_emb_dim = 64,\\n    s1_emb_kernel = 7,\\n    s1_emb_stride = 4,\\n    s1_proj_kernel = 3,\\n    s1_kv_proj_stride = 2,\\n    s1_heads = 1,\\n    s1_depth = 1,\\n    s1_mlp_mult = 4,\\n    s2_emb_dim = 192,\\n    s2_emb_kernel = 3,\\n    s2_emb_stride = 2,\\n    s2_proj_kernel = 3,\\n    s2_kv_proj_stride = 2,\\n    s2_heads = 3,\\n    s2_depth = 2,\\n    s2_mlp_mult = 4,\\n    s3_emb_dim = 384,\\n    s3_emb_kernel = 3,\\n    s3_emb_stride = 2,\\n    s3_proj_kernel = 3,\\n    s3_kv_proj_stride = 2,\\n    s3_heads = 4,\\n    s3_depth = 10,\\n    s3_mlp_mult = 4,\\n    dropout = 0.\\n)\\n\\nlearner = EsViTTrainer(\\n    cvt,\\n    image_size = 256,\\n    hidden_layer = 'layers',           # hidden layer name or index, from which to extract the embedding\\n    projection_hidden_size = 256,      # projector network hidden dimension\\n    projection_layers = 4,             # number of layers in projection network\\n    num_classes_K = 65336,             # output logits dimensions (referenced as K in paper)\\n    student_temp = 0.9,                # student temperature\\n    teacher_temp = 0.04,               # teacher temperature, needs to be annealed from 0.04 to 0.07 over 30 epochs\\n    local_upper_crop_scale = 0.4,      # upper bound for local crop - 0.4 was recommended in the paper\\n    global_lower_crop_scale = 0.5,     # lower bound for global crop - 0.5 was recommended in the paper\\n    moving_average_decay = 0.9,        # moving average of encoder - paper showed anywhere from 0.9 to 0.999 was ok\\n    center_moving_average_decay = 0.9, # moving average of teacher centers - paper showed anywhere from 0.9 to 0.999 was ok\\n)\\n\\nopt = torch.optim.AdamW(learner.parameters(), lr = 3e-4)\\n\\ndef sample_unlabelled_images():\\n    return torch.randn(8, 3, 256, 256)\\n\\nfor _ in range(1000):\\n    images = sample_unlabelled_images()\\n    loss = learner(images)\\n    opt.zero_grad()\\n    loss.backward()\\n    opt.step()\\n    learner.update_moving_average() # update moving average of teacher encoder and teacher centers\\n\\n# save your improved network\\ntorch.save(cvt.state_dict(), './pretrained-net.pt')\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003ecvt\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eCvT\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003ees_vit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eEsViTTrainer\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ecvt\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eCvT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_emb_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e64\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_emb_kernel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e7\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_emb_stride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_proj_kernel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_kv_proj_stride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es1_mlp_mult\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_emb_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e192\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_emb_kernel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_emb_stride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_proj_kernel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_kv_proj_stride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es2_mlp_mult\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_emb_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e384\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_emb_kernel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_emb_stride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_proj_kernel\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_kv_proj_stride\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e10\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003es3_mlp_mult\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003elearner\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eEsViTTrainer\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecvt\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ehidden_layer\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s\\\"\\u003e'layers'\\u003c/span\\u003e,           \\u003cspan class=\\\"pl-c\\\"\\u003e# hidden layer name or index, from which to extract the embedding\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eprojection_hidden_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# projector network hidden dimension\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eprojection_layers\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,             \\u003cspan class=\\\"pl-c\\\"\\u003e# number of layers in projection network\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes_K\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e65336\\u003c/span\\u003e,             \\u003cspan class=\\\"pl-c\\\"\\u003e# output logits dimensions (referenced as K in paper)\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003estudent_temp\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.9\\u003c/span\\u003e,                \\u003cspan class=\\\"pl-c\\\"\\u003e# student temperature\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eteacher_temp\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.04\\u003c/span\\u003e,               \\u003cspan class=\\\"pl-c\\\"\\u003e# teacher temperature, needs to be annealed from 0.04 to 0.07 over 30 epochs\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elocal_upper_crop_scale\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.4\\u003c/span\\u003e,      \\u003cspan class=\\\"pl-c\\\"\\u003e# upper bound for local crop - 0.4 was recommended in the paper\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eglobal_lower_crop_scale\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.5\\u003c/span\\u003e,     \\u003cspan class=\\\"pl-c\\\"\\u003e# lower bound for global crop - 0.5 was recommended in the paper\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emoving_average_decay\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.9\\u003c/span\\u003e,        \\u003cspan class=\\\"pl-c\\\"\\u003e# moving average of encoder - paper showed anywhere from 0.9 to 0.999 was ok\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecenter_moving_average_decay\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.9\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c\\\"\\u003e# moving average of teacher centers - paper showed anywhere from 0.9 to 0.999 was ok\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eopt\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eoptim\\u003c/span\\u003e.\\u003cspan class=\\\"pl-v\\\"\\u003eAdamW\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003elearner\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eparameters\\u003c/span\\u003e(), \\u003cspan class=\\\"pl-s1\\\"\\u003elr\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3e-4\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003edef\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003esample_unlabelled_images\\u003c/span\\u003e():\\n    \\u003cspan class=\\\"pl-k\\\"\\u003ereturn\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003efor\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003e_\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003ein\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003erange\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e):\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimages\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003esample_unlabelled_images\\u003c/span\\u003e()\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003elearner\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimages\\u003c/span\\u003e)\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eopt\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ezero_grad\\u003c/span\\u003e()\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eloss\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003ebackward\\u003c/span\\u003e()\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eopt\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003estep\\u003c/span\\u003e()\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elearner\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eupdate_moving_average\\u003c/span\\u003e() \\u003cspan class=\\\"pl-c\\\"\\u003e# update moving average of teacher encoder and teacher centers\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# save your improved network\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003esave\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ecvt\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003estate_dict\\u003c/span\\u003e(), \\u003cspan class=\\\"pl-s\\\"\\u003e'./pretrained-net.pt'\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eAccessing Attention\\u003c/h2\\u003e\\u003ca id=\\\"user-content-accessing-attention\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Accessing Attention\\\" href=\\\"#accessing-attention\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eIf you would like to visualize the attention weights (post-softmax) for your research, just follow the procedure below\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.vit import ViT\\n\\nv = ViT(\\n    image_size = 256,\\n    patch_size = 32,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 6,\\n    heads = 16,\\n    mlp_dim = 2048,\\n    dropout = 0.1,\\n    emb_dropout = 0.1\\n)\\n\\n# import Recorder and wrap the ViT\\n\\nfrom vit_pytorch.recorder import Recorder\\nv = Recorder(v)\\n\\n# forward pass now returns predictions and the attention maps\\n\\nimg = torch.randn(1, 3, 256, 256)\\npreds, attns = v(img)\\n\\n# there is one extra patch due to the CLS token\\n\\nattns # (1, 6, 16, 65, 65) - (batch x layers x heads x patch x patch)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003evit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# import Recorder and wrap the ViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003erecorder\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eRecorder\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eRecorder\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# forward pass now returns predictions and the attention maps\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eattns\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# there is one extra patch due to the CLS token\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eattns\\u003c/span\\u003e \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 6, 16, 65, 65) - (batch x layers x heads x patch x patch)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eto cleanup the class and the hooks once you have collected enough data\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"v = v.eject()  # wrapper is discarded and original ViT instance is returned\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003eeject\\u003c/span\\u003e()  \\u003cspan class=\\\"pl-c\\\"\\u003e# wrapper is discarded and original ViT instance is returned\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eAccessing Embeddings\\u003c/h2\\u003e\\u003ca id=\\\"user-content-accessing-embeddings\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Accessing Embeddings\\\" href=\\\"#accessing-embeddings\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can similarly access the embeddings with the \\u003ccode\\u003eExtractor\\u003c/code\\u003e wrapper\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.vit import ViT\\n\\nv = ViT(\\n    image_size = 256,\\n    patch_size = 32,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 6,\\n    heads = 16,\\n    mlp_dim = 2048,\\n    dropout = 0.1,\\n    emb_dropout = 0.1\\n)\\n\\n# import Recorder and wrap the ViT\\n\\nfrom vit_pytorch.extractor import Extractor\\nv = Extractor(v)\\n\\n# forward pass now returns predictions and the attention maps\\n\\nimg = torch.randn(1, 3, 256, 256)\\nlogits, embeddings = v(img)\\n\\n# there is one extra token due to the CLS token\\n\\nembeddings # (1, 65, 1024) - (batch x patches x model dim)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003evit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# import Recorder and wrap the ViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eextractor\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eExtractor\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eExtractor\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# forward pass now returns predictions and the attention maps\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003elogits\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eembeddings\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# there is one extra token due to the CLS token\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eembeddings\\u003c/span\\u003e \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 65, 1024) - (batch x patches x model dim)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eOr say for \\u003ccode\\u003eCrossViT\\u003c/code\\u003e, which has a multi-scale encoder that outputs two sets of embeddings for 'large' and 'small' scales\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.cross_vit import CrossViT\\n\\nv = CrossViT(\\n    image_size = 256,\\n    num_classes = 1000,\\n    depth = 4,\\n    sm_dim = 192,\\n    sm_patch_size = 16,\\n    sm_enc_depth = 2,\\n    sm_enc_heads = 8,\\n    sm_enc_mlp_dim = 2048,\\n    lg_dim = 384,\\n    lg_patch_size = 64,\\n    lg_enc_depth = 3,\\n    lg_enc_heads = 8,\\n    lg_enc_mlp_dim = 2048,\\n    cross_attn_depth = 2,\\n    cross_attn_heads = 8,\\n    dropout = 0.1,\\n    emb_dropout = 0.1\\n)\\n\\n# wrap the CrossViT\\n\\nfrom vit_pytorch.extractor import Extractor\\nv = Extractor(v, layer_name = 'multi_scale_encoder') # take embedding coming from the output of multi-scale-encoder\\n\\n# forward pass now returns predictions and the attention maps\\n\\nimg = torch.randn(1, 3, 256, 256)\\nlogits, embeddings = v(img)\\n\\n# there is one extra token due to the CLS token\\n\\nembeddings # ((1, 257, 192), (1, 17, 384)) - (batch x patches x dimension) \\u0026lt;- large and small scales respectively\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003ecross_vit\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eCrossViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eCrossViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e4\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003esm_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e192\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003esm_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003esm_enc_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003esm_enc_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003esm_enc_mlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elg_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e384\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elg_patch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e64\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elg_enc_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elg_enc_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003elg_enc_mlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecross_attn_depth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003ecross_attn_heads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# wrap the CrossViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eextractor\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eExtractor\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eExtractor\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003elayer_name\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s\\\"\\u003e'multi_scale_encoder'\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# take embedding coming from the output of multi-scale-encoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# forward pass now returns predictions and the attention maps\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-s1\\\"\\u003elogits\\u003c/span\\u003e, \\u003cspan class=\\\"pl-s1\\\"\\u003eembeddings\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-c\\\"\\u003e# there is one extra token due to the CLS token\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eembeddings\\u003c/span\\u003e \\u003cspan class=\\\"pl-c\\\"\\u003e# ((1, 257, 192), (1, 17, 384)) - (batch x patches x dimension) \\u0026lt;- large and small scales respectively\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eResearch Ideas\\u003c/h2\\u003e\\u003ca id=\\\"user-content-research-ideas\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Research Ideas\\\" href=\\\"#research-ideas\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eEfficient Attention\\u003c/h3\\u003e\\u003ca id=\\\"user-content-efficient-attention\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Efficient Attention\\\" href=\\\"#efficient-attention\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThere may be some coming from computer vision who think attention still suffers from quadratic costs. Fortunately, we have a lot of new techniques that may help. This repository offers a way for you to plugin your own sparse attention transformer.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eAn example with \\u003ca href=\\\"https://arxiv.org/abs/2102.03902\\\" rel=\\\"nofollow\\\"\\u003eNystromformer\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"$ pip install nystrom-attention\\\"\\u003e\\u003cpre\\u003e$ pip install nystrom-attention\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.efficient import ViT\\nfrom nystrom_attention import Nystromformer\\n\\nefficient_transformer = Nystromformer(\\n    dim = 512,\\n    depth = 12,\\n    heads = 8,\\n    num_landmarks = 256\\n)\\n\\nv = ViT(\\n    dim = 512,\\n    image_size = 2048,\\n    patch_size = 32,\\n    num_classes = 1000,\\n    transformer = efficient_transformer\\n)\\n\\nimg = torch.randn(1, 3, 2048, 2048) # your high resolution picture\\nv(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eefficient\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003enystrom_attention\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eNystromformer\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eefficient_transformer\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eNystromformer\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e12\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_landmarks\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003etransformer\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003eefficient_transformer\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# your high resolution picture\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eOther sparse attention frameworks I would highly recommend is \\u003ca href=\\\"https://github.com/lucidrains/routing-transformer\\\"\\u003eRouting Transformer\\u003c/a\\u003e or \\u003ca href=\\\"https://github.com/lucidrains/sinkhorn-transformer\\\"\\u003eSinkhorn Transformer\\u003c/a\\u003e\\u003c/p\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eCombining with other Transformer improvements\\u003c/h3\\u003e\\u003ca id=\\\"user-content-combining-with-other-transformer-improvements\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Combining with other Transformer improvements\\\" href=\\\"#combining-with-other-transformer-improvements\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eThis paper purposely used the most vanilla of attention networks to make a statement. If you would like to use some of the latest improvements for attention nets, please use the \\u003ccode\\u003eEncoder\\u003c/code\\u003e from \\u003ca href=\\\"https://github.com/lucidrains/x-transformers\\\"\\u003ethis repository\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eex.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"$ pip install x-transformers\\\"\\u003e\\u003cpre\\u003e$ pip install x-transformers\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch.efficient import ViT\\nfrom x_transformers import Encoder\\n\\nv = ViT(\\n    dim = 512,\\n    image_size = 224,\\n    patch_size = 16,\\n    num_classes = 1000,\\n    transformer = Encoder(\\n        dim = 512,                  # set to be the same as the wrapper\\n        depth = 12,\\n        heads = 8,\\n        ff_glu = True,              # ex. feed forward GLU variant https://arxiv.org/abs/2002.05202\\n        residual_attn = True        # ex. residual attention https://arxiv.org/abs/2012.11747\\n    )\\n)\\n\\nimg = torch.randn(1, 3, 224, 224)\\nv(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-s1\\\"\\u003eefficient\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003ex_transformers\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003etransformer\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eEncoder\\u003c/span\\u003e(\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e512\\u003c/span\\u003e,                  \\u003cspan class=\\\"pl-c\\\"\\u003e# set to be the same as the wrapper\\u003c/span\\u003e\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e12\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e8\\u003c/span\\u003e,\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eff_glu\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e,              \\u003cspan class=\\\"pl-c\\\"\\u003e# ex. feed forward GLU variant https://arxiv.org/abs/2002.05202\\u003c/span\\u003e\\n        \\u003cspan class=\\\"pl-s1\\\"\\u003eresidual_attn\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003eTrue\\u003c/span\\u003e        \\u003cspan class=\\\"pl-c\\\"\\u003e# ex. residual attention https://arxiv.org/abs/2012.11747\\u003c/span\\u003e\\n    )\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e224\\u003c/span\\u003e)\\n\\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eFAQ\\u003c/h2\\u003e\\u003ca id=\\\"user-content-faq\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: FAQ\\\" href=\\\"#faq\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cul dir=\\\"auto\\\"\\u003e\\n\\u003cli\\u003eHow do I pass in non-square images?\\u003c/li\\u003e\\n\\u003c/ul\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eYou can already pass in non-square images - you just have to make sure your height and width is less than or equal to the \\u003ccode\\u003eimage_size\\u003c/code\\u003e, and both divisible by the \\u003ccode\\u003epatch_size\\u003c/code\\u003e\\u003c/p\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eex.\\u003c/p\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch import ViT\\n\\nv = ViT(\\n    image_size = 256,\\n    patch_size = 32,\\n    num_classes = 1000,\\n    dim = 1024,\\n    depth = 6,\\n    heads = 16,\\n    mlp_dim = 2048,\\n    dropout = 0.1,\\n    emb_dropout = 0.1\\n)\\n\\nimg = torch.randn(1, 3, 256, 128) # \\u0026lt;-- not a square\\n\\npreds = v(img) # (1, 1000)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# \\u0026lt;-- not a square\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e) \\u003cspan class=\\\"pl-c\\\"\\u003e# (1, 1000)\\u003c/span\\u003e\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cul dir=\\\"auto\\\"\\u003e\\n\\u003cli\\u003eHow do I pass in non-square patches?\\u003c/li\\u003e\\n\\u003c/ul\\u003e\\n\\u003cdiv class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import torch\\nfrom vit_pytorch import ViT\\n\\nv = ViT(\\n    num_classes = 1000,\\n    image_size = (256, 128),  # image size is a tuple of (height, width)\\n    patch_size = (32, 16),    # patch size is a tuple of (height, width)\\n    dim = 1024,\\n    depth = 6,\\n    heads = 16,\\n    mlp_dim = 2048,\\n    dropout = 0.1,\\n    emb_dropout = 0.1\\n)\\n\\nimg = torch.randn(1, 3, 256, 128)\\n\\npreds = v(img)\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e\\n\\u003cspan class=\\\"pl-k\\\"\\u003efrom\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003evit_pytorch\\u003c/span\\u003e \\u003cspan class=\\\"pl-k\\\"\\u003eimport\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003ev\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-v\\\"\\u003eViT\\u003c/span\\u003e(\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003enum_classes\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1000\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eimage_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e),  \\u003cspan class=\\\"pl-c\\\"\\u003e# image size is a tuple of (height, width)\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003epatch_size\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e (\\u003cspan class=\\\"pl-c1\\\"\\u003e32\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e),    \\u003cspan class=\\\"pl-c\\\"\\u003e# patch size is a tuple of (height, width)\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e1024\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edepth\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e6\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eheads\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e16\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003emlp_dim\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e2048\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003edropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s1\\\"\\u003eemb_dropout\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e0.1\\u003c/span\\u003e\\n)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-s1\\\"\\u003etorch\\u003c/span\\u003e.\\u003cspan class=\\\"pl-en\\\"\\u003erandn\\u003c/span\\u003e(\\u003cspan class=\\\"pl-c1\\\"\\u003e1\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e3\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e256\\u003c/span\\u003e, \\u003cspan class=\\\"pl-c1\\\"\\u003e128\\u003c/span\\u003e)\\n\\n\\u003cspan class=\\\"pl-s1\\\"\\u003epreds\\u003c/span\\u003e \\u003cspan class=\\\"pl-c1\\\"\\u003e=\\u003c/span\\u003e \\u003cspan class=\\\"pl-en\\\"\\u003ev\\u003c/span\\u003e(\\u003cspan class=\\\"pl-s1\\\"\\u003eimg\\u003c/span\\u003e)\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eResources\\u003c/h2\\u003e\\u003ca id=\\\"user-content-resources\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Resources\\\" href=\\\"#resources\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003eComing from computer vision and new to transformers? Here are some resources that greatly accelerated my learning.\\u003c/p\\u003e\\n\\u003col dir=\\\"auto\\\"\\u003e\\n\\u003cli\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"http://jalammar.github.io/illustrated-transformer/\\\" rel=\\\"nofollow\\\"\\u003eIllustrated Transformer\\u003c/a\\u003e - Jay Alammar\\u003c/p\\u003e\\n\\u003c/li\\u003e\\n\\u003cli\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"http://peterbloem.nl/blog/transformers\\\" rel=\\\"nofollow\\\"\\u003eTransformers from Scratch\\u003c/a\\u003e  - Peter Bloem\\u003c/p\\u003e\\n\\u003c/li\\u003e\\n\\u003cli\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003ca href=\\\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\\\" rel=\\\"nofollow\\\"\\u003eThe Annotated Transformer\\u003c/a\\u003e - Harvard NLP\\u003c/p\\u003e\\n\\u003c/li\\u003e\\n\\u003c/ol\\u003e\\n\\u003cdiv class=\\\"markdown-heading\\\" dir=\\\"auto\\\"\\u003e\\u003ch2 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\"\\u003eCitations\\u003c/h2\\u003e\\u003ca id=\\\"user-content-citations\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Citations\\\" href=\\\"#citations\\\"\\u003e\\u003csvg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"\\u003e\\u003cpath d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"\\u003e\\u003c/path\\u003e\\u003c/svg\\u003e\\u003c/a\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{hassani2021escaping,\\n    title   = {Escaping the Big Data Paradigm with Compact Transformers},\\n    author  = {Ali Hassani and Steven Walton and Nikhil Shah and Abulikemu Abuduweili and Jiachen Li and Humphrey Shi},\\n    year    = 2021,\\n    url     = {https://arxiv.org/abs/2104.05704},\\n    eprint  = {2104.05704},\\n    archiveprefix = {arXiv},\\n    primaryclass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ehassani2021escaping\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eEscaping the Big Data Paradigm with Compact Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAli Hassani and Steven Walton and Nikhil Shah and Abulikemu Abuduweili and Jiachen Li and Humphrey Shi\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-c1\\\"\\u003e2021\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://arxiv.org/abs/2104.05704\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2104.05704\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchiveprefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryclass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{dosovitskiy2020image,\\n    title   = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\\n    author  = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},\\n    year    = {2020},\\n    eprint  = {2010.11929},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003edosovitskiy2020image\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAlexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2020\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2010.11929\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{touvron2020training,\\n    title   = {Training data-efficient image transformers \\u0026amp; distillation through attention}, \\n    author  = {Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herv\u00e9 J\u00e9gou},\\n    year    = {2020},\\n    eprint  = {2012.12877},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003etouvron2020training\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eTraining data-efficient image transformers \\u0026amp; distillation through attention\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eHugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herv\u00e9 J\u00e9gou\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2020\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2012.12877\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{yuan2021tokenstotoken,\\n    title   = {Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet},\\n    author  = {Li Yuan and Yunpeng Chen and Tao Wang and Weihao Yu and Yujun Shi and Francis EH Tay and Jiashi Feng and Shuicheng Yan},\\n    year    = {2021},\\n    eprint  = {2101.11986},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eyuan2021tokenstotoken\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eTokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eLi Yuan and Yunpeng Chen and Tao Wang and Weihao Yu and Yujun Shi and Francis EH Tay and Jiashi Feng and Shuicheng Yan\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2101.11986\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{zhou2021deepvit,\\n    title   = {DeepViT: Towards Deeper Vision Transformer},\\n    author  = {Daquan Zhou and Bingyi Kang and Xiaojie Jin and Linjie Yang and Xiaochen Lian and Qibin Hou and Jiashi Feng},\\n    year    = {2021},\\n    eprint  = {2103.11886},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ezhou2021deepvit\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eDeepViT: Towards Deeper Vision Transformer\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eDaquan Zhou and Bingyi Kang and Xiaojie Jin and Linjie Yang and Xiaochen Lian and Qibin Hou and Jiashi Feng\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2103.11886\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{touvron2021going,\\n    title   = {Going deeper with Image Transformers}, \\n    author  = {Hugo Touvron and Matthieu Cord and Alexandre Sablayrolles and Gabriel Synnaeve and Herv\u00e9 J\u00e9gou},\\n    year    = {2021},\\n    eprint  = {2103.17239},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003etouvron2021going\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eGoing deeper with Image Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eHugo Touvron and Matthieu Cord and Alexandre Sablayrolles and Gabriel Synnaeve and Herv\u00e9 J\u00e9gou\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2103.17239\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{chen2021crossvit,\\n    title   = {CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification},\\n    author  = {Chun-Fu Chen and Quanfu Fan and Rameswar Panda},\\n    year    = {2021},\\n    eprint  = {2103.14899},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003echen2021crossvit\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eCrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eChun-Fu Chen and Quanfu Fan and Rameswar Panda\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2103.14899\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{wu2021cvt,\\n    title   = {CvT: Introducing Convolutions to Vision Transformers},\\n    author  = {Haiping Wu and Bin Xiao and Noel Codella and Mengchen Liu and Xiyang Dai and Lu Yuan and Lei Zhang},\\n    year    = {2021},\\n    eprint  = {2103.15808},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ewu2021cvt\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eCvT: Introducing Convolutions to Vision Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eHaiping Wu and Bin Xiao and Noel Codella and Mengchen Liu and Xiyang Dai and Lu Yuan and Lei Zhang\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2103.15808\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{heo2021rethinking,\\n    title   = {Rethinking Spatial Dimensions of Vision Transformers}, \\n    author  = {Byeongho Heo and Sangdoo Yun and Dongyoon Han and Sanghyuk Chun and Junsuk Choe and Seong Joon Oh},\\n    year    = {2021},\\n    eprint  = {2103.16302},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eheo2021rethinking\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eRethinking Spatial Dimensions of Vision Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eByeongho Heo and Sangdoo Yun and Dongyoon Han and Sanghyuk Chun and Junsuk Choe and Seong Joon Oh\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2103.16302\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{graham2021levit,\\n    title   = {LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference},\\n    author  = {Ben Graham and Alaaeldin El-Nouby and Hugo Touvron and Pierre Stock and Armand Joulin and Herv\u00e9 J\u00e9gou and Matthijs Douze},\\n    year    = {2021},\\n    eprint  = {2104.01136},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003egraham2021levit\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eLeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eBen Graham and Alaaeldin El-Nouby and Hugo Touvron and Pierre Stock and Armand Joulin and Herv\u00e9 J\u00e9gou and Matthijs Douze\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2104.01136\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{li2021localvit,\\n    title   = {LocalViT: Bringing Locality to Vision Transformers},\\n    author  = {Yawei Li and Kai Zhang and Jiezhang Cao and Radu Timofte and Luc Van Gool},\\n    year    = {2021},\\n    eprint  = {2104.05707},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eli2021localvit\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eLocalViT: Bringing Locality to Vision Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eYawei Li and Kai Zhang and Jiezhang Cao and Radu Timofte and Luc Van Gool\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2104.05707\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{chu2021twins,\\n    title   = {Twins: Revisiting Spatial Attention Design in Vision Transformers},\\n    author  = {Xiangxiang Chu and Zhi Tian and Yuqing Wang and Bo Zhang and Haibing Ren and Xiaolin Wei and Huaxia Xia and Chunhua Shen},\\n    year    = {2021},\\n    eprint  = {2104.13840},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003echu2021twins\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eTwins: Revisiting Spatial Attention Design in Vision Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eXiangxiang Chu and Zhi Tian and Yuqing Wang and Bo Zhang and Haibing Ren and Xiaolin Wei and Huaxia Xia and Chunhua Shen\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2104.13840\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{su2021roformer,\\n    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding}, \\n    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\\n    year    = {2021},\\n    eprint  = {2104.09864},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CL}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003esu2021roformer\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eRoFormer: Enhanced Transformer with Rotary Position Embedding\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eJianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2104.09864\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CL\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{zhang2021aggregating,\\n    title   = {Aggregating Nested Transformers},\\n    author  = {Zizhao Zhang and Han Zhang and Long Zhao and Ting Chen and Tomas Pfister},\\n    year    = {2021},\\n    eprint  = {2105.12723},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ezhang2021aggregating\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAggregating Nested Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eZizhao Zhang and Han Zhang and Long Zhao and Ting Chen and Tomas Pfister\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2105.12723\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{chen2021regionvit,\\n    title   = {RegionViT: Regional-to-Local Attention for Vision Transformers}, \\n    author  = {Chun-Fu Chen and Rameswar Panda and Quanfu Fan},\\n    year    = {2021},\\n    eprint  = {2106.02689},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003echen2021regionvit\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eRegionViT: Regional-to-Local Attention for Vision Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eChun-Fu Chen and Rameswar Panda and Quanfu Fan\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2106.02689\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{wang2021crossformer,\\n    title   = {CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention}, \\n    author  = {Wenxiao Wang and Lu Yao and Long Chen and Binbin Lin and Deng Cai and Xiaofei He and Wei Liu},\\n    year    = {2021},\\n    eprint  = {2108.00154},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ewang2021crossformer\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eCrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eWenxiao Wang and Lu Yao and Long Chen and Binbin Lin and Deng Cai and Xiaofei He and Wei Liu\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2108.00154\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{caron2021emerging,\\n    title   = {Emerging Properties in Self-Supervised Vision Transformers},\\n    author  = {Mathilde Caron and Hugo Touvron and Ishan Misra and Herv\u00e9 J\u00e9gou and Julien Mairal and Piotr Bojanowski and Armand Joulin},\\n    year    = {2021},\\n    eprint  = {2104.14294},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ecaron2021emerging\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eEmerging Properties in Self-Supervised Vision Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eMathilde Caron and Hugo Touvron and Ishan Misra and Herv\u00e9 J\u00e9gou and Julien Mairal and Piotr Bojanowski and Armand Joulin\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2104.14294\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{he2021masked,\\n    title   = {Masked Autoencoders Are Scalable Vision Learners}, \\n    author  = {Kaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr Doll\u00e1r and Ross Girshick},\\n    year    = {2021},\\n    eprint  = {2111.06377},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ehe2021masked\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eMasked Autoencoders Are Scalable Vision Learners\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eKaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr Doll\u00e1r and Ross Girshick\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2111.06377\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{xie2021simmim,\\n    title   = {SimMIM: A Simple Framework for Masked Image Modeling}, \\n    author  = {Zhenda Xie and Zheng Zhang and Yue Cao and Yutong Lin and Jianmin Bao and Zhuliang Yao and Qi Dai and Han Hu},\\n    year    = {2021},\\n    eprint  = {2111.09886},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003exie2021simmim\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eSimMIM: A Simple Framework for Masked Image Modeling\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eZhenda Xie and Zheng Zhang and Yue Cao and Yutong Lin and Jianmin Bao and Zhuliang Yao and Qi Dai and Han Hu\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2111.09886\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{fayyaz2021ats,\\n    title   = {ATS: Adaptive Token Sampling For Efficient Vision Transformers},\\n    author  = {Mohsen Fayyaz and Soroush Abbasi Kouhpayegani and Farnoush Rezaei Jafari and Eric Sommerlade and Hamid Reza Vaezi Joze and Hamed Pirsiavash and Juergen Gall},\\n    year    = {2021},\\n    eprint  = {2111.15667},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003efayyaz2021ats\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eATS: Adaptive Token Sampling For Efficient Vision Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eMohsen Fayyaz and Soroush Abbasi Kouhpayegani and Farnoush Rezaei Jafari and Eric Sommerlade and Hamid Reza Vaezi Joze and Hamed Pirsiavash and Juergen Gall\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2111.15667\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{mehta2021mobilevit,\\n    title   = {MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},\\n    author  = {Sachin Mehta and Mohammad Rastegari},\\n    year    = {2021},\\n    eprint  = {2110.02178},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003emehta2021mobilevit\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eMobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eSachin Mehta and Mohammad Rastegari\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2110.02178\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{lee2021vision,\\n    title   = {Vision Transformer for Small-Size Datasets}, \\n    author  = {Seung Hoon Lee and Seunghyun Lee and Byung Cheol Song},\\n    year    = {2021},\\n    eprint  = {2112.13492},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003elee2021vision\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eVision Transformer for Small-Size Datasets\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eSeung Hoon Lee and Seunghyun Lee and Byung Cheol Song\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2112.13492\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{renggli2022learning,\\n    title   = {Learning to Merge Tokens in Vision Transformers},\\n    author  = {Cedric Renggli and Andr\u00e9 Susano Pinto and Neil Houlsby and Basil Mustafa and Joan Puigcerver and Carlos Riquelme},\\n    year    = {2022},\\n    eprint  = {2202.12015},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003erenggli2022learning\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eLearning to Merge Tokens in Vision Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eCedric Renggli and Andr\u00e9 Susano Pinto and Neil Houlsby and Basil Mustafa and Joan Puigcerver and Carlos Riquelme\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2202.12015\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{yang2022scalablevit,\\n    title   = {ScalableViT: Rethinking the Context-oriented Generalization of Vision Transformer}, \\n    author  = {Rui Yang and Hailong Ma and Jie Wu and Yansong Tang and Xuefeng Xiao and Min Zheng and Xiu Li},\\n    year    = {2022},\\n    eprint  = {2203.10790},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CV}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eyang2022scalablevit\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eScalableViT: Rethinking the Context-oriented Generalization of Vision Transformer\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e, \\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eRui Yang and Hailong Ma and Jie Wu and Yansong Tang and Xuefeng Xiao and Min Zheng and Xiu Li\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2203.10790\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CV\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Touvron2022ThreeTE,\\n    title   = {Three things everyone should know about Vision Transformers},\\n    author  = {Hugo Touvron and Matthieu Cord and Alaaeldin El-Nouby and Jakob Verbeek and Herv'e J'egou},\\n    year    = {2022}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eTouvron2022ThreeTE\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eThree things everyone should know about Vision Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eHugo Touvron and Matthieu Cord and Alaaeldin El-Nouby and Jakob Verbeek and Herv'e J'egou\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Sandler2022FinetuningIT,\\n    title   = {Fine-tuning Image Transformers using Learnable Memory},\\n    author  = {Mark Sandler and Andrey Zhmoginov and Max Vladymyrov and Andrew Jackson},\\n    year    = {2022}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eSandler2022FinetuningIT\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eFine-tuning Image Transformers using Learnable Memory\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eMark Sandler and Andrey Zhmoginov and Max Vladymyrov and Andrew Jackson\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Li2022SepViTSV,\\n    title   = {SepViT: Separable Vision Transformer},\\n    author  = {Wei Li and Xing Wang and Xin Xia and Jie Wu and Xuefeng Xiao and Minghang Zheng and Shiping Wen},\\n    year    = {2022}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eLi2022SepViTSV\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eSepViT: Separable Vision Transformer\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eWei Li and Xing Wang and Xin Xia and Jie Wu and Xuefeng Xiao and Minghang Zheng and Shiping Wen\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Tu2022MaxViTMV,\\n    title   = {MaxViT: Multi-Axis Vision Transformer},\\n    author  = {Zhengzhong Tu and Hossein Talebi and Han Zhang and Feng Yang and Peyman Milanfar and Alan Conrad Bovik and Yinxiao Li},\\n    year    = {2022}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eTu2022MaxViTMV\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eMaxViT: Multi-Axis Vision Transformer\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eZhengzhong Tu and Hossein Talebi and Han Zhang and Feng Yang and Peyman Milanfar and Alan Conrad Bovik and Yinxiao Li\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{Li2021EfficientSV,\\n    title   = {Efficient Self-supervised Vision Transformers for Representation Learning},\\n    author  = {Chunyuan Li and Jianwei Yang and Pengchuan Zhang and Mei Gao and Bin Xiao and Xiyang Dai and Lu Yuan and Jianfeng Gao},\\n    journal = {ArXiv},\\n    year    = {2021},\\n    volume  = {abs/2106.09785}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eLi2021EfficientSV\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eEfficient Self-supervised Vision Transformers for Representation Learning\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eChunyuan Li and Jianwei Yang and Pengchuan Zhang and Mei Gao and Bin Xiao and Xiyang Dai and Lu Yuan and Jianfeng Gao\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eArXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003evolume\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eabs/2106.09785\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{Beyer2022BetterPlainViT\\n    title     = {Better plain ViT baselines for ImageNet-1k},\\n    author    = {Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},\\n    publisher = {arXiv},\\n    year      = {2022}\\n}\\n\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eBeyer2022BetterPlainViT\\u003c/span\\u003e\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eBetter plain ViT baselines for ImageNet-1k\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eBeyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003epublisher\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e      = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\n\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{Arnab2021ViViTAV,\\n    title   = {ViViT: A Video Vision Transformer},\\n    author  = {Anurag Arnab and Mostafa Dehghani and Georg Heigold and Chen Sun and Mario Lucic and Cordelia Schmid},\\n    journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\\n    year    = {2021},\\n    pages   = {6816-6826}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eArnab2021ViViTAV\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eViViT: A Video Vision Transformer\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAnurag Arnab and Mostafa Dehghani and Georg Heigold and Chen Sun and Mario Lucic and Cordelia Schmid\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021 IEEE/CVF International Conference on Computer Vision (ICCV)\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003epages\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e6816-6826\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@article{Liu2022PatchDropoutEV,\\n    title   = {PatchDropout: Economizing Vision Transformers Using Patch Dropout},\\n    author  = {Yue Liu and Christos Matsoukas and Fredrik Strand and Hossein Azizpour and Kevin Smith},\\n    journal = {ArXiv},\\n    year    = {2022},\\n    volume  = {abs/2208.07220}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@article\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eLiu2022PatchDropoutEV\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ePatchDropout: Economizing Vision Transformers Using Patch Dropout\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eYue Liu and Christos Matsoukas and Fredrik Strand and Hossein Azizpour and Kevin Smith\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ejournal\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eArXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003evolume\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eabs/2208.07220\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{https://doi.org/10.48550/arxiv.2302.01327,\\n    doi     = {10.48550/ARXIV.2302.01327},\\n    url     = {https://arxiv.org/abs/2302.01327},\\n    author  = {Kumar, Manoj and Dehghani, Mostafa and Houlsby, Neil},\\n    title   = {Dual PatchNorm},\\n    publisher = {arXiv},\\n    year    = {2023},\\n    copyright = {Creative Commons Attribution 4.0 International}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003ehttps://doi.org/10.48550/arxiv.2302.01327\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003edoi\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e10.48550/ARXIV.2302.01327\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://arxiv.org/abs/2302.01327\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eKumar, Manoj and Dehghani, Mostafa and Houlsby, Neil\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eDual PatchNorm\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003epublisher\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ecopyright\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eCreative Commons Attribution 4.0 International\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Dehghani2023PatchNP,\\n    title   = {Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution},\\n    author  = {Mostafa Dehghani and Basil Mustafa and Josip Djolonga and Jonathan Heek and Matthias Minderer and Mathilde Caron and Andreas Steiner and Joan Puigcerver and Robert Geirhos and Ibrahim M. Alabdulmohsin and Avital Oliver and Piotr Padlewski and Alexey A. Gritsenko and Mario Luvci'c and Neil Houlsby},\\n    year    = {2023}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eDehghani2023PatchNP\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ePatch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eMostafa Dehghani and Basil Mustafa and Josip Djolonga and Jonathan Heek and Matthias Minderer and Mathilde Caron and Andreas Steiner and Joan Puigcerver and Robert Geirhos and Ibrahim M. Alabdulmohsin and Avital Oliver and Piotr Padlewski and Alexey A. Gritsenko and Mario Luvci'c and Neil Houlsby\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@misc{vaswani2017attention,\\n    title   = {Attention Is All You Need},\\n    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\\n    year    = {2017},\\n    eprint  = {1706.03762},\\n    archivePrefix = {arXiv},\\n    primaryClass = {cs.CL}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@misc\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003evaswani2017attention\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAttention Is All You Need\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAshish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2017\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eeprint\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e1706.03762\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003earchivePrefix\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003earXiv\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eprimaryClass\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ecs.CL\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{dao2022flashattention,\\n    title   = {Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},\\n    author  = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\\\\'e}, Christopher},\\n    booktitle = {Advances in Neural Information Processing Systems},\\n    year    = {2022}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003edao2022flashattention\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eFlash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eDao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\\\\'e}, Christopher\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ebooktitle\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAdvances in Neural Information Processing Systems\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2022\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{Darcet2023VisionTN,\\n    title   = {Vision Transformers Need Registers},\\n    author  = {Timoth'ee Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski},\\n    year    = {2023},\\n    url     = {https://api.semanticscholar.org/CorpusID:263134283}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eDarcet2023VisionTN\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eVision Transformers Need Registers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eTimoth'ee Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2023\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://api.semanticscholar.org/CorpusID:263134283\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cdiv class=\\\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"@inproceedings{ElNouby2021XCiTCI,\\n    title   = {XCiT: Cross-Covariance Image Transformers},\\n    author  = {Alaaeldin El-Nouby and Hugo Touvron and Mathilde Caron and Piotr Bojanowski and Matthijs Douze and Armand Joulin and Ivan Laptev and Natalia Neverova and Gabriel Synnaeve and Jakob Verbeek and Herv{\\\\'e} J{\\\\'e}gou},\\n    booktitle = {Neural Information Processing Systems},\\n    year    = {2021},\\n    url     = {https://api.semanticscholar.org/CorpusID:235458262}\\n}\\\"\\u003e\\u003cpre\\u003e\\u003cspan class=\\\"pl-k\\\"\\u003e@inproceedings\\u003c/span\\u003e{\\u003cspan class=\\\"pl-en\\\"\\u003eElNouby2021XCiTCI\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003etitle\\u003c/span\\u003e   = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eXCiT: Cross-Covariance Image Transformers\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eauthor\\u003c/span\\u003e  = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eAlaaeldin El-Nouby and Hugo Touvron and Mathilde Caron and Piotr Bojanowski and Matthijs Douze and Armand Joulin and Ivan Laptev and Natalia Neverova and Gabriel Synnaeve and Jakob Verbeek and Herv{\\\\'e} J{\\\\'e}gou\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003ebooktitle\\u003c/span\\u003e = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003eNeural Information Processing Systems\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eyear\\u003c/span\\u003e    = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003e2021\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e,\\n    \\u003cspan class=\\\"pl-s\\\"\\u003eurl\\u003c/span\\u003e     = \\u003cspan class=\\\"pl-s\\\"\\u003e\\u003cspan class=\\\"pl-pds\\\"\\u003e{\\u003c/span\\u003ehttps://api.semanticscholar.org/CorpusID:235458262\\u003cspan class=\\\"pl-pds\\\"\\u003e}\\u003c/span\\u003e\\u003c/span\\u003e\\n}\\u003c/pre\\u003e\\u003c/div\\u003e\\n\\u003cp dir=\\\"auto\\\"\\u003e\\u003cem\\u003eI visualise a time when we will be to robots what dogs are to humans, and I\u2019m rooting for the machines.\\u003c/em\\u003e \u2014 Claude Shannon\\u003c/p\\u003e\\n\\u003c/article\\u003e\",\"loaded\":true,\"timedOut\":false,\"errorMessage\":null,\"headerInfo\":{\"toc\":[{\"level\":2,\"text\":\"Table of Contents\",\"anchor\":\"table-of-contents\",\"htmlText\":\"Table of Contents\"},{\"level\":2,\"text\":\"Vision Transformer - Pytorch\",\"anchor\":\"vision-transformer---pytorch\",\"htmlText\":\"Vision Transformer - Pytorch\"},{\"level\":2,\"text\":\"Install\",\"anchor\":\"install\",\"htmlText\":\"Install\"},{\"level\":2,\"text\":\"Usage\",\"anchor\":\"usage\",\"htmlText\":\"Usage\"},{\"level\":2,\"text\":\"Parameters\",\"anchor\":\"parameters\",\"htmlText\":\"Parameters\"},{\"level\":2,\"text\":\"Simple ViT\",\"anchor\":\"simple-vit\",\"htmlText\":\"Simple ViT\"},{\"level\":2,\"text\":\"NaViT\",\"anchor\":\"navit\",\"htmlText\":\"NaViT\"},{\"level\":2,\"text\":\"Distillation\",\"anchor\":\"distillation\",\"htmlText\":\"Distillation\"},{\"level\":2,\"text\":\"Deep ViT\",\"anchor\":\"deep-vit\",\"htmlText\":\"Deep ViT\"},{\"level\":2,\"text\":\"CaiT\",\"anchor\":\"cait\",\"htmlText\":\"CaiT\"},{\"level\":2,\"text\":\"Token-to-Token ViT\",\"anchor\":\"token-to-token-vit\",\"htmlText\":\"Token-to-Token ViT\"},{\"level\":2,\"text\":\"CCT\",\"anchor\":\"cct\",\"htmlText\":\"CCT\"},{\"level\":2,\"text\":\"Cross ViT\",\"anchor\":\"cross-vit\",\"htmlText\":\"Cross ViT\"},{\"level\":2,\"text\":\"PiT\",\"anchor\":\"pit\",\"htmlText\":\"PiT\"},{\"level\":2,\"text\":\"LeViT\",\"anchor\":\"levit\",\"htmlText\":\"LeViT\"},{\"level\":2,\"text\":\"CvT\",\"anchor\":\"cvt\",\"htmlText\":\"CvT\"},{\"level\":2,\"text\":\"Twins SVT\",\"anchor\":\"twins-svt\",\"htmlText\":\"Twins SVT\"},{\"level\":2,\"text\":\"RegionViT\",\"anchor\":\"regionvit\",\"htmlText\":\"RegionViT\"},{\"level\":2,\"text\":\"CrossFormer\",\"anchor\":\"crossformer\",\"htmlText\":\"CrossFormer\"},{\"level\":2,\"text\":\"ScalableViT\",\"anchor\":\"scalablevit\",\"htmlText\":\"ScalableViT\"},{\"level\":2,\"text\":\"SepViT\",\"anchor\":\"sepvit\",\"htmlText\":\"SepViT\"},{\"level\":2,\"text\":\"MaxViT\",\"anchor\":\"maxvit\",\"htmlText\":\"MaxViT\"},{\"level\":2,\"text\":\"NesT\",\"anchor\":\"nest\",\"htmlText\":\"NesT\"},{\"level\":2,\"text\":\"MobileViT\",\"anchor\":\"mobilevit\",\"htmlText\":\"MobileViT\"},{\"level\":2,\"text\":\"XCiT\",\"anchor\":\"xcit\",\"htmlText\":\"XCiT\"},{\"level\":2,\"text\":\"Simple Masked Image Modeling\",\"anchor\":\"simple-masked-image-modeling\",\"htmlText\":\"Simple Masked Image Modeling\"},{\"level\":2,\"text\":\"Masked Autoencoder\",\"anchor\":\"masked-autoencoder\",\"htmlText\":\"Masked Autoencoder\"},{\"level\":2,\"text\":\"Masked Patch Prediction\",\"anchor\":\"masked-patch-prediction\",\"htmlText\":\"Masked Patch Prediction\"},{\"level\":2,\"text\":\"Masked Position Prediction\",\"anchor\":\"masked-position-prediction\",\"htmlText\":\"Masked Position Prediction\"},{\"level\":2,\"text\":\"Adaptive Token Sampling\",\"anchor\":\"adaptive-token-sampling\",\"htmlText\":\"Adaptive Token Sampling\"},{\"level\":2,\"text\":\"Patch Merger\",\"anchor\":\"patch-merger\",\"htmlText\":\"Patch Merger\"},{\"level\":2,\"text\":\"Vision Transformer for Small Datasets\",\"anchor\":\"vision-transformer-for-small-datasets\",\"htmlText\":\"Vision Transformer for Small Datasets\"},{\"level\":2,\"text\":\"3D ViT\",\"anchor\":\"3d-vit\",\"htmlText\":\"3D ViT\"},{\"level\":2,\"text\":\"ViViT\",\"anchor\":\"vivit\",\"htmlText\":\"ViViT\"},{\"level\":2,\"text\":\"Parallel ViT\",\"anchor\":\"parallel-vit\",\"htmlText\":\"Parallel ViT\"},{\"level\":2,\"text\":\"Learnable Memory ViT\",\"anchor\":\"learnable-memory-vit\",\"htmlText\":\"Learnable Memory ViT\"},{\"level\":2,\"text\":\"Dino\",\"anchor\":\"dino\",\"htmlText\":\"Dino\"},{\"level\":2,\"text\":\"EsViT\",\"anchor\":\"esvit\",\"htmlText\":\"EsViT\"},{\"level\":2,\"text\":\"Accessing Attention\",\"anchor\":\"accessing-attention\",\"htmlText\":\"Accessing Attention\"},{\"level\":2,\"text\":\"Accessing Embeddings\",\"anchor\":\"accessing-embeddings\",\"htmlText\":\"Accessing Embeddings\"},{\"level\":2,\"text\":\"Research Ideas\",\"anchor\":\"research-ideas\",\"htmlText\":\"Research Ideas\"},{\"level\":3,\"text\":\"Efficient Attention\",\"anchor\":\"efficient-attention\",\"htmlText\":\"Efficient Attention\"},{\"level\":3,\"text\":\"Combining with other Transformer improvements\",\"anchor\":\"combining-with-other-transformer-improvements\",\"htmlText\":\"Combining with other Transformer improvements\"},{\"level\":2,\"text\":\"FAQ\",\"anchor\":\"faq\",\"htmlText\":\"FAQ\"},{\"level\":2,\"text\":\"Resources\",\"anchor\":\"resources\",\"htmlText\":\"Resources\"},{\"level\":2,\"text\":\"Citations\",\"anchor\":\"citations\",\"htmlText\":\"Citations\"}],\"siteNavLoginPath\":\"/login?return_to=https%3A%2F%2Fgithub.com%2Flucidrains%2Fvit-pytorch\"}},{\"displayName\":\"LICENSE\",\"repoName\":\"vit-pytorch\",\"refName\":\"main\",\"path\":\"LICENSE\",\"preferredFileType\":\"license\",\"tabName\":\"MIT\",\"richText\":null,\"loaded\":false,\"timedOut\":false,\"errorMessage\":null,\"headerInfo\":{\"toc\":null,\"siteNavLoginPath\":\"/login?return_to=https%3A%2F%2Fgithub.com%2Flucidrains%2Fvit-pytorch\"}}],\"overviewFilesProcessingTime\":45.960471000000005}},\"appPayload\":{\"helpUrl\":\"https://docs.github.com\",\"findFileWorkerPath\":\"/assets-cdn/worker/find-file-worker-32bb159cc57c.js\",\"findInFileWorkerPath\":\"/assets-cdn/worker/find-in-file-worker-c6704d501c10.js\",\"githubDevUrl\":null,\"enabled_features\":{\"code_nav_ui_events\":false,\"copilot_conversational_ux\":false,\"copilot_conversational_ux_embedding_update\":false,\"copilot_popover_file_editor_header\":false,\"copilot_smell_icebreaker_ux\":true,\"copilot_workspace\":false,\"codeview_firefox_inert\":true}}}}</script>  <div data-target=\"react-partial.reactRoot\"><style data-styled=\"true\" data-styled-version=\"5.3.6\">.cgQnMS{font-weight:600;font-size:32px;margin:0;}/*!sc*/data-styled.g1[id=\"Heading__StyledHeading-sc-1c1dgg0-0\"]{content:\"cgQnMS,\"}/*!sc*/.izjvBm{margin-top:16px;margin-bottom:16px;}/*!sc*/.rPQgy{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/.eUMEDg{margin-bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;row-gap:16px;}/*!sc*/.eLcVee{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;padding-bottom:16px;padding-top:8px;}/*!sc*/.hsfLlq{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;gap:8px;}/*!sc*/@media screen and (max-width:320px){.hsfLlq{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}}/*!sc*/.gpKoUz{position:relative;}/*!sc*/@media screen and (max-width:380px){.gpKoUz .ref-selector-button-text-container{max-width:80px;}}/*!sc*/@media screen and (max-width:320px){.gpKoUz{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}.gpKoUz .overview-ref-selector{width:100%;}.gpKoUz .overview-ref-selector > span{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;}.gpKoUz .overview-ref-selector > span > span[data-component=\"text\"]{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}}/*!sc*/.kkrdEu{-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}/*!sc*/.bKgizp{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;}/*!sc*/.iPGYsi{margin-right:4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.dKmYfk{font-size:14px;min-width:0;max-width:125px;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}/*!sc*/.trpoQ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;pointer-events:none;}/*!sc*/.laYubZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (max-width:1079px){.laYubZ{display:none;}}/*!sc*/.swnaL{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (min-width:1080px){.swnaL{display:none;}}/*!sc*/@media screen and (max-width:543px){.swnaL{display:none;}}/*!sc*/.bWpuBf{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-left:8px;gap:8px;}/*!sc*/.grHjNb{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:8px;}/*!sc*/@media screen and (max-width:543px){.grHjNb{display:none;}}/*!sc*/.dXTsqj{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (max-width:1011px){.dXTsqj{display:none;}}/*!sc*/.dCOrmu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (min-width:1012px){.dCOrmu{display:none;}}/*!sc*/@media screen and (max-width:544px){.bVvbgP{display:none;}}/*!sc*/.bNDvfp{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/@media screen and (min-width:544px){.bNDvfp{display:none;}}/*!sc*/.yfPnm{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;gap:16px;}/*!sc*/.cAQuiW{width:100%;border-collapse:separate;border-spacing:0;border:1px solid;border-color:var(--borderColor-default,var(--color-border-default,#d0d7de));border-radius:6px;table-layout:fixed;overflow:unset;}/*!sc*/.iiUlLN{height:0px;line-height:0px;}/*!sc*/.iiUlLN tr{height:0px;font-size:0px;}/*!sc*/.jmggSN{padding:16px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));font-size:12px;text-align:left;height:40px;}/*!sc*/.jmggSN th{padding-left:16px;background-color:var(--bgColor-muted,var(--color-canvas-subtle,#f6f8fa));}/*!sc*/.kvYunM{width:100%;border-top-left-radius:6px;}/*!sc*/@media screen and (min-width:544px){.kvYunM{display:none;}}/*!sc*/.hrLuxA{width:40%;border-top-left-radius:6px;}/*!sc*/@media screen and (max-width:543px){.hrLuxA{display:none;}}/*!sc*/@media screen and (max-width:543px){.ePjhhA{display:none;}}/*!sc*/.cuEKae{text-align:right;padding-right:16px;width:136px;border-top-right-radius:6px;}/*!sc*/.jEbBOT{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));font-size:12px;height:40px;}/*!sc*/.bTxCvM{background-color:var(--bgColor-muted,var(--color-canvas-subtle,#f6f8fa));padding:4px;border-top-left-radius:6px;border-top-right-radius:6px;}/*!sc*/.eYedVD{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;gap:8px;min-width:273px;padding-right:8px;padding-left:16px;padding-top:8px;padding-bottom:8px;}/*!sc*/.jGfYmh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:8px;}/*!sc*/.lhFvfi{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/.bqgLjk{display:inherit;}/*!sc*/@media screen and (min-width:544px){.bqgLjk{display:none;}}/*!sc*/@media screen and (min-width:768px){.bqgLjk{display:none;}}/*!sc*/.epsqEd{text-align:center;vertical-align:center;height:40px;border-top:1px solid;border-color:var(--borderColor-default,var(--color-border-default,#d0d7de));}/*!sc*/.ldpruc{border-top:1px solid var(--borderColor-default,var(--color-border-default));cursor:pointer;}/*!sc*/.ehcSsh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;gap:16px;}/*!sc*/.iGmlUb{border:1px solid;border-color:var(--borderColor-default,var(--color-border-default,#d0d7de));border-radius:6px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}/*!sc*/@media screen and (max-width:543px){.iGmlUb{margin-left:-16px;margin-right:-16px;max-width:calc(100% + 32px);}}/*!sc*/@media screen and (min-width:544px){.iGmlUb{max-width:100%;}}/*!sc*/.iRQGXA{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;border-bottom:1px solid;border-bottom-color:var(--borderColor-default,var(--color-border-default,#d0d7de));-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-right:8px;position:-webkit-sticky;position:sticky;top:0;background-color:var(--bgColor-default,var(--color-canvas-default,#ffffff));z-index:1;border-top-left-radius:6px;border-top-right-radius:6px;}/*!sc*/.dvTdPK{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-left:8px;padding-right:8px;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;border-bottom:none;border-bottom-color:var(--borderColor-muted,var(--color-border-muted,hsla(210,18%,87%,1)));align:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;min-height:48px;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;max-width:100%;}/*!sc*/.gwuIGu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/.kOxwQs{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;margin-right:8px;}/*!sc*/.kOgeFj{font-weight:600;}/*!sc*/.bJMeLZ{padding:32px;overflow:auto;}/*!sc*/data-styled.g2[id=\"Box-sc-g0xbh4-0\"]{content:\"izjvBm,rPQgy,eUMEDg,eLcVee,hsfLlq,gpKoUz,kkrdEu,bKgizp,iPGYsi,dKmYfk,trpoQ,laYubZ,swnaL,bWpuBf,grHjNb,dXTsqj,dCOrmu,bVvbgP,bNDvfp,yfPnm,cAQuiW,iiUlLN,jmggSN,kvYunM,hrLuxA,ePjhhA,cuEKae,jEbBOT,bTxCvM,eYedVD,jGfYmh,lhFvfi,bqgLjk,epsqEd,ldpruc,ehcSsh,iGmlUb,iRQGXA,dvTdPK,gwuIGu,kOxwQs,kOgeFj,bJMeLZ,\"}/*!sc*/.bOMzPg{min-width:0;}/*!sc*/.eUGNHp{font-weight:600;}/*!sc*/.dALsKK{color:var(--fgColor-default,var(--color-fg-default,#1F2328));}/*!sc*/data-styled.g6[id=\"Text-sc-17v1xeu-0\"]{content:\"bOMzPg,eUGNHp,dALsKK,\"}/*!sc*/.dheQRw{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));-webkit-text-decoration:none;text-decoration:none;}/*!sc*/[data-a11y-link-underlines='true'] .Link__StyledLink-sc-14289xe-0[data-inline='true']{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.dheQRw:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.dheQRw:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/.vLMkZ{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));-webkit-text-decoration:none;text-decoration:none;position:relative;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;color:var(--fgColor-default,var(--color-fg-default,#1F2328));text-align:center;-webkit-text-decoration:none;text-decoration:none;line-height:calc(20/14);border-radius:6px;font-size:14px;padding-left:8px;padding-right:8px;padding-top:calc((2rem - 1.25rem) / 2);padding-bottom:calc((2rem - 1.25rem) / 2);}/*!sc*/[data-a11y-link-underlines='true'] .Link__StyledLink-sc-14289xe-0[data-inline='true']{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.vLMkZ:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.vLMkZ:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/.vLMkZ span[data-component=\"icon\"]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/@media (hover:hover){.vLMkZ:hover{background-color:var(--bgColor-neutral-muted,var(--color-neutral-muted,rgba(175,184,193,0.2)));-webkit-transition:background .12s ease-out;transition:background .12s ease-out;-webkit-text-decoration:none;text-decoration:none;}}/*!sc*/.vLMkZ:focus{outline:2px solid transparent;}/*!sc*/.vLMkZ:focus{box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.vLMkZ:focus:not(:focus-visible){box-shadow:none;}/*!sc*/.vLMkZ:focus-visible{outline:2px solid transparent;box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.vLMkZ span[data-content]::before{content:attr(data-content);display:block;height:0;font-weight:600;visibility:hidden;white-space:nowrap;}/*!sc*/.vLMkZ::after{position:absolute;right:50%;bottom:calc(50% - 25px);width:100%;height:2px;content:\"\";background-color:var(--underlineNav-borderColor-active,var(--color-primer-border-active,#fd8c73));border-radius:0;-webkit-transform:translate(50%,-50%);-ms-transform:translate(50%,-50%);transform:translate(50%,-50%);}/*!sc*/@media (forced-colors:active){.vLMkZ::after{background-color:LinkText;}}/*!sc*/.bhqztV{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));-webkit-text-decoration:none;text-decoration:none;position:relative;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;color:var(--fgColor-default,var(--color-fg-default,#1F2328));text-align:center;-webkit-text-decoration:none;text-decoration:none;line-height:calc(20/14);border-radius:6px;font-size:14px;padding-left:8px;padding-right:8px;padding-top:calc((2rem - 1.25rem) / 2);padding-bottom:calc((2rem - 1.25rem) / 2);}/*!sc*/[data-a11y-link-underlines='true'] .Link__StyledLink-sc-14289xe-0[data-inline='true']{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.bhqztV:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/.bhqztV:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/.bhqztV span[data-component=\"icon\"]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/@media (hover:hover){.bhqztV:hover{background-color:var(--bgColor-neutral-muted,var(--color-neutral-muted,rgba(175,184,193,0.2)));-webkit-transition:background .12s ease-out;transition:background .12s ease-out;-webkit-text-decoration:none;text-decoration:none;}}/*!sc*/.bhqztV:focus{outline:2px solid transparent;}/*!sc*/.bhqztV:focus{box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.bhqztV:focus:not(:focus-visible){box-shadow:none;}/*!sc*/.bhqztV:focus-visible{outline:2px solid transparent;box-shadow:inset 0 0 0 2px var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.bhqztV span[data-content]::before{content:attr(data-content);display:block;height:0;font-weight:600;visibility:hidden;white-space:nowrap;}/*!sc*/.bhqztV::after{position:absolute;right:50%;bottom:calc(50% - 25px);width:100%;height:2px;content:\"\";background-color:transparent;border-radius:0;-webkit-transform:translate(50%,-50%);-ms-transform:translate(50%,-50%);transform:translate(50%,-50%);}/*!sc*/@media (forced-colors:active){.bhqztV::after{background-color:transparent;}}/*!sc*/data-styled.g8[id=\"Link__StyledLink-sc-14289xe-0\"]{content:\"dheQRw,vLMkZ,bhqztV,\"}/*!sc*/.izDscS{border-radius:6px;border:1px solid;border-color:var(--button-default-borderColor-rest,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:var(--button-default-bgColor-rest,var(--color-btn-bg,#f6f8fa));box-shadow:var(--button-default-shadow-resting,var(--color-btn-shadow,0 1px 0 rgba(31,35,40,0.04))),var(--button-default-shadow-inset,var(--color-btn-inset-shadow,inset 0 1px 0 rgba(255,255,255,0.25)));}/*!sc*/.izDscS:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.izDscS:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.izDscS:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.izDscS[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.izDscS[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.izDscS:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.izDscS:active{-webkit-transition:none;transition:none;}/*!sc*/.izDscS[data-inactive]{cursor:auto;}/*!sc*/.izDscS:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));border-color:var(--button-default-borderColor-disabled,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));}/*!sc*/.izDscS:disabled [data-component=ButtonCounter]{color:inherit;}/*!sc*/@media (forced-colors:active){.izDscS:focus{outline:solid 1px transparent;}}/*!sc*/.izDscS [data-component=ButtonCounter]{font-size:12px;background-color:var(--buttonCounter-default-bgColor-rest,var(--color-btn-counter-bg,rgba(31,35,40,0.08)));}/*!sc*/.izDscS[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.izDscS[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.izDscS[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.izDscS[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.izDscS[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.izDscS[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.izDscS[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.izDscS[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.izDscS[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.izDscS[data-block=\"block\"]{width:100%;}/*!sc*/.izDscS[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.izDscS[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.izDscS [data-component=\"leadingVisual\"]{grid-area:leadingVisual;}/*!sc*/.izDscS [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.izDscS [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.izDscS [data-component=\"trailingAction\"]{margin-right:-4px;}/*!sc*/.izDscS [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.izDscS [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.izDscS:hover:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-hover,var(--color-btn-hover-bg,#f3f4f6));border-color:var(--button-default-borderColor-hover,var(--button-default-borderColor-hover,var(--color-btn-hover-border,rgba(31,35,40,0.15))));}/*!sc*/.izDscS:active:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/.izDscS[aria-expanded=true]{background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/.izDscS [data-component=\"leadingVisual\"],.izDscS [data-component=\"trailingVisual\"],.izDscS [data-component=\"trailingAction\"]{color:var(--button-color,var(--fgColor-muted,var(--color-fg-muted,#656d76)));}/*!sc*/.izDscS{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/.izDscS svg{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.izDscS > span{width:inherit;}/*!sc*/.cuOWTR{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:transparent;box-shadow:none;}/*!sc*/.cuOWTR:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.cuOWTR:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.cuOWTR:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.cuOWTR[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.cuOWTR[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.cuOWTR:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.cuOWTR:active{-webkit-transition:none;transition:none;}/*!sc*/.cuOWTR[data-inactive]{cursor:auto;}/*!sc*/.cuOWTR:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.cuOWTR:disabled [data-component=ButtonCounter],.cuOWTR:disabled [data-component=\"leadingVisual\"],.cuOWTR:disabled [data-component=\"trailingAction\"]{color:inherit;}/*!sc*/@media (forced-colors:active){.cuOWTR:focus{outline:solid 1px transparent;}}/*!sc*/.cuOWTR [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.cuOWTR[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.cuOWTR[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.cuOWTR[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.cuOWTR[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.cuOWTR[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.cuOWTR[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.cuOWTR[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.cuOWTR[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.cuOWTR[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.cuOWTR[data-block=\"block\"]{width:100%;}/*!sc*/.cuOWTR[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.cuOWTR[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.cuOWTR [data-component=\"leadingVisual\"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.cuOWTR [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.cuOWTR [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.cuOWTR [data-component=\"trailingAction\"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.cuOWTR [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.cuOWTR [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.cuOWTR:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));}/*!sc*/.cuOWTR:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));}/*!sc*/.cuOWTR[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/.cuOWTR[data-component=\"IconButton\"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.cuOWTR[data-no-visuals]{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.cuOWTR:has([data-component=\"ButtonCounter\"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/.cuOWTR:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.cuOWTR:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/.cuOWTR{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));padding-left:4px;padding-right:4px;}/*!sc*/.cuOWTR span[data-component=\"leadingVisual\"]{margin-right:4px !important;}/*!sc*/.tDSzd{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:transparent;box-shadow:none;}/*!sc*/.tDSzd:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.tDSzd:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.tDSzd:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.tDSzd[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.tDSzd[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.tDSzd:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.tDSzd:active{-webkit-transition:none;transition:none;}/*!sc*/.tDSzd[data-inactive]{cursor:auto;}/*!sc*/.tDSzd:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.tDSzd:disabled [data-component=ButtonCounter],.tDSzd:disabled [data-component=\"leadingVisual\"],.tDSzd:disabled [data-component=\"trailingAction\"]{color:inherit;}/*!sc*/@media (forced-colors:active){.tDSzd:focus{outline:solid 1px transparent;}}/*!sc*/.tDSzd [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.tDSzd[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.tDSzd[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.tDSzd[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.tDSzd[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.tDSzd[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.tDSzd[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.tDSzd[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.tDSzd[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.tDSzd[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.tDSzd[data-block=\"block\"]{width:100%;}/*!sc*/.tDSzd[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.tDSzd[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.tDSzd [data-component=\"leadingVisual\"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.tDSzd [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.tDSzd [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.tDSzd [data-component=\"trailingAction\"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.tDSzd [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.tDSzd [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.tDSzd:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));}/*!sc*/.tDSzd:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));}/*!sc*/.tDSzd[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/.tDSzd[data-component=\"IconButton\"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.tDSzd[data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.tDSzd:has([data-component=\"ButtonCounter\"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/.tDSzd:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.tDSzd:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/.ftZGca{border-radius:6px;border:1px solid;border-color:var(--button-default-borderColor-rest,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:var(--button-default-bgColor-rest,var(--color-btn-bg,#f6f8fa));box-shadow:var(--button-default-shadow-resting,var(--color-btn-shadow,0 1px 0 rgba(31,35,40,0.04))),var(--button-default-shadow-inset,var(--color-btn-inset-shadow,inset 0 1px 0 rgba(255,255,255,0.25)));}/*!sc*/.ftZGca:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.ftZGca:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.ftZGca:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.ftZGca[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.ftZGca[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.ftZGca:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.ftZGca:active{-webkit-transition:none;transition:none;}/*!sc*/.ftZGca[data-inactive]{cursor:auto;}/*!sc*/.ftZGca:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));border-color:var(--button-default-borderColor-disabled,var(--button-default-borderColor-rest,var(--color-btn-border,rgba(31,35,40,0.15))));}/*!sc*/.ftZGca:disabled [data-component=ButtonCounter]{color:inherit;}/*!sc*/@media (forced-colors:active){.ftZGca:focus{outline:solid 1px transparent;}}/*!sc*/.ftZGca [data-component=ButtonCounter]{font-size:12px;background-color:var(--buttonCounter-default-bgColor-rest,var(--color-btn-counter-bg,rgba(31,35,40,0.08)));}/*!sc*/.ftZGca[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.ftZGca[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.ftZGca[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.ftZGca[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.ftZGca[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.ftZGca[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.ftZGca[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.ftZGca[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.ftZGca[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.ftZGca[data-block=\"block\"]{width:100%;}/*!sc*/.ftZGca[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.ftZGca[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.ftZGca [data-component=\"leadingVisual\"]{grid-area:leadingVisual;}/*!sc*/.ftZGca [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.ftZGca [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.ftZGca [data-component=\"trailingAction\"]{margin-right:-4px;}/*!sc*/.ftZGca [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.ftZGca [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.ftZGca:hover:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-hover,var(--color-btn-hover-bg,#f3f4f6));border-color:var(--button-default-borderColor-hover,var(--button-default-borderColor-hover,var(--color-btn-hover-border,rgba(31,35,40,0.15))));}/*!sc*/.ftZGca:active:not([disabled]):not([data-inactive]){background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/.ftZGca[aria-expanded=true]{background-color:var(--button-default-bgColor-active,var(--color-btn-active-bg,hsla(220,14%,93%,1)));border-color:var(--button-default-borderColor-active,var(--button-default-borderColor-active,var(--color-btn-active-border,rgba(31,35,40,0.15))));}/*!sc*/.ftZGca [data-component=\"leadingVisual\"],.ftZGca [data-component=\"trailingVisual\"],.ftZGca [data-component=\"trailingAction\"]{color:var(--button-color,var(--fgColor-muted,var(--color-fg-muted,#656d76)));}/*!sc*/.gYvpXq{border-radius:6px;border:1px solid;border-color:var(--button-primary-borderColor-rest,var(--color-btn-primary-border,rgba(31,35,40,0.15)));font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-primary-fgColor-rest,var(--color-btn-primary-text,#ffffff));background-color:var(--button-primary-bgColor-rest,var(--color-btn-primary-bg,#1f883d));box-shadow:var(--shadow-resting-small,var(--color-btn-primary-shadow,0 1px 0 rgba(31,35,40,0.1)));}/*!sc*/.gYvpXq:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.gYvpXq:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.gYvpXq:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.gYvpXq[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.gYvpXq[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.gYvpXq:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.gYvpXq:active{-webkit-transition:none;transition:none;}/*!sc*/.gYvpXq[data-inactive]{cursor:auto;}/*!sc*/.gYvpXq:disabled{cursor:not-allowed;box-shadow:none;color:var(--button-primary-fgColor-disabled,var(--color-btn-primary-disabled-text,rgba(255,255,255,0.8)));background-color:var(--button-primary-bgColor-disabled,var(--color-btn-primary-disabled-bg,#94d3a2));border-color:var(--button-primary-borderColor-disabled,var(--color-btn-primary-disabled-border,rgba(31,35,40,0.15)));}/*!sc*/.gYvpXq:disabled [data-component=ButtonCounter]{color:inherit;}/*!sc*/@media (forced-colors:active){.gYvpXq:focus{outline:solid 1px transparent;}}/*!sc*/.gYvpXq [data-component=ButtonCounter]{font-size:12px;background-color:var(--buttonCounter-primary-bgColor-rest,var(--color-btn-primary-counter-bg,rgba(0,45,17,0.2)));color:var(--button-primary-fgColor-rest,var(--color-btn-primary-text,#ffffff));}/*!sc*/.gYvpXq[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.gYvpXq[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.gYvpXq[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.gYvpXq[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.gYvpXq[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.gYvpXq[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.gYvpXq[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.gYvpXq[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.gYvpXq[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.gYvpXq[data-block=\"block\"]{width:100%;}/*!sc*/.gYvpXq[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.gYvpXq[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.gYvpXq [data-component=\"leadingVisual\"]{grid-area:leadingVisual;}/*!sc*/.gYvpXq [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.gYvpXq [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.gYvpXq [data-component=\"trailingAction\"]{margin-right:-4px;}/*!sc*/.gYvpXq [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.gYvpXq [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.gYvpXq:hover:not([disabled]):not([data-inactive]){color:btn.primary.hoverText;background-color:var(--button-primary-bgColor-hover,var(--color-btn-primary-hover-bg,#1a7f37));}/*!sc*/.gYvpXq:focus:not([disabled]){box-shadow:inset 0 0 0 3px;}/*!sc*/.gYvpXq:focus-visible:not([disabled]){box-shadow:inset 0 0 0 3px;}/*!sc*/.gYvpXq:active:not([disabled]):not([data-inactive]){background-color:var(--button-primary-bgColor-active,var(--color-btn-primary-selected-bg,hsla(137,66%,28%,1)));box-shadow:var(--button-primary-shadow-selected,var(--color-btn-primary-selected-shadow,inset 0 1px 0 rgba(0,45,17,0.2)));}/*!sc*/.gYvpXq[aria-expanded=true]{background-color:var(--button-primary-bgColor-active,var(--color-btn-primary-selected-bg,hsla(137,66%,28%,1)));box-shadow:var(--button-primary-shadow-selected,var(--color-btn-primary-selected-shadow,inset 0 1px 0 rgba(0,45,17,0.2)));}/*!sc*/.gYvpXq svg{color:fg.primary;}/*!sc*/.fAkXQN{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--fgColor-default,var(--color-fg-default,#1F2328));background-color:transparent;box-shadow:none;}/*!sc*/.fAkXQN:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.fAkXQN:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.fAkXQN:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.fAkXQN[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.fAkXQN[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.fAkXQN:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.fAkXQN:active{-webkit-transition:none;transition:none;}/*!sc*/.fAkXQN[data-inactive]{cursor:auto;}/*!sc*/.fAkXQN:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.fAkXQN:disabled [data-component=ButtonCounter],.fAkXQN:disabled [data-component=\"leadingVisual\"],.fAkXQN:disabled [data-component=\"trailingAction\"]{color:inherit;}/*!sc*/@media (forced-colors:active){.fAkXQN:focus{outline:solid 1px transparent;}}/*!sc*/.fAkXQN [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.fAkXQN[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.fAkXQN[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.fAkXQN[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.fAkXQN[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.fAkXQN[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.fAkXQN[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.fAkXQN[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.fAkXQN[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.fAkXQN[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.fAkXQN[data-block=\"block\"]{width:100%;}/*!sc*/.fAkXQN[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.fAkXQN[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.fAkXQN [data-component=\"leadingVisual\"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.fAkXQN [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.fAkXQN [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.fAkXQN [data-component=\"trailingAction\"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.fAkXQN [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.fAkXQN [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.fAkXQN:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.fAkXQN:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.fAkXQN[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/.fAkXQN[data-component=\"IconButton\"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.fAkXQN[data-no-visuals]{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.fAkXQN:has([data-component=\"ButtonCounter\"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/.fAkXQN:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.fAkXQN:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/.fAkXQN:focus:not([disabled]){-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.jPraEl{border-radius:6px;border:1px solid;border-color:transparent;font-family:inherit;font-weight:500;font-size:14px;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;height:32px;padding:0 12px;gap:8px;min-width:-webkit-max-content;min-width:-moz-max-content;min-width:max-content;-webkit-transition:80ms cubic-bezier(0.65,0,0.35,1);transition:80ms cubic-bezier(0.65,0,0.35,1);-webkit-transition-property:color,fill,background-color,border-color;transition-property:color,fill,background-color,border-color;color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));background-color:transparent;box-shadow:none;}/*!sc*/.jPraEl:focus:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.jPraEl:focus:not(:disabled):not(:focus-visible){outline:solid 1px transparent;}/*!sc*/.jPraEl:focus-visible:not(:disabled){box-shadow:none;outline:2px solid var(--fgColor-accent,var(--color-accent-fg,#0969da));outline-offset:-2px;}/*!sc*/.jPraEl[href]{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}/*!sc*/.jPraEl[href]:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/.jPraEl:hover{-webkit-transition-duration:80ms;transition-duration:80ms;}/*!sc*/.jPraEl:active{-webkit-transition:none;transition:none;}/*!sc*/.jPraEl[data-inactive]{cursor:auto;}/*!sc*/.jPraEl:disabled{cursor:not-allowed;box-shadow:none;color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.jPraEl:disabled [data-component=ButtonCounter],.jPraEl:disabled [data-component=\"leadingVisual\"],.jPraEl:disabled [data-component=\"trailingAction\"]{color:inherit;}/*!sc*/@media (forced-colors:active){.jPraEl:focus{outline:solid 1px transparent;}}/*!sc*/.jPraEl [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.jPraEl[data-component=IconButton]{display:inline-grid;padding:unset;place-content:center;width:32px;min-width:unset;}/*!sc*/.jPraEl[data-size=\"small\"]{padding:0 8px;height:28px;gap:4px;font-size:12px;}/*!sc*/.jPraEl[data-size=\"small\"] [data-component=\"text\"]{line-height:calc(20 / 12);}/*!sc*/.jPraEl[data-size=\"small\"] [data-component=ButtonCounter]{font-size:12px;}/*!sc*/.jPraEl[data-size=\"small\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:4px;}/*!sc*/.jPraEl[data-size=\"small\"][data-component=IconButton]{width:28px;padding:unset;}/*!sc*/.jPraEl[data-size=\"large\"]{padding:0 16px;height:40px;gap:8px;}/*!sc*/.jPraEl[data-size=\"large\"] [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.jPraEl[data-size=\"large\"][data-component=IconButton]{width:40px;padding:unset;}/*!sc*/.jPraEl[data-block=\"block\"]{width:100%;}/*!sc*/.jPraEl[data-inactive]:not([disabled]){background-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));border-color:var(--button-inactive-bgColor,var(--button-inactive-bgColor-rest,var(--color-btn-inactive-bg,#eaeef2)));color:var(--button-inactive-fgColor,var(--button-inactive-fgColor-rest,var(--color-btn-inactive-text,#57606a)));}/*!sc*/.jPraEl[data-inactive]:not([disabled]):focus-visible{box-shadow:none;}/*!sc*/.jPraEl [data-component=\"leadingVisual\"]{grid-area:leadingVisual;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.jPraEl [data-component=\"text\"]{grid-area:text;line-height:calc(20/14);white-space:nowrap;}/*!sc*/.jPraEl [data-component=\"trailingVisual\"]{grid-area:trailingVisual;}/*!sc*/.jPraEl [data-component=\"trailingAction\"]{margin-right:-4px;color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.jPraEl [data-component=\"buttonContent\"]{-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;display:grid;grid-template-areas:\"leadingVisual text trailingVisual\";grid-template-columns:min-content minmax(0,auto) min-content;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;}/*!sc*/.jPraEl [data-component=\"buttonContent\"] > :not(:last-child){margin-right:8px;}/*!sc*/.jPraEl:hover:not([disabled]){background-color:var(--control-transparent-bgColor-hover,var(--color-action-list-item-default-hover-bg,rgba(208,215,222,0.32)));}/*!sc*/.jPraEl:active:not([disabled]){background-color:var(--control-transparent-bgColor-active,var(--color-action-list-item-default-active-bg,rgba(208,215,222,0.48)));}/*!sc*/.jPraEl[aria-expanded=true]{background-color:var(--control-transparent-bgColor-selected,var(--color-action-list-item-default-selected-bg,rgba(208,215,222,0.24)));}/*!sc*/.jPraEl[data-component=\"IconButton\"][data-no-visuals]{color:var(--fgColor-muted,var(--color-fg-muted,#656d76));}/*!sc*/.jPraEl[data-no-visuals]{color:var(--fgColor-accent,var(--color-accent-fg,#0969da));}/*!sc*/.jPraEl:has([data-component=\"ButtonCounter\"]){color:var(--button-default-fgColor-rest,var(--color-btn-text,#24292f));}/*!sc*/.jPraEl:disabled[data-no-visuals]{color:var(--fgColor-disabled,var(--color-primer-fg-disabled,#8c959f));}/*!sc*/.jPraEl:disabled[data-no-visuals] [data-component=ButtonCounter]{color:inherit;}/*!sc*/.jPraEl{color:var(--fgColor-muted,var(--color-fg-subtle,#6e7781));padding-left:8px;padding-right:8px;}/*!sc*/data-styled.g9[id=\"types__StyledButton-sc-ws60qy-0\"]{content:\"izDscS,cuOWTR,tDSzd,ftZGca,gYvpXq,fAkXQN,jPraEl,\"}/*!sc*/.rTZSs{position:absolute;width:1px;height:1px;padding:0;margin:-1px;overflow:hidden;-webkit-clip:rect(0,0,0,0);clip:rect(0,0,0,0);white-space:nowrap;border-width:0;}/*!sc*/data-styled.g10[id=\"_VisuallyHidden__VisuallyHidden-sc-11jhm7a-0\"]{content:\"rTZSs,\"}/*!sc*/.fUpWeN{display:inline-block;overflow:hidden;text-overflow:ellipsis;vertical-align:top;white-space:nowrap;max-width:125px;max-width:100%;}/*!sc*/data-styled.g15[id=\"Truncate__StyledTruncate-sc-23o1d2-0\"]{content:\"fUpWeN,\"}/*!sc*/.dMjscx{position:relative;display:inline-block;}/*!sc*/.dMjscx::before{position:absolute;z-index:1000001;display:none;width:0px;height:0px;color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));pointer-events:none;content:'';border:6px solid transparent;opacity:0;}/*!sc*/.dMjscx::after{position:absolute;z-index:1000000;display:none;padding:0.5em 0.75em;font:normal normal 11px/1.5 -apple-system,BlinkMacSystemFont,\"Segoe UI\",\"Noto Sans\",Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\";-webkit-font-smoothing:subpixel-antialiased;color:var(--fgColor-onEmphasis,var(--color-fg-on-emphasis,#ffffff));text-align:center;-webkit-text-decoration:none;text-decoration:none;text-shadow:none;text-transform:none;-webkit-letter-spacing:normal;-moz-letter-spacing:normal;-ms-letter-spacing:normal;letter-spacing:normal;word-wrap:break-word;white-space:pre;pointer-events:none;content:attr(aria-label);background:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));border-radius:6px;opacity:0;}/*!sc*/@-webkit-keyframes tooltip-appear{from{opacity:0;}to{opacity:1;}}/*!sc*/@keyframes tooltip-appear{from{opacity:0;}to{opacity:1;}}/*!sc*/.dMjscx:hover::before,.dMjscx:active::before,.dMjscx:focus::before,.dMjscx:focus-within::before,.dMjscx:hover::after,.dMjscx:active::after,.dMjscx:focus::after,.dMjscx:focus-within::after{display:inline-block;-webkit-text-decoration:none;text-decoration:none;-webkit-animation-name:tooltip-appear;animation-name:tooltip-appear;-webkit-animation-duration:0.1s;animation-duration:0.1s;-webkit-animation-fill-mode:forwards;animation-fill-mode:forwards;-webkit-animation-timing-function:ease-in;animation-timing-function:ease-in;-webkit-animation-delay:0.4s;animation-delay:0.4s;}/*!sc*/.dMjscx.tooltipped-no-delay:hover::before,.dMjscx.tooltipped-no-delay:active::before,.dMjscx.tooltipped-no-delay:focus::before,.dMjscx.tooltipped-no-delay:focus-within::before,.dMjscx.tooltipped-no-delay:hover::after,.dMjscx.tooltipped-no-delay:active::after,.dMjscx.tooltipped-no-delay:focus::after,.dMjscx.tooltipped-no-delay:focus-within::after{-webkit-animation-delay:0s;animation-delay:0s;}/*!sc*/.dMjscx.tooltipped-multiline:hover::after,.dMjscx.tooltipped-multiline:active::after,.dMjscx.tooltipped-multiline:focus::after,.dMjscx.tooltipped-multiline:focus-within::after{display:table-cell;}/*!sc*/.dMjscx.tooltipped-s::after,.dMjscx.tooltipped-se::after,.dMjscx.tooltipped-sw::after{top:100%;right:50%;margin-top:6px;}/*!sc*/.dMjscx.tooltipped-s::before,.dMjscx.tooltipped-se::before,.dMjscx.tooltipped-sw::before{top:auto;right:50%;bottom:-7px;margin-right:-6px;border-bottom-color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));}/*!sc*/.dMjscx.tooltipped-se::after{right:auto;left:50%;margin-left:-16px;}/*!sc*/.dMjscx.tooltipped-sw::after{margin-right:-16px;}/*!sc*/.dMjscx.tooltipped-n::after,.dMjscx.tooltipped-ne::after,.dMjscx.tooltipped-nw::after{right:50%;bottom:100%;margin-bottom:6px;}/*!sc*/.dMjscx.tooltipped-n::before,.dMjscx.tooltipped-ne::before,.dMjscx.tooltipped-nw::before{top:-7px;right:50%;bottom:auto;margin-right:-6px;border-top-color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));}/*!sc*/.dMjscx.tooltipped-ne::after{right:auto;left:50%;margin-left:-16px;}/*!sc*/.dMjscx.tooltipped-nw::after{margin-right:-16px;}/*!sc*/.dMjscx.tooltipped-s::after,.dMjscx.tooltipped-n::after{-webkit-transform:translateX(50%);-ms-transform:translateX(50%);transform:translateX(50%);}/*!sc*/.dMjscx.tooltipped-w::after{right:100%;bottom:50%;margin-right:6px;-webkit-transform:translateY(50%);-ms-transform:translateY(50%);transform:translateY(50%);}/*!sc*/.dMjscx.tooltipped-w::before{top:50%;bottom:50%;left:-7px;margin-top:-6px;border-left-color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));}/*!sc*/.dMjscx.tooltipped-e::after{bottom:50%;left:100%;margin-left:6px;-webkit-transform:translateY(50%);-ms-transform:translateY(50%);transform:translateY(50%);}/*!sc*/.dMjscx.tooltipped-e::before{top:50%;right:-7px;bottom:50%;margin-top:-6px;border-right-color:var(--bgColor-emphasis,var(--color-neutral-emphasis-plus,#24292f));}/*!sc*/.dMjscx.tooltipped-multiline::after{width:-webkit-max-content;width:-moz-max-content;width:max-content;max-width:250px;word-wrap:break-word;white-space:pre-line;border-collapse:separate;}/*!sc*/.dMjscx.tooltipped-multiline.tooltipped-s::after,.dMjscx.tooltipped-multiline.tooltipped-n::after{right:auto;left:50%;-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%);transform:translateX(-50%);}/*!sc*/.dMjscx.tooltipped-multiline.tooltipped-w::after,.dMjscx.tooltipped-multiline.tooltipped-e::after{right:100%;}/*!sc*/.dMjscx.tooltipped-align-right-2::after{right:0;margin-right:0;}/*!sc*/.dMjscx.tooltipped-align-right-2::before{right:15px;}/*!sc*/.dMjscx.tooltipped-align-left-2::after{left:0;margin-left:0;}/*!sc*/.dMjscx.tooltipped-align-left-2::before{left:10px;}/*!sc*/data-styled.g18[id=\"Tooltip__TooltipBase-sc-17tf59c-0\"]{content:\"dMjscx,\"}/*!sc*/.bPgibo{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;list-style:none;white-space:nowrap;padding-top:0;padding-bottom:0;padding-left:0;padding-right:0;margin:0;margin-bottom:-1px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;gap:8px;position:relative;}/*!sc*/data-styled.g103[id=\"UnderlineNav__NavigationList-sc-1jfr31k-0\"]{content:\"bPgibo,\"}/*!sc*/</style> <!-- --> <!-- --> <div class=\"Box-sc-g0xbh4-0 izjvBm\"><div class=\"Box-sc-g0xbh4-0 rPQgy\"><div class=\"Box-sc-g0xbh4-0 eUMEDg\"></div></div><div class=\"Box-sc-g0xbh4-0 eLcVee\"><div class=\"Box-sc-g0xbh4-0 hsfLlq\"><div class=\"Box-sc-g0xbh4-0 gpKoUz\"><button type=\"button\" id=\"branch-picker-repos-header-ref-selector\" aria-haspopup=\"true\" tabindex=\"0\" aria-label=\"main branch\" data-testid=\"anchor-button\" class=\"types__StyledButton-sc-ws60qy-0 izDscS overview-ref-selector\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"text\"><div class=\"Box-sc-g0xbh4-0 bKgizp\"><div class=\"Box-sc-g0xbh4-0 iPGYsi\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-git-branch\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z\"></path></svg></div><div class=\"Box-sc-g0xbh4-0 dKmYfk ref-selector-button-text-container\"><span class=\"Text-sc-17v1xeu-0 bOMzPg\">\u00a0<!-- -->main</span></div></div></span><span data-component=\"trailingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-triangle-down\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z\"></path></svg></span></span></button><button hidden=\"\" data-hotkey-scope=\"read-only-cursor-text-area\"></button></div><div class=\"Box-sc-g0xbh4-0 laYubZ\"><a style=\"--button-color:fg.muted\" type=\"button\" href=\"/lucidrains/vit-pytorch/branches\" class=\"types__StyledButton-sc-ws60qy-0 cuOWTR\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-git-branch\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z\"></path></svg></span><span data-component=\"text\">Branches</span></span></a><a style=\"--button-color:fg.muted\" type=\"button\" href=\"/lucidrains/vit-pytorch/tags\" class=\"types__StyledButton-sc-ws60qy-0 cuOWTR\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-tag\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z\"></path></svg></span><span data-component=\"text\">Tags</span></span></a></div><div class=\"Box-sc-g0xbh4-0 swnaL\"><a style=\"--button-color:fg.muted\" type=\"button\" aria-label=\"Go to Branches page\" href=\"/lucidrains/vit-pytorch/branches\" data-no-visuals=\"true\" class=\"types__StyledButton-sc-ws60qy-0 tDSzd\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-git-branch\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z\"></path></svg></a><a style=\"--button-color:fg.muted\" type=\"button\" aria-label=\"Go to Tags page\" href=\"/lucidrains/vit-pytorch/tags\" data-no-visuals=\"true\" class=\"types__StyledButton-sc-ws60qy-0 tDSzd\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-tag\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z\"></path></svg></a></div></div><div class=\"Box-sc-g0xbh4-0 bWpuBf\"><div class=\"Box-sc-g0xbh4-0 grHjNb\"><div class=\"Box-sc-g0xbh4-0 dXTsqj\"><!--$!--><template></template><!--/$--></div><div class=\"Box-sc-g0xbh4-0 dCOrmu\"><button type=\"button\" data-no-visuals=\"true\" class=\"types__StyledButton-sc-ws60qy-0 ftZGca\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"text\">Go to file</span></span></button></div><div class=\"react-directory-add-file-icon\"></div><div class=\"react-directory-remove-file-icon\"></div></div><button type=\"button\" id=\":R2il5:\" aria-haspopup=\"true\" tabindex=\"0\" class=\"types__StyledButton-sc-ws60qy-0 gYvpXq\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><div class=\"Box-sc-g0xbh4-0 bVvbgP\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-code\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z\"></path></svg></div></span><span data-component=\"text\">Code</span></span><span data-component=\"trailingAction\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-triangle-down\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z\"></path></svg></span></button><div class=\"Box-sc-g0xbh4-0 bNDvfp\"><button data-component=\"IconButton\" type=\"button\" aria-label=\"Open more actions menu\" id=\":R3il5:\" aria-haspopup=\"true\" tabindex=\"0\" data-no-visuals=\"true\" class=\"types__StyledButton-sc-ws60qy-0 ftZGca\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-kebab-horizontal\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z\"></path></svg></button></div></div></div><div class=\"Box-sc-g0xbh4-0 yfPnm\"><div data-hpc=\"true\" class=\"Box-sc-g0xbh4-0\"><button hidden=\"\" data-testid=\"focus-next-element-button\" data-hotkey=\"j\" data-hotkey-scope=\"read-only-cursor-text-area\"></button><button hidden=\"\" data-hotkey=\"j\"></button><button hidden=\"\" data-testid=\"focus-previous-element-button\" data-hotkey=\"k\" data-hotkey-scope=\"read-only-cursor-text-area\"></button><button hidden=\"\" data-hotkey=\"k\"></button><h2 class=\"Heading__StyledHeading-sc-1c1dgg0-0 cgQnMS sr-only\" data-testid=\"screen-reader-heading\" id=\"folders-and-files\">Folders and files</h2><table aria-labelledby=\"folders-and-files\" class=\"Box-sc-g0xbh4-0 cAQuiW\"><thead class=\"Box-sc-g0xbh4-0 iiUlLN\"><tr class=\"Box-sc-g0xbh4-0 jmggSN\"><th colSpan=\"2\" class=\"Box-sc-g0xbh4-0 kvYunM\"><span class=\"Text-sc-17v1xeu-0 eUGNHp\">Name</span></th><th colSpan=\"1\" class=\"Box-sc-g0xbh4-0 hrLuxA\"><span class=\"Text-sc-17v1xeu-0 eUGNHp\">Name</span></th><th class=\"Box-sc-g0xbh4-0 ePjhhA\"><div title=\"Last commit message\" class=\"Truncate__StyledTruncate-sc-23o1d2-0 fUpWeN\"><span class=\"Text-sc-17v1xeu-0 eUGNHp\">Last commit message</span></div></th><th colSpan=\"1\" class=\"Box-sc-g0xbh4-0 cuEKae\"><div title=\"Last commit date\" class=\"Truncate__StyledTruncate-sc-23o1d2-0 fUpWeN\"><span class=\"Text-sc-17v1xeu-0 eUGNHp\">Last commit date</span></div></th></tr></thead><tbody><tr class=\"Box-sc-g0xbh4-0 jEbBOT\"><td colSpan=\"3\" class=\"Box-sc-g0xbh4-0 bTxCvM\"><div class=\"Box-sc-g0xbh4-0 eYedVD\"><h2 class=\"Heading__StyledHeading-sc-1c1dgg0-0 cgQnMS sr-only\" data-testid=\"screen-reader-heading\">Latest commit</h2><div style=\"width:120px\" class=\"Skeleton Skeleton--text\" data-testid=\"loading\">\u00a0</div><div class=\"Box-sc-g0xbh4-0 jGfYmh\"><div data-testid=\"latest-commit-details\" class=\"Box-sc-g0xbh4-0 lhFvfi\"></div><h2 class=\"Heading__StyledHeading-sc-1c1dgg0-0 cgQnMS sr-only\" data-testid=\"screen-reader-heading\">History</h2><a class=\"types__StyledButton-sc-ws60qy-0 fAkXQN react-last-commit-history-group\" href=\"/lucidrains/vit-pytorch/commits/main/\" data-size=\"small\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-history\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z\"></path></svg></span><span data-component=\"text\"><span class=\"Text-sc-17v1xeu-0 dALsKK\">302 Commits</span></span></span></a><div class=\"Box-sc-g0xbh4-0 bqgLjk\"></div><span role=\"tooltip\" aria-label=\"Commit history\" class=\"Tooltip__TooltipBase-sc-17tf59c-0 dMjscx tooltipped-n\"><a class=\"types__StyledButton-sc-ws60qy-0 fAkXQN react-last-commit-history-icon\" href=\"/lucidrains/vit-pytorch/commits/main/\"><span data-component=\"buttonContent\" class=\"Box-sc-g0xbh4-0 kkrdEu\"><span data-component=\"leadingVisual\" class=\"Box-sc-g0xbh4-0 trpoQ\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-history\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z\"></path></svg></span></span></a></span></div></div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-0\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\".github\" aria-label=\".github, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/tree/main/.github\">.github</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\".github\" aria-label=\".github, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/tree/main/.github\">.github</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-1\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"examples\" aria-label=\"examples, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/tree/main/examples\">examples</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"examples\" aria-label=\"examples, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/tree/main/examples\">examples</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-2\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"images\" aria-label=\"images, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/tree/main/images\">images</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"images\" aria-label=\"images, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/tree/main/images\">images</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-3\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"tests\" aria-label=\"tests, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/tree/main/tests\">tests</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"tests\" aria-label=\"tests, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/tree/main/tests\">tests</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-4\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"vit_pytorch\" aria-label=\"vit_pytorch, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/tree/main/vit_pytorch\">vit_pytorch</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"icon-directory\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"vit_pytorch\" aria-label=\"vit_pytorch, (Directory)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/tree/main/vit_pytorch\">vit_pytorch</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-5\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\".gitignore\" aria-label=\".gitignore, (File)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/blob/main/.gitignore\">.gitignore</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\".gitignore\" aria-label=\".gitignore, (File)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/blob/main/.gitignore\">.gitignore</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-6\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"LICENSE\" aria-label=\"LICENSE, (File)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/blob/main/LICENSE\">LICENSE</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"LICENSE\" aria-label=\"LICENSE, (File)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/blob/main/LICENSE\">LICENSE</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-7\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"MANIFEST.in\" aria-label=\"MANIFEST.in, (File)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/blob/main/MANIFEST.in\">MANIFEST.in</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"MANIFEST.in\" aria-label=\"MANIFEST.in, (File)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/blob/main/MANIFEST.in\">MANIFEST.in</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-8\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"README.md\" aria-label=\"README.md, (File)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/blob/main/README.md\">README.md</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"README.md\" aria-label=\"README.md, (File)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/blob/main/README.md\">README.md</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"react-directory-row undefined\" id=\"folder-row-9\"><td class=\"react-directory-row-name-cell-small-screen\" colSpan=\"2\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"setup.py\" aria-label=\"setup.py, (File)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/blob/main/setup.py\">setup.py</a></div></h3></div></div></td><td class=\"react-directory-row-name-cell-large-screen\" colSpan=\"1\"><div class=\"react-directory-filename-column\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"color-fg-muted\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z\"></path></svg><div class=\"overflow-hidden\"><h3><div class=\"react-directory-truncate\"><a title=\"setup.py\" aria-label=\"setup.py, (File)\" class=\"Link--primary\" href=\"/lucidrains/vit-pytorch/blob/main/setup.py\">setup.py</a></div></h3></div></div></td><td class=\"react-directory-row-commit-cell\"><div class=\"Skeleton Skeleton--text\">\u00a0</div></td><td><div class=\"Skeleton Skeleton--text\">\u00a0</div></td></tr><tr class=\"Box-sc-g0xbh4-0 epsqEd d-none\" data-testid=\"view-all-files-row\"><td colSpan=\"3\" class=\"Box-sc-g0xbh4-0 ldpruc\"><div><button class=\"Link__StyledLink-sc-14289xe-0 dheQRw\">View all files</button></div></td></tr></tbody></table></div><div class=\"Box-sc-g0xbh4-0 ehcSsh\"><div class=\"Box-sc-g0xbh4-0 iGmlUb\"><div class=\"Box-sc-g0xbh4-0 iRQGXA\"><h2 class=\"_VisuallyHidden__VisuallyHidden-sc-11jhm7a-0 rTZSs\">Repository files navigation</h2><nav aria-label=\"Repository files\" class=\"Box-sc-g0xbh4-0 dvTdPK\"><ul role=\"list\" class=\"UnderlineNav__NavigationList-sc-1jfr31k-0 bPgibo\"><li class=\"Box-sc-g0xbh4-0 gwuIGu\"><a href=\"#\" aria-current=\"page\" class=\"Link__StyledLink-sc-14289xe-0 vLMkZ\"><span data-component=\"icon\" class=\"Box-sc-g0xbh4-0 kOxwQs\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-book\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z\"></path></svg></span><span data-component=\"text\" data-content=\"README\" class=\"Box-sc-g0xbh4-0 kOgeFj\">README</span></a></li><li class=\"Box-sc-g0xbh4-0 gwuIGu\"><a href=\"#\" class=\"Link__StyledLink-sc-14289xe-0 bhqztV\"><span data-component=\"icon\" class=\"Box-sc-g0xbh4-0 kOxwQs\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-law\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M8.75.75V2h.985c.304 0 .603.08.867.231l1.29.736c.038.022.08.033.124.033h2.234a.75.75 0 0 1 0 1.5h-.427l2.111 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.006.005-.01.01-.045.04c-.21.176-.441.327-.686.45C14.556 10.78 13.88 11 13 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L12.178 4.5h-.162c-.305 0-.604-.079-.868-.231l-1.29-.736a.245.245 0 0 0-.124-.033H8.75V13h2.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1 0-1.5h2.5V3.5h-.984a.245.245 0 0 0-.124.033l-1.289.737c-.265.15-.564.23-.869.23h-.162l2.112 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.016.015-.045.04c-.21.176-.441.327-.686.45C4.556 10.78 3.88 11 3 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L2.178 4.5H1.75a.75.75 0 0 1 0-1.5h2.234a.249.249 0 0 0 .125-.033l1.288-.737c.265-.15.564-.23.869-.23h.984V.75a.75.75 0 0 1 1.5 0Zm2.945 8.477c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L13 6.327Zm-10 0c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L3 6.327Z\"></path></svg></span><span data-component=\"text\" data-content=\"MIT license\" class=\"Box-sc-g0xbh4-0\">MIT license</span></a></li></ul></nav><button style=\"--button-color:fg.subtle\" type=\"button\" aria-label=\"Outline\" id=\":Rdkl5:\" aria-haspopup=\"true\" tabindex=\"0\" class=\"types__StyledButton-sc-ws60qy-0 jPraEl\"><svg aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"octicon octicon-list-unordered\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\" fill=\"currentColor\" style=\"display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible\"><path d=\"M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z\"></path></svg></button></div><div class=\"Box-sc-g0xbh4-0 bJMeLZ js-snippet-clipboard-copy-unpositioned\" data-hpc=\"true\"><article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/vit.gif\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/vit.gif\" width=\"500px\" data-animated-image=\"\" style=\"max-width: 100%;\"></a></p><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Table of Contents</h2><a id=\"user-content-table-of-contents\" class=\"anchor-element\" aria-label=\"Permalink: Table of Contents\" href=\"#table-of-contents\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><ul dir=\"auto\"><li><a href=\"#vision-transformer---pytorch\">Vision Transformer - Pytorch</a></li><li><a href=\"#install\">Install</a></li><li><a href=\"#usage\">Usage</a></li><li><a href=\"#parameters\">Parameters</a></li><li><a href=\"#simple-vit\">Simple ViT</a></li><li><a href=\"#navit\">NaViT</a></li><li><a href=\"#distillation\">Distillation</a></li><li><a href=\"#deep-vit\">Deep ViT</a></li><li><a href=\"#cait\">CaiT</a></li><li><a href=\"#token-to-token-vit\">Token-to-Token ViT</a></li><li><a href=\"#cct\">CCT</a></li><li><a href=\"#cross-vit\">Cross ViT</a></li><li><a href=\"#pit\">PiT</a></li><li><a href=\"#levit\">LeViT</a></li><li><a href=\"#cvt\">CvT</a></li><li><a href=\"#twins-svt\">Twins SVT</a></li><li><a href=\"#crossformer\">CrossFormer</a></li><li><a href=\"#regionvit\">RegionViT</a></li><li><a href=\"#scalablevit\">ScalableViT</a></li><li><a href=\"#sepvit\">SepViT</a></li><li><a href=\"#maxvit\">MaxViT</a></li><li><a href=\"#nest\">NesT</a></li><li><a href=\"#mobilevit\">MobileViT</a></li><li><a href=\"#xcit\">XCiT</a></li><li><a href=\"#masked-autoencoder\">Masked Autoencoder</a></li><li><a href=\"#simple-masked-image-modeling\">Simple Masked Image Modeling</a></li><li><a href=\"#masked-patch-prediction\">Masked Patch Prediction</a></li><li><a href=\"#masked-position-prediction\">Masked Position Prediction</a></li><li><a href=\"#adaptive-token-sampling\">Adaptive Token Sampling</a></li><li><a href=\"#patch-merger\">Patch Merger</a></li><li><a href=\"#vision-transformer-for-small-datasets\">Vision Transformer for Small Datasets</a></li><li><a href=\"#3d-vit\">3D Vit</a></li><li><a href=\"#vivit\">ViVit</a></li><li><a href=\"#parallel-vit\">Parallel ViT</a></li><li><a href=\"#learnable-memory-vit\">Learnable Memory ViT</a></li><li><a href=\"#dino\">Dino</a></li><li><a href=\"#esvit\">EsViT</a></li><li><a href=\"#accessing-attention\">Accessing Attention</a></li><li><a href=\"#research-ideas\">Research Ideas</a><ul dir=\"auto\"><li><a href=\"#efficient-attention\">Efficient Attention</a></li><li><a href=\"#combining-with-other-transformer-improvements\">Combining with other Transformer improvements</a></li></ul></li><li><a href=\"#faq\">FAQ</a></li><li><a href=\"#resources\">Resources</a></li><li><a href=\"#citations\">Citations</a></li></ul><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Vision Transformer - Pytorch</h2><a id=\"user-content-vision-transformer---pytorch\" class=\"anchor-element\" aria-label=\"Permalink: Vision Transformer - Pytorch\" href=\"#vision-transformer---pytorch\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">Implementation of <a href=\"https://openreview.net/pdf?id=YicbFdNTTy\" rel=\"nofollow\">Vision Transformer</a>, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch. Significance is further explained in <a href=\"https://www.youtube.com/watch?v=TrdevFK_am4\" rel=\"nofollow\">Yannic Kilcher's</a> video. There's really not much to code here, but may as well lay it out for everyone so we expedite the attention revolution.</p><p dir=\"auto\">For a Pytorch implementation with pretrained models, please see Ross Wightman's repository <a href=\"https://github.com/rwightman/pytorch-image-models\">here</a>.</p><p dir=\"auto\">The official Jax repository is <a href=\"https://github.com/google-research/vision_transformer\">here</a>.</p><p dir=\"auto\">A tensorflow2 translation also exists <a href=\"https://github.com/taki0112/vit-tensorflow\">here</a>, created by research scientist <a href=\"https://github.com/taki0112\">Junho Kim</a>! \ud83d\ude4f</p><p dir=\"auto\"><a href=\"https://github.com/conceptofmind/vit-flax\">Flax translation</a> by <a href=\"https://github.com/conceptofmind\">Enrico Shippole</a>!</p><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Install</h2><a id=\"user-content-install\" class=\"anchor-element\" aria-label=\"Permalink: Install\" href=\"#install\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"$ pip install vit-pytorch\"><pre>$ pip install vit-pytorch</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Usage</h2><a id=\"user-content-usage\" class=\"anchor-element\" aria-label=\"Permalink: Usage\" href=\"#usage\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch import ViTv = ViT(    image_size = 256,    patch_size = 32,    num_classes = 1000,    dim = 1024,    depth = 6,    heads = 16,    mlp_dim = 2048,    dropout = 0.1,    emb_dropout = 0.1)img = torch.randn(1, 3, 256, 256)preds = v(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Parameters</h2><a id=\"user-content-parameters\" class=\"anchor-element\" aria-label=\"Permalink: Parameters\" href=\"#parameters\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><ul dir=\"auto\"><li><code>image_size</code>: int.<br>Image size. If you have rectangular images, make sure your image size is the maximum of the width and height</li><li><code>patch_size</code>: int.<br>Size of patches. <code>image_size</code> must be divisible by <code>patch_size</code>.<br>The number of patches is: <code> n = (image_size // patch_size) ** 2</code> and <code>n</code> <strong>must be greater than 16</strong>.</li><li><code>num_classes</code>: int.<br>Number of classes to classify.</li><li><code>dim</code>: int.<br>Last dimension of output tensor after linear transformation <code>nn.Linear(..., dim)</code>.</li><li><code>depth</code>: int.<br>Number of Transformer blocks.</li><li><code>heads</code>: int.<br>Number of heads in Multi-head Attention layer.</li><li><code>mlp_dim</code>: int.<br>Dimension of the MLP (FeedForward) layer.</li><li><code>channels</code>: int, default <code>3</code>.<br>Number of image's channels.</li><li><code>dropout</code>: float between <code>[0, 1]</code>, default <code>0.</code>.<br>Dropout rate.</li><li><code>emb_dropout</code>: float between <code>[0, 1]</code>, default <code>0</code>.<br>Embedding dropout rate.</li><li><code>pool</code>: string, either <code>cls</code> token pooling or <code>mean</code> pooling</li></ul><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Simple ViT</h2><a id=\"user-content-simple-vit\" class=\"anchor-element\" aria-label=\"Permalink: Simple ViT\" href=\"#simple-vit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2205.01580\" rel=\"nofollow\">An update</a> from some of the same authors of the original paper proposes simplifications to <code>ViT</code> that allows it to train faster and better.</p><p dir=\"auto\">Among these simplifications include 2d sinusoidal positional embedding, global average pooling (no CLS token), no dropout, batch sizes of 1024 rather than 4096, and use of RandAugment and MixUp augmentations. They also show that a simple linear at the end is not significantly worse than the original MLP head</p><p dir=\"auto\">You can use it by importing the <code>SimpleViT</code> as shown below</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch import SimpleViTv = SimpleViT(    image_size = 256,    patch_size = 32,    num_classes = 1000,    dim = 1024,    depth = 6,    heads = 16,    mlp_dim = 2048)img = torch.randn(1, 3, 256, 256)preds = v(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">SimpleViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">SimpleViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">NaViT</h2><a id=\"user-content-navit\" class=\"anchor-element\" aria-label=\"Permalink: NaViT\" href=\"#navit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/navit.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/navit.png\" width=\"450px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2307.06304\" rel=\"nofollow\">This paper</a> proposes to leverage the flexibility of attention and masking for variable lengthed sequences to train images of multiple resolution, packed into a single batch. They demonstrate much faster training and improved accuracies, with the only cost being extra complexity in the architecture and dataloading. They use factorized 2d positional encodings, token dropping, as well as query-key normalization.</p><p dir=\"auto\">You can use it as follows</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.na_vit import NaViTv = NaViT(    image_size = 256,    patch_size = 32,    num_classes = 1000,    dim = 1024,    depth = 6,    heads = 16,    mlp_dim = 2048,    dropout = 0.1,    emb_dropout = 0.1,    token_dropout_prob = 0.1  # token dropout of 10% (keep 90% of tokens))# 5 images of different resolutions - List[List[Tensor]]# for now, you'll have to correctly place images in same batch element as to not exceed maximum allowed sequence length for self-attention w/ maskingimages = [    [torch.randn(3, 256, 256), torch.randn(3, 128, 128)],    [torch.randn(3, 128, 256), torch.randn(3, 256, 128)],    [torch.randn(3, 64, 256)]]preds = v(images) # (5, 1000) - 5, because 5 images of different resolution above\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">na_vit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">NaViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">NaViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">token_dropout_prob</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>  <span class=\"pl-c\"># token dropout of 10% (keep 90% of tokens)</span>)<span class=\"pl-c\"># 5 images of different resolutions - List[List[Tensor]]</span><span class=\"pl-c\"># for now, you'll have to correctly place images in same batch element as to not exceed maximum allowed sequence length for self-attention w/ masking</span><span class=\"pl-s1\">images</span> <span class=\"pl-c1\">=</span> [    [<span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>), <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>)],    [<span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">256</span>), <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">128</span>)],    [<span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">256</span>)]]<span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">images</span>) <span class=\"pl-c\"># (5, 1000) - 5, because 5 images of different resolution above</span></pre></div><p dir=\"auto\">Or if you would rather that the framework auto group the images into variable lengthed sequences that do not exceed a certain max length</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"images = [    torch.randn(3, 256, 256),    torch.randn(3, 128, 128),    torch.randn(3, 128, 256),    torch.randn(3, 256, 128),    torch.randn(3, 64, 256)]preds = v(    images,    group_images = True,    group_max_seq_len = 64) # (5, 1000)\"><pre><span class=\"pl-s1\">images</span> <span class=\"pl-c1\">=</span> [    <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>),    <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>),    <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">256</span>),    <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">128</span>),    <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">256</span>)]<span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(    <span class=\"pl-s1\">images</span>,    <span class=\"pl-s1\">group_images</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>,    <span class=\"pl-s1\">group_max_seq_len</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">64</span>) <span class=\"pl-c\"># (5, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Distillation</h2><a id=\"user-content-distillation\" class=\"anchor-element\" aria-label=\"Permalink: Distillation\" href=\"#distillation\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/distill.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/distill.png\" width=\"300px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">A recent <a href=\"https://arxiv.org/abs/2012.12877\" rel=\"nofollow\">paper</a> has shown that use of a distillation token for distilling knowledge from convolutional nets to vision transformer can yield small and efficient vision transformers. This repository offers the means to do distillation easily.</p><p dir=\"auto\">ex. distilling from Resnet50 (or any teacher) to a vision transformer</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom torchvision.models import resnet50from vit_pytorch.distill import DistillableViT, DistillWrapperteacher = resnet50(pretrained = True)v = DistillableViT(    image_size = 256,    patch_size = 32,    num_classes = 1000,    dim = 1024,    depth = 6,    heads = 8,    mlp_dim = 2048,    dropout = 0.1,    emb_dropout = 0.1)distiller = DistillWrapper(    student = v,    teacher = teacher,    temperature = 3,           # temperature of distillation    alpha = 0.5,               # trade between main loss and distillation loss    hard = False               # whether to use soft or hard distillation)img = torch.randn(2, 3, 256, 256)labels = torch.randint(0, 1000, (2,))loss = distiller(img, labels)loss.backward()# after lots of training above ...pred = v(img) # (2, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">torchvision</span>.<span class=\"pl-s1\">models</span> <span class=\"pl-k\">import</span> <span class=\"pl-s1\">resnet50</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">distill</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">DistillableViT</span>, <span class=\"pl-v\">DistillWrapper</span><span class=\"pl-s1\">teacher</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">resnet50</span>(<span class=\"pl-s1\">pretrained</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>)<span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">DistillableViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>)<span class=\"pl-s1\">distiller</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">DistillWrapper</span>(    <span class=\"pl-s1\">student</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">v</span>,    <span class=\"pl-s1\">teacher</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">teacher</span>,    <span class=\"pl-s1\">temperature</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,           <span class=\"pl-c\"># temperature of distillation</span>    <span class=\"pl-s1\">alpha</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.5</span>,               <span class=\"pl-c\"># trade between main loss and distillation loss</span>    <span class=\"pl-s1\">hard</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">False</span>               <span class=\"pl-c\"># whether to use soft or hard distillation</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">labels</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randint</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1000</span>, (<span class=\"pl-c1\">2</span>,))<span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">distiller</span>(<span class=\"pl-s1\">img</span>, <span class=\"pl-s1\">labels</span>)<span class=\"pl-s1\">loss</span>.<span class=\"pl-en\">backward</span>()<span class=\"pl-c\"># after lots of training above ...</span><span class=\"pl-s1\">pred</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (2, 1000)</span></pre></div><p dir=\"auto\">The <code>DistillableViT</code> class is identical to <code>ViT</code> except for how the forward pass is handled, so you should be able to load the parameters back to <code>ViT</code> after you have completed distillation training.</p><p dir=\"auto\">You can also use the handy <code>.to_vit</code> method on the <code>DistillableViT</code> instance to get back a <code>ViT</code> instance.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"v = v.to_vit()type(v) # &lt;class 'vit_pytorch.vit_pytorch.ViT'&gt;\"><pre><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">v</span>.<span class=\"pl-en\">to_vit</span>()<span class=\"pl-en\">type</span>(<span class=\"pl-s1\">v</span>) <span class=\"pl-c\"># &lt;class 'vit_pytorch.vit_pytorch.ViT'&gt;</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Deep ViT</h2><a id=\"user-content-deep-vit\" class=\"anchor-element\" aria-label=\"Permalink: Deep ViT\" href=\"#deep-vit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">This <a href=\"https://arxiv.org/abs/2103.11886\" rel=\"nofollow\">paper</a> notes that ViT struggles to attend at greater depths (past 12 layers), and suggests mixing the attention of each head post-softmax as a solution, dubbed Re-attention. The results line up with the <a href=\"https://github.com/lucidrains/x-transformers#talking-heads-attention\">Talking Heads</a> paper from NLP.</p><p dir=\"auto\">You can use it as follows</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.deepvit import DeepViTv = DeepViT(    image_size = 256,    patch_size = 32,    num_classes = 1000,    dim = 1024,    depth = 6,    heads = 16,    mlp_dim = 2048,    dropout = 0.1,    emb_dropout = 0.1)img = torch.randn(1, 3, 256, 256)preds = v(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">deepvit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">DeepViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">DeepViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">CaiT</h2><a id=\"user-content-cait\" class=\"anchor-element\" aria-label=\"Permalink: CaiT\" href=\"#cait\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2103.17239\" rel=\"nofollow\">This paper</a> also notes difficulty in training vision transformers at greater depths and proposes two solutions. First it proposes to do per-channel multiplication of the output of the residual block. Second, it proposes to have the patches attend to one another, and only allow the CLS token to attend to the patches in the last few layers.</p><p dir=\"auto\">They also add <a href=\"https://github.com/lucidrains/x-transformers#talking-heads-attention\">Talking Heads</a>, noting improvements</p><p dir=\"auto\">You can use this scheme as follows</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.cait import CaiTv = CaiT(    image_size = 256,    patch_size = 32,    num_classes = 1000,    dim = 1024,    depth = 12,             # depth of transformer for patch to patch attention only    cls_depth = 2,          # depth of cross attention of CLS tokens to patch    heads = 16,    mlp_dim = 2048,    dropout = 0.1,    emb_dropout = 0.1,    layer_dropout = 0.05    # randomly dropout 5% of the layers)img = torch.randn(1, 3, 256, 256)preds = v(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">cait</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">CaiT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">CaiT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">12</span>,             <span class=\"pl-c\"># depth of transformer for patch to patch attention only</span>    <span class=\"pl-s1\">cls_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,          <span class=\"pl-c\"># depth of cross attention of CLS tokens to patch</span>    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">layer_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.05</span>    <span class=\"pl-c\"># randomly dropout 5% of the layers</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Token-to-Token ViT</h2><a id=\"user-content-token-to-token-vit\" class=\"anchor-element\" aria-label=\"Permalink: Token-to-Token ViT\" href=\"#token-to-token-vit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/t2t.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/t2t.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2101.11986\" rel=\"nofollow\">This paper</a> proposes that the first couple layers should downsample the image sequence by unfolding, leading to overlapping image data in each token as shown in the figure above. You can use this variant of the <code>ViT</code> as follows.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.t2t import T2TViTv = T2TViT(    dim = 512,    image_size = 224,    depth = 5,    heads = 8,    mlp_dim = 512,    num_classes = 1000,    t2t_layers = ((7, 4), (3, 2), (3, 2)) # tuples of the kernel size and stride of each consecutive layers of the initial token to token module)img = torch.randn(1, 3, 224, 224)preds = v(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">t2t</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">T2TViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">T2TViT</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">224</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">5</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">t2t_layers</span> <span class=\"pl-c1\">=</span> ((<span class=\"pl-c1\">7</span>, <span class=\"pl-c1\">4</span>), (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>), (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>)) <span class=\"pl-c\"># tuples of the kernel size and stride of each consecutive layers of the initial token to token module</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>)<span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">CCT</h2><a id=\"user-content-cct\" class=\"anchor-element\" aria-label=\"Permalink: CCT\" href=\"#cct\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://raw.githubusercontent.com/SHI-Labs/Compact-Transformers/main/images/model_sym.png\"><img src=\"https://raw.githubusercontent.com/SHI-Labs/Compact-Transformers/main/images/model_sym.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2104.05704\" rel=\"nofollow\">CCT</a> proposes compact transformersby using convolutions instead of patching and performing sequence pooling. Thisallows for CCT to have high accuracy and a low number of parameters.</p><p dir=\"auto\">You can use this with two methods</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.cct import CCTcct = CCT(    img_size = (224, 448),    embedding_dim = 384,    n_conv_layers = 2,    kernel_size = 7,    stride = 2,    padding = 3,    pooling_kernel_size = 3,    pooling_stride = 2,    pooling_padding = 1,    num_layers = 14,    num_heads = 6,    mlp_ratio = 3.,    num_classes = 1000,    positional_embedding = 'learnable', # ['sine', 'learnable', 'none'])img = torch.randn(1, 3, 224, 448)pred = cct(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">cct</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">CCT</span><span class=\"pl-s1\">cct</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">CCT</span>(    <span class=\"pl-s1\">img_size</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">448</span>),    <span class=\"pl-s1\">embedding_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">384</span>,    <span class=\"pl-s1\">n_conv_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">kernel_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">7</span>,    <span class=\"pl-s1\">stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">padding</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">pooling_kernel_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">pooling_stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">pooling_padding</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1</span>,    <span class=\"pl-s1\">num_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">14</span>,    <span class=\"pl-s1\">num_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">mlp_ratio</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3.</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">positional_embedding</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">'learnable'</span>, <span class=\"pl-c\"># ['sine', 'learnable', 'none']</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">448</span>)<span class=\"pl-s1\">pred</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">cct</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><p dir=\"auto\">Alternatively you can use one of several pre-defined models <code>[2,4,6,7,8,14,16]</code>which pre-define the number of layers, number of attention heads, the mlp ratio,and the embedding dimension.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.cct import cct_14cct = cct_14(    img_size = 224,    n_conv_layers = 1,    kernel_size = 7,    stride = 2,    padding = 3,    pooling_kernel_size = 3,    pooling_stride = 2,    pooling_padding = 1,    num_classes = 1000,    positional_embedding = 'learnable', # ['sine', 'learnable', 'none'])\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">cct</span> <span class=\"pl-k\">import</span> <span class=\"pl-s1\">cct_14</span><span class=\"pl-s1\">cct</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">cct_14</span>(    <span class=\"pl-s1\">img_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">224</span>,    <span class=\"pl-s1\">n_conv_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1</span>,    <span class=\"pl-s1\">kernel_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">7</span>,    <span class=\"pl-s1\">stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">padding</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">pooling_kernel_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">pooling_stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">pooling_padding</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">positional_embedding</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">'learnable'</span>, <span class=\"pl-c\"># ['sine', 'learnable', 'none']</span>)</pre></div><p dir=\"auto\"><a href=\"https://github.com/SHI-Labs/Compact-Transformers\">OfficialRepository</a> includes links to pretrained model checkpoints.</p><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Cross ViT</h2><a id=\"user-content-cross-vit\" class=\"anchor-element\" aria-label=\"Permalink: Cross ViT\" href=\"#cross-vit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/cross_vit.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/cross_vit.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2103.14899\" rel=\"nofollow\">This paper</a> proposes to have two vision transformers processing the image at different scales, cross attending to one every so often. They show improvements on top of the base vision transformer.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.cross_vit import CrossViTv = CrossViT(    image_size = 256,    num_classes = 1000,    depth = 4,               # number of multi-scale encoding blocks    sm_dim = 192,            # high res dimension    sm_patch_size = 16,      # high res patch size (should be smaller than lg_patch_size)    sm_enc_depth = 2,        # high res depth    sm_enc_heads = 8,        # high res heads    sm_enc_mlp_dim = 2048,   # high res feedforward dimension    lg_dim = 384,            # low res dimension    lg_patch_size = 64,      # low res patch size    lg_enc_depth = 3,        # low res depth    lg_enc_heads = 8,        # low res heads    lg_enc_mlp_dim = 2048,   # low res feedforward dimensions    cross_attn_depth = 2,    # cross attention rounds    cross_attn_heads = 8,    # cross attention heads    dropout = 0.1,    emb_dropout = 0.1)img = torch.randn(1, 3, 256, 256)pred = v(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">cross_vit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">CrossViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">CrossViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,               <span class=\"pl-c\"># number of multi-scale encoding blocks</span>    <span class=\"pl-s1\">sm_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">192</span>,            <span class=\"pl-c\"># high res dimension</span>    <span class=\"pl-s1\">sm_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,      <span class=\"pl-c\"># high res patch size (should be smaller than lg_patch_size)</span>    <span class=\"pl-s1\">sm_enc_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,        <span class=\"pl-c\"># high res depth</span>    <span class=\"pl-s1\">sm_enc_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-c\"># high res heads</span>    <span class=\"pl-s1\">sm_enc_mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,   <span class=\"pl-c\"># high res feedforward dimension</span>    <span class=\"pl-s1\">lg_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">384</span>,            <span class=\"pl-c\"># low res dimension</span>    <span class=\"pl-s1\">lg_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">64</span>,      <span class=\"pl-c\"># low res patch size</span>    <span class=\"pl-s1\">lg_enc_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,        <span class=\"pl-c\"># low res depth</span>    <span class=\"pl-s1\">lg_enc_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-c\"># low res heads</span>    <span class=\"pl-s1\">lg_enc_mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,   <span class=\"pl-c\"># low res feedforward dimensions</span>    <span class=\"pl-s1\">cross_attn_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-c\"># cross attention rounds</span>    <span class=\"pl-s1\">cross_attn_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-c\"># cross attention heads</span>    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">pred</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">PiT</h2><a id=\"user-content-pit\" class=\"anchor-element\" aria-label=\"Permalink: PiT\" href=\"#pit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/pit.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/pit.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2103.16302\" rel=\"nofollow\">This paper</a> proposes to downsample the tokens through a pooling procedure using depth-wise convolutions.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.pit import PiTv = PiT(    image_size = 224,    patch_size = 14,    dim = 256,    num_classes = 1000,    depth = (3, 3, 3),     # list of depths, indicating the number of rounds of each stage before a downsample    heads = 16,    mlp_dim = 2048,    dropout = 0.1,    emb_dropout = 0.1)# forward pass now returns predictions and the attention mapsimg = torch.randn(1, 3, 224, 224)preds = v(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">pit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">PiT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">PiT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">224</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">14</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>),     <span class=\"pl-c\"># list of depths, indicating the number of rounds of each stage before a downsample</span>    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>)<span class=\"pl-c\"># forward pass now returns predictions and the attention maps</span><span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>)<span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">LeViT</h2><a id=\"user-content-levit\" class=\"anchor-element\" aria-label=\"Permalink: LeViT\" href=\"#levit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/levit.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/levit.png\" width=\"300px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2104.01136\" rel=\"nofollow\">This paper</a> proposes a number of changes, including (1) convolutional embedding instead of patch-wise projection (2) downsampling in stages (3) extra non-linearity in attention (4) 2d relative positional biases instead of initial absolute positional bias (5) batchnorm in place of layernorm.</p><p dir=\"auto\"><a href=\"https://github.com/facebookresearch/LeViT\">Official repository</a></p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.levit import LeViTlevit = LeViT(    image_size = 224,    num_classes = 1000,    stages = 3,             # number of stages    dim = (256, 384, 512),  # dimensions at each stage    depth = 4,              # transformer of depth 4 at each stage    heads = (4, 6, 8),      # heads at each stage    mlp_mult = 2,    dropout = 0.1)img = torch.randn(1, 3, 224, 224)levit(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">levit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">LeViT</span><span class=\"pl-s1\">levit</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">LeViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">224</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">stages</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,             <span class=\"pl-c\"># number of stages</span>    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">384</span>, <span class=\"pl-c1\">512</span>),  <span class=\"pl-c\"># dimensions at each stage</span>    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,              <span class=\"pl-c\"># transformer of depth 4 at each stage</span>    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">8</span>),      <span class=\"pl-c\"># heads at each stage</span>    <span class=\"pl-s1\">mlp_mult</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>)<span class=\"pl-en\">levit</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">CvT</h2><a id=\"user-content-cvt\" class=\"anchor-element\" aria-label=\"Permalink: CvT\" href=\"#cvt\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/cvt.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/cvt.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2103.15808\" rel=\"nofollow\">This paper</a> proposes mixing convolutions and attention. Specifically, convolutions are used to embed and downsample the image / feature map in three stages. Depthwise-convoltion is also used to project the queries, keys, and values for attention.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.cvt import CvTv = CvT(    num_classes = 1000,    s1_emb_dim = 64,        # stage 1 - dimension    s1_emb_kernel = 7,      # stage 1 - conv kernel    s1_emb_stride = 4,      # stage 1 - conv stride    s1_proj_kernel = 3,     # stage 1 - attention ds-conv kernel size    s1_kv_proj_stride = 2,  # stage 1 - attention key / value projection stride    s1_heads = 1,           # stage 1 - heads    s1_depth = 1,           # stage 1 - depth    s1_mlp_mult = 4,        # stage 1 - feedforward expansion factor    s2_emb_dim = 192,       # stage 2 - (same as above)    s2_emb_kernel = 3,    s2_emb_stride = 2,    s2_proj_kernel = 3,    s2_kv_proj_stride = 2,    s2_heads = 3,    s2_depth = 2,    s2_mlp_mult = 4,    s3_emb_dim = 384,       # stage 3 - (same as above)    s3_emb_kernel = 3,    s3_emb_stride = 2,    s3_proj_kernel = 3,    s3_kv_proj_stride = 2,    s3_heads = 4,    s3_depth = 10,    s3_mlp_mult = 4,    dropout = 0.)img = torch.randn(1, 3, 224, 224)pred = v(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">cvt</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">CvT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">CvT</span>(    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">s1_emb_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">64</span>,        <span class=\"pl-c\"># stage 1 - dimension</span>    <span class=\"pl-s1\">s1_emb_kernel</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">7</span>,      <span class=\"pl-c\"># stage 1 - conv kernel</span>    <span class=\"pl-s1\">s1_emb_stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,      <span class=\"pl-c\"># stage 1 - conv stride</span>    <span class=\"pl-s1\">s1_proj_kernel</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,     <span class=\"pl-c\"># stage 1 - attention ds-conv kernel size</span>    <span class=\"pl-s1\">s1_kv_proj_stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,  <span class=\"pl-c\"># stage 1 - attention key / value projection stride</span>    <span class=\"pl-s1\">s1_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1</span>,           <span class=\"pl-c\"># stage 1 - heads</span>    <span class=\"pl-s1\">s1_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1</span>,           <span class=\"pl-c\"># stage 1 - depth</span>    <span class=\"pl-s1\">s1_mlp_mult</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,        <span class=\"pl-c\"># stage 1 - feedforward expansion factor</span>    <span class=\"pl-s1\">s2_emb_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">192</span>,       <span class=\"pl-c\"># stage 2 - (same as above)</span>    <span class=\"pl-s1\">s2_emb_kernel</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">s2_emb_stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">s2_proj_kernel</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">s2_kv_proj_stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">s2_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">s2_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">s2_mlp_mult</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,    <span class=\"pl-s1\">s3_emb_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">384</span>,       <span class=\"pl-c\"># stage 3 - (same as above)</span>    <span class=\"pl-s1\">s3_emb_kernel</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">s3_emb_stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">s3_proj_kernel</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">s3_kv_proj_stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">s3_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,    <span class=\"pl-s1\">s3_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">10</span>,    <span class=\"pl-s1\">s3_mlp_mult</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>)<span class=\"pl-s1\">pred</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Twins SVT</h2><a id=\"user-content-twins-svt\" class=\"anchor-element\" aria-label=\"Permalink: Twins SVT\" href=\"#twins-svt\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/twins_svt.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/twins_svt.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This <a href=\"https://arxiv.org/abs/2104.13840\" rel=\"nofollow\">paper</a> proposes mixing local and global attention, along with position encoding generator (proposed in <a href=\"https://arxiv.org/abs/2102.10882\" rel=\"nofollow\">CPVT</a>) and global average pooling, to achieve the same results as <a href=\"https://arxiv.org/abs/2103.14030\" rel=\"nofollow\">Swin</a>, without the extra complexity of shifted windows, CLS tokens, nor positional embeddings.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.twins_svt import TwinsSVTmodel = TwinsSVT(    num_classes = 1000,       # number of output classes    s1_emb_dim = 64,          # stage 1 - patch embedding projected dimension    s1_patch_size = 4,        # stage 1 - patch size for patch embedding    s1_local_patch_size = 7,  # stage 1 - patch size for local attention    s1_global_k = 7,          # stage 1 - global attention key / value reduction factor, defaults to 7 as specified in paper    s1_depth = 1,             # stage 1 - number of transformer blocks (local attn -&gt; ff -&gt; global attn -&gt; ff)    s2_emb_dim = 128,         # stage 2 (same as above)    s2_patch_size = 2,    s2_local_patch_size = 7,    s2_global_k = 7,    s2_depth = 1,    s3_emb_dim = 256,         # stage 3 (same as above)    s3_patch_size = 2,    s3_local_patch_size = 7,    s3_global_k = 7,    s3_depth = 5,    s4_emb_dim = 512,         # stage 4 (same as above)    s4_patch_size = 2,    s4_local_patch_size = 7,    s4_global_k = 7,    s4_depth = 4,    peg_kernel_size = 3,      # positional encoding generator kernel size    dropout = 0.              # dropout)img = torch.randn(1, 3, 224, 224)pred = model(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">twins_svt</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">TwinsSVT</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">TwinsSVT</span>(    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,       <span class=\"pl-c\"># number of output classes</span>    <span class=\"pl-s1\">s1_emb_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">64</span>,          <span class=\"pl-c\"># stage 1 - patch embedding projected dimension</span>    <span class=\"pl-s1\">s1_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,        <span class=\"pl-c\"># stage 1 - patch size for patch embedding</span>    <span class=\"pl-s1\">s1_local_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">7</span>,  <span class=\"pl-c\"># stage 1 - patch size for local attention</span>    <span class=\"pl-s1\">s1_global_k</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">7</span>,          <span class=\"pl-c\"># stage 1 - global attention key / value reduction factor, defaults to 7 as specified in paper</span>    <span class=\"pl-s1\">s1_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1</span>,             <span class=\"pl-c\"># stage 1 - number of transformer blocks (local attn -&gt; ff -&gt; global attn -&gt; ff)</span>    <span class=\"pl-s1\">s2_emb_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">128</span>,         <span class=\"pl-c\"># stage 2 (same as above)</span>    <span class=\"pl-s1\">s2_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">s2_local_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">7</span>,    <span class=\"pl-s1\">s2_global_k</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">7</span>,    <span class=\"pl-s1\">s2_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1</span>,    <span class=\"pl-s1\">s3_emb_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,         <span class=\"pl-c\"># stage 3 (same as above)</span>    <span class=\"pl-s1\">s3_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">s3_local_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">7</span>,    <span class=\"pl-s1\">s3_global_k</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">7</span>,    <span class=\"pl-s1\">s3_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">5</span>,    <span class=\"pl-s1\">s4_emb_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,         <span class=\"pl-c\"># stage 4 (same as above)</span>    <span class=\"pl-s1\">s4_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">s4_local_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">7</span>,    <span class=\"pl-s1\">s4_global_k</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">7</span>,    <span class=\"pl-s1\">s4_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,    <span class=\"pl-s1\">peg_kernel_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,      <span class=\"pl-c\"># positional encoding generator kernel size</span>    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.</span>              <span class=\"pl-c\"># dropout</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>)<span class=\"pl-s1\">pred</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">model</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">RegionViT</h2><a id=\"user-content-regionvit\" class=\"anchor-element\" aria-label=\"Permalink: RegionViT\" href=\"#regionvit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/regionvit.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/regionvit.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/regionvit2.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/regionvit2.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2106.02689\" rel=\"nofollow\">This paper</a> proposes to divide up the feature map into local regions, whereby the local tokens attend to each other. Each local region has its own regional token which then attends to all its local tokens, as well as other regional tokens.</p><p dir=\"auto\">You can use it as follows</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.regionvit import RegionViTmodel = RegionViT(    dim = (64, 128, 256, 512),      # tuple of size 4, indicating dimension at each stage    depth = (2, 2, 8, 2),           # depth of the region to local transformer at each stage    window_size = 7,                # window size, which should be either 7 or 14    num_classes = 1000,             # number of output classes    tokenize_local_3_conv = False,  # whether to use a 3 layer convolution to encode the local tokens from the image. the paper uses this for the smaller models, but uses only 1 conv (set to False) for the larger models    use_peg = False,                # whether to use positional generating module. they used this for object detection for a boost in performance)img = torch.randn(1, 3, 224, 224)pred = model(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">regionvit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">RegionViT</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">RegionViT</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">512</span>),      <span class=\"pl-c\"># tuple of size 4, indicating dimension at each stage</span>    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">2</span>),           <span class=\"pl-c\"># depth of the region to local transformer at each stage</span>    <span class=\"pl-s1\">window_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">7</span>,                <span class=\"pl-c\"># window size, which should be either 7 or 14</span>    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,             <span class=\"pl-c\"># number of output classes</span>    <span class=\"pl-s1\">tokenize_local_3_conv</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">False</span>,  <span class=\"pl-c\"># whether to use a 3 layer convolution to encode the local tokens from the image. the paper uses this for the smaller models, but uses only 1 conv (set to False) for the larger models</span>    <span class=\"pl-s1\">use_peg</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">False</span>,                <span class=\"pl-c\"># whether to use positional generating module. they used this for object detection for a boost in performance</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>)<span class=\"pl-s1\">pred</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">model</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">CrossFormer</h2><a id=\"user-content-crossformer\" class=\"anchor-element\" aria-label=\"Permalink: CrossFormer\" href=\"#crossformer\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/crossformer.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/crossformer.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/crossformer2.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/crossformer2.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This <a href=\"https://arxiv.org/abs/2108.00154\" rel=\"nofollow\">paper</a> beats PVT and Swin using alternating local and global attention. The global attention is done across the windowing dimension for reduced complexity, much like the scheme used for axial attention.</p><p dir=\"auto\">They also have cross-scale embedding layer, which they shown to be a generic layer that can improve all vision transformers. Dynamic relative positional bias was also formulated to allow the net to generalize to images of greater resolution.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.crossformer import CrossFormermodel = CrossFormer(    num_classes = 1000,                # number of output classes    dim = (64, 128, 256, 512),         # dimension at each stage    depth = (2, 2, 8, 2),              # depth of transformer at each stage    global_window_size = (8, 4, 2, 1), # global window sizes at each stage    local_window_size = 7,             # local window size (can be customized for each stage, but in paper, held constant at 7 for all stages))img = torch.randn(1, 3, 224, 224)pred = model(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">crossformer</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">CrossFormer</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">CrossFormer</span>(    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,                <span class=\"pl-c\"># number of output classes</span>    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">512</span>),         <span class=\"pl-c\"># dimension at each stage</span>    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">2</span>),              <span class=\"pl-c\"># depth of transformer at each stage</span>    <span class=\"pl-s1\">global_window_size</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-c\"># global window sizes at each stage</span>    <span class=\"pl-s1\">local_window_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">7</span>,             <span class=\"pl-c\"># local window size (can be customized for each stage, but in paper, held constant at 7 for all stages)</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>)<span class=\"pl-s1\">pred</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">model</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">ScalableViT</h2><a id=\"user-content-scalablevit\" class=\"anchor-element\" aria-label=\"Permalink: ScalableViT\" href=\"#scalablevit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/scalable-vit-1.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/scalable-vit-1.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/scalable-vit-2.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/scalable-vit-2.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This Bytedance AI <a href=\"https://arxiv.org/abs/2203.10790\" rel=\"nofollow\">paper</a> proposes the Scalable Self Attention (SSA) and the Interactive Windowed Self Attention (IWSA) modules. The SSA alleviates the computation needed at earlier stages by reducing the key / value feature map by some factor (<code>reduction_factor</code>), while modulating the dimension of the queries and keys (<code>ssa_dim_key</code>). The IWSA performs self attention within local windows, similar to other vision transformer papers. However, they add a residual of the values, passed through a convolution of kernel size 3, which they named Local Interactive Module (LIM).</p><p dir=\"auto\">They make the claim in this paper that this scheme outperforms Swin Transformer, and also demonstrate competitive performance against Crossformer.</p><p dir=\"auto\">You can use it as follows (ex. ScalableViT-S)</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.scalable_vit import ScalableViTmodel = ScalableViT(    num_classes = 1000,    dim = 64,                               # starting model dimension. at every stage, dimension is doubled    heads = (2, 4, 8, 16),                  # number of attention heads at each stage    depth = (2, 2, 20, 2),                  # number of transformer blocks at each stage    ssa_dim_key = (40, 40, 40, 32),         # the dimension of the attention keys (and queries) for SSA. in the paper, they represented this as a scale factor on the base dimension per key (ssa_dim_key / dim_key)    reduction_factor = (8, 4, 2, 1),        # downsampling of the key / values in SSA. in the paper, this was represented as (reduction_factor ** -2)    window_size = (64, 32, None, None),     # window size of the IWSA at each stage. None means no windowing needed    dropout = 0.1,                          # attention and feedforward dropout)img = torch.randn(1, 3, 256, 256)preds = model(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">scalable_vit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ScalableViT</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ScalableViT</span>(    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">64</span>,                               <span class=\"pl-c\"># starting model dimension. at every stage, dimension is doubled</span>    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">16</span>),                  <span class=\"pl-c\"># number of attention heads at each stage</span>    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">2</span>),                  <span class=\"pl-c\"># number of transformer blocks at each stage</span>    <span class=\"pl-s1\">ssa_dim_key</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">40</span>, <span class=\"pl-c1\">40</span>, <span class=\"pl-c1\">40</span>, <span class=\"pl-c1\">32</span>),         <span class=\"pl-c\"># the dimension of the attention keys (and queries) for SSA. in the paper, they represented this as a scale factor on the base dimension per key (ssa_dim_key / dim_key)</span>    <span class=\"pl-s1\">reduction_factor</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>),        <span class=\"pl-c\"># downsampling of the key / values in SSA. in the paper, this was represented as (reduction_factor ** -2)</span>    <span class=\"pl-s1\">window_size</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>),     <span class=\"pl-c\"># window size of the IWSA at each stage. None means no windowing needed</span>    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,                          <span class=\"pl-c\"># attention and feedforward dropout</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">model</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">SepViT</h2><a id=\"user-content-sepvit\" class=\"anchor-element\" aria-label=\"Permalink: SepViT\" href=\"#sepvit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/sep-vit.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/sep-vit.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">Another <a href=\"https://arxiv.org/abs/2203.15380\" rel=\"nofollow\">Bytedance AI paper</a>, it proposes a depthwise-pointwise self-attention layer that seems largely inspired by mobilenet's depthwise-separable convolution. The most interesting aspect is the reuse of the feature map from the depthwise self-attention stage as the values for the pointwise self-attention, as shown in the diagram above.</p><p dir=\"auto\">I have decided to include only the version of <code>SepViT</code> with this specific self-attention layer, as the grouped attention layers are not remarkable nor novel, and the authors were not clear on how they treated the window tokens for the group self-attention layer. Besides, it seems like with <code>DSSA</code> layer alone, they were able to beat Swin.</p><p dir=\"auto\">ex. SepViT-Lite</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.sep_vit import SepViTv = SepViT(    num_classes = 1000,    dim = 32,               # dimensions of first stage, which doubles every stage (32, 64, 128, 256) for SepViT-Lite    dim_head = 32,          # attention head dimension    heads = (1, 2, 4, 8),   # number of heads per stage    depth = (1, 2, 6, 2),   # number of transformer blocks per stage    window_size = 7,        # window size of DSS Attention block    dropout = 0.1           # dropout)img = torch.randn(1, 3, 224, 224)preds = v(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">sep_vit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">SepViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">SepViT</span>(    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,               <span class=\"pl-c\"># dimensions of first stage, which doubles every stage (32, 64, 128, 256) for SepViT-Lite</span>    <span class=\"pl-s1\">dim_head</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,          <span class=\"pl-c\"># attention head dimension</span>    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">8</span>),   <span class=\"pl-c\"># number of heads per stage</span>    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">2</span>),   <span class=\"pl-c\"># number of transformer blocks per stage</span>    <span class=\"pl-s1\">window_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">7</span>,        <span class=\"pl-c\"># window size of DSS Attention block</span>    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>           <span class=\"pl-c\"># dropout</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>)<span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">MaxViT</h2><a id=\"user-content-maxvit\" class=\"anchor-element\" aria-label=\"Permalink: MaxViT\" href=\"#maxvit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/max-vit.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/max-vit.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2204.01697\" rel=\"nofollow\">This paper</a> proposes a hybrid convolutional / attention network, using MBConv from the convolution side, and then block / grid axial sparse attention.</p><p dir=\"auto\">They also claim this specific vision transformer is good for generative models (GANs).</p><p dir=\"auto\">ex. MaxViT-S</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.max_vit import MaxViTv = MaxViT(    num_classes = 1000,    dim_conv_stem = 64,               # dimension of the convolutional stem, would default to dimension of first layer if not specified    dim = 96,                         # dimension of first layer, doubles every layer    dim_head = 32,                    # dimension of attention heads, kept at 32 in paper    depth = (2, 2, 5, 2),             # number of MaxViT blocks per stage, which consists of MBConv, block-like attention, grid-like attention    window_size = 7,                  # window size for block and grids    mbconv_expansion_rate = 4,        # expansion rate of MBConv    mbconv_shrinkage_rate = 0.25,     # shrinkage rate of squeeze-excitation in MBConv    dropout = 0.1                     # dropout)img = torch.randn(2, 3, 224, 224)preds = v(img) # (2, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">max_vit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">MaxViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">MaxViT</span>(    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim_conv_stem</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">64</span>,               <span class=\"pl-c\"># dimension of the convolutional stem, would default to dimension of first layer if not specified</span>    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">96</span>,                         <span class=\"pl-c\"># dimension of first layer, doubles every layer</span>    <span class=\"pl-s1\">dim_head</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,                    <span class=\"pl-c\"># dimension of attention heads, kept at 32 in paper</span>    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">2</span>),             <span class=\"pl-c\"># number of MaxViT blocks per stage, which consists of MBConv, block-like attention, grid-like attention</span>    <span class=\"pl-s1\">window_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">7</span>,                  <span class=\"pl-c\"># window size for block and grids</span>    <span class=\"pl-s1\">mbconv_expansion_rate</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,        <span class=\"pl-c\"># expansion rate of MBConv</span>    <span class=\"pl-s1\">mbconv_shrinkage_rate</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.25</span>,     <span class=\"pl-c\"># shrinkage rate of squeeze-excitation in MBConv</span>    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>                     <span class=\"pl-c\"># dropout</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>)<span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (2, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">NesT</h2><a id=\"user-content-nest\" class=\"anchor-element\" aria-label=\"Permalink: NesT\" href=\"#nest\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/nest.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/nest.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This <a href=\"https://arxiv.org/abs/2105.12723\" rel=\"nofollow\">paper</a> decided to process the image in hierarchical stages, with attention only within tokens of local blocks, which aggregate as it moves up the hierarchy. The aggregation is done in the image plane, and contains a convolution and subsequent maxpool to allow it to pass information across the boundary.</p><p dir=\"auto\">You can use it with the following code (ex. NesT-T)</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.nest import NesTnest = NesT(    image_size = 224,    patch_size = 4,    dim = 96,    heads = 3,    num_hierarchies = 3,        # number of hierarchies    block_repeats = (2, 2, 8),  # the number of transformer blocks at each hierarchy, starting from the bottom    num_classes = 1000)img = torch.randn(1, 3, 224, 224)pred = nest(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">nest</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">NesT</span><span class=\"pl-s1\">nest</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">NesT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">224</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">96</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">num_hierarchies</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,        <span class=\"pl-c\"># number of hierarchies</span>    <span class=\"pl-s1\">block_repeats</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">8</span>),  <span class=\"pl-c\"># the number of transformer blocks at each hierarchy, starting from the bottom</span>    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>)<span class=\"pl-s1\">pred</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">nest</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">MobileViT</h2><a id=\"user-content-mobilevit\" class=\"anchor-element\" aria-label=\"Permalink: MobileViT\" href=\"#mobilevit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/mbvit.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/mbvit.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This <a href=\"https://arxiv.org/abs/2110.02178\" rel=\"nofollow\">paper</a> introduce MobileViT, a light-weight and general purpose vision transformer for mobile devices. MobileViT presents a differentperspective for the global processing of information with transformers.</p><p dir=\"auto\">You can use it with the following code (ex. mobilevit_xs)</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.mobile_vit import MobileViTmbvit_xs = MobileViT(    image_size = (256, 256),    dims = [96, 120, 144],    channels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],    num_classes = 1000)img = torch.randn(1, 3, 256, 256)pred = mbvit_xs(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">mobile_vit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">MobileViT</span><span class=\"pl-s1\">mbvit_xs</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">MobileViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>),    <span class=\"pl-s1\">dims</span> <span class=\"pl-c1\">=</span> [<span class=\"pl-c1\">96</span>, <span class=\"pl-c1\">120</span>, <span class=\"pl-c1\">144</span>],    <span class=\"pl-s1\">channels</span> <span class=\"pl-c1\">=</span> [<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">48</span>, <span class=\"pl-c1\">48</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">80</span>, <span class=\"pl-c1\">80</span>, <span class=\"pl-c1\">96</span>, <span class=\"pl-c1\">96</span>, <span class=\"pl-c1\">384</span>],    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">pred</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">mbvit_xs</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">XCiT</h2><a id=\"user-content-xcit\" class=\"anchor-element\" aria-label=\"Permalink: XCiT\" href=\"#xcit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/xcit.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/xcit.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This <a href=\"https://arxiv.org/abs/2106.09681\" rel=\"nofollow\">paper</a> introduces the cross covariance attention (abbreviated XCA). One can think of it as doing attention across the features dimension rather than the spatial one (another perspective would be a dynamic 1x1 convolution, the kernel being attention map defined by spatial correlations).</p><p dir=\"auto\">Technically, this amounts to simply transposing the query, key, values before executing cosine similarity attention with learned temperature.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.xcit import XCiTv = XCiT(    image_size = 256,    patch_size = 32,    num_classes = 1000,    dim = 1024,    depth = 12,                     # depth of xcit transformer    cls_depth = 2,                  # depth of cross attention of CLS tokens to patch, attention pool at end    heads = 16,    mlp_dim = 2048,    dropout = 0.1,    emb_dropout = 0.1,    layer_dropout = 0.05,           # randomly dropout 5% of the layers    local_patch_kernel_size = 3     # kernel size of the local patch interaction module (depthwise convs))img = torch.randn(1, 3, 256, 256)preds = v(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">xcit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">XCiT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">XCiT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">12</span>,                     <span class=\"pl-c\"># depth of xcit transformer</span>    <span class=\"pl-s1\">cls_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,                  <span class=\"pl-c\"># depth of cross attention of CLS tokens to patch, attention pool at end</span>    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">layer_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.05</span>,           <span class=\"pl-c\"># randomly dropout 5% of the layers</span>    <span class=\"pl-s1\">local_patch_kernel_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>     <span class=\"pl-c\"># kernel size of the local patch interaction module (depthwise convs)</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Simple Masked Image Modeling</h2><a id=\"user-content-simple-masked-image-modeling\" class=\"anchor-element\" aria-label=\"Permalink: Simple Masked Image Modeling\" href=\"#simple-masked-image-modeling\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/simmim.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/simmim.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This <a href=\"https://arxiv.org/abs/2111.09886\" rel=\"nofollow\">paper</a> proposes a simple masked image modeling (SimMIM) scheme, using only a linear projection off the masked tokens into pixel space followed by an L1 loss with the pixel values of the masked patches. Results are competitive with other more complicated approaches.</p><p dir=\"auto\">You can use this as follows</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch import ViTfrom vit_pytorch.simmim import SimMIMv = ViT(    image_size = 256,    patch_size = 32,    num_classes = 1000,    dim = 1024,    depth = 6,    heads = 8,    mlp_dim = 2048)mim = SimMIM(    encoder = v,    masking_ratio = 0.5  # they found 50% to yield the best results)images = torch.randn(8, 3, 256, 256)loss = mim(images)loss.backward()# that's all!# do the above in a for loop many times with a lot of images and your vision transformer will learntorch.save(v.state_dict(), './trained-vit.pt')\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">simmim</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">SimMIM</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>)<span class=\"pl-s1\">mim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">SimMIM</span>(    <span class=\"pl-s1\">encoder</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">v</span>,    <span class=\"pl-s1\">masking_ratio</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.5</span>  <span class=\"pl-c\"># they found 50% to yield the best results</span>)<span class=\"pl-s1\">images</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">mim</span>(<span class=\"pl-s1\">images</span>)<span class=\"pl-s1\">loss</span>.<span class=\"pl-en\">backward</span>()<span class=\"pl-c\"># that's all!</span><span class=\"pl-c\"># do the above in a for loop many times with a lot of images and your vision transformer will learn</span><span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">save</span>(<span class=\"pl-s1\">v</span>.<span class=\"pl-en\">state_dict</span>(), <span class=\"pl-s\">'./trained-vit.pt'</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Masked Autoencoder</h2><a id=\"user-content-masked-autoencoder\" class=\"anchor-element\" aria-label=\"Permalink: Masked Autoencoder\" href=\"#masked-autoencoder\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/mae.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/mae.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">A new <a href=\"https://arxiv.org/abs/2111.06377\" rel=\"nofollow\">Kaiming He paper</a> proposes a simple autoencoder scheme where the vision transformer attends to a set of unmasked patches, and a smaller decoder tries to reconstruct the masked pixel values.</p><p dir=\"auto\"><a href=\"https://www.youtube.com/watch?v=LKixq2S2Pz8\" rel=\"nofollow\">DeepReader quick paper review</a></p><p dir=\"auto\"><a href=\"https://www.youtube.com/watch?v=Dp6iICL2dVI\" rel=\"nofollow\">AI Coffeebreak with Letitia</a></p><p dir=\"auto\">You can use it with the following code</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch import ViT, MAEv = ViT(    image_size = 256,    patch_size = 32,    num_classes = 1000,    dim = 1024,    depth = 6,    heads = 8,    mlp_dim = 2048)mae = MAE(    encoder = v,    masking_ratio = 0.75,   # the paper recommended 75% masked patches    decoder_dim = 512,      # paper showed good results with just 512    decoder_depth = 6       # anywhere from 1 to 8)images = torch.randn(8, 3, 256, 256)loss = mae(images)loss.backward()# that's all!# do the above in a for loop many times with a lot of images and your vision transformer will learn# save your improved vision transformertorch.save(v.state_dict(), './trained-vit.pt')\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span>, <span class=\"pl-v\">MAE</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>)<span class=\"pl-s1\">mae</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">MAE</span>(    <span class=\"pl-s1\">encoder</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">v</span>,    <span class=\"pl-s1\">masking_ratio</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.75</span>,   <span class=\"pl-c\"># the paper recommended 75% masked patches</span>    <span class=\"pl-s1\">decoder_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,      <span class=\"pl-c\"># paper showed good results with just 512</span>    <span class=\"pl-s1\">decoder_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>       <span class=\"pl-c\"># anywhere from 1 to 8</span>)<span class=\"pl-s1\">images</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">mae</span>(<span class=\"pl-s1\">images</span>)<span class=\"pl-s1\">loss</span>.<span class=\"pl-en\">backward</span>()<span class=\"pl-c\"># that's all!</span><span class=\"pl-c\"># do the above in a for loop many times with a lot of images and your vision transformer will learn</span><span class=\"pl-c\"># save your improved vision transformer</span><span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">save</span>(<span class=\"pl-s1\">v</span>.<span class=\"pl-en\">state_dict</span>(), <span class=\"pl-s\">'./trained-vit.pt'</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Masked Patch Prediction</h2><a id=\"user-content-masked-patch-prediction\" class=\"anchor-element\" aria-label=\"Permalink: Masked Patch Prediction\" href=\"#masked-patch-prediction\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">Thanks to <a href=\"https://github.com/zankner\">Zach</a>, you can train using the original masked patch prediction task presented in the paper, with the following code.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch import ViTfrom vit_pytorch.mpp import MPPmodel = ViT(    image_size=256,    patch_size=32,    num_classes=1000,    dim=1024,    depth=6,    heads=8,    mlp_dim=2048,    dropout=0.1,    emb_dropout=0.1)mpp_trainer = MPP(    transformer=model,    patch_size=32,    dim=1024,    mask_prob=0.15,          # probability of using token in masked prediction task    random_patch_prob=0.30,  # probability of randomly replacing a token being used for mpp    replace_prob=0.50,       # probability of replacing a token being used for mpp with the mask token)opt = torch.optim.Adam(mpp_trainer.parameters(), lr=3e-4)def sample_unlabelled_images():    return torch.FloatTensor(20, 3, 256, 256).uniform_(0., 1.)for _ in range(100):    images = sample_unlabelled_images()    loss = mpp_trainer(images)    opt.zero_grad()    loss.backward()    opt.step()# save your improved networktorch.save(model.state_dict(), './pretrained-net.pt')\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">mpp</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">MPP</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">image_size</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">num_classes</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">mlp_dim</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">0.1</span>)<span class=\"pl-s1\">mpp_trainer</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">MPP</span>(    <span class=\"pl-s1\">transformer</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">model</span>,    <span class=\"pl-s1\">patch_size</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">dim</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">mask_prob</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">0.15</span>,          <span class=\"pl-c\"># probability of using token in masked prediction task</span>    <span class=\"pl-s1\">random_patch_prob</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">0.30</span>,  <span class=\"pl-c\"># probability of randomly replacing a token being used for mpp</span>    <span class=\"pl-s1\">replace_prob</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">0.50</span>,       <span class=\"pl-c\"># probability of replacing a token being used for mpp with the mask token</span>)<span class=\"pl-s1\">opt</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-s1\">optim</span>.<span class=\"pl-v\">Adam</span>(<span class=\"pl-s1\">mpp_trainer</span>.<span class=\"pl-en\">parameters</span>(), <span class=\"pl-s1\">lr</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">3e-4</span>)<span class=\"pl-k\">def</span> <span class=\"pl-en\">sample_unlabelled_images</span>():    <span class=\"pl-k\">return</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-v\">FloatTensor</span>(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>).<span class=\"pl-en\">uniform_</span>(<span class=\"pl-c1\">0.</span>, <span class=\"pl-c1\">1.</span>)<span class=\"pl-k\">for</span> <span class=\"pl-s1\">_</span> <span class=\"pl-c1\">in</span> <span class=\"pl-en\">range</span>(<span class=\"pl-c1\">100</span>):    <span class=\"pl-s1\">images</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">sample_unlabelled_images</span>()    <span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">mpp_trainer</span>(<span class=\"pl-s1\">images</span>)    <span class=\"pl-s1\">opt</span>.<span class=\"pl-en\">zero_grad</span>()    <span class=\"pl-s1\">loss</span>.<span class=\"pl-en\">backward</span>()    <span class=\"pl-s1\">opt</span>.<span class=\"pl-en\">step</span>()<span class=\"pl-c\"># save your improved network</span><span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">save</span>(<span class=\"pl-s1\">model</span>.<span class=\"pl-en\">state_dict</span>(), <span class=\"pl-s\">'./pretrained-net.pt'</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Masked Position Prediction</h2><a id=\"user-content-masked-position-prediction\" class=\"anchor-element\" aria-label=\"Permalink: Masked Position Prediction\" href=\"#masked-position-prediction\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/mp3.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/mp3.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">New <a href=\"https://arxiv.org/abs/2207.07611\" rel=\"nofollow\">paper</a> that introduces masked position prediction pre-training criteria. This strategy is more efficient than the Masked Autoencoder strategy and has comparable performance.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.mp3 import ViT, MP3v = ViT(    num_classes = 1000,    image_size = 256,    patch_size = 8,    dim = 1024,    depth = 6,    heads = 8,    mlp_dim = 2048,    dropout = 0.1,)mp3 = MP3(    vit = v,    masking_ratio = 0.75)images = torch.randn(8, 3, 256, 256)loss = mp3(images)loss.backward()# that's all!# do the above in a for loop many times with a lot of images and your vision transformer will learn# save your improved vision transformertorch.save(v.state_dict(), './trained-vit.pt')\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">mp3</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span>, <span class=\"pl-v\">MP3</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,)<span class=\"pl-s1\">mp3</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">MP3</span>(    <span class=\"pl-s1\">vit</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">v</span>,    <span class=\"pl-s1\">masking_ratio</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.75</span>)<span class=\"pl-s1\">images</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">mp3</span>(<span class=\"pl-s1\">images</span>)<span class=\"pl-s1\">loss</span>.<span class=\"pl-en\">backward</span>()<span class=\"pl-c\"># that's all!</span><span class=\"pl-c\"># do the above in a for loop many times with a lot of images and your vision transformer will learn</span><span class=\"pl-c\"># save your improved vision transformer</span><span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">save</span>(<span class=\"pl-s1\">v</span>.<span class=\"pl-en\">state_dict</span>(), <span class=\"pl-s\">'./trained-vit.pt'</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Adaptive Token Sampling</h2><a id=\"user-content-adaptive-token-sampling\" class=\"anchor-element\" aria-label=\"Permalink: Adaptive Token Sampling\" href=\"#adaptive-token-sampling\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/ats.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/ats.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This <a href=\"https://arxiv.org/abs/2111.15667\" rel=\"nofollow\">paper</a> proposes to use the CLS attention scores, re-weighed by the norms of the value heads, as means to discard unimportant tokens at different layers.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.ats_vit import ViTv = ViT(    image_size = 256,    patch_size = 16,    num_classes = 1000,    dim = 1024,    depth = 6,    max_tokens_per_depth = (256, 128, 64, 32, 16, 8), # a tuple that denotes the maximum number of tokens that any given layer should have. if the layer has greater than this amount, it will undergo adaptive token sampling    heads = 16,    mlp_dim = 2048,    dropout = 0.1,    emb_dropout = 0.1)img = torch.randn(4, 3, 256, 256)preds = v(img) # (4, 1000)# you can also get a list of the final sampled patch ids# a value of -1 denotes paddingpreds, token_ids = v(img, return_sampled_token_ids = True) # (4, 1000), (4, &lt;=8)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">ats_vit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">max_tokens_per_depth</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">8</span>), <span class=\"pl-c\"># a tuple that denotes the maximum number of tokens that any given layer should have. if the layer has greater than this amount, it will undergo adaptive token sampling</span>    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (4, 1000)</span><span class=\"pl-c\"># you can also get a list of the final sampled patch ids</span><span class=\"pl-c\"># a value of -1 denotes padding</span><span class=\"pl-s1\">preds</span>, <span class=\"pl-s1\">token_ids</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>, <span class=\"pl-s1\">return_sampled_token_ids</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>) <span class=\"pl-c\"># (4, 1000), (4, &lt;=8)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Patch Merger</h2><a id=\"user-content-patch-merger\" class=\"anchor-element\" aria-label=\"Permalink: Patch Merger\" href=\"#patch-merger\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/patch_merger.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/patch_merger.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This <a href=\"https://arxiv.org/abs/2202.12015\" rel=\"nofollow\">paper</a> proposes a simple module (Patch Merger) for reducing the number of tokens at any layer of a vision transformer without sacrificing performance.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.vit_with_patch_merger import ViTv = ViT(    image_size = 256,    patch_size = 16,    num_classes = 1000,    dim = 1024,    depth = 12,    heads = 8,    patch_merge_layer = 6,        # at which transformer layer to do patch merging    patch_merge_num_tokens = 8,   # the output number of tokens from the patch merge    mlp_dim = 2048,    dropout = 0.1,    emb_dropout = 0.1)img = torch.randn(4, 3, 256, 256)preds = v(img) # (4, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">vit_with_patch_merger</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">12</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">patch_merge_layer</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-c\"># at which transformer layer to do patch merging</span>    <span class=\"pl-s1\">patch_merge_num_tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,   <span class=\"pl-c\"># the output number of tokens from the patch merge</span>    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (4, 1000)</span></pre></div><p dir=\"auto\">One can also use the <code>PatchMerger</code> module by itself</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.vit_with_patch_merger import PatchMergermerger = PatchMerger(    dim = 1024,    num_tokens_out = 8   # output number of tokens)features = torch.randn(4, 256, 1024) # (batch, num tokens, dimension)out = merger(features) # (4, 8, 1024)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">vit_with_patch_merger</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">PatchMerger</span><span class=\"pl-s1\">merger</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">PatchMerger</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">num_tokens_out</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>   <span class=\"pl-c\"># output number of tokens</span>)<span class=\"pl-s1\">features</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">1024</span>) <span class=\"pl-c\"># (batch, num tokens, dimension)</span><span class=\"pl-s1\">out</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">merger</span>(<span class=\"pl-s1\">features</span>) <span class=\"pl-c\"># (4, 8, 1024)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Vision Transformer for Small Datasets</h2><a id=\"user-content-vision-transformer-for-small-datasets\" class=\"anchor-element\" aria-label=\"Permalink: Vision Transformer for Small Datasets\" href=\"#vision-transformer-for-small-datasets\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/vit_for_small_datasets.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/vit_for_small_datasets.png\" width=\"400px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This <a href=\"https://arxiv.org/abs/2112.13492\" rel=\"nofollow\">paper</a> proposes a new image to patch function that incorporates shifts of the image, before normalizing and dividing the image into patches. I have found shifting to be extremely helpful in some other transformers work, so decided to include this for further explorations. It also includes the <code>LSA</code> with the learned temperature and masking out of a token's attention to itself.</p><p dir=\"auto\">You can use as follows:</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.vit_for_small_dataset import ViTv = ViT(    image_size = 256,    patch_size = 16,    num_classes = 1000,    dim = 1024,    depth = 6,    heads = 16,    mlp_dim = 2048,    dropout = 0.1,    emb_dropout = 0.1)img = torch.randn(4, 3, 256, 256)preds = v(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">vit_for_small_dataset</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><p dir=\"auto\">You can also use the <code>SPT</code> from this paper as a standalone module</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.vit_for_small_dataset import SPTspt = SPT(    dim = 1024,    patch_size = 16,    channels = 3)img = torch.randn(4, 3, 256, 256)tokens = spt(img) # (4, 256, 1024)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">vit_for_small_dataset</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">SPT</span><span class=\"pl-s1\">spt</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">SPT</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">channels</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">tokens</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">spt</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (4, 256, 1024)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">3D ViT</h2><a id=\"user-content-3d-vit\" class=\"anchor-element\" aria-label=\"Permalink: 3D ViT\" href=\"#3d-vit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">By popular request, I will start extending a few of the architectures in this repository to 3D ViTs, for use with video, medical imaging, etc.</p><p dir=\"auto\">You will need to pass in two additional hyperparameters: (1) the number of frames <code>frames</code> and (2) patch size along the frame dimension <code>frame_patch_size</code></p><p dir=\"auto\">For starters, 3D ViT</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.vit_3d import ViTv = ViT(    image_size = 128,          # image size    frames = 16,               # number of frames    image_patch_size = 16,     # image patch size    frame_patch_size = 2,      # frame patch size    num_classes = 1000,    dim = 1024,    depth = 6,    heads = 8,    mlp_dim = 2048,    dropout = 0.1,    emb_dropout = 0.1)video = torch.randn(4, 3, 16, 128, 128) # (batch, channels, frames, height, width)preds = v(video) # (4, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">vit_3d</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">128</span>,          <span class=\"pl-c\"># image size</span>    <span class=\"pl-s1\">frames</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,               <span class=\"pl-c\"># number of frames</span>    <span class=\"pl-s1\">image_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,     <span class=\"pl-c\"># image patch size</span>    <span class=\"pl-s1\">frame_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,      <span class=\"pl-c\"># frame patch size</span>    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>)<span class=\"pl-s1\">video</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>) <span class=\"pl-c\"># (batch, channels, frames, height, width)</span><span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">video</span>) <span class=\"pl-c\"># (4, 1000)</span></pre></div><p dir=\"auto\">3D Simple ViT</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.simple_vit_3d import SimpleViTv = SimpleViT(    image_size = 128,          # image size    frames = 16,               # number of frames    image_patch_size = 16,     # image patch size    frame_patch_size = 2,      # frame patch size    num_classes = 1000,    dim = 1024,    depth = 6,    heads = 8,    mlp_dim = 2048)video = torch.randn(4, 3, 16, 128, 128) # (batch, channels, frames, height, width)preds = v(video) # (4, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">simple_vit_3d</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">SimpleViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">SimpleViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">128</span>,          <span class=\"pl-c\"># image size</span>    <span class=\"pl-s1\">frames</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,               <span class=\"pl-c\"># number of frames</span>    <span class=\"pl-s1\">image_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,     <span class=\"pl-c\"># image patch size</span>    <span class=\"pl-s1\">frame_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,      <span class=\"pl-c\"># frame patch size</span>    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>)<span class=\"pl-s1\">video</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>) <span class=\"pl-c\"># (batch, channels, frames, height, width)</span><span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">video</span>) <span class=\"pl-c\"># (4, 1000)</span></pre></div><p dir=\"auto\">3D version of <a href=\"https://github.com/lucidrains/vit-pytorch#cct\">CCT</a></p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.cct_3d import CCTcct = CCT(    img_size = 224,    num_frames = 8,    embedding_dim = 384,    n_conv_layers = 2,    frame_kernel_size = 3,    kernel_size = 7,    stride = 2,    padding = 3,    pooling_kernel_size = 3,    pooling_stride = 2,    pooling_padding = 1,    num_layers = 14,    num_heads = 6,    mlp_ratio = 3.,    num_classes = 1000,    positional_embedding = 'learnable')video = torch.randn(1, 3, 8, 224, 224) # (batch, channels, frames, height, width)pred = cct(video)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">cct_3d</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">CCT</span><span class=\"pl-s1\">cct</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">CCT</span>(    <span class=\"pl-s1\">img_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">224</span>,    <span class=\"pl-s1\">num_frames</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">embedding_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">384</span>,    <span class=\"pl-s1\">n_conv_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">frame_kernel_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">kernel_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">7</span>,    <span class=\"pl-s1\">stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">padding</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">pooling_kernel_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">pooling_stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">pooling_padding</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1</span>,    <span class=\"pl-s1\">num_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">14</span>,    <span class=\"pl-s1\">num_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">mlp_ratio</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3.</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">positional_embedding</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">'learnable'</span>)<span class=\"pl-s1\">video</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>) <span class=\"pl-c\"># (batch, channels, frames, height, width)</span><span class=\"pl-s1\">pred</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">cct</span>(<span class=\"pl-s1\">video</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">ViViT</h2><a id=\"user-content-vivit\" class=\"anchor-element\" aria-label=\"Permalink: ViViT\" href=\"#vivit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/vivit.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/vivit.png\" width=\"350px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This <a href=\"https://arxiv.org/abs/2103.15691\" rel=\"nofollow\">paper</a> offers 3 different types of architectures for efficient attention of videos, with the main theme being factorizing the attention across space and time. This repository will offer the first variant, which is a spatial transformer followed by a temporal one.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.vivit import ViTv = ViT(    image_size = 128,          # image size    frames = 16,               # number of frames    image_patch_size = 16,     # image patch size    frame_patch_size = 2,      # frame patch size    num_classes = 1000,    dim = 1024,    spatial_depth = 6,         # depth of the spatial transformer    temporal_depth = 6,        # depth of the temporal transformer    heads = 8,    mlp_dim = 2048)video = torch.randn(4, 3, 16, 128, 128) # (batch, channels, frames, height, width)preds = v(video) # (4, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">vivit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">128</span>,          <span class=\"pl-c\"># image size</span>    <span class=\"pl-s1\">frames</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,               <span class=\"pl-c\"># number of frames</span>    <span class=\"pl-s1\">image_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,     <span class=\"pl-c\"># image patch size</span>    <span class=\"pl-s1\">frame_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,      <span class=\"pl-c\"># frame patch size</span>    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">spatial_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,         <span class=\"pl-c\"># depth of the spatial transformer</span>    <span class=\"pl-s1\">temporal_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,        <span class=\"pl-c\"># depth of the temporal transformer</span>    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>)<span class=\"pl-s1\">video</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>) <span class=\"pl-c\"># (batch, channels, frames, height, width)</span><span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">video</span>) <span class=\"pl-c\"># (4, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Parallel ViT</h2><a id=\"user-content-parallel-vit\" class=\"anchor-element\" aria-label=\"Permalink: Parallel ViT\" href=\"#parallel-vit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/parallel-vit.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/parallel-vit.png\" width=\"350px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This <a href=\"https://arxiv.org/abs/2203.09795\" rel=\"nofollow\">paper</a> propose parallelizing multiple attention and feedforward blocks per layer (2 blocks), claiming that it is easier to train without loss of performance.</p><p dir=\"auto\">You can try this variant as follows</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.parallel_vit import ViTv = ViT(    image_size = 256,    patch_size = 16,    num_classes = 1000,    dim = 1024,    depth = 6,    heads = 8,    mlp_dim = 2048,    num_parallel_branches = 2,  # in paper, they claimed 2 was optimal    dropout = 0.1,    emb_dropout = 0.1)img = torch.randn(4, 3, 256, 256)preds = v(img) # (4, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">parallel_vit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">num_parallel_branches</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,  <span class=\"pl-c\"># in paper, they claimed 2 was optimal</span>    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (4, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Learnable Memory ViT</h2><a id=\"user-content-learnable-memory-vit\" class=\"anchor-element\" aria-label=\"Permalink: Learnable Memory ViT\" href=\"#learnable-memory-vit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/learnable-memory-vit.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/learnable-memory-vit.png\" width=\"350px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">This <a href=\"https://arxiv.org/abs/2203.15243\" rel=\"nofollow\">paper</a> shows that adding learnable memory tokens at each layer of a vision transformer can greatly enhance fine-tuning results (in addition to learnable task specific CLS token and adapter head).</p><p dir=\"auto\">You can use this with a specially modified <code>ViT</code> as follows</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.learnable_memory_vit import ViT, Adapter# normal base ViTv = ViT(    image_size = 256,    patch_size = 16,    num_classes = 1000,    dim = 1024,    depth = 6,    heads = 8,    mlp_dim = 2048,    dropout = 0.1,    emb_dropout = 0.1)img = torch.randn(4, 3, 256, 256)logits = v(img) # (4, 1000)# do your usual training with ViT# ...# then, to finetune, just pass the ViT into the Adapter class# you can do this for multiple Adapters, as shown belowadapter1 = Adapter(    vit = v,    num_classes = 2,               # number of output classes for this specific task    num_memories_per_layer = 5     # number of learnable memories per layer, 10 was sufficient in paper)logits1 = adapter1(img) # (4, 2) - predict 2 classes off frozen ViT backbone with learnable memories and task specific head# yet another task to finetune on, this time with 4 classesadapter2 = Adapter(    vit = v,    num_classes = 4,    num_memories_per_layer = 10)logits2 = adapter2(img) # (4, 4) - predict 4 classes off frozen ViT backbone with learnable memories and task specific head\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">learnable_memory_vit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span>, <span class=\"pl-v\">Adapter</span><span class=\"pl-c\"># normal base ViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">logits</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (4, 1000)</span><span class=\"pl-c\"># do your usual training with ViT</span><span class=\"pl-c\"># ...</span><span class=\"pl-c\"># then, to finetune, just pass the ViT into the Adapter class</span><span class=\"pl-c\"># you can do this for multiple Adapters, as shown below</span><span class=\"pl-s1\">adapter1</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Adapter</span>(    <span class=\"pl-s1\">vit</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">v</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,               <span class=\"pl-c\"># number of output classes for this specific task</span>    <span class=\"pl-s1\">num_memories_per_layer</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">5</span>     <span class=\"pl-c\"># number of learnable memories per layer, 10 was sufficient in paper</span>)<span class=\"pl-s1\">logits1</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">adapter1</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (4, 2) - predict 2 classes off frozen ViT backbone with learnable memories and task specific head</span><span class=\"pl-c\"># yet another task to finetune on, this time with 4 classes</span><span class=\"pl-s1\">adapter2</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Adapter</span>(    <span class=\"pl-s1\">vit</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">v</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,    <span class=\"pl-s1\">num_memories_per_layer</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">10</span>)<span class=\"pl-s1\">logits2</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">adapter2</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (4, 4) - predict 4 classes off frozen ViT backbone with learnable memories and task specific head</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Dino</h2><a id=\"user-content-dino\" class=\"anchor-element\" aria-label=\"Permalink: Dino\" href=\"#dino\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/dino.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/dino.png\" width=\"350px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\">You can train <code>ViT</code> with the recent SOTA self-supervised learning technique, <a href=\"https://arxiv.org/abs/2104.14294\" rel=\"nofollow\">Dino</a>, with the following code.</p><p dir=\"auto\"><a href=\"https://www.youtube.com/watch?v=h3ij3F3cPIk\" rel=\"nofollow\">Yannic Kilcher</a> video</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch import ViT, Dinomodel = ViT(    image_size = 256,    patch_size = 32,    num_classes = 1000,    dim = 1024,    depth = 6,    heads = 8,    mlp_dim = 2048)learner = Dino(    model,    image_size = 256,    hidden_layer = 'to_latent',        # hidden layer name or index, from which to extract the embedding    projection_hidden_size = 256,      # projector network hidden dimension    projection_layers = 4,             # number of layers in projection network    num_classes_K = 65336,             # output logits dimensions (referenced as K in paper)    student_temp = 0.9,                # student temperature    teacher_temp = 0.04,               # teacher temperature, needs to be annealed from 0.04 to 0.07 over 30 epochs    local_upper_crop_scale = 0.4,      # upper bound for local crop - 0.4 was recommended in the paper     global_lower_crop_scale = 0.5,     # lower bound for global crop - 0.5 was recommended in the paper    moving_average_decay = 0.9,        # moving average of encoder - paper showed anywhere from 0.9 to 0.999 was ok    center_moving_average_decay = 0.9, # moving average of teacher centers - paper showed anywhere from 0.9 to 0.999 was ok)opt = torch.optim.Adam(learner.parameters(), lr = 3e-4)def sample_unlabelled_images():    return torch.randn(20, 3, 256, 256)for _ in range(100):    images = sample_unlabelled_images()    loss = learner(images)    opt.zero_grad()    loss.backward()    opt.step()    learner.update_moving_average() # update moving average of teacher encoder and teacher centers# save your improved networktorch.save(model.state_dict(), './pretrained-net.pt')\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span>, <span class=\"pl-v\">Dino</span><span class=\"pl-s1\">model</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>)<span class=\"pl-s1\">learner</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Dino</span>(    <span class=\"pl-s1\">model</span>,    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">hidden_layer</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">'to_latent'</span>,        <span class=\"pl-c\"># hidden layer name or index, from which to extract the embedding</span>    <span class=\"pl-s1\">projection_hidden_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,      <span class=\"pl-c\"># projector network hidden dimension</span>    <span class=\"pl-s1\">projection_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,             <span class=\"pl-c\"># number of layers in projection network</span>    <span class=\"pl-s1\">num_classes_K</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">65336</span>,             <span class=\"pl-c\"># output logits dimensions (referenced as K in paper)</span>    <span class=\"pl-s1\">student_temp</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.9</span>,                <span class=\"pl-c\"># student temperature</span>    <span class=\"pl-s1\">teacher_temp</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.04</span>,               <span class=\"pl-c\"># teacher temperature, needs to be annealed from 0.04 to 0.07 over 30 epochs</span>    <span class=\"pl-s1\">local_upper_crop_scale</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.4</span>,      <span class=\"pl-c\"># upper bound for local crop - 0.4 was recommended in the paper </span>    <span class=\"pl-s1\">global_lower_crop_scale</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.5</span>,     <span class=\"pl-c\"># lower bound for global crop - 0.5 was recommended in the paper</span>    <span class=\"pl-s1\">moving_average_decay</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.9</span>,        <span class=\"pl-c\"># moving average of encoder - paper showed anywhere from 0.9 to 0.999 was ok</span>    <span class=\"pl-s1\">center_moving_average_decay</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.9</span>, <span class=\"pl-c\"># moving average of teacher centers - paper showed anywhere from 0.9 to 0.999 was ok</span>)<span class=\"pl-s1\">opt</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-s1\">optim</span>.<span class=\"pl-v\">Adam</span>(<span class=\"pl-s1\">learner</span>.<span class=\"pl-en\">parameters</span>(), <span class=\"pl-s1\">lr</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3e-4</span>)<span class=\"pl-k\">def</span> <span class=\"pl-en\">sample_unlabelled_images</span>():    <span class=\"pl-k\">return</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-k\">for</span> <span class=\"pl-s1\">_</span> <span class=\"pl-c1\">in</span> <span class=\"pl-en\">range</span>(<span class=\"pl-c1\">100</span>):    <span class=\"pl-s1\">images</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">sample_unlabelled_images</span>()    <span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">learner</span>(<span class=\"pl-s1\">images</span>)    <span class=\"pl-s1\">opt</span>.<span class=\"pl-en\">zero_grad</span>()    <span class=\"pl-s1\">loss</span>.<span class=\"pl-en\">backward</span>()    <span class=\"pl-s1\">opt</span>.<span class=\"pl-en\">step</span>()    <span class=\"pl-s1\">learner</span>.<span class=\"pl-en\">update_moving_average</span>() <span class=\"pl-c\"># update moving average of teacher encoder and teacher centers</span><span class=\"pl-c\"># save your improved network</span><span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">save</span>(<span class=\"pl-s1\">model</span>.<span class=\"pl-en\">state_dict</span>(), <span class=\"pl-s\">'./pretrained-net.pt'</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">EsViT</h2><a id=\"user-content-esvit\" class=\"anchor-element\" aria-label=\"Permalink: EsViT\" href=\"#esvit\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"/lucidrains/vit-pytorch/blob/main/images/esvit.png\"><img src=\"/lucidrains/vit-pytorch/raw/main/images/esvit.png\" width=\"350px\" style=\"max-width: 100%;\"></a></p><p dir=\"auto\"><a href=\"https://arxiv.org/abs/2106.09785\" rel=\"nofollow\"><code>EsViT</code></a> is a variant of Dino (from above) re-engineered to support efficient <code>ViT</code>s with patch merging / downsampling by taking into an account an extra regional loss between the augmented views. To quote the abstract, it <code>outperforms its supervised counterpart on 17 out of 18 datasets</code> at 3 times higher throughput.</p><p dir=\"auto\">Even though it is named as though it were a new <code>ViT</code> variant, it actually is just a strategy for training any multistage <code>ViT</code> (in the paper, they focused on Swin). The example below will show how to use it with <code>CvT</code>. You'll need to set the <code>hidden_layer</code> to the name of the layer within your efficient ViT that outputs the non-average pooled visual representations, just before the global pooling and projection to logits.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.cvt import CvTfrom vit_pytorch.es_vit import EsViTTrainercvt = CvT(    num_classes = 1000,    s1_emb_dim = 64,    s1_emb_kernel = 7,    s1_emb_stride = 4,    s1_proj_kernel = 3,    s1_kv_proj_stride = 2,    s1_heads = 1,    s1_depth = 1,    s1_mlp_mult = 4,    s2_emb_dim = 192,    s2_emb_kernel = 3,    s2_emb_stride = 2,    s2_proj_kernel = 3,    s2_kv_proj_stride = 2,    s2_heads = 3,    s2_depth = 2,    s2_mlp_mult = 4,    s3_emb_dim = 384,    s3_emb_kernel = 3,    s3_emb_stride = 2,    s3_proj_kernel = 3,    s3_kv_proj_stride = 2,    s3_heads = 4,    s3_depth = 10,    s3_mlp_mult = 4,    dropout = 0.)learner = EsViTTrainer(    cvt,    image_size = 256,    hidden_layer = 'layers',           # hidden layer name or index, from which to extract the embedding    projection_hidden_size = 256,      # projector network hidden dimension    projection_layers = 4,             # number of layers in projection network    num_classes_K = 65336,             # output logits dimensions (referenced as K in paper)    student_temp = 0.9,                # student temperature    teacher_temp = 0.04,               # teacher temperature, needs to be annealed from 0.04 to 0.07 over 30 epochs    local_upper_crop_scale = 0.4,      # upper bound for local crop - 0.4 was recommended in the paper    global_lower_crop_scale = 0.5,     # lower bound for global crop - 0.5 was recommended in the paper    moving_average_decay = 0.9,        # moving average of encoder - paper showed anywhere from 0.9 to 0.999 was ok    center_moving_average_decay = 0.9, # moving average of teacher centers - paper showed anywhere from 0.9 to 0.999 was ok)opt = torch.optim.AdamW(learner.parameters(), lr = 3e-4)def sample_unlabelled_images():    return torch.randn(8, 3, 256, 256)for _ in range(1000):    images = sample_unlabelled_images()    loss = learner(images)    opt.zero_grad()    loss.backward()    opt.step()    learner.update_moving_average() # update moving average of teacher encoder and teacher centers# save your improved networktorch.save(cvt.state_dict(), './pretrained-net.pt')\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">cvt</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">CvT</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">es_vit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">EsViTTrainer</span><span class=\"pl-s1\">cvt</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">CvT</span>(    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">s1_emb_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">64</span>,    <span class=\"pl-s1\">s1_emb_kernel</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">7</span>,    <span class=\"pl-s1\">s1_emb_stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,    <span class=\"pl-s1\">s1_proj_kernel</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">s1_kv_proj_stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">s1_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1</span>,    <span class=\"pl-s1\">s1_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1</span>,    <span class=\"pl-s1\">s1_mlp_mult</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,    <span class=\"pl-s1\">s2_emb_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">192</span>,    <span class=\"pl-s1\">s2_emb_kernel</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">s2_emb_stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">s2_proj_kernel</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">s2_kv_proj_stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">s2_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">s2_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">s2_mlp_mult</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,    <span class=\"pl-s1\">s3_emb_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">384</span>,    <span class=\"pl-s1\">s3_emb_kernel</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">s3_emb_stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">s3_proj_kernel</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">s3_kv_proj_stride</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">s3_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,    <span class=\"pl-s1\">s3_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">10</span>,    <span class=\"pl-s1\">s3_mlp_mult</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.</span>)<span class=\"pl-s1\">learner</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">EsViTTrainer</span>(    <span class=\"pl-s1\">cvt</span>,    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">hidden_layer</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">'layers'</span>,           <span class=\"pl-c\"># hidden layer name or index, from which to extract the embedding</span>    <span class=\"pl-s1\">projection_hidden_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,      <span class=\"pl-c\"># projector network hidden dimension</span>    <span class=\"pl-s1\">projection_layers</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,             <span class=\"pl-c\"># number of layers in projection network</span>    <span class=\"pl-s1\">num_classes_K</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">65336</span>,             <span class=\"pl-c\"># output logits dimensions (referenced as K in paper)</span>    <span class=\"pl-s1\">student_temp</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.9</span>,                <span class=\"pl-c\"># student temperature</span>    <span class=\"pl-s1\">teacher_temp</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.04</span>,               <span class=\"pl-c\"># teacher temperature, needs to be annealed from 0.04 to 0.07 over 30 epochs</span>    <span class=\"pl-s1\">local_upper_crop_scale</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.4</span>,      <span class=\"pl-c\"># upper bound for local crop - 0.4 was recommended in the paper</span>    <span class=\"pl-s1\">global_lower_crop_scale</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.5</span>,     <span class=\"pl-c\"># lower bound for global crop - 0.5 was recommended in the paper</span>    <span class=\"pl-s1\">moving_average_decay</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.9</span>,        <span class=\"pl-c\"># moving average of encoder - paper showed anywhere from 0.9 to 0.999 was ok</span>    <span class=\"pl-s1\">center_moving_average_decay</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.9</span>, <span class=\"pl-c\"># moving average of teacher centers - paper showed anywhere from 0.9 to 0.999 was ok</span>)<span class=\"pl-s1\">opt</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-s1\">optim</span>.<span class=\"pl-v\">AdamW</span>(<span class=\"pl-s1\">learner</span>.<span class=\"pl-en\">parameters</span>(), <span class=\"pl-s1\">lr</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3e-4</span>)<span class=\"pl-k\">def</span> <span class=\"pl-en\">sample_unlabelled_images</span>():    <span class=\"pl-k\">return</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-k\">for</span> <span class=\"pl-s1\">_</span> <span class=\"pl-c1\">in</span> <span class=\"pl-en\">range</span>(<span class=\"pl-c1\">1000</span>):    <span class=\"pl-s1\">images</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">sample_unlabelled_images</span>()    <span class=\"pl-s1\">loss</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">learner</span>(<span class=\"pl-s1\">images</span>)    <span class=\"pl-s1\">opt</span>.<span class=\"pl-en\">zero_grad</span>()    <span class=\"pl-s1\">loss</span>.<span class=\"pl-en\">backward</span>()    <span class=\"pl-s1\">opt</span>.<span class=\"pl-en\">step</span>()    <span class=\"pl-s1\">learner</span>.<span class=\"pl-en\">update_moving_average</span>() <span class=\"pl-c\"># update moving average of teacher encoder and teacher centers</span><span class=\"pl-c\"># save your improved network</span><span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">save</span>(<span class=\"pl-s1\">cvt</span>.<span class=\"pl-en\">state_dict</span>(), <span class=\"pl-s\">'./pretrained-net.pt'</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Accessing Attention</h2><a id=\"user-content-accessing-attention\" class=\"anchor-element\" aria-label=\"Permalink: Accessing Attention\" href=\"#accessing-attention\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">If you would like to visualize the attention weights (post-softmax) for your research, just follow the procedure below</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.vit import ViTv = ViT(    image_size = 256,    patch_size = 32,    num_classes = 1000,    dim = 1024,    depth = 6,    heads = 16,    mlp_dim = 2048,    dropout = 0.1,    emb_dropout = 0.1)# import Recorder and wrap the ViTfrom vit_pytorch.recorder import Recorderv = Recorder(v)# forward pass now returns predictions and the attention mapsimg = torch.randn(1, 3, 256, 256)preds, attns = v(img)# there is one extra patch due to the CLS tokenattns # (1, 6, 16, 65, 65) - (batch x layers x heads x patch x patch)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">vit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>)<span class=\"pl-c\"># import Recorder and wrap the ViT</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">recorder</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Recorder</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Recorder</span>(<span class=\"pl-s1\">v</span>)<span class=\"pl-c\"># forward pass now returns predictions and the attention maps</span><span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">preds</span>, <span class=\"pl-s1\">attns</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>)<span class=\"pl-c\"># there is one extra patch due to the CLS token</span><span class=\"pl-s1\">attns</span> <span class=\"pl-c\"># (1, 6, 16, 65, 65) - (batch x layers x heads x patch x patch)</span></pre></div><p dir=\"auto\">to cleanup the class and the hooks once you have collected enough data</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"v = v.eject()  # wrapper is discarded and original ViT instance is returned\"><pre><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">v</span>.<span class=\"pl-en\">eject</span>()  <span class=\"pl-c\"># wrapper is discarded and original ViT instance is returned</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Accessing Embeddings</h2><a id=\"user-content-accessing-embeddings\" class=\"anchor-element\" aria-label=\"Permalink: Accessing Embeddings\" href=\"#accessing-embeddings\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">You can similarly access the embeddings with the <code>Extractor</code> wrapper</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.vit import ViTv = ViT(    image_size = 256,    patch_size = 32,    num_classes = 1000,    dim = 1024,    depth = 6,    heads = 16,    mlp_dim = 2048,    dropout = 0.1,    emb_dropout = 0.1)# import Recorder and wrap the ViTfrom vit_pytorch.extractor import Extractorv = Extractor(v)# forward pass now returns predictions and the attention mapsimg = torch.randn(1, 3, 256, 256)logits, embeddings = v(img)# there is one extra token due to the CLS tokenembeddings # (1, 65, 1024) - (batch x patches x model dim)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">vit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>)<span class=\"pl-c\"># import Recorder and wrap the ViT</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">extractor</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Extractor</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Extractor</span>(<span class=\"pl-s1\">v</span>)<span class=\"pl-c\"># forward pass now returns predictions and the attention maps</span><span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">logits</span>, <span class=\"pl-s1\">embeddings</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>)<span class=\"pl-c\"># there is one extra token due to the CLS token</span><span class=\"pl-s1\">embeddings</span> <span class=\"pl-c\"># (1, 65, 1024) - (batch x patches x model dim)</span></pre></div><p dir=\"auto\">Or say for <code>CrossViT</code>, which has a multi-scale encoder that outputs two sets of embeddings for 'large' and 'small' scales</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.cross_vit import CrossViTv = CrossViT(    image_size = 256,    num_classes = 1000,    depth = 4,    sm_dim = 192,    sm_patch_size = 16,    sm_enc_depth = 2,    sm_enc_heads = 8,    sm_enc_mlp_dim = 2048,    lg_dim = 384,    lg_patch_size = 64,    lg_enc_depth = 3,    lg_enc_heads = 8,    lg_enc_mlp_dim = 2048,    cross_attn_depth = 2,    cross_attn_heads = 8,    dropout = 0.1,    emb_dropout = 0.1)# wrap the CrossViTfrom vit_pytorch.extractor import Extractorv = Extractor(v, layer_name = 'multi_scale_encoder') # take embedding coming from the output of multi-scale-encoder# forward pass now returns predictions and the attention mapsimg = torch.randn(1, 3, 256, 256)logits, embeddings = v(img)# there is one extra token due to the CLS tokenembeddings # ((1, 257, 192), (1, 17, 384)) - (batch x patches x dimension) &lt;- large and small scales respectively\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">cross_vit</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">CrossViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">CrossViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">4</span>,    <span class=\"pl-s1\">sm_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">192</span>,    <span class=\"pl-s1\">sm_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">sm_enc_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">sm_enc_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">sm_enc_mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">lg_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">384</span>,    <span class=\"pl-s1\">lg_patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">64</span>,    <span class=\"pl-s1\">lg_enc_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">3</span>,    <span class=\"pl-s1\">lg_enc_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">lg_enc_mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">cross_attn_depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2</span>,    <span class=\"pl-s1\">cross_attn_heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>)<span class=\"pl-c\"># wrap the CrossViT</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">extractor</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Extractor</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Extractor</span>(<span class=\"pl-s1\">v</span>, <span class=\"pl-s1\">layer_name</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">'multi_scale_encoder'</span>) <span class=\"pl-c\"># take embedding coming from the output of multi-scale-encoder</span><span class=\"pl-c\"># forward pass now returns predictions and the attention maps</span><span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">logits</span>, <span class=\"pl-s1\">embeddings</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>)<span class=\"pl-c\"># there is one extra token due to the CLS token</span><span class=\"pl-s1\">embeddings</span> <span class=\"pl-c\"># ((1, 257, 192), (1, 17, 384)) - (batch x patches x dimension) &lt;- large and small scales respectively</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Research Ideas</h2><a id=\"user-content-research-ideas\" class=\"anchor-element\" aria-label=\"Permalink: Research Ideas\" href=\"#research-ideas\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Efficient Attention</h3><a id=\"user-content-efficient-attention\" class=\"anchor-element\" aria-label=\"Permalink: Efficient Attention\" href=\"#efficient-attention\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">There may be some coming from computer vision who think attention still suffers from quadratic costs. Fortunately, we have a lot of new techniques that may help. This repository offers a way for you to plugin your own sparse attention transformer.</p><p dir=\"auto\">An example with <a href=\"https://arxiv.org/abs/2102.03902\" rel=\"nofollow\">Nystromformer</a></p><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"$ pip install nystrom-attention\"><pre>$ pip install nystrom-attention</pre></div><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.efficient import ViTfrom nystrom_attention import Nystromformerefficient_transformer = Nystromformer(    dim = 512,    depth = 12,    heads = 8,    num_landmarks = 256)v = ViT(    dim = 512,    image_size = 2048,    patch_size = 32,    num_classes = 1000,    transformer = efficient_transformer)img = torch.randn(1, 3, 2048, 2048) # your high resolution picturev(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">efficient</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">nystrom_attention</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Nystromformer</span><span class=\"pl-s1\">efficient_transformer</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Nystromformer</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">12</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,    <span class=\"pl-s1\">num_landmarks</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>)<span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">transformer</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">efficient_transformer</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2048</span>, <span class=\"pl-c1\">2048</span>) <span class=\"pl-c\"># your high resolution picture</span><span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><p dir=\"auto\">Other sparse attention frameworks I would highly recommend is <a href=\"https://github.com/lucidrains/routing-transformer\">Routing Transformer</a> or <a href=\"https://github.com/lucidrains/sinkhorn-transformer\">Sinkhorn Transformer</a></p><div class=\"markdown-heading\" dir=\"auto\"><h3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Combining with other Transformer improvements</h3><a id=\"user-content-combining-with-other-transformer-improvements\" class=\"anchor-element\" aria-label=\"Permalink: Combining with other Transformer improvements\" href=\"#combining-with-other-transformer-improvements\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">This paper purposely used the most vanilla of attention networks to make a statement. If you would like to use some of the latest improvements for attention nets, please use the <code>Encoder</code> from <a href=\"https://github.com/lucidrains/x-transformers\">this repository</a>.</p><p dir=\"auto\">ex.</p><div class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"$ pip install x-transformers\"><pre>$ pip install x-transformers</pre></div><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch.efficient import ViTfrom x_transformers import Encoderv = ViT(    dim = 512,    image_size = 224,    patch_size = 16,    num_classes = 1000,    transformer = Encoder(        dim = 512,                  # set to be the same as the wrapper        depth = 12,        heads = 8,        ff_glu = True,              # ex. feed forward GLU variant https://arxiv.org/abs/2002.05202        residual_attn = True        # ex. residual attention https://arxiv.org/abs/2012.11747    ))img = torch.randn(1, 3, 224, 224)v(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span>.<span class=\"pl-s1\">efficient</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">x_transformers</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Encoder</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">224</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">transformer</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Encoder</span>(        <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">512</span>,                  <span class=\"pl-c\"># set to be the same as the wrapper</span>        <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">12</span>,        <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">8</span>,        <span class=\"pl-s1\">ff_glu</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>,              <span class=\"pl-c\"># ex. feed forward GLU variant https://arxiv.org/abs/2002.05202</span>        <span class=\"pl-s1\">residual_attn</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">True</span>        <span class=\"pl-c\"># ex. residual attention https://arxiv.org/abs/2012.11747</span>    ))<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>)<span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">FAQ</h2><a id=\"user-content-faq\" class=\"anchor-element\" aria-label=\"Permalink: FAQ\" href=\"#faq\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><ul dir=\"auto\"><li>How do I pass in non-square images?</li></ul><p dir=\"auto\">You can already pass in non-square images - you just have to make sure your height and width is less than or equal to the <code>image_size</code>, and both divisible by the <code>patch_size</code></p><p dir=\"auto\">ex.</p><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch import ViTv = ViT(    image_size = 256,    patch_size = 32,    num_classes = 1000,    dim = 1024,    depth = 6,    heads = 16,    mlp_dim = 2048,    dropout = 0.1,    emb_dropout = 0.1)img = torch.randn(1, 3, 256, 128) # &lt;-- not a squarepreds = v(img) # (1, 1000)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">256</span>,    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">32</span>,    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">128</span>) <span class=\"pl-c\"># &lt;-- not a square</span><span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>) <span class=\"pl-c\"># (1, 1000)</span></pre></div><ul dir=\"auto\"><li>How do I pass in non-square patches?</li></ul><div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import torchfrom vit_pytorch import ViTv = ViT(    num_classes = 1000,    image_size = (256, 128),  # image size is a tuple of (height, width)    patch_size = (32, 16),    # patch size is a tuple of (height, width)    dim = 1024,    depth = 6,    heads = 16,    mlp_dim = 2048,    dropout = 0.1,    emb_dropout = 0.1)img = torch.randn(1, 3, 256, 128)preds = v(img)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">torch</span><span class=\"pl-k\">from</span> <span class=\"pl-s1\">vit_pytorch</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ViT</span><span class=\"pl-s1\">v</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ViT</span>(    <span class=\"pl-s1\">num_classes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1000</span>,    <span class=\"pl-s1\">image_size</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">128</span>),  <span class=\"pl-c\"># image size is a tuple of (height, width)</span>    <span class=\"pl-s1\">patch_size</span> <span class=\"pl-c1\">=</span> (<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">16</span>),    <span class=\"pl-c\"># patch size is a tuple of (height, width)</span>    <span class=\"pl-s1\">dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">1024</span>,    <span class=\"pl-s1\">depth</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">6</span>,    <span class=\"pl-s1\">heads</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">16</span>,    <span class=\"pl-s1\">mlp_dim</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">2048</span>,    <span class=\"pl-s1\">dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>,    <span class=\"pl-s1\">emb_dropout</span> <span class=\"pl-c1\">=</span> <span class=\"pl-c1\">0.1</span>)<span class=\"pl-s1\">img</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">torch</span>.<span class=\"pl-en\">randn</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">128</span>)<span class=\"pl-s1\">preds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">v</span>(<span class=\"pl-s1\">img</span>)</pre></div><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Resources</h2><a id=\"user-content-resources\" class=\"anchor-element\" aria-label=\"Permalink: Resources\" href=\"#resources\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><p dir=\"auto\">Coming from computer vision and new to transformers? Here are some resources that greatly accelerated my learning.</p><ol dir=\"auto\"><li><p dir=\"auto\"><a href=\"http://jalammar.github.io/illustrated-transformer/\" rel=\"nofollow\">Illustrated Transformer</a> - Jay Alammar</p></li><li><p dir=\"auto\"><a href=\"http://peterbloem.nl/blog/transformers\" rel=\"nofollow\">Transformers from Scratch</a>  - Peter Bloem</p></li><li><p dir=\"auto\"><a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\" rel=\"nofollow\">The Annotated Transformer</a> - Harvard NLP</p></li></ol><div class=\"markdown-heading\" dir=\"auto\"><h2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\">Citations</h2><a id=\"user-content-citations\" class=\"anchor-element\" aria-label=\"Permalink: Citations\" href=\"#citations\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{hassani2021escaping,    title   = {Escaping the Big Data Paradigm with Compact Transformers},    author  = {Ali Hassani and Steven Walton and Nikhil Shah and Abulikemu Abuduweili and Jiachen Li and Humphrey Shi},    year    = 2021,    url     = {https://arxiv.org/abs/2104.05704},    eprint  = {2104.05704},    archiveprefix = {arXiv},    primaryclass = {cs.CV}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">hassani2021escaping</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Escaping the Big Data Paradigm with Compact Transformers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Ali Hassani and Steven Walton and Nikhil Shah and Abulikemu Abuduweili and Jiachen Li and Humphrey Shi<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-c1\">2021</span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://arxiv.org/abs/2104.05704<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2104.05704<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archiveprefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryclass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{dosovitskiy2020image,    title   = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},    author  = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},    year    = {2020},    eprint  = {2010.11929},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">dosovitskiy2020image</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2020<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2010.11929<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{touvron2020training,    title   = {Training data-efficient image transformers &amp; distillation through attention},     author  = {Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herv\u00e9 J\u00e9gou},    year    = {2020},    eprint  = {2012.12877},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">touvron2020training</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Training data-efficient image transformers &amp; distillation through attention<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herv\u00e9 J\u00e9gou<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2020<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2012.12877<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{yuan2021tokenstotoken,    title   = {Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet},    author  = {Li Yuan and Yunpeng Chen and Tao Wang and Weihao Yu and Yujun Shi and Francis EH Tay and Jiashi Feng and Shuicheng Yan},    year    = {2021},    eprint  = {2101.11986},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">yuan2021tokenstotoken</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Li Yuan and Yunpeng Chen and Tao Wang and Weihao Yu and Yujun Shi and Francis EH Tay and Jiashi Feng and Shuicheng Yan<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2101.11986<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{zhou2021deepvit,    title   = {DeepViT: Towards Deeper Vision Transformer},    author  = {Daquan Zhou and Bingyi Kang and Xiaojie Jin and Linjie Yang and Xiaochen Lian and Qibin Hou and Jiashi Feng},    year    = {2021},    eprint  = {2103.11886},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">zhou2021deepvit</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>DeepViT: Towards Deeper Vision Transformer<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Daquan Zhou and Bingyi Kang and Xiaojie Jin and Linjie Yang and Xiaochen Lian and Qibin Hou and Jiashi Feng<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2103.11886<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{touvron2021going,    title   = {Going deeper with Image Transformers},     author  = {Hugo Touvron and Matthieu Cord and Alexandre Sablayrolles and Gabriel Synnaeve and Herv\u00e9 J\u00e9gou},    year    = {2021},    eprint  = {2103.17239},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">touvron2021going</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Going deeper with Image Transformers<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Hugo Touvron and Matthieu Cord and Alexandre Sablayrolles and Gabriel Synnaeve and Herv\u00e9 J\u00e9gou<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2103.17239<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{chen2021crossvit,    title   = {CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification},    author  = {Chun-Fu Chen and Quanfu Fan and Rameswar Panda},    year    = {2021},    eprint  = {2103.14899},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">chen2021crossvit</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Chun-Fu Chen and Quanfu Fan and Rameswar Panda<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2103.14899<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{wu2021cvt,    title   = {CvT: Introducing Convolutions to Vision Transformers},    author  = {Haiping Wu and Bin Xiao and Noel Codella and Mengchen Liu and Xiyang Dai and Lu Yuan and Lei Zhang},    year    = {2021},    eprint  = {2103.15808},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">wu2021cvt</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>CvT: Introducing Convolutions to Vision Transformers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Haiping Wu and Bin Xiao and Noel Codella and Mengchen Liu and Xiyang Dai and Lu Yuan and Lei Zhang<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2103.15808<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{heo2021rethinking,    title   = {Rethinking Spatial Dimensions of Vision Transformers},     author  = {Byeongho Heo and Sangdoo Yun and Dongyoon Han and Sanghyuk Chun and Junsuk Choe and Seong Joon Oh},    year    = {2021},    eprint  = {2103.16302},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">heo2021rethinking</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Rethinking Spatial Dimensions of Vision Transformers<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Byeongho Heo and Sangdoo Yun and Dongyoon Han and Sanghyuk Chun and Junsuk Choe and Seong Joon Oh<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2103.16302<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{graham2021levit,    title   = {LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference},    author  = {Ben Graham and Alaaeldin El-Nouby and Hugo Touvron and Pierre Stock and Armand Joulin and Herv\u00e9 J\u00e9gou and Matthijs Douze},    year    = {2021},    eprint  = {2104.01136},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">graham2021levit</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Ben Graham and Alaaeldin El-Nouby and Hugo Touvron and Pierre Stock and Armand Joulin and Herv\u00e9 J\u00e9gou and Matthijs Douze<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2104.01136<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{li2021localvit,    title   = {LocalViT: Bringing Locality to Vision Transformers},    author  = {Yawei Li and Kai Zhang and Jiezhang Cao and Radu Timofte and Luc Van Gool},    year    = {2021},    eprint  = {2104.05707},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">li2021localvit</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>LocalViT: Bringing Locality to Vision Transformers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Yawei Li and Kai Zhang and Jiezhang Cao and Radu Timofte and Luc Van Gool<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2104.05707<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{chu2021twins,    title   = {Twins: Revisiting Spatial Attention Design in Vision Transformers},    author  = {Xiangxiang Chu and Zhi Tian and Yuqing Wang and Bo Zhang and Haibing Ren and Xiaolin Wei and Huaxia Xia and Chunhua Shen},    year    = {2021},    eprint  = {2104.13840},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">chu2021twins</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Twins: Revisiting Spatial Attention Design in Vision Transformers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Xiangxiang Chu and Zhi Tian and Yuqing Wang and Bo Zhang and Haibing Ren and Xiaolin Wei and Huaxia Xia and Chunhua Shen<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2104.13840<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{su2021roformer,    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},     author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},    year    = {2021},    eprint  = {2104.09864},    archivePrefix = {arXiv},    primaryClass = {cs.CL}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">su2021roformer</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>RoFormer: Enhanced Transformer with Rotary Position Embedding<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2104.09864<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CL<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{zhang2021aggregating,    title   = {Aggregating Nested Transformers},    author  = {Zizhao Zhang and Han Zhang and Long Zhao and Ting Chen and Tomas Pfister},    year    = {2021},    eprint  = {2105.12723},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">zhang2021aggregating</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Aggregating Nested Transformers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Zizhao Zhang and Han Zhang and Long Zhao and Ting Chen and Tomas Pfister<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2105.12723<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{chen2021regionvit,    title   = {RegionViT: Regional-to-Local Attention for Vision Transformers},     author  = {Chun-Fu Chen and Rameswar Panda and Quanfu Fan},    year    = {2021},    eprint  = {2106.02689},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">chen2021regionvit</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>RegionViT: Regional-to-Local Attention for Vision Transformers<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Chun-Fu Chen and Rameswar Panda and Quanfu Fan<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2106.02689<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{wang2021crossformer,    title   = {CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention},     author  = {Wenxiao Wang and Lu Yao and Long Chen and Binbin Lin and Deng Cai and Xiaofei He and Wei Liu},    year    = {2021},    eprint  = {2108.00154},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">wang2021crossformer</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Wenxiao Wang and Lu Yao and Long Chen and Binbin Lin and Deng Cai and Xiaofei He and Wei Liu<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2108.00154<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{caron2021emerging,    title   = {Emerging Properties in Self-Supervised Vision Transformers},    author  = {Mathilde Caron and Hugo Touvron and Ishan Misra and Herv\u00e9 J\u00e9gou and Julien Mairal and Piotr Bojanowski and Armand Joulin},    year    = {2021},    eprint  = {2104.14294},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">caron2021emerging</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Emerging Properties in Self-Supervised Vision Transformers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Mathilde Caron and Hugo Touvron and Ishan Misra and Herv\u00e9 J\u00e9gou and Julien Mairal and Piotr Bojanowski and Armand Joulin<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2104.14294<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{he2021masked,    title   = {Masked Autoencoders Are Scalable Vision Learners},     author  = {Kaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr Doll\u00e1r and Ross Girshick},    year    = {2021},    eprint  = {2111.06377},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">he2021masked</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Masked Autoencoders Are Scalable Vision Learners<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Kaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr Doll\u00e1r and Ross Girshick<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2111.06377<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{xie2021simmim,    title   = {SimMIM: A Simple Framework for Masked Image Modeling},     author  = {Zhenda Xie and Zheng Zhang and Yue Cao and Yutong Lin and Jianmin Bao and Zhuliang Yao and Qi Dai and Han Hu},    year    = {2021},    eprint  = {2111.09886},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">xie2021simmim</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>SimMIM: A Simple Framework for Masked Image Modeling<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Zhenda Xie and Zheng Zhang and Yue Cao and Yutong Lin and Jianmin Bao and Zhuliang Yao and Qi Dai and Han Hu<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2111.09886<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{fayyaz2021ats,    title   = {ATS: Adaptive Token Sampling For Efficient Vision Transformers},    author  = {Mohsen Fayyaz and Soroush Abbasi Kouhpayegani and Farnoush Rezaei Jafari and Eric Sommerlade and Hamid Reza Vaezi Joze and Hamed Pirsiavash and Juergen Gall},    year    = {2021},    eprint  = {2111.15667},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">fayyaz2021ats</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ATS: Adaptive Token Sampling For Efficient Vision Transformers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Mohsen Fayyaz and Soroush Abbasi Kouhpayegani and Farnoush Rezaei Jafari and Eric Sommerlade and Hamid Reza Vaezi Joze and Hamed Pirsiavash and Juergen Gall<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2111.15667<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{mehta2021mobilevit,    title   = {MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},    author  = {Sachin Mehta and Mohammad Rastegari},    year    = {2021},    eprint  = {2110.02178},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">mehta2021mobilevit</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Sachin Mehta and Mohammad Rastegari<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2110.02178<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{lee2021vision,    title   = {Vision Transformer for Small-Size Datasets},     author  = {Seung Hoon Lee and Seunghyun Lee and Byung Cheol Song},    year    = {2021},    eprint  = {2112.13492},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">lee2021vision</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Vision Transformer for Small-Size Datasets<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Seung Hoon Lee and Seunghyun Lee and Byung Cheol Song<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2112.13492<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{renggli2022learning,    title   = {Learning to Merge Tokens in Vision Transformers},    author  = {Cedric Renggli and Andr\u00e9 Susano Pinto and Neil Houlsby and Basil Mustafa and Joan Puigcerver and Carlos Riquelme},    year    = {2022},    eprint  = {2202.12015},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">renggli2022learning</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Learning to Merge Tokens in Vision Transformers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Cedric Renggli and Andr\u00e9 Susano Pinto and Neil Houlsby and Basil Mustafa and Joan Puigcerver and Carlos Riquelme<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2202.12015<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{yang2022scalablevit,    title   = {ScalableViT: Rethinking the Context-oriented Generalization of Vision Transformer},     author  = {Rui Yang and Hailong Ma and Jie Wu and Yansong Tang and Xuefeng Xiao and Min Zheng and Xiu Li},    year    = {2022},    eprint  = {2203.10790},    archivePrefix = {arXiv},    primaryClass = {cs.CV}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">yang2022scalablevit</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ScalableViT: Rethinking the Context-oriented Generalization of Vision Transformer<span class=\"pl-pds\">}</span></span>,     <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Rui Yang and Hailong Ma and Jie Wu and Yansong Tang and Xuefeng Xiao and Min Zheng and Xiu Li<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2203.10790<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CV<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Touvron2022ThreeTE,    title   = {Three things everyone should know about Vision Transformers},    author  = {Hugo Touvron and Matthieu Cord and Alaaeldin El-Nouby and Jakob Verbeek and Herv'e J'egou},    year    = {2022}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Touvron2022ThreeTE</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Three things everyone should know about Vision Transformers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Hugo Touvron and Matthieu Cord and Alaaeldin El-Nouby and Jakob Verbeek and Herv'e J'egou<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Sandler2022FinetuningIT,    title   = {Fine-tuning Image Transformers using Learnable Memory},    author  = {Mark Sandler and Andrey Zhmoginov and Max Vladymyrov and Andrew Jackson},    year    = {2022}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Sandler2022FinetuningIT</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Fine-tuning Image Transformers using Learnable Memory<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Mark Sandler and Andrey Zhmoginov and Max Vladymyrov and Andrew Jackson<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Li2022SepViTSV,    title   = {SepViT: Separable Vision Transformer},    author  = {Wei Li and Xing Wang and Xin Xia and Jie Wu and Xuefeng Xiao and Minghang Zheng and Shiping Wen},    year    = {2022}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Li2022SepViTSV</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>SepViT: Separable Vision Transformer<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Wei Li and Xing Wang and Xin Xia and Jie Wu and Xuefeng Xiao and Minghang Zheng and Shiping Wen<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Tu2022MaxViTMV,    title   = {MaxViT: Multi-Axis Vision Transformer},    author  = {Zhengzhong Tu and Hossein Talebi and Han Zhang and Feng Yang and Peyman Milanfar and Alan Conrad Bovik and Yinxiao Li},    year    = {2022}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Tu2022MaxViTMV</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>MaxViT: Multi-Axis Vision Transformer<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Zhengzhong Tu and Hossein Talebi and Han Zhang and Feng Yang and Peyman Milanfar and Alan Conrad Bovik and Yinxiao Li<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{Li2021EfficientSV,    title   = {Efficient Self-supervised Vision Transformers for Representation Learning},    author  = {Chunyuan Li and Jianwei Yang and Pengchuan Zhang and Mei Gao and Bin Xiao and Xiyang Dai and Lu Yuan and Jianfeng Gao},    journal = {ArXiv},    year    = {2021},    volume  = {abs/2106.09785}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">Li2021EfficientSV</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Efficient Self-supervised Vision Transformers for Representation Learning<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Chunyuan Li and Jianwei Yang and Pengchuan Zhang and Mei Gao and Bin Xiao and Xiyang Dai and Lu Yuan and Jianfeng Gao<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ArXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">volume</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>abs/2106.09785<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{Beyer2022BetterPlainViT    title     = {Better plain ViT baselines for ImageNet-1k},    author    = {Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},    publisher = {arXiv},    year      = {2022}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">Beyer2022BetterPlainViT</span>    <span class=\"pl-s\">title</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Better plain ViT baselines for ImageNet-1k<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">publisher</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>      = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{Arnab2021ViViTAV,    title   = {ViViT: A Video Vision Transformer},    author  = {Anurag Arnab and Mostafa Dehghani and Georg Heigold and Chen Sun and Mario Lucic and Cordelia Schmid},    journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},    year    = {2021},    pages   = {6816-6826}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">Arnab2021ViViTAV</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ViViT: A Video Vision Transformer<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Anurag Arnab and Mostafa Dehghani and Georg Heigold and Chen Sun and Mario Lucic and Cordelia Schmid<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021 IEEE/CVF International Conference on Computer Vision (ICCV)<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">pages</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>6816-6826<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{Liu2022PatchDropoutEV,    title   = {PatchDropout: Economizing Vision Transformers Using Patch Dropout},    author  = {Yue Liu and Christos Matsoukas and Fredrik Strand and Hossein Azizpour and Kevin Smith},    journal = {ArXiv},    year    = {2022},    volume  = {abs/2208.07220}}\"><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">Liu2022PatchDropoutEV</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>PatchDropout: Economizing Vision Transformers Using Patch Dropout<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Yue Liu and Christos Matsoukas and Fredrik Strand and Hossein Azizpour and Kevin Smith<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">journal</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>ArXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">volume</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>abs/2208.07220<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{https://doi.org/10.48550/arxiv.2302.01327,    doi     = {10.48550/ARXIV.2302.01327},    url     = {https://arxiv.org/abs/2302.01327},    author  = {Kumar, Manoj and Dehghani, Mostafa and Houlsby, Neil},    title   = {Dual PatchNorm},    publisher = {arXiv},    year    = {2023},    copyright = {Creative Commons Attribution 4.0 International}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">https://doi.org/10.48550/arxiv.2302.01327</span>,    <span class=\"pl-s\">doi</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>10.48550/ARXIV.2302.01327<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://arxiv.org/abs/2302.01327<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Kumar, Manoj and Dehghani, Mostafa and Houlsby, Neil<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Dual PatchNorm<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">publisher</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">copyright</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Creative Commons Attribution 4.0 International<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Dehghani2023PatchNP,    title   = {Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution},    author  = {Mostafa Dehghani and Basil Mustafa and Josip Djolonga and Jonathan Heek and Matthias Minderer and Mathilde Caron and Andreas Steiner and Joan Puigcerver and Robert Geirhos and Ibrahim M. Alabdulmohsin and Avital Oliver and Piotr Padlewski and Alexey A. Gritsenko and Mario Luvci'c and Neil Houlsby},    year    = {2023}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Dehghani2023PatchNP</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Mostafa Dehghani and Basil Mustafa and Josip Djolonga and Jonathan Heek and Matthias Minderer and Mathilde Caron and Andreas Steiner and Joan Puigcerver and Robert Geirhos and Ibrahim M. Alabdulmohsin and Avital Oliver and Piotr Padlewski and Alexey A. Gritsenko and Mario Luvci'c and Neil Houlsby<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@misc{vaswani2017attention,    title   = {Attention Is All You Need},    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},    year    = {2017},    eprint  = {1706.03762},    archivePrefix = {arXiv},    primaryClass = {cs.CL}}\"><pre><span class=\"pl-k\">@misc</span>{<span class=\"pl-en\">vaswani2017attention</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Attention Is All You Need<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2017<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">eprint</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>1706.03762<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">archivePrefix</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>arXiv<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">primaryClass</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>cs.CL<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{dao2022flashattention,    title   = {Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},    author  = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\\'e}, Christopher},    booktitle = {Advances in Neural Information Processing Systems},    year    = {2022}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">dao2022flashattention</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\\'e}, Christopher<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">booktitle</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Advances in Neural Information Processing Systems<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2022<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{Darcet2023VisionTN,    title   = {Vision Transformers Need Registers},    author  = {Timoth'ee Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski},    year    = {2023},    url     = {https://api.semanticscholar.org/CorpusID:263134283}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">Darcet2023VisionTN</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Vision Transformers Need Registers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Timoth'ee Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2023<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://api.semanticscholar.org/CorpusID:263134283<span class=\"pl-pds\">}</span></span>}</pre></div><div class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@inproceedings{ElNouby2021XCiTCI,    title   = {XCiT: Cross-Covariance Image Transformers},    author  = {Alaaeldin El-Nouby and Hugo Touvron and Mathilde Caron and Piotr Bojanowski and Matthijs Douze and Armand Joulin and Ivan Laptev and Natalia Neverova and Gabriel Synnaeve and Jakob Verbeek and Herv{\\'e} J{\\'e}gou},    booktitle = {Neural Information Processing Systems},    year    = {2021},    url     = {https://api.semanticscholar.org/CorpusID:235458262}}\"><pre><span class=\"pl-k\">@inproceedings</span>{<span class=\"pl-en\">ElNouby2021XCiTCI</span>,    <span class=\"pl-s\">title</span>   = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>XCiT: Cross-Covariance Image Transformers<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">author</span>  = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Alaaeldin El-Nouby and Hugo Touvron and Mathilde Caron and Piotr Bojanowski and Matthijs Douze and Armand Joulin and Ivan Laptev and Natalia Neverova and Gabriel Synnaeve and Jakob Verbeek and Herv{\\'e} J{\\'e}gou<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">booktitle</span> = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Neural Information Processing Systems<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">year</span>    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>,    <span class=\"pl-s\">url</span>     = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>https://api.semanticscholar.org/CorpusID:235458262<span class=\"pl-pds\">}</span></span>}</pre></div><p dir=\"auto\"><em>I visualise a time when we will be to robots what dogs are to humans, and I\u2019m rooting for the machines.</em> \u2014 Claude Shannon</p></article></div></div></div></div></div> <!-- --> <!-- --> <script type=\"application/json\" id=\"__PRIMER_DATA__\">{\"resolvedServerColorMode\":\"day\"}</script></div></react-partial>        <input type=\"hidden\" data-csrf=\"true\" value=\"YKgEglw96L2kbKJvoX9yuyV+MvzgoVrh1XfIFZiyClbsDEMTr46IMtShRhc2B+EuA0AY7Dk6SkMgu47VCCxM/g==\" /></div>  <div data-view-component=\"true\" class=\"Layout-sidebar\">            <div class=\"BorderGrid about-margin\" data-pjax>        <div class=\"BorderGrid-row\">          <div class=\"BorderGrid-cell\">            <div class=\"hide-sm hide-md\">  <h2 class=\"mb-3 h4\">About</h2>      <p class=\"f4 my-3\">        Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch      </p>    <h3 class=\"sr-only\">Topics</h3>    <div class=\"my-3\">        <div class=\"f6\">      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:computer-vision\" href=\"/topics/computer-vision\" title=\"Topic: computer-vision\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  computer-vision</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:transformers\" href=\"/topics/transformers\" title=\"Topic: transformers\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  transformers</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:artificial-intelligence\" href=\"/topics/artificial-intelligence\" title=\"Topic: artificial-intelligence\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  artificial-intelligence</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:image-classification\" href=\"/topics/image-classification\" title=\"Topic: image-classification\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  image-classification</a>      <a data-ga-click=\"Topic, repository page\" data-octo-click=\"topic_click\" data-octo-dimensions=\"topic:attention-mechanism\" href=\"/topics/attention-mechanism\" title=\"Topic: attention-mechanism\" data-view-component=\"true\" class=\"topic-tag topic-tag-link\">  attention-mechanism</a>  </div>    </div>    <h3 class=\"sr-only\">Resources</h3>    <div class=\"mt-2\">      <a class=\"Link--muted\" data-analytics-event=\"{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:readme&quot;}\" href=\"#readme-ov-file\">        <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-book mr-2\">    <path d=\"M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z\"></path></svg>        Readme</a>    </div>      <h3 class=\"sr-only\">License</h3>  <div class=\"mt-2\">    <a href=\"#MIT-1-ov-file\"      class=\"Link--muted\"            data-analytics-event=\"{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:license&quot;}\"    >      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-law mr-2\">    <path d=\"M8.75.75V2h.985c.304 0 .603.08.867.231l1.29.736c.038.022.08.033.124.033h2.234a.75.75 0 0 1 0 1.5h-.427l2.111 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.006.005-.01.01-.045.04c-.21.176-.441.327-.686.45C14.556 10.78 13.88 11 13 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L12.178 4.5h-.162c-.305 0-.604-.079-.868-.231l-1.29-.736a.245.245 0 0 0-.124-.033H8.75V13h2.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1 0-1.5h2.5V3.5h-.984a.245.245 0 0 0-.124.033l-1.289.737c-.265.15-.564.23-.869.23h-.162l2.112 4.692a.75.75 0 0 1-.154.838l-.53-.53.529.531-.001.002-.002.002-.006.006-.016.015-.045.04c-.21.176-.441.327-.686.45C4.556 10.78 3.88 11 3 11a4.498 4.498 0 0 1-2.023-.454 3.544 3.544 0 0 1-.686-.45l-.045-.04-.016-.015-.006-.006-.004-.004v-.001a.75.75 0 0 1-.154-.838L2.178 4.5H1.75a.75.75 0 0 1 0-1.5h2.234a.249.249 0 0 0 .125-.033l1.288-.737c.265-.15.564-.23.869-.23h.984V.75a.75.75 0 0 1 1.5 0Zm2.945 8.477c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L13 6.327Zm-10 0c.285.135.718.273 1.305.273s1.02-.138 1.305-.273L3 6.327Z\"></path></svg>     MIT license    </a>  </div>  <include-fragment  src=\"/lucidrains/vit-pytorch/hovercards/citation/sidebar_partial?tree_name=main\">  </include-fragment>  <div class=\"mt-2\">    <a href=\"/lucidrains/vit-pytorch/activity\" data-view-component=\"true\" class=\"Link Link--muted\">      <svg text=\"gray\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-pulse mr-2\">    <path d=\"M6 2c.306 0 .582.187.696.471L10 10.731l1.304-3.26A.751.751 0 0 1 12 7h3.25a.75.75 0 0 1 0 1.5h-2.742l-1.812 4.528a.751.751 0 0 1-1.392 0L6 4.77 4.696 8.03A.75.75 0 0 1 4 8.5H.75a.75.75 0 0 1 0-1.5h2.742l1.812-4.529A.751.751 0 0 1 6 2Z\"></path></svg>      <span class=\"color-fg-muted\">Activity</span></a>  </div>  <h3 class=\"sr-only\">Stars</h3>  <div class=\"mt-2\">    <a href=\"/lucidrains/vit-pytorch/stargazers\" data-view-component=\"true\" class=\"Link Link--muted\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-star mr-2\">    <path d=\"M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z\"></path></svg>      <strong>17.3k</strong>      stars</a>  </div>  <h3 class=\"sr-only\">Watchers</h3>  <div class=\"mt-2\">    <a href=\"/lucidrains/vit-pytorch/watchers\" data-view-component=\"true\" class=\"Link Link--muted\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-eye mr-2\">    <path d=\"M8 2c1.981 0 3.671.992 4.933 2.078 1.27 1.091 2.187 2.345 2.637 3.023a1.62 1.62 0 0 1 0 1.798c-.45.678-1.367 1.932-2.637 3.023C11.67 13.008 9.981 14 8 14c-1.981 0-3.671-.992-4.933-2.078C1.797 10.83.88 9.576.43 8.898a1.62 1.62 0 0 1 0-1.798c.45-.677 1.367-1.931 2.637-3.022C4.33 2.992 6.019 2 8 2ZM1.679 7.932a.12.12 0 0 0 0 .136c.411.622 1.241 1.75 2.366 2.717C5.176 11.758 6.527 12.5 8 12.5c1.473 0 2.825-.742 3.955-1.715 1.124-.967 1.954-2.096 2.366-2.717a.12.12 0 0 0 0-.136c-.412-.621-1.242-1.75-2.366-2.717C10.824 4.242 9.473 3.5 8 3.5c-1.473 0-2.825.742-3.955 1.715-1.124.967-1.954 2.096-2.366 2.717ZM8 10a2 2 0 1 1-.001-3.999A2 2 0 0 1 8 10Z\"></path></svg>      <strong>141</strong>      watching</a>  </div>  <h3 class=\"sr-only\">Forks</h3>  <div class=\"mt-2\">    <a href=\"/lucidrains/vit-pytorch/forks\" data-view-component=\"true\" class=\"Link Link--muted\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-repo-forked mr-2\">    <path d=\"M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z\"></path></svg>      <strong>2.7k</strong>      forks</a>  </div>    <div class=\"mt-2\">      <a class=\"Link--muted\" href=\"/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Flucidrains%2Fvit-pytorch&amp;report=lucidrains+%28user%29\">          Report repository</a>    </div></div>          </div>        </div>                    <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\">  <a href=\"/lucidrains/vit-pytorch/releases\" data-view-component=\"true\" class=\"Link--primary no-underline Link\">    Releases      <span title=\"185\" data-view-component=\"true\" class=\"Counter\">185</span></a></h2>  <a class=\"Link--primary d-flex no-underline\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" href=\"/lucidrains/vit-pytorch/releases/tag/1.6.5\">    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-tag flex-shrink-0 mt-1 color-fg-success\">    <path d=\"M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z\"></path></svg>    <div class=\"ml-2 min-width-0\">      <div class=\"d-flex\">        <span class=\"css-truncate css-truncate-target text-bold mr-2\" style=\"max-width: none;\">1.6.5</span>        <span title=\"Label: Latest\" data-view-component=\"true\" class=\"Label Label--success flex-shrink-0\">          Latest</span>      </div>      <div class=\"text-small color-fg-muted\"><relative-time datetime=\"2023-12-23T16:11:47Z\" class=\"no-wrap\">Dec 23, 2023</relative-time></div>    </div></a>    <div data-view-component=\"true\" class=\"mt-3\">      <a text=\"small\" data-pjax=\"#repo-content-pjax-container\" data-turbo-frame=\"repo-content-turbo-frame\" href=\"/lucidrains/vit-pytorch/releases\" data-view-component=\"true\" class=\"Link\">        + 184 releases</a></div>              </div>            </div>                    <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\">Sponsor this project</h2><include-fragment src=\"/lucidrains/vit-pytorch/sponsors_list?block_button=false&amp;current_repository=vit-pytorch\" aria-busy=\"true\" aria-label=\"Loading sponsorable links\">        <ul class=\"list-style-none\">        </ul></include-fragment>              </div>            </div>                    <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\">  <a href=\"/users/lucidrains/packages?repo_name=vit-pytorch\" data-view-component=\"true\" class=\"Link--primary no-underline Link d-flex flex-items-center\">    Packages      <span title=\"0\" hidden=\"hidden\" data-view-component=\"true\" class=\"Counter ml-1\">0</span></a></h2>      <div class=\"text-small color-fg-muted\">        No packages published <br>      </div>              </div>            </div>                    <div class=\"BorderGrid-row\" >              <div class=\"BorderGrid-cell\">                  <h2 class=\"h4 mb-3\">    <a href=\"/lucidrains/vit-pytorch/network/dependents\" data-view-component=\"true\" class=\"Link--primary no-underline Link\">      Used by <span title=\"402\" data-view-component=\"true\" class=\"Counter\">402</span></a>  </h2>  <a class=\"d-flex flex-items-center\" href=\"/lucidrains/vit-pytorch/network/dependents\">    <ul class=\"hx_flex-avatar-stack list-style-none min-width-0\">          <li class=\"hx_flex-avatar-stack-item\">            <img class=\"avatar avatar-user\" src=\"https://avatars.githubusercontent.com/u/149308916?s=64&amp;v=4\" width=\"32\" height=\"32\" alt=\"@hserocks\" />          </li>          <li class=\"hx_flex-avatar-stack-item\">            <img class=\"avatar avatar-user\" src=\"https://avatars.githubusercontent.com/u/80376609?s=64&amp;v=4\" width=\"32\" height=\"32\" alt=\"@koya75\" />          </li>          <li class=\"hx_flex-avatar-stack-item\">            <img class=\"avatar avatar-user\" src=\"https://avatars.githubusercontent.com/u/70981410?s=64&amp;v=4\" width=\"32\" height=\"32\" alt=\"@behradbeyglo\" />          </li>          <li class=\"hx_flex-avatar-stack-item\">            <img class=\"avatar\" src=\"https://avatars.githubusercontent.com/u/119270446?s=64&amp;v=4\" width=\"32\" height=\"32\" alt=\"@KDEGroup\" />          </li>          <li class=\"hx_flex-avatar-stack-item\">            <img class=\"avatar avatar-user\" src=\"https://avatars.githubusercontent.com/u/71670255?s=64&amp;v=4\" width=\"32\" height=\"32\" alt=\"@tmvllrrl\" />          </li>          <li class=\"hx_flex-avatar-stack-item\">            <img class=\"avatar\" src=\"https://avatars.githubusercontent.com/u/57018659?s=64&amp;v=4\" width=\"32\" height=\"32\" alt=\"@buildingamind\" />          </li>          <li class=\"hx_flex-avatar-stack-item\">            <img class=\"avatar avatar-user\" src=\"https://avatars.githubusercontent.com/u/71554118?s=64&amp;v=4\" width=\"32\" height=\"32\" alt=\"@marco-peer\" />          </li>          <li class=\"hx_flex-avatar-stack-item\">            <img class=\"avatar avatar-user\" src=\"https://avatars.githubusercontent.com/u/70945480?s=64&amp;v=4\" width=\"32\" height=\"32\" alt=\"@Cryptology-Algorithm-Lab\" />          </li>    </ul>      <span class=\"px-2 text-bold text-small no-wrap\">        + 394      </span>  </a>              </div>            </div>                    <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\">  <a href=\"/lucidrains/vit-pytorch/graphs/contributors\" data-view-component=\"true\" class=\"Link--primary no-underline Link d-flex flex-items-center\">    Contributors      <span title=\"20\" data-view-component=\"true\" class=\"Counter ml-1\">20</span></a></h2>      <ul class=\"list-style-none d-flex flex-wrap mb-n2\">    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/lucidrains\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/lucidrains/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/108653?s=64&amp;v=4\" alt=\"@lucidrains\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/zankner\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/zankner/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/39166683?s=64&amp;v=4\" alt=\"@zankner\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/developer0hye\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/developer0hye/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/35001605?s=64&amp;v=4\" alt=\"@developer0hye\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/murufeng\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/murufeng/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/26592269?s=64&amp;v=4\" alt=\"@murufeng\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/stevenwalton\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/stevenwalton/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/4933582?s=64&amp;v=4\" alt=\"@stevenwalton\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/Vishu26\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/Vishu26/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/24605821?s=64&amp;v=4\" alt=\"@Vishu26\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/minhlong94\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/minhlong94/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/55311435?s=64&amp;v=4\" alt=\"@minhlong94\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/alihassanijr\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/alihassanijr/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/68103095?s=64&amp;v=4\" alt=\"@alihassanijr\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/ryanrussell\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/ryanrussell/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/523300?s=64&amp;v=4\" alt=\"@ryanrussell\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/EIFY\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/EIFY/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/2584418?s=64&amp;v=4\" alt=\"@EIFY\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/loctruong96\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/loctruong96/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/8007433?s=64&amp;v=4\" alt=\"@loctruong96\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/umbertov\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/umbertov/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/12353864?s=64&amp;v=4\" alt=\"@umbertov\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/adimyth\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/adimyth/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/26377913?s=64&amp;v=4\" alt=\"@adimyth\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li>    <li class=\"mb-2 mr-2\"        >      <a href=\"https://github.com/shabie\"          class=\"\"            data-hovercard-type=\"user\" data-hovercard-url=\"/users/shabie/hovercard\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\"                  >        <img src=\"https://avatars.githubusercontent.com/u/30535146?s=64&amp;v=4\" alt=\"@shabie\" size=\"32\" height=\"32\" width=\"32\" data-view-component=\"true\" class=\"avatar circle\" />      </a>    </li></ul>  <div data-view-component=\"true\" class=\"mt-3\">    <a text=\"small\" href=\"/lucidrains/vit-pytorch/graphs/contributors\" data-view-component=\"true\" class=\"Link\">      + 6 contributors</a></div>              </div>            </div>                            <div class=\"BorderGrid-row\">              <div class=\"BorderGrid-cell\">                <h2 class=\"h4 mb-3\">Languages</h2><div class=\"mb-2\">  <span data-view-component=\"true\" class=\"Progress\">    <span style=\"background-color:#3572A5 !important;;width: 100.0%;\" itemprop=\"keywords\" aria-label=\"Python 100.0\" data-view-component=\"true\" class=\"Progress-item color-bg-success-emphasis\"></span></span></div><ul class=\"list-style-none\">    <li class=\"d-inline\">        <a class=\"d-inline-flex flex-items-center flex-nowrap Link--secondary no-underline text-small mr-3\" href=\"/lucidrains/vit-pytorch/search?l=python\"  data-ga-click=\"Repository, language stats search click, location:repo overview\">          <svg style=\"color:#3572A5;\" aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-dot-fill mr-2\">    <path d=\"M8 4a4 4 0 1 1 0 8 4 4 0 0 1 0-8Z\"></path></svg>          <span class=\"color-fg-default text-bold mr-1\">Python</span>          <span>100.0%</span>        </a>    </li></ul>              </div>            </div>              </div></div>  </div></div>  </div>  </div></turbo-frame>    </main>  </div>  </div>          <footer class=\"footer pt-8 pb-6 f6 color-fg-muted p-responsive\" role=\"contentinfo\" >  <h2 class='sr-only'>Footer</h2>    <div class=\"d-flex flex-justify-center flex-items-center flex-column-reverse flex-lg-row flex-wrap flex-lg-nowrap\">    <div class=\"d-flex flex-items-center flex-shrink-0 mx-2\">      <a aria-label=\"Homepage\" title=\"GitHub\" class=\"footer-octicon mr-2\" href=\"https://github.com\">        <svg aria-hidden=\"true\" height=\"24\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"24\" data-view-component=\"true\" class=\"octicon octicon-mark-github\">    <path d=\"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z\"></path></svg></a>      <span>        &copy; 2024 GitHub,&nbsp;Inc.      </span>    </div>    <nav aria-label=\"Footer\">      <h3 class=\"sr-only\" id=\"sr-footer-heading\">Footer navigation</h3>      <ul class=\"list-style-none d-flex flex-justify-center flex-wrap mb-2 mb-lg-0\" aria-labelledby=\"sr-footer-heading\">          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to Terms&quot;,&quot;label&quot;:&quot;text:terms&quot;}\" href=\"https://docs.github.com/site-policy/github-terms/github-terms-of-service\" data-view-component=\"true\" class=\"Link--secondary Link\">Terms</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to privacy&quot;,&quot;label&quot;:&quot;text:privacy&quot;}\" href=\"https://docs.github.com/site-policy/privacy-policies/github-privacy-statement\" data-view-component=\"true\" class=\"Link--secondary Link\">Privacy</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to security&quot;,&quot;label&quot;:&quot;text:security&quot;}\" href=\"/security\" data-view-component=\"true\" class=\"Link--secondary Link\">Security</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to status&quot;,&quot;label&quot;:&quot;text:status&quot;}\" href=\"https://www.githubstatus.com/\" data-view-component=\"true\" class=\"Link--secondary Link\">Status</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to docs&quot;,&quot;label&quot;:&quot;text:docs&quot;}\" href=\"https://docs.github.com\" data-view-component=\"true\" class=\"Link--secondary Link\">Docs</a>          </li>          <li class=\"mx-2\">            <a data-analytics-event=\"{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to contact&quot;,&quot;label&quot;:&quot;text:contact&quot;}\" href=\"https://support.github.com?tags=dotcom-footer\" data-view-component=\"true\" class=\"Link--secondary Link\">Contact</a>          </li>          <li class=\"mx-2\" >  <cookie-consent-link>    <button type=\"button\" class=\"Link--secondary underline-on-hover border-0 p-0 color-bg-transparent\" data-action=\"click:cookie-consent-link#showConsentManagement\">      Manage cookies    </button>  </cookie-consent-link></li><li class=\"mx-2\">  <cookie-consent-link>    <button type=\"button\" class=\"Link--secondary underline-on-hover border-0 p-0 color-bg-transparent\" data-action=\"click:cookie-consent-link#showConsentManagement\">      Do not share my personal information    </button>  </cookie-consent-link></li>      </ul>    </nav>  </div></footer>    <cookie-consent id=\"cookie-consent-banner\" class=\"position-fixed bottom-0 left-0\" style=\"z-index: 999999\" data-initial-cookie-consent-allowed=\"\" data-cookie-consent-required=\"false\"></cookie-consent>  <div id=\"ajax-error-message\" class=\"ajax-error-message flash flash-error\" hidden>    <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-alert\">    <path d=\"M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\"></path></svg>    <button type=\"button\" class=\"flash-close js-ajax-error-dismiss\" aria-label=\"Dismiss error\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg>    </button>    You can\u2019t perform that action at this time.  </div>    <template id=\"site-details-dialog\">  <details class=\"details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm\" open>    <summary role=\"button\" aria-label=\"Close dialog\"></summary>    <details-dialog class=\"Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal\">      <button class=\"Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0\" type=\"button\" aria-label=\"Close dialog\" data-close-dialog>        <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-x\">    <path d=\"M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z\"></path></svg>      </button>      <div class=\"octocat-spinner my-6 js-details-dialog-spinner\"></div>    </details-dialog>  </details></template>    <div class=\"Popover js-hovercard-content position-absolute\" style=\"display: none; outline: none;\" tabindex=\"0\">  <div class=\"Popover-message Popover-message--bottom-left Popover-message--large Box color-shadow-large\" style=\"width:360px;\">  </div></div>    <template id=\"snippet-clipboard-copy-button\">  <div class=\"zeroclipboard-container position-absolute right-0 top-0\">    <clipboard-copy aria-label=\"Copy\" class=\"ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay\" data-copy-feedback=\"Copied!\" data-tooltip-direction=\"w\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copy js-clipboard-copy-icon m-2\">    <path d=\"M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z\"></path><path d=\"M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path></svg>      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2\">    <path d=\"M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z\"></path></svg>    </clipboard-copy>  </div></template><template id=\"snippet-clipboard-copy-button-unpositioned\">  <div class=\"zeroclipboard-container\">    <clipboard-copy aria-label=\"Copy\" class=\"ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center\" data-copy-feedback=\"Copied!\" data-tooltip-direction=\"w\">      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-copy js-clipboard-copy-icon\">    <path d=\"M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z\"></path><path d=\"M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z\"></path></svg>      <svg aria-hidden=\"true\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" data-view-component=\"true\" class=\"octicon octicon-check js-clipboard-check-icon color-fg-success d-none\">    <path d=\"M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z\"></path></svg>    </clipboard-copy>  </div></template>    </div>    <div id=\"js-global-screen-reader-notice\" class=\"sr-only\" aria-live=\"polite\" aria-atomic=\"true\" ></div>    <div id=\"js-global-screen-reader-notice-assertive\" class=\"sr-only\" aria-live=\"assertive\" aria-atomic=\"true\"></div>  </body></html>",
  "embeddings": []
}