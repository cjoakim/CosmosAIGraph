{
  "libtype": "pypi",
  "libname": "torch-tb-profiler",
  "url": "https://github.com/pytorch/kineto/tree/main/tb_plugin",
  "html": "{\"payload\":{\"allShortcutsEnabled\":false,\"path\":\"tb_plugin\",\"repo\":{\"id\":280276446,\"defaultBranch\":\"main\",\"name\":\"kineto\",\"ownerLogin\":\"pytorch\",\"currentUserCanPush\":false,\"isFork\":false,\"isEmpty\":false,\"createdAt\":\"2020-07-16T23:03:00.000Z\",\"ownerAvatar\":\"https://avatars.githubusercontent.com/u/21003710?v=4\",\"public\":true,\"private\":false,\"isOrgOwned\":true},\"currentUser\":null,\"refInfo\":{\"name\":\"main\",\"listCacheKey\":\"v0:1699402980.0\",\"canEdit\":false,\"refType\":\"branch\",\"currentOid\":\"eeafa44b312c0a06fe1f343e62c3f2ccdc1c2fd0\"},\"tree\":{\"items\":[{\"name\":\"ci_scripts\",\"path\":\"tb_plugin/ci_scripts\",\"contentType\":\"directory\"},{\"name\":\"docs\",\"path\":\"tb_plugin/docs\",\"contentType\":\"directory\"},{\"name\":\"examples\",\"path\":\"tb_plugin/examples\",\"contentType\":\"directory\"},{\"name\":\"fe\",\"path\":\"tb_plugin/fe\",\"contentType\":\"directory\"},{\"name\":\"packaging/torch_tb_profiler\",\"path\":\"tb_plugin/packaging/torch_tb_profiler\",\"contentType\":\"directory\",\"hasSimplifiedPath\":true},{\"name\":\"samples\",\"path\":\"tb_plugin/samples\",\"contentType\":\"directory\"},{\"name\":\"test\",\"path\":\"tb_plugin/test\",\"contentType\":\"directory\"},{\"name\":\"torch_tb_profiler\",\"path\":\"tb_plugin/torch_tb_profiler\",\"contentType\":\"directory\"},{\"name\":\".flake8\",\"path\":\"tb_plugin/.flake8\",\"contentType\":\"file\"},{\"name\":\".gitignore\",\"path\":\"tb_plugin/.gitignore\",\"contentType\":\"file\"},{\"name\":\".pre-commit-config.yaml\",\"path\":\"tb_plugin/.pre-commit-config.yaml\",\"contentType\":\"file\"},{\"name\":\"LICENSE\",\"path\":\"tb_plugin/LICENSE\",\"contentType\":\"file\"},{\"name\":\"README.md\",\"path\":\"tb_plugin/README.md\",\"contentType\":\"file\"},{\"name\":\"setup.py\",\"path\":\"tb_plugin/setup.py\",\"contentType\":\"file\"}],\"templateDirectorySuggestionUrl\":null,\"readme\":{\"displayName\":\"README.md\",\"richText\":\"<article class=\\\"markdown-body entry-content container-lg\\\" itemprop=\\\"text\\\"><div class=\\\"markdown-heading\\\" dir=\\\"auto\\\"><h1 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\">PyTorch Profiler TensorBoard Plugin</h1><a id=\\\"user-content-pytorch-profiler-tensorboard-plugin\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: PyTorch Profiler TensorBoard Plugin\\\" href=\\\"#pytorch-profiler-tensorboard-plugin\\\"><svg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"><path d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"></path></svg></a></div>\\n<p dir=\\\"auto\\\">This is a TensorBoard Plugin that provides visualization of PyTorch profiling.\\nIt can parse, process and visualize the PyTorch Profiler's dumped profiling result,\\nand give optimization recommendations.</p>\\n<div class=\\\"markdown-heading\\\" dir=\\\"auto\\\"><h3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\">Quick Installation Instructions</h3><a id=\\\"user-content-quick-installation-instructions\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Quick Installation Instructions\\\" href=\\\"#quick-installation-instructions\\\"><svg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"><path d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"></path></svg></a></div>\\n<ul dir=\\\"auto\\\">\\n<li>\\n<p dir=\\\"auto\\\">Install from pypi</p>\\n<p dir=\\\"auto\\\"><code>pip install torch-tb-profiler</code></p>\\n<p dir=\\\"auto\\\">To install with S3 / AzureBlob / GCS / HDFS extension, <code>pip install torch-tb-profiler[s3/blob/gs/hdfs]</code>, for example <code>pip install torch-tb-profiler[s3]</code></p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Or you can install from source</p>\\n<p dir=\\\"auto\\\">Clone the git repository:</p>\\n<p dir=\\\"auto\\\"><code>git clone https://github.com/pytorch/kineto.git</code></p>\\n<p dir=\\\"auto\\\">Navigate to the <code>kineto/tb_plugin</code> directory.</p>\\n<p dir=\\\"auto\\\">Install with command:</p>\\n<p dir=\\\"auto\\\"><code>pip install .</code></p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Build the wheel</p>\\n<ul dir=\\\"auto\\\">\\n<li><code>python setup.py build_fe sdist bdist_wheel</code> <br>\\n <strong><em>Note</em></strong>: the build_fe step need setup yarn and Node.js</li>\\n<li><code>python setup.py sdist bdist_wheel</code></li>\\n</ul>\\n</li>\\n</ul>\\n<div class=\\\"markdown-heading\\\" dir=\\\"auto\\\"><h3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\">Quick Start Instructions</h3><a id=\\\"user-content-quick-start-instructions\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Quick Start Instructions\\\" href=\\\"#quick-start-instructions\\\"><svg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"><path d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"></path></svg></a></div>\\n<ul dir=\\\"auto\\\">\\n<li>\\n<p dir=\\\"auto\\\">Prepare profiling data</p>\\n<p dir=\\\"auto\\\">We have prepared some sample profiling data at <a href=\\\"/pytorch/kineto/blob/main/tb_plugin/samples\\\">kineto/tb_plugin/samples</a>\\nYou can download it directly.\\nOr you can generate these profiling samples yourself by running\\n<a href=\\\"/pytorch/kineto/blob/main/tb_plugin/examples/resnet50_profiler_api.py\\\">kineto/tb_plugin/examples/resnet50_profiler_api.py</a>.\\nAlso you can learn how to profile your model and generate profiling data from <a href=\\\"https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?highlight=tensorboard\\\" rel=\\\"nofollow\\\">PyTorch Profiler</a>.</p>\\n<p dir=\\\"auto\\\">Note: The recommended way to produce profiling data is assigning <code>torch.profiler.tensorboard_trace_handler</code>\\nto <code>on_trace_ready</code> on creation of <code>torch.profiler.profile</code>.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Start TensorBoard</p>\\n<p dir=\\\"auto\\\">Specify the profiling data folder to <code>logdir</code> in TensorBoard. If you use the above samples data, start TensorBoard with:</p>\\n<p dir=\\\"auto\\\"><code>tensorboard --logdir=./samples</code></p>\\n<p dir=\\\"auto\\\">If your web browser is not in the same machine that you start TensorBoard,\\nyou can add <code>--bind_all</code> option, such as:</p>\\n<p dir=\\\"auto\\\"><code>tensorboard --logdir=./samples --bind_all</code></p>\\n<p dir=\\\"auto\\\">Note: Make sure the default port 6006 is open to the browser's host.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Open TensorBoard in Chrome browser</p>\\n<p dir=\\\"auto\\\">Open URL <code>http://localhost:6006</code> in the browser.\\nIf you use <code>--bind_all</code> in tensorboard start command, the hostname may not be 'localhost'. You may find it in the log printed after the cmd.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Navigate to the PYTORCH_PROFILER tab</p>\\n<p dir=\\\"auto\\\">If the files under <code>--logdir</code> are too big or too many,\\nplease wait a while and refresh the browser to check latest loaded result.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Loading profiling data from the cloud</p>\\n<ul dir=\\\"auto\\\">\\n<li>\\n<p dir=\\\"auto\\\">AWS S3 (S3://)</p>\\n<p dir=\\\"auto\\\">Install <code>boto3</code>. Set environment variables:  <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>. Optionally, <code>S3_ENDPOINT</code> can be set as well.<br>\\nFor minio, the S3 url should start with the bucket name <code>s3://&lt;bucket&gt;/&lt;folder&gt;/</code> instead of minio prefix <code>s3://minio/&lt;bucket&gt;/&lt;folder&gt;</code>. At the same time, the <code>S3_ENDPOINT</code> is needed as well. <br>\\nFollow these guides to get set-up with minio:</p>\\n<ul dir=\\\"auto\\\">\\n<li>Server: <a href=\\\"https://docs.min.io/docs/minio-quickstart-guide.html\\\" rel=\\\"nofollow\\\">https://docs.min.io/docs/minio-quickstart-guide.html</a></li>\\n<li>MC Client: <a href=\\\"https://docs.min.io/docs/minio-client-quickstart-guide.html\\\" rel=\\\"nofollow\\\">https://docs.min.io/docs/minio-client-quickstart-guide.html</a></li>\\n</ul>\\n<p dir=\\\"auto\\\">For example, the following commands can be used to create minio storage:</p>\\n</li>\\n</ul>\\n<div class=\\\"highlight highlight-source-shell notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"   ./mc alias set s3  http://10.150.148.189:9000 minioadmin  minioadmin\\n   ./mc mb s3/profiler --region=us-east-1\\n   ./mc cp ~/notebook/version_2 s3/profiler/ --recursive\\n   export AWS_ACCESS_KEY_ID=minioadmin\\n   export AWS_SECRET_ACCESS_KEY=minioadmin\\n   export AWS_REGION=us-east-1\\n   export S3_USE_HTTPS=0\\n   export S3_VERIFY_SSL=0\\n   export S3_ENDPOINT=http://localhost:9000\\n   tensorboard --logdir=s3://profiler/version_2/ --bind_all\\\"><pre>   ./mc <span class=\\\"pl-c1\\\">alias</span> <span class=\\\"pl-c1\\\">set</span> s3  http://10.150.148.189:9000 minioadmin  minioadmin\\n   ./mc mb s3/profiler --region=us-east-1\\n   ./mc cp <span class=\\\"pl-k\\\">~</span>/notebook/version_2 s3/profiler/ --recursive\\n   <span class=\\\"pl-k\\\">export</span> AWS_ACCESS_KEY_ID=minioadmin\\n   <span class=\\\"pl-k\\\">export</span> AWS_SECRET_ACCESS_KEY=minioadmin\\n   <span class=\\\"pl-k\\\">export</span> AWS_REGION=us-east-1\\n   <span class=\\\"pl-k\\\">export</span> S3_USE_HTTPS=0\\n   <span class=\\\"pl-k\\\">export</span> S3_VERIFY_SSL=0\\n   <span class=\\\"pl-k\\\">export</span> S3_ENDPOINT=http://localhost:9000\\n   tensorboard --logdir=s3://profiler/version_2/ --bind_all</pre></div>\\n<ul dir=\\\"auto\\\">\\n<li>\\n<p dir=\\\"auto\\\">Azure Blob (https://&lt;account&gt;.blob.core.windows.net)</p>\\n<p dir=\\\"auto\\\">Install <code>azure-storage-blob</code>. Optionally, set environment variable <code>AZURE_STORAGE_CONNECTION_STRING</code>.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Google Cloud (GS://)</p>\\n<p dir=\\\"auto\\\">Install <code>google-cloud-storage</code>.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">HDFS (hdfs://)</p>\\n<p dir=\\\"auto\\\">Install <code>fsspec</code> and <code>pyarrow</code>. Optionally set environment variable <code>HADOOP_HOME</code>.</p>\\n</li>\\n</ul>\\n<hr>\\n<blockquote>\\n<p dir=\\\"auto\\\"><strong><em>NOTES:</em></strong> For AWS S3, Google Cloud and Azure Blob, the trace files need to be put on a top level folder under bucket/container.</p>\\n</blockquote>\\n<hr>\\n<p dir=\\\"auto\\\">We prepared some sample data in blob, you can also access it using the command</p>\\n<div class=\\\"snippet-clipboard-content notranslate position-relative overflow-auto\\\" data-snippet-clipboard-copy-content=\\\"tensorboard --logdir=https://torchtbprofiler.blob.core.windows.net/torchtbprofiler/demo/ --bind_all\\\"><pre class=\\\"notranslate\\\"><code>tensorboard --logdir=https://torchtbprofiler.blob.core.windows.net/torchtbprofiler/demo/ --bind_all\\n</code></pre></div>\\n<p dir=\\\"auto\\\">and open TensorBoard your browser to see all the views described below.</p>\\n<p dir=\\\"auto\\\">Note: for accessing data in Azure Blob, you need to install torch-tb-profiler with <code>pip install torch-tb-profiler[blob]</code></p>\\n</li>\\n</ul>\\n<div class=\\\"markdown-heading\\\" dir=\\\"auto\\\"><h3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\">Quick Usage Instructions</h3><a id=\\\"user-content-quick-usage-instructions\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: Quick Usage Instructions\\\" href=\\\"#quick-usage-instructions\\\"><svg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"><path d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"></path></svg></a></div>\\n<p dir=\\\"auto\\\">We regard each running with profiler enabled as a \\\"run\\\".\\nIn most cases a run is a single process. If DDP is enabled, then a run includes multiple processes.\\nWe name each process a \\\"worker\\\".</p>\\n<p dir=\\\"auto\\\">Each run corresponds to a sub-folder under the folder specified by \\\"--logdir\\\".\\nEach sub-folder contains one or more chrome trace files, one for each process.\\nThe kineto/tb_plugin/samples is an example of how the files are organized.</p>\\n<p dir=\\\"auto\\\">You can select the run and worker on the left control panel.</p>\\n<p dir=\\\"auto\\\"><a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/control_panel.PNG\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/control_panel.PNG\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a></p>\\n<p dir=\\\"auto\\\">Runs: Select a run. Each run is one execution of a PyTorch application with profiling enabled.</p>\\n<p dir=\\\"auto\\\">Views: We organize the profiling result into multiple views,\\nfrom coarse-grained (overview-level) to fine-grained (kernel-level).</p>\\n<p dir=\\\"auto\\\">Workers: Select a worker. Each worker is a process. There could be multiple workers when DDP is used.</p>\\n<p dir=\\\"auto\\\">Span: There may be multiple profiling trace files of different spans to be generated when using <a href=\\\"https://github.com/pytorch/pytorch/blob/master/torch/profiler/profiler.py#L24\\\">torch.profiler.schedule</a> as schedule of torch.profiler.\\nYou can select them with this selection box.</p>\\n<p dir=\\\"auto\\\">Currently we have the following performance diagnosis views:</p>\\n<ul dir=\\\"auto\\\">\\n<li>Overall View</li>\\n<li>Operator View</li>\\n<li>Kernel View</li>\\n<li>Trace View</li>\\n<li>Memory View</li>\\n<li>Distributed View</li>\\n</ul>\\n<p dir=\\\"auto\\\">We describe each of these views below.</p>\\n<ul dir=\\\"auto\\\">\\n<li>\\n<p dir=\\\"auto\\\">Overall View</p>\\n<p dir=\\\"auto\\\">The overall view is a top level view of the process in your profiling run.\\nIt shows an overview of time cost, including both host and GPU devices.\\nYou can select the current worker in the left panel's \\\"Workers\\\" dropdown menu.</p>\\n<p dir=\\\"auto\\\">An example of overall view:\\n<a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/overall_view.PNG\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/overall_view.PNG\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a></p>\\n<p dir=\\\"auto\\\">The 'GPU Summary' panel shows GPU information and usage metrics of this run, include name, global memory, compute capability of this GPU.\\nThe 'GPU Utilization', 'Est. SM Efficiency' and 'Est. Achieved Occupancy' shows GPU usage efficiency of this run at different levels.\\nThe 'Kernel Time using Tensor Cores' shows percent of the time Tensor Core kernels are active.\\nThe detailed information about the above four metrics can be found at <a href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/gpu_utilization.md\\\">gpu_utilization</a>.</p>\\n<p dir=\\\"auto\\\">The 'Step Time Breakdown' panel shows the performance summary. We regard each iteration (usually a mini-batch) as a step.\\nThe time spent on each step is broken down into multiple categories as follows:</p>\\n<ol dir=\\\"auto\\\">\\n<li>\\n<p dir=\\\"auto\\\">Kernel: Kernels execution time on GPU device;</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Memcpy: GPU involved memory copy time (either D2D, D2H or H2D);</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Memset: GPU involved memory set time;</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Communication: Communication time only appear in DDP case;</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Runtime: CUDA runtime execution time on host side;\\nSuch as cudaLaunchKernel, cudaMemcpyAsync, cudaStreamSynchronize, ...</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">DataLoader: The data loading time spent in PyTorch DataLoader object;</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">CPU Exec: Host compute time, including every PyTorch operator running time;</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Other: The time not included in any of the above.</p>\\n</li>\\n</ol>\\n<p dir=\\\"auto\\\">Note: The summary of all the above categories is end-to-end wall-clock time.</p>\\n<p dir=\\\"auto\\\">The above list is ranked by priority from high to low. We count time in priority order.\\nThe time cost with highest priority category(Kernel) is counted first,\\nthen Memcpy, then Memset, ...,  and Other is counted last.\\nIn the following example, the \\\"Kernel\\\" is counted first as 7-2=5 seconds;\\nThen the \\\"Memcpy\\\" is counted as 0 seconds, because it is fully hidden by \\\"Kernel\\\";\\nThen \\\"CPU Exec\\\" is counted as 2-1=1 seconds, because the [2,3] interval is hidden by \\\"Kernel\\\", only [1,2] interval is counted.</p>\\n<p dir=\\\"auto\\\">In this way, summarization of all the 7 categories' counted time in a step\\nwill be the same with this step's total wall clock time.</p>\\n<p dir=\\\"auto\\\"><a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/time_breakdown_priority.PNG\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/time_breakdown_priority.PNG\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a></p>\\n<p dir=\\\"auto\\\">Performance Recommendation: Leverage the profiling result to automatically highlight likely bottlenecks,\\nand give users actionable optimization suggestions.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Operator View</p>\\n<p dir=\\\"auto\\\">This view displays the performance of every PyTorch operator that is executed either on the host or device.</p>\\n<p dir=\\\"auto\\\"><a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/operator_view.PNG\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/operator_view.PNG\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a>\\nEach table row is a PyTorch operator, which is a computation operator implemented by C++,\\nsuch as \\\"aten::relu_\\\", \\\"aten::convolution\\\".</p>\\n<p dir=\\\"auto\\\">Calls: How many times the operator is called in this run.</p>\\n<p dir=\\\"auto\\\">Device Self Duration: The accumulated time spent on GPU, not including this operator\u2019s child operators.</p>\\n<p dir=\\\"auto\\\">Device Total Duration: The accumulated time spent on GPU, including this operator\u2019s child operators.</p>\\n<p dir=\\\"auto\\\">Host Self Duration: The accumulated time spent on Host, not including this operator\u2019s child operators.</p>\\n<p dir=\\\"auto\\\">Host Total Duration: The accumulated time spent on Host, including this operator\u2019s child operators.</p>\\n<p dir=\\\"auto\\\">Tensor Cores Eligible: Whether this operator is eligible to use Tensor Cores.</p>\\n<p dir=\\\"auto\\\">Tensor Cores Self (%): Time of self-kernels with Tensor Cores / Time of self-kernels.\\nSelf-kernels don't include kernels launched by this operator\u2019s child operators.</p>\\n<p dir=\\\"auto\\\">Tensor Cores Total (%): Time of kernels with Tensor Cores / Time of kernels.</p>\\n<p dir=\\\"auto\\\">CallStack: All call stacks of this operator if it has been recorded in profiling trace file.\\nTo dump this call stack information, you should set the 'with_stack' parameter in torch.profiler API.\\nThe TensorBoard has integrated to VSCode, if you launch TensorBoard in VSCode, clicking this CallStack will forward to corresponding line of source code as below:</p>\\n<p dir=\\\"auto\\\"><a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/vscode_stack.PNG\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/vscode_stack.PNG\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a></p>\\n<p dir=\\\"auto\\\">Note: Each above duration means wall-clock time. It doesn't mean the GPU or CPU during this period is fully utilized.</p>\\n<p dir=\\\"auto\\\">The top 4 pie charts are visualizations of the above 4 columns of durations.\\nThey make the breakdowns visible at a glance.\\nOnly the top N operators sorted by duration (configurable in the text box) will be shown in the pie charts.</p>\\n<p dir=\\\"auto\\\">The search box enables searching operators by name.</p>\\n<p dir=\\\"auto\\\">\\\"Group By\\\" could choose between \\\"Operator\\\" and \\\"Operator + Input Shape\\\".\\nThe \\\"Input Shape\\\" is shapes of tensors in this operator\u2019s input argument list.\\nThe empty \\\"[]\\\" means argument with scalar type.\\nFor example, \\\"[[32, 256, 14, 14], [1024, 256, 1, 1], [], [], [], [], [], [], []]\\\"\\nmeans this operator has 9 input arguments,\\n1st is a tensor of size 32*256*14*14,\\n2nd is a tensor of size 1024*256*1*1,\\nthe following 7 ones are scalar variables.</p>\\n<p dir=\\\"auto\\\"><a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/operator_view_group_by_inputshape.PNG\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/operator_view_group_by_inputshape.PNG\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a></p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Kernel View</p>\\n<p dir=\\\"auto\\\">This view shows all kernels\u2019 time spent on GPU.\\nThe time is calculated by subtracting the kernel's start time from the end time.</p>\\n<p dir=\\\"auto\\\">Note: This view does not include cudaMemcpy or cudaMemset. Because they are not kernels.</p>\\n<p dir=\\\"auto\\\"><a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/kernel_view.PNG\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/kernel_view.PNG\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a></p>\\n<ul dir=\\\"auto\\\">\\n<li>\\n<p dir=\\\"auto\\\">Tensor Cores Used: Whether this kernel uses Tensor Cores.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Total Duration: The accumulated time of all calls of this kernel.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Mean Duration: The average time duration of all calls. That's \\\"Total Duration\\\" divided by \\\"Calls\\\".</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Max Duration: The maximum time duration among all calls.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Min Duration: The minimum time duration among all calls.</p>\\n<p dir=\\\"auto\\\">Note: These durations only include a kernel's elapsed time on GPU device.\\nIt does not mean the GPU is fully busy executing instructions during this time interval.\\nSome of the GPU cores may be idle due to reasons such as memory access latency or insufficient parallelism.\\nFor example, there may be insufficient number of available warps per SM for the GPU to effectively\\nhide memory access latencies, or some SMs may be entirely idle due to an insufficient number of blocks.\\nPlease refer to <a href=\\\"https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html\\\" rel=\\\"nofollow\\\">Nvidia's best-practices guide</a>.\\nTo investigate efficiency for each kernel, we calculate and show the 'Mean Blocks Per SM' and 'Mean Est. Achieved Occupancy' in the last two column.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Mean Blocks Per SM: Blocks per SM = Blocks of this kernel / SM number of this GPU. If this number is less than 1, it indicates the GPU multiprocessors are not fully utilized. \\\"Mean Blocks per SM\\\" is weighted average of all runs of this kernel name, using each run\u2019s duration as weight.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Mean Est. Achieved Occupancy: The definition of Est. Achieved Occupancy can refer to <a href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/gpu_utilization.md\\\">gpu_utilization</a>, It is weighted average of all runs of this kernel name, using each run\u2019s duration as weight.</p>\\n</li>\\n</ul>\\n<p dir=\\\"auto\\\">The top left pie chart is a visualization of \\\"Total Duration\\\" column.\\nIt makes the breakdowns visible at a glance.\\nOnly the top N kernels sorted by accumulated time (configurable in the text box) will be shown in the pie chart.</p>\\n<p dir=\\\"auto\\\">The top right pie chart is percent of the kernel time using and without using Tensor Cores.</p>\\n<p dir=\\\"auto\\\">The search box enables searching kernels by name.</p>\\n<p dir=\\\"auto\\\">\\\"Group By\\\" could choose between \\\"Kernel Name\\\" and \\\"Kernel Properties + Op Name\\\".</p>\\n<p dir=\\\"auto\\\">\\\"Kernel Name\\\" will group kernels by kernel name.</p>\\n<p dir=\\\"auto\\\">\\\"Kernel Properties + Op Name\\\" will group kernels by combination of kernel name, launching operator name,\\ngrid, block, registers per thread, and shared memory.</p>\\n<p dir=\\\"auto\\\"><a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/kernel_view_group_by_properties_and_op.PNG\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/kernel_view_group_by_properties_and_op.PNG\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a></p>\\n<ul dir=\\\"auto\\\">\\n<li>\\n<p dir=\\\"auto\\\">Operator: The name of PyTorch operator which launches this kernel.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Grid: Grid size of this kernel.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Block: Block size of this kernel.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Register Per Thread: Number of registers required for each thread executing the kernel.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Shared Memory: Sum of dynamic shared memory reserved, and static shared memory allocated for this kernel.</p>\\n</li>\\n</ul>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Trace View</p>\\n<p dir=\\\"auto\\\">This view shows timeline using the chrome tracing plugin. Each horizontal area represents a thread or a CUDA stream.\\nEach colored rectangle represents an operator, or a CUDA runtime, or a GPU op which executes on GPU\\n(such as a kernel, a CUDA memory copy, a CUDA memory set, ...)</p>\\n<p dir=\\\"auto\\\"><a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/trace_view.PNG\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/trace_view.PNG\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a></p>\\n<p dir=\\\"auto\\\">In the above example:</p>\\n<p dir=\\\"auto\\\">The \\\"thread 25772\\\" is the CPU thread that do \\\"backward\\\" of neural network.</p>\\n<p dir=\\\"auto\\\">The \\\"thread 25738\\\" is the main CPU thread, which mainly do data loading, forward of neural network, and model update.</p>\\n<p dir=\\\"auto\\\">The \\\"stream 7\\\" is a CUDA stream, which shows all kernels of this stream.</p>\\n<p dir=\\\"auto\\\">You can see there are 6 \\\"ProfilerStep\\\" at the top of \\\"thread 1\\\". Each \\\"ProfilerStep\\\" represents a mini-batch step.</p>\\n<p dir=\\\"auto\\\">The suspended toolbar has functionalities to help view the trace line.\\nFor example, when the up-down arrow is enabled,\\nyou can zoom in by dragging the mouse up and keeping mouse's left button pushed down.</p>\\n<p dir=\\\"auto\\\"><a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/trace_view_one_step.PNG\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/trace_view_one_step.PNG\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a></p>\\n<p dir=\\\"auto\\\">The \\\"Optimizer.step#SGD.step\\\" and \\\"enumerate(DataLoader)#_SingleProcessDataLoaderIter._<em>next_</em>\\\"\\nare high-level python side functions.</p>\\n<p dir=\\\"auto\\\">When you select the top-right corner's \\\"Flow events\\\" to \\\"async\\\",\\nyou can see the relationship between an operator and its launched kernels.\\n<a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/trace_view_launch.PNG\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/trace_view_launch.PNG\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a></p>\\n<p dir=\\\"auto\\\">You can also view the gpu utilization and Est. SM Efficiency in the trace view. They are drawn alongside the timeline:</p>\\n<p dir=\\\"auto\\\"><a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/trace_view_gpu_utilization.PNG\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/trace_view_gpu_utilization.PNG\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a></p>\\n<p dir=\\\"auto\\\">When you select the top-right corner's \\\"Flow events\\\" to \\\"fwd_bwd_correlation\\\",\\nyou can see the relationship between forward operator and its launched backward operator.\\nNote: Only the backward operator's direct launching forward operator will be connected by line,\\nits ancestor operators which call this operator as child will not be connected.\\n<a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/trace_view_fwd_bwd_correlation.PNG\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/trace_view_fwd_bwd_correlation.PNG\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a></p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Memory View</p>\\n<p dir=\\\"auto\\\">The Pytorch profiler records all memory allocation/release events and allocator's internal state during profiling. For\\neach operator, the plugin aggregates all the events inside its lifespan.</p>\\n<p dir=\\\"auto\\\"><a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/memory_view.PNG\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/memory_view.PNG\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a></p>\\n<p dir=\\\"auto\\\">The memory kind could be selected in 'Device' selection box. For example, 'GPU0' means the following plot and tables only shows each\\noperator's memory usage on GPU 0, not including CPU or other GPUs.</p>\\n<ul dir=\\\"auto\\\">\\n<li>\\n<p dir=\\\"auto\\\">Memory Curve</p>\\n<p dir=\\\"auto\\\">Memory curve shows the memory usage trends. It helps the user get an overview on memory consumption. The 'Allocated' plot is the\\ntotal memory requested from the allocator, for example, used by tensors. The 'Reserved' plot only makes sense if the underlying\\nallocator make use of caching mechanism. It represents the total memory that is allocated from the operating system by the allocator.</p>\\n<p dir=\\\"auto\\\">User can select on the memory curve plot and zoom into the selected range by pressing left mouse button and dragging on the curve.\\nRight click will reset the plot to the initial state. The selection will affect 'Memory Events' table and 'Memory Statistics' table\\nas mentioned in the following sections.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Memory Events</p>\\n<p dir=\\\"auto\\\">Memory events table shows the memory allocation and release event pairs. Definition of each field in the table:</p>\\n<ul dir=\\\"auto\\\">\\n<li>\\n<p dir=\\\"auto\\\">Operator: The immediate operator causing allocation from allocator. In pytorch, some operators such as\\n<code>aten::empty</code> is widely used as an API for tensor creation, in this case, we show it as <code>&lt;parent-op&gt; (&lt;op&gt;)</code>.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Size: The allocated memory size.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Allocation Time: Memory allocation time point relative to profiler start. It maybe missing from the table if the allocation event\\nis not included in the selected range.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Release Time: Memory deallocation time point relative to profiler start. It maybe missing from the table if the release event is\\nnot included in the selected range. Notice, released memory block might still be cached by the underlying allocator.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Duration: The life duration of the allocated memory. It maybe missing from the table if Allocation Time or Release Time is absent.</p>\\n</li>\\n</ul>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Memory Statistics</p>\\n<p dir=\\\"auto\\\">Definition of each field in the table:</p>\\n<ul dir=\\\"auto\\\">\\n<li>\\n<p dir=\\\"auto\\\">Calls: How many times this operator is called.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Size Increase: The memory increase size includes all children operators. It sums up all allocation bytes and minus all the memory release bytes.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Self Size Increase: The memory increase size associated with the operator itself excluding that of its children. It sums up all allocation bytes and minus all the memory release bytes.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Allocation Count: The allocation count including all children operators.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Self Allocation Count: The allocation count belonging to the operator itself excluding its children.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Allocation Size: The allocation size including all children operators. It sums up all allocation bytes without considering the memory free.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Self Allocation Size: The allocation size belonging to the operator itself. It sums up all allocation bytes without considering the memory free.</p>\\n</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Distributed View</p>\\n<p dir=\\\"auto\\\">This view will appear automatically only for DDP jobs that use nccl for communication.\\nThere are four panels in this view:</p>\\n<p dir=\\\"auto\\\"><a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/distributed_view.PNG\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/distributed_view.PNG\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a></p>\\n<ul dir=\\\"auto\\\">\\n<li>\\n<p dir=\\\"auto\\\">The top panel shows the information about nodes/processes/GPU hierarchy of this job.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">The left panel in the middle is 'Computation/Communication Overview'. Definition of each legend:</p>\\n<ul dir=\\\"auto\\\">\\n<li>Computation: the sum of kernel time on GPU minus the overlapping time.</li>\\n<li>Overlapping: the overlapping time of computation and communication. More overlapping represents better parallelism between computation and communication. Ideally the communication would be totally overlapped with computation.</li>\\n<li>Communication: the total communication time minus the overlapping time.</li>\\n<li>Other: step time minus computation and communication time. Maybe includes initialization, data loader, CPU computation, and so on.</li>\\n</ul>\\n<p dir=\\\"auto\\\">From this view, you can know computation-to-communication ratio of each worker and load balance between workers. For example, if the computation + overlapping time of\\none worker is much larger than others, there may be a problem of loading balance or this worker may be a straggler.</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">The right panel in the middle is 'Synchronizing/Communication Overview'. Definition of each legend:</p>\\n<ul dir=\\\"auto\\\">\\n<li>Data Transfer Time: part in the total communication time for actual data exchanging.</li>\\n<li>Synchronizing Time: part in the total communication time for waiting and synchronizing with other workers.</li>\\n</ul>\\n<p dir=\\\"auto\\\">From this view, you can know the efficiency of communication (how much ratio of total communication time is really used for exchanging data and how much is just waiting for data from other workers)</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">The 'Communication Operations Stats' summarizes the detailed statistics of all communication ops in each worker. Definition of each field:</p>\\n<ul dir=\\\"auto\\\">\\n<li>Calls: How many times this operator is called in this run.</li>\\n<li>Total Size (bytes): Total data size transferred in operators of this type.</li>\\n<li>Avg Size (bytes): Average data size transferred in each operator of this type.</li>\\n<li>Total Latency (us): Total latency of all operators of this type.</li>\\n<li>Avg Latency (us): Average latency of each operator of this type.</li>\\n<li>Data Transfer Time (us): Total time actually used for data transfer in operator of this type.</li>\\n<li>Ave Data Transfer Time (us): Average time actually used for data transfer in each operator of this type.</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Module View</p>\\n<p dir=\\\"auto\\\">If the torch.nn.Module information is dumped into the result Chrome tracing file by Pytorch profiler, the plugin could display the nn.Module hierarchy and summary.</p>\\n<p dir=\\\"auto\\\"><a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/module_view.png\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/module_view.png\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a></p>\\n<ul dir=\\\"auto\\\">\\n<li>\\n<p dir=\\\"auto\\\">The top table shows each torch.nn.Module statistics information including:</p>\\n<ul dir=\\\"auto\\\">\\n<li>Occurrences: how many times the module is called in the training process.</li>\\n<li>Operators: how many operators the module invokes.</li>\\n<li>Host Total Time: The accumulated time spent on Host, including the child submodule.</li>\\n<li>Host Self Time: The accumulated time spent on Host, not including the child submodule.</li>\\n<li>Device Total Time: The accumulated time spent on GPU of the operators contained in the module, including the child submodule.</li>\\n<li>Device Self Time: The accumulated time spent on GPU of the operators contained in the module, not including the child submodule.</li>\\n</ul>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">The middle flamegraph shows the torch.nn.Module hierarchy information</p>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">The bottom graph shows the main thread operators tree.</p>\\n</li>\\n</ul>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Lightning View</p>\\n<p dir=\\\"auto\\\">If the Chrome tracing file is from PytorchLightning job, the plugin will show a Lightning View which is customized for Pytorch Lightning.\\nAll the data of this view is from PytorchLightning framework.</p>\\n<p dir=\\\"auto\\\"><a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/lightning_view.png\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/lightning_view.png\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a></p>\\n<ul dir=\\\"auto\\\">\\n<li>The top table shows the model structure. The meaning of metrics in the table is same as Module View.</li>\\n<li>The middle flamegraph shows the model hierarchy information.</li>\\n<li>The bottom graph shows the call tree of all hooks in PytorchLightning.</li>\\n</ul>\\n</li>\\n<li>\\n<p dir=\\\"auto\\\">Diff Run View</p>\\n<p dir=\\\"auto\\\">The diff run feature helps to compare two run by logical timeline. The key comparision operators include backward, dataloader, torch.nn.Module, optimizer. If each operator contains these sub-operators internally, the diff run could be zoom in by click the bar.</p>\\n<p dir=\\\"auto\\\"><a target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\" href=\\\"/pytorch/kineto/blob/main/tb_plugin/docs/images/diff_view.png\\\"><img src=\\\"/pytorch/kineto/raw/main/tb_plugin/docs/images/diff_view.png\\\" alt=\\\"Alt text\\\" style=\\\"max-width: 100%;\\\"></a></p>\\n<ul dir=\\\"auto\\\">\\n<li>The top bar chart shows each operator type and trend comparision result.</li>\\n<li>The middle line chart shows the delta and accumulated execution time difference against each operator type.</li>\\n<li>The bottom table show the operators difference for the following categories:\\n<ul dir=\\\"auto\\\">\\n<li>Host Total Duration: The accumulated time spent on Host, including this operator\u2019s child operators.</li>\\n<li>Host Self Duration: The accumulated time spent on Host, not including this operator\u2019s child operators.</li>\\n<li>Device Total Duration: The accumulated time spent on GPU, including this operator\u2019s child operators.</li>\\n<li>Device Self Duration: The accumulated time spent on GPU, not including this operator\u2019s child operators.</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n</ul>\\n<div class=\\\"markdown-heading\\\" dir=\\\"auto\\\"><h3 tabindex=\\\"-1\\\" class=\\\"heading-element\\\" dir=\\\"auto\\\">PyTorch Profiler TensorBoard Plugin 0.2 Release Notes</h3><a id=\\\"user-content-pytorch-profiler-tensorboard-plugin-02-release-notes\\\" class=\\\"anchor-element\\\" aria-label=\\\"Permalink: PyTorch Profiler TensorBoard Plugin 0.2 Release Notes\\\" href=\\\"#pytorch-profiler-tensorboard-plugin-02-release-notes\\\"><svg class=\\\"octicon octicon-link\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" height=\\\"16\\\" aria-hidden=\\\"true\\\"><path d=\\\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\\\"></path></svg></a></div>\\n<p dir=\\\"auto\\\">Known Issues: This software does not support Python 3.9.0, 3.9.1, 3.9.2.\\nIf the TensorBoard launching reports error message \\\"ImportError\\\" and \\\"circular import\\\",\\nplease update your Python to higher version.</p>\\n</article>\",\"errorMessage\":null,\"headerInfo\":{\"toc\":[{\"level\":1,\"text\":\"PyTorch Profiler TensorBoard Plugin\",\"anchor\":\"pytorch-profiler-tensorboard-plugin\",\"htmlText\":\"PyTorch Profiler TensorBoard Plugin\"},{\"level\":3,\"text\":\"Quick Installation Instructions\",\"anchor\":\"quick-installation-instructions\",\"htmlText\":\"Quick Installation Instructions\"},{\"level\":3,\"text\":\"Quick Start Instructions\",\"anchor\":\"quick-start-instructions\",\"htmlText\":\"Quick Start Instructions\"},{\"level\":3,\"text\":\"Quick Usage Instructions\",\"anchor\":\"quick-usage-instructions\",\"htmlText\":\"Quick Usage Instructions\"},{\"level\":3,\"text\":\"PyTorch Profiler TensorBoard Plugin 0.2 Release Notes\",\"anchor\":\"pytorch-profiler-tensorboard-plugin-02-release-notes\",\"htmlText\":\"PyTorch Profiler TensorBoard Plugin 0.2 Release Notes\"}],\"siteNavLoginPath\":\"/login?return_to=https%3A%2F%2Fgithub.com%2Fpytorch%2Fkineto%2Ftree%2Fmain%2Ftb_plugin\"}},\"totalCount\":14,\"showBranchInfobar\":false},\"fileTree\":{\"\":{\"items\":[{\"name\":\".github\",\"path\":\".github\",\"contentType\":\"directory\"},{\"name\":\"libkineto\",\"path\":\"libkineto\",\"contentType\":\"directory\"},{\"name\":\"tb_plugin\",\"path\":\"tb_plugin\",\"contentType\":\"directory\"},{\"name\":\".gitignore\",\"path\":\".gitignore\",\"contentType\":\"file\"},{\"name\":\".gitmodules\",\"path\":\".gitmodules\",\"contentType\":\"file\"},{\"name\":\"CODE_OF_CONDUCT.md\",\"path\":\"CODE_OF_CONDUCT.md\",\"contentType\":\"file\"},{\"name\":\"CONTRIBUTING.md\",\"path\":\"CONTRIBUTING.md\",\"contentType\":\"file\"},{\"name\":\"LICENSE\",\"path\":\"LICENSE\",\"contentType\":\"file\"},{\"name\":\"README.md\",\"path\":\"README.md\",\"contentType\":\"file\"}],\"totalCount\":9}},\"fileTreeProcessingTime\":2.278113,\"foldersToFetch\":[],\"treeExpanded\":true,\"symbolsExpanded\":false,\"csrf_tokens\":{\"/pytorch/kineto/branches\":{\"post\":\"LDmXv7DyZ-Cy7KFRpCRkzY0FXB9u8QojQNTrkJdIqDDcRoZegFMkuAMvr5oEVApBz2xhggzF4OMnyfd6xsPzEw\"},\"/pytorch/kineto/branches/fetch_and_merge/main\":{\"post\":\"2rpgp7kpOGzMZk1J8HD-5OFrWfMGxia3pyHoW3D8OAp9pO4xsOLNJ7pdEqHf0xEktGBL8Jnaknk0XyAQ5UxpWQ\"},\"/pytorch/kineto/branches/fetch_and_merge/main?discard_changes=true\":{\"post\":\"mAHIF5C2LiU1ftkSR7ZuPvfsy2Ne3Y4yXclOU6o8Jro_H0aBmX3bbkNFhvpoFYH-oufZYMHBOvzOt4YYP4x36Q\"}}},\"title\":\"kineto/tb_plugin at main \u00b7 pytorch/kineto\"}",
  "embeddings": []
}